<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>K8S中微服务踩坑分享</title>
    <link href="/2020/01/05/K8S%E4%B8%AD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%B8%A9%E5%9D%91%E5%88%86%E4%BA%AB/"/>
    <url>/2020/01/05/K8S%E4%B8%AD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%B8%A9%E5%9D%91%E5%88%86%E4%BA%AB/</url>
    
    <content type="html"><![CDATA[<h3 id="一、资源限制"><a href="#一、资源限制" class="headerlink" title="一、资源限制"></a>一、资源限制</h3><ul><li>我们先来谈下 “资源限制”：</li></ul><p>我们线上目前多数为java应用，java应用对于K8S来说，资源限制特别重要，如果不做资源限制，会影响整个宿主机，然后整个宿主机资源不够会实现飘移，会转移到其他主机上，然后再异常，可能会起到一种雪崩的效应。</p><p>通常我们会通过如下设置进行对容器的资源限制，但是通常对于java应用来说这个是和jvm中设置的是不搭嘎的，且docker容器也抓不到jvm的设置；这就造成了，假如设置不合理就会导致业务启动故障。</p><p>如：如下内存我们设置了最大1G，如果jvm可以读取到limits的内存，一旦应用使用内存即将到达1G，就会自动触发堆内存回收，大量GCC。如果读取不到，且jvm内存设置的2G，这时候多数的应用会OOM；<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">resources:</span><br><span class="line">  requests:</span><br><span class="line">    cpu: 0.5</span><br><span class="line">    memory: 256Mi</span><br><span class="line">  limits:</span><br><span class="line">    cpu: 1</span><br><span class="line">    memory: 1Gi</span><br></pre></td></tr></table></figure></p><p>当然不是没有解决方案，我们目前是使用的2种解决方案：</p><ul><li>默认将JVM参数通过镜像写死在镜像中，且JVM参数是根据内存自动分配大小，放在/etc/profile中如果使用就加载，不使用不加载：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export NewSize=`expr $&#123;DAOKEMEM&#125; / 4`</span><br><span class="line">export DirectMemorySize=`expr $&#123;DAOKEMEM&#125; / 16`</span><br><span class="line">export XM=`expr $&#123;DAOKEMEM&#125; - $&#123;DirectMemorySize&#125;`</span><br><span class="line">export MetaspaceSize=`expr $&#123;NewSize&#125; / 8`</span><br><span class="line">export MaxMetaspaceSize=`expr $&#123;NewSize&#125; / 4`</span><br><span class="line">export JAVA_OPTS=&quot;-Duser.timezone=GMT+08 -server -Xms$&#123;XM&#125;m  -Xmx$&#123;XM&#125;m -XX:NewSize=$&#123;NewSize&#125;m -XX:MaxNewSize=$&#123;NewSize&#125;m -XX:MaxDirectMemorySize=$&#123;DirectMemorySize&#125;m -XX:MetaspaceSize=$&#123;MetaspaceSize&#125;m -XX:MaxMetaspaceSize=$&#123;MaxMetaspaceSize&#125;m -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Xloggc:/data/logs/skynet-$&#123;DAOKEAPPUK&#125;/$&#123;DAOKEAPPUK&#125;_gc.log -Dapm.applicationName=$&#123;DAOKEAPPUK&#125; -Dapm.agentId=$&#123;HOST&#125;-$&#123;PORT0&#125; -Dapm.env=$&#123;DAOKEENVTYPE&#125;&quot;</span><br></pre></td></tr></table></figure></li></ul><p>系统变量：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># env</span><br><span class="line">HOSTNAME=ASD-18-XXX-XXX.linux.XXX.com</span><br><span class="line">XM=7680</span><br><span class="line">NewSize=2048</span><br><span class="line">HOST=172.XX.XXX.XX</span><br><span class="line">TERM=xterm</span><br><span class="line">PORT0=18645</span><br><span class="line">DAOKEAPPNAME=XXX.hbase.XXX.efs</span><br><span class="line">APPNAME=XXX.hbase.XXX.efs</span><br><span class="line">JAVA_OPTS=-Duser.timezone=GMT+08 -server -Xms7680m  -Xmx7680m -XX:NewSize=2048m -XX:MaxNewSize=2048m -XX:MaxDirectMemorySize=512m -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Xloggc:/data/logs/skynet-tcbase.java.dss.hbase.infra.efs/tcbase.java.XXX.hbase.XXX.efs_gc.log -Dapm.applicationName=tcbase.java.XXX.hbase.XXX.efs -Dapm.agentId=172.18.191.95-18645 -Dapm.env=product</span><br><span class="line">DirectMemorySize=512</span><br><span class="line">JRE_HOME=/usr/java/default/jre</span><br><span class="line">LS_COLORS=</span><br><span class="line">DAOKE_REGION=cn_east</span><br><span class="line">DAOKE_LOGIC_IDC=logicidc_hd1</span><br><span class="line">DAOKEDOWNCMD=</span><br><span class="line">DAOKEIDC=logicidc_hd1</span><br><span class="line">PATH=/usr/java/default/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">PWD=/usr/local/tomcat</span><br><span class="line">DAOKEENV=product</span><br><span class="line">JAVA_HOME=/usr/java/default</span><br><span class="line">DAOKEID=73918</span><br><span class="line">LANG=en_US.UTF-8</span><br><span class="line">TZ=Asia/Shanghai</span><br><span class="line">DAOKE_IDC=XHY</span><br><span class="line">DAOKEREGION=cn_east</span><br><span class="line">DAOKEENVTYPE=product</span><br><span class="line">DAOKECPU=4</span><br><span class="line">DAOKEWAITTIME=10</span><br><span class="line">SHLVL=1</span><br><span class="line">HOME=/root</span><br><span class="line">MaxMetaspaceSize=512</span><br><span class="line">MetaspaceSize=256</span><br><span class="line">CLASSPATH=.:/usr/java/default/lib/tools.jar</span><br><span class="line">LESSOPEN=||/usr/bin/lesspipe.sh %s</span><br><span class="line">DAOKEIP=172.18.191.95</span><br><span class="line">DAOKEMEM=8192</span><br><span class="line">DAOKEAPPUK=tcbase.java.XXX.hbase.XXX.efs</span><br><span class="line">_=/usr/bin/env</span><br></pre></td></tr></table></figure></p><ul><li>通过yaml文件限定jvm参数：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">env:</span><br><span class="line">  - name: JAVA_OPTS</span><br><span class="line">    value: &quot;-Duser.timezone=GMT+08 -server -Xms7680m  -Xmx7680m -XX:NewSize=2048m -XX:MaxNewSize=2048m -XX:MaxDirectMemorySize=512m -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=512m -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Xloggc:/data/logs/XXX-tcbase.java.XXX.hbase.XXX.efs/tcbase.java.XXX.hbase.XXX.efs_gc.log -Dapm.applicationName=tcbase.java.XXX.hbase.XXX.efs -Dapm.agentId=172.18.191.95-18645 -Dapm.env=product&quot;</span><br></pre></td></tr></table></figure></li></ul><p>当然，你也许会有疑问，那如何加载呢？<br>先来看一个示例的Dockerfile：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat product-service/product-service-biz/Dockerfile</span><br><span class="line">FROM java:8-jdk-alpine</span><br><span class="line">ENV JAVA_OPTS=&quot;$JAVA_OPTS&quot;</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">COPY ./target/product-service-biz.jar ./</span><br><span class="line">COPY pinpoint /pinpoint</span><br><span class="line">EXPOSE 8010</span><br><span class="line">CMD java -jar -javaagent:/pinpoint/pinpoint-bootstrap-1.8.3.jar -Dpinpoint.agentId=$&#123;HOSTNAME&#125; -Dpinpoint.applicationName=ms-product $JAVA_OPTS /product-service-biz.jar</span><br></pre></td></tr></table></figure></p><h3 id="二、滚动更新之健康检查重要性"><a href="#二、滚动更新之健康检查重要性" class="headerlink" title="二、滚动更新之健康检查重要性"></a>二、滚动更新之健康检查重要性</h3><h4 id="2-1、探针机制"><a href="#2-1、探针机制" class="headerlink" title="2.1、探针机制"></a>2.1、探针机制</h4><p>k8s提供了是两种探针的机制，分别为<strong>就绪探针</strong> readinessProbe、<strong>存活探针</strong> livenessProbe。</p><p>探针机制可以通过http接口、shell指令、tcp确认容器的状态。探针还可以配置延迟探测时间、探测间隔、探测成功或失败条件延后时间等参数。使用http接口探测时，可以配置header参数，如果响应的状态码大于等于200且小于400，则诊断被认为是成功的。</p><ul><li><p>存活探针，主要用于检测pod是否异常，如果k8s通过健康探针检测到服务异常后会替换或重启容器。</p></li><li><p>就绪探针，这个探测通过时才会将其加入到service匹配的endpoint列表中，并向该容器发送http请求，否则会将pod从列表移除直到就绪探针再次通过</p></li></ul><p>就绪探针和存活探针比较类似，都会持续执行检测，只是检测会导致的结果不一样，一个会导致容器重启或被替换，一个会导致http请求停止分发到容器。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">readinessProbe: ##就绪检查</span><br><span class="line">  tcpSocket:</span><br><span class="line">    port: 8010</span><br><span class="line">  initialDelaySeconds: 60</span><br><span class="line">  periodSeconds: 3</span><br><span class="line">livenessProbe:  ##存活检查（探活）</span><br><span class="line">  tcpSocket:</span><br><span class="line">    port: 8010</span><br><span class="line">  initialDelaySeconds: 60   ##启动pod后的多少秒开始检查</span><br><span class="line">  periodSeconds: 3     ##每隔3s进行探活</span><br></pre></td></tr></table></figure><p>如上针对单体java应用还是有必要的，因为如果没有readinessProbe（就绪检查），一旦容器启动就会有业务流量进入；但是对于微服务来说可有可无，因为有注册中心和配置中心来管控！</p><h3 id="三、滚动更新之流量丢失"><a href="#三、滚动更新之流量丢失" class="headerlink" title="三、滚动更新之流量丢失"></a>三、滚动更新之流量丢失</h3><p>一般故障就是：</p><ul><li>连接拒绝</li><li>响应错误</li><li>调用不到</li></ul><p>使用k8s发布服务默认使用的滚动发布方案，这个方案本身已经有一定机制减少发布的影响。滚动发布时发布完一个新版本的pod后才会下线一个旧的pod，并把指向sevice的请求经负载均衡指向新pod，直到所有旧的pod下线，新的pod全部发布完毕。</p><p>所以只要k8s在pod的启停时做到和微服务联动，就可以做到无感发布。关键在于探知微服务是否准备好了、通知服务将要停止、配置启停过程预留的时间。这几个方面k8s都有相关的机制，所以我们先了解这些机制，再整合得出解决思路。</p><p>一般滚动更新是关闭现有的pod，再起一新的pod，关闭现有的其实是就是删除了一个pod,然后apiserver会通知给kubelet,然后kubelet会关闭这个容器，然后从service后端摘掉；</p><p>关闭pod之后会有一个等待时间，在这个时间呢，可能还会接入一些新的流量，但是它的服务已经不再处理新的请求了，所以会导致连接拒绝，怎么去解决这个问题呢？实际上readiness探针在整个过程中并起到关键的作用，一旦endpoint收到pod 的删除事件后，这已经就与readiness探测结果不相关了。</p><p>如何解决呢？</p><p>其实之需要在关闭这个pod时加个休眠的时间，其实就可以解决这个问题了，在关闭和启动都是有一个钩子存在的，所有可以在关闭容器前，执行这个钩子，钩子这个定义一个shell,y也可以定义一个http请求，也就是支持者两种类型，也就是在container同级，因为这里休眠5秒也就是你关闭的容器不会马上退出，然后休眠5秒钟，再去关闭着应用，这5秒能够足够让kube-proxy刷新这个规则，这样的话，就不会将新加入的流量转发到这个刚关闭的pod上，增加这个钩子就能暂缓你关闭pod的时间，从而让kube-proxy增加刷新规则的时间！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lifecycle:</span><br><span class="line">  preStop:</span><br><span class="line">    httpGet:</span><br><span class="line">      host: 192.168.4.170</span><br><span class="line">      path: api/v2/devops/pkg/upload_hooks</span><br><span class="line">      port: 8090</span><br><span class="line"></span><br><span class="line">或者：</span><br><span class="line">lifecycle :</span><br><span class="line">  preStop :</span><br><span class="line">    exec :</span><br><span class="line">     command :</span><br><span class="line">      - sh</span><br><span class="line">      - -c</span><br><span class="line">      - “sleep 5”</span><br></pre></td></tr></table></figure></p><h4 id="3-1、terminationGracePeriodSeconds-配置延迟关闭时间"><a href="#3-1、terminationGracePeriodSeconds-配置延迟关闭时间" class="headerlink" title="3.1、terminationGracePeriodSeconds 配置延迟关闭时间"></a>3.1、terminationGracePeriodSeconds 配置延迟关闭时间</h4><p>该属性默认30s，只配置terminationGracePeriodSeconds这个属性而没有配置prestop时，k8s会先发送SIGTERM信号给主进程，然后然后等待terminationGracePeriodSeconds 属性的时间，会被使用SIGKILL杀死。这个机制相对简单粗暴。</p><h4 id="3-2、提供上述几个机制的deployment文件配置示例如下"><a href="#3-2、提供上述几个机制的deployment文件配置示例如下" class="headerlink" title="3.2、提供上述几个机制的deployment文件配置示例如下"></a>3.2、提供上述几个机制的deployment文件配置示例如下</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: review-demo</span><br><span class="line">  namespace: scm</span><br><span class="line">  labels:</span><br><span class="line">    app: review-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">#  minReadySeconds: 60     #滚动升级时60s后认为该pod就绪</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:  ##由于replicas为3,则整个升级,pod个数在2-4个之间</span><br><span class="line">      maxSurge: 1      #滚动升级时会先启动1个pod</span><br><span class="line">      maxUnavailable: 1 #滚动升级时允许的最大Unavailable的pod个数</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: review-demo</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60 ##k8s将会给应用发送SIGTERM信号，可以用来正确、优雅地关闭应用,默认为30秒</span><br><span class="line">      containers:</span><br><span class="line">      - name: review-demo</span><br><span class="line">        image: library/review-demo:0.0.1-SNAPSHOT</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        lifecycle:</span><br><span class="line">          preStop:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /prestop</span><br><span class="line">              port: 8080</span><br><span class="line">              scheme: HTTP            </span><br><span class="line">        livenessProbe: #kubernetes认为该pod是存活的,不存活则需要重启</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /health</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">            httpHeaders:</span><br><span class="line">              - name: Custom-Header</span><br><span class="line">              value: Awesome               </span><br><span class="line">          initialDelaySeconds: 60 ## equals to the max startup time of the application + couple of seconds</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          failureThreshold: 5   # 连续失败次数</span><br><span class="line">          periodSeconds: 5 # 多少秒执行一次检测</span><br><span class="line">        readinessProbe: #kubernetes认为该pod是准备好接收http请求了的</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /ifready</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">            httpHeaders:</span><br><span class="line">              - name: Custom-Header</span><br><span class="line">              value: Awesome            </span><br><span class="line">          initialDelaySeconds: 30 #equals to min startup time of the app</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">          periodSeconds: 5 # 多少秒执行一次检测</span><br><span class="line">        resources:</span><br><span class="line">          # keep request = limit to keep this container in guaranteed class</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 50m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 500Mi</span><br><span class="line">        env:</span><br><span class="line">          - name: PROFILE</span><br><span class="line">            value: &quot;test&quot;</span><br><span class="line">        ports:</span><br><span class="line">          - name: http</span><br><span class="line">            containerPort: 8080</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S中微服务链路监控系统</title>
    <link href="/2020/01/04/K8S%E4%B8%AD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"/>
    <url>/2020/01/04/K8S%E4%B8%AD%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%93%BE%E8%B7%AF%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h3 id="一、全链路监控"><a href="#一、全链路监控" class="headerlink" title="一、全链路监控"></a>一、全链路监控</h3><h4 id="1-1、全链路监控是什么"><a href="#1-1、全链路监控是什么" class="headerlink" title="1.1、全链路监控是什么"></a>1.1、全链路监控是什么</h4><p>随着微服务架构的流行，<strong>服务按照不同的维度进行拆分</strong>，一次请求往往需要涉及到多个服务。这些服务可能不同编程语言开发，不同团队开发，可能部署很多副本。因此，就需要一些可以帮助理解系统行为、用于分析性能问题的工具，以便发生故障的时候，能够快速定位和解决问题。全链路监控组件就在这样的问题背景下产生了。</p><p>全链路性能监控<strong>从整体维度到局部维度展示各项指标</strong>，将跨应用的所有调用链性能信息集中展现，可方便度量整体和局部性能，并且方便找到故障产生的源头，生产上可极大缩短故障排除时间。</p><p><img src="http://myimage.okay686.cn/okay686cn/20200113/esMg9n7qFBpt.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="1-2、全链路监控解决什么问题"><a href="#1-2、全链路监控解决什么问题" class="headerlink" title="1.2、全链路监控解决什么问题"></a>1.2、全链路监控解决什么问题</h4><ul><li><strong>请求链路追踪，故障快速定位</strong>：可以通过调用链结合业务日志快速定位错误信息。</li><li><strong>可视化</strong>： 各个阶段耗时，进行性能分析。</li><li><strong>依赖优化</strong>：各个调用环节的可用性、梳理服务依赖关系以及优化。</li><li><strong>数据分析，优化链路</strong>：可以得到用户的行为路径，汇总分析应用在很多业务场景。</li></ul><h4 id="1-3、全链路监控选择依据"><a href="#1-3、全链路监控选择依据" class="headerlink" title="1.3、全链路监控选择依据"></a>1.3、全链路监控选择依据</h4><p>全链路监控系统有很多，应从这几方面选择：</p><ul><li>探针的性能消耗</li></ul><p>APM组件服务的影响应该做到足够小，数据分析要快，性能占用小。</p><ul><li>代码的侵入性</li></ul><p>即也作为业务组件，应当尽可能少入侵或者无入侵其他业务系统，<br>对于使用方透明，减少开发人员的负担。</p><ul><li>监控维度</li></ul><p>分析的维度尽可能多。</p><ul><li>可扩展性</li></ul><p>一个优秀的调用跟踪系统必须支持分布式部署，具备良好的可扩展<br>性。能够支持的组件越多当然越好。</p><p>主流系统： zipkin、 skywalking、 pinpoint</p><h3 id="二、Pinpoint"><a href="#二、Pinpoint" class="headerlink" title="二、Pinpoint"></a>二、Pinpoint</h3><p>Pinpoint是一个APM（应用程序性能管理）工具，适用于用Java/PHP编写的大型分布式系统。</p><h4 id="2-1、特性："><a href="#2-1、特性：" class="headerlink" title="2.1、特性："></a>2.1、特性：</h4><ul><li><p>服务器地图（ServerMap）通过可视化分布式系统的模块和他们之间的相互联系来理解系统拓扑。点击某个节点会展示这个模块的详情，比如它当前的状态和请求数量。</p></li><li><p>实时活动线程图 （Realtime Active Thread Chart） ：实时监控应用内部的活动线程。</p></li><li><p>请求/响应分布图（Request/Response Scatter Chart ） ：长期可视化请求数量和应答模式来定位潜在问题。通过在图表上拉拽可以选择请求查看 更多的详细信息。</p></li><li><p>调用栈（CallStack ）：在分布式环境中为每个调用生成代码级别的可视图，在单个视图中定位瓶颈和失败点。</p></li><li><p>检查器（Inspector ） ：查看应用上的其他详细信息，比如CPU使用率，内存/垃圾回收，TPS，和JVM参数。</p></li></ul><h4 id="2-2、Pinpoint-部署"><a href="#2-2、Pinpoint-部署" class="headerlink" title="2.2、Pinpoint 部署"></a>2.2、Pinpoint 部署</h4><p><img src="http://myimage.okay686.cn/okay686cn/20200113/qDBEPfku09Ht.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>如上就是pinpoint一个整体架构，也是基于c/s。所有的应用在启动的时候通过JVM参数或者jar的方式启动，服务端的collector采集器通过agent采集各个指标并写入到HBase中，然后进行数据分析和展示。</p><p>Docker部署：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/naver/pinpoint-docker.git</span><br><span class="line">cd pinpoint-docker</span><br><span class="line">docker-compose pull &amp;&amp; docker-compose up -d</span><br><span class="line"></span><br><span class="line">docker-compose ps #查看启动的服务</span><br></pre></td></tr></table></figure></p><p>通过安装服务端的IP:8079 访问：</p><p><img src="http://myimage.okay686.cn/okay686cn/20200114/cexf1UUmcA6Y.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>当然启动后官方会给我们启动一个简单的quickapp的一个demon示例；</p><p>如果需要将现有的业务接入咱们的pinpoint就需要按照如下方式：</p><h4 id="2-3、Pinpoint-Agent部署"><a href="#2-3、Pinpoint-Agent部署" class="headerlink" title="2.3、Pinpoint Agent部署"></a>2.3、Pinpoint Agent部署</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tomcat：</span><br><span class="line"># catalina.sh</span><br><span class="line">CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar&quot;</span><br><span class="line">CATALINA_OPTS=&quot;$CATALINA_OPTS -Dpinpoint.agentId=$AGENT_ID&quot;</span><br><span class="line">CATALINA_OPTS=&quot;$CATALINA_OPTS -Dpinpoint.applicationName=$APPLICATION_NAME&quot;</span><br><span class="line"></span><br><span class="line">Jar：</span><br><span class="line">java -jar -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar -Dpinpoint.agentId=$AGENT_ID -Dpinpoint.applicationName=$APPLICATION_NAME xxx.jar</span><br></pre></td></tr></table></figure><p>如上agent 在我们安装好pinpoint后会提供一个下载连接：</p><p><img src="http://myimage.okay686.cn/okay686cn/20200114/uXHEfo5rHyMG.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>下载完成后，2种目前我们在用的方式接入：</p><ul><li>①将下载好的jar文件打入应用的基础镜像；</li><li>②将下载好的jar包推到仓库，配置下载链接并解压缩，然后开发配置启动即可；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第二种方式：</span><br><span class="line">ARG envType</span><br><span class="line">ADD http://XXX.XXXX.com/download/apmagent/apm.agent_$&#123;envType&#125;.tar.gz /usr/local/apm_agent/apm.agent.tar.gz</span><br><span class="line">RUN tar -xzf /usr/local/apm_agent/apm.agent.tar.gz -C /usr/local/apm_agent/</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3、Pinpoint-组件加载"><a href="#2-3、Pinpoint-组件加载" class="headerlink" title="2.3、Pinpoint 组件加载"></a>2.3、Pinpoint 组件加载</h4><p>回到微服务，我在dev4环境中自动引入了pinpoint组件：</p><p>先看一个典型的前端portal示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># pwd</span><br><span class="line">/root/microservic-code/simple-microservice-dev4/portal-service</span><br><span class="line"></span><br><span class="line"># cat Dockerfile</span><br><span class="line">FROM java:8-jdk-alpine</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">COPY ./target/portal-service.jar ./</span><br><span class="line">COPY pinpoint /pinpoint</span><br><span class="line">EXPOSE 8080</span><br><span class="line">CMD java -jar -javaagent:/pinpoint/pinpoint-bootstrap-1.8.3.jar -Dpinpoint.agentId=$&#123;HOSTNAME&#125; -Dpinpoint.applicationName=ms-protal /portal-service.jar</span><br><span class="line"></span><br><span class="line"># ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-r--r-- 1 root root  379 8月   3 23:03 Dockerfile</span><br><span class="line">drwxr-xr-x 7 root root  129 8月   3 23:03 pinpoint  ##组件所在的位置</span><br><span class="line">-rw-r--r-- 1 root root 1154 8月   3 23:03 pom.xml</span><br><span class="line">drwxr-xr-x 3 root root   18 8月   3 23:03 src</span><br><span class="line"></span><br><span class="line">### 进入pinpoint目录配置pinpoint.config（**每个微服都需要修改下**）</span><br><span class="line"></span><br><span class="line"># vim pinpoint.config</span><br><span class="line"></span><br><span class="line">profiler.collector.ip=192.168.171.10    ##指定采集端的server_IP</span><br></pre></td></tr></table></figure></p><p>然后只需要按照我之前修改的docker_build.sh <strong>重新部署</strong> 即可：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat docker_build.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">docker_registry=192.168.171.10</span><br><span class="line">kubectl create secret docker-registry registry-pull-secret --docker-server=$docker_registry --docker-username=admin --docker-password=XXXX --docker-email=zhdya@zhdya.cn -n ms</span><br><span class="line"></span><br><span class="line">service_list=&quot;gateway-service order-service product-service stock-service portal-service&quot;</span><br><span class="line">service_list=$&#123;1:-$&#123;service_list&#125;&#125;</span><br><span class="line">work_dir=$(dirname $PWD)</span><br><span class="line">current_dir=$PWD</span><br><span class="line"></span><br><span class="line">cd $work_dir</span><br><span class="line">mvn clean package -Dmaven.test.skip=true</span><br><span class="line"></span><br><span class="line">for service in $service_list; do</span><br><span class="line">   cd $work_dir/$service</span><br><span class="line">   if ls |grep biz &amp;&gt;/dev/null; then</span><br><span class="line">      cd $&#123;service&#125;-biz</span><br><span class="line">   fi</span><br><span class="line">   service=$&#123;service%-*&#125;</span><br><span class="line">   image_name=$docker_registry/microservice/$&#123;service&#125;:$(date +%F-%H-%M-%S)</span><br><span class="line">   docker build -t $&#123;image_name&#125; .</span><br><span class="line">   docker push $&#123;image_name&#125;</span><br><span class="line">  sed -i -r &quot;s#(image: )(.*)#\1$image_name#&quot; $&#123;current_dir&#125;/$&#123;service&#125;.yaml</span><br><span class="line">  kubectl apply -f $&#123;current_dir&#125;/$&#123;service&#125;.yaml</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>最后在dashboard中查看各个应用的调用关系（其中ms-portal）：</p><p><img src="http://myimage.okay686.cn/okay686cn/20200118/nuUbLhuWHmMJ.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><hr><h4 id="写在最后："><a href="#写在最后：" class="headerlink" title="写在最后："></a>写在最后：</h4><ul><li>线上应用主要监控哪些指标？<ul><li>堆内存：年轻代、老年代、非堆内存（PermSize，默认是物理内存的1/4）；</li><li>线程数量；</li><li>GCC（垃圾回收）；</li><li>CPU，内存，利用率；</li><li>堆栈跟踪；</li><li>web请求，上级调用，外部调用；</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动态PV, PVC在K8S的工作流程</title>
    <link href="/2020/01/03/%E5%8A%A8%E6%80%81PV,%20PVC%E5%9C%A8K8S%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    <url>/2020/01/03/%E5%8A%A8%E6%80%81PV,%20PVC%E5%9C%A8K8S%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h4 id="整个过程："><a href="#整个过程：" class="headerlink" title="整个过程："></a>整个过程：</h4><p><img src="http://myimage.okay686.cn/okay686cn/20200120/PQbNaOxusWEP.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>1）集群管理员预先创建存储类（StorageClass）；</p><p>2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)；</p><p>3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)；</p><p>4）系统读取存储类的信息；</p><p>5）系统基于存储类的信息，在后台自动创建PVC需要的PV；</p><p>6）用户创建一个使用PVC的Pod；</p><p>7）Pod中的应用通过PVC进行数据的持久化；</p><p>8）而PVC使用PV进行数据的最终持久化处理。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S中部署SpringCloud微服务项目</title>
    <link href="/2020/01/02/K8S%E4%B8%AD%E9%83%A8%E7%BD%B2SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE/"/>
    <url>/2020/01/02/K8S%E4%B8%AD%E9%83%A8%E7%BD%B2SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="一、熟悉SpringCloud项目"><a href="#一、熟悉SpringCloud项目" class="headerlink" title="一、熟悉SpringCloud项目"></a>一、熟悉SpringCloud项目</h3><h4 id="1-1、主机分配"><a href="#1-1、主机分配" class="headerlink" title="1.1、主机分配"></a>1.1、主机分配</h4><table><thead><tr><th>主机IP</th><th>角色</th></tr></thead><tbody><tr><td>192.168.171.11</td><td>k8s-master</td></tr><tr><td>192.168.171.12</td><td>k8s-node1</td></tr><tr><td>192.168.171.13</td><td>k8s-node2</td></tr><tr><td>192.168.171.10</td><td>harbor-mysql</td></tr></tbody></table><h4 id="1-2、代码分支详情："><a href="#1-2、代码分支详情：" class="headerlink" title="1.2、代码分支详情："></a>1.2、代码分支详情：</h4><ul><li>Dev1 交付代码；</li><li>Dev2 编写Dockerfile构建镜像；</li><li>Dev3 k8s资源编排文件；</li><li>Dev4 微服务链路监控；</li><li>Dev5 新功能测试；</li><li>Master 最终上线；</li></ul><h3 id="二、代码编译构建（Maven）"><a href="#二、代码编译构建（Maven）" class="headerlink" title="二、代码编译构建（Maven）"></a>二、代码编译构建（Maven）</h3><h4 id="2-1、拉取项目代码"><a href="#2-1、拉取项目代码" class="headerlink" title="2.1、拉取项目代码"></a>2.1、拉取项目代码</h4><p>代码分支：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master ~]# git clone -b dev1 https://github.com/xxxxx/simple-microservice</span><br></pre></td></tr></table></figure></p><h4 id="2-2、部署须知"><a href="#2-2、部署须知" class="headerlink" title="2.2、部署须知"></a>2.2、部署须知</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、导入db目录下数据库文件到自己的MySQL服务器</span><br><span class="line">2、修改配置环境（xxx-service/src/main/resources/application.yml，active值决定启用环境配置文件）</span><br><span class="line">3、修改连接数据库配置（xxx-service/src/main/resources/application-fat.yml）</span><br><span class="line">4、修改前端页面连接网关地址（portal-service/src/main/resources/static/js/productList.js和orderList.js）</span><br><span class="line">5、服务启动顺序：eureka -&gt; mysql -&gt; product,stock,order -&gt; gateway -&gt; portal</span><br></pre></td></tr></table></figure><h4 id="2-3、架构介绍："><a href="#2-3、架构介绍：" class="headerlink" title="2.3、架构介绍："></a>2.3、架构介绍：</h4><p><img src="http://myimage.okay686.cn/okay686cn/20200109/gNlmApv33vAW.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><ul><li>用户访问==》portal（前端）；</li><li>通过负载均衡调用后端api（通过ingress把网关的service暴露下）；</li><li>后端服务分别注册到Eureka，并由Eureka进行服务发现，注册，心跳等；</li></ul><h4 id="2-4、准备编译"><a href="#2-4、准备编译" class="headerlink" title="2.4、准备编译"></a>2.4、准备编译</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">编译需要安装jdk , jdk版本要看开发那边使用什么，注意一下。我这里用的1.8.0</span><br><span class="line"></span><br><span class="line"># yum install java-1.8.0-openjdk maven -y</span><br><span class="line"></span><br><span class="line"># java -version</span><br><span class="line">openjdk version &quot;1.8.0_232&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_232-b09)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode)</span><br></pre></td></tr></table></figure><p>进行编译：</p><p>进入源代码里，我们先进入交付的分支，开发工程师原封不动的把源代码交给我们，然后我们运维进行编译构建，把源代码打成jar包。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 simple-microservice-dev1]# ll</span><br><span class="line">总用量 28</span><br><span class="line">drwxr-xr-x 4 root root    70 7月  28 11:56 basic-common</span><br><span class="line">drwxr-xr-x 2 root root    59 7月  28 11:56 db</span><br><span class="line">drwxr-xr-x 3 root root    32 7月  28 11:56 eureka-service</span><br><span class="line">drwxr-xr-x 3 root root    32 7月  28 11:56 gateway-service</span><br><span class="line">-rw-r--r-- 1 root root 11357 7月  28 11:56 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root   420 7月  28 11:56 lombok.config</span><br><span class="line">drwxr-xr-x 4 root root    71 7月  28 11:56 order-service</span><br><span class="line">-rw-r--r-- 1 root root  5419 7月  28 11:56 pom.xml</span><br><span class="line">drwxr-xr-x 3 root root    32 7月  28 11:56 portal-service</span><br><span class="line">drwxr-xr-x 4 root root    75 7月  28 11:56 product-service</span><br><span class="line">-rw-r--r-- 1 root root    24 7月  28 11:56 README.md</span><br><span class="line">drwxr-xr-x 4 root root    71 7月  28 11:56 stock-service</span><br><span class="line"></span><br><span class="line">每个微服务的配置文件均在：</span><br><span class="line">XXX/src/main/resources/application-fat.yml</span><br><span class="line"></span><br><span class="line">需要修改的微服务为：stock-service，product-service， order-service中的mysql配置文件；</span><br></pre></td></tr></table></figure><p>Maven项目对象模型(POM),可以通过一小段描述信息来观念里项目的构建，报告和文档的项目管理工具软件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -D maven.test.skip=true -P prod</span><br><span class="line"></span><br><span class="line">mvn clean package：清除目录中生成的结果，做一个清除，重新打新的包。</span><br><span class="line">-D maven.test.skip: 跳过单元测试，写的测试用例，如果写的有问题，是编译不过去的</span><br><span class="line">-P prod: 使用哪一套配置文件</span><br><span class="line"></span><br><span class="line">[root@k8s-master simple-microservice-dev1]# mvn clean package -D maven.test.skip=true</span><br><span class="line"></span><br><span class="line">构建完成会多出一个target且根据pom文件生成指定jar包：</span><br><span class="line"># cd product-service/product-service-api/target/</span><br><span class="line">classes/                 maven-archiver/          maven-status/            product-service-api.jar</span><br></pre></td></tr></table></figure></p><h3 id="三、构建项目镜像并推送到镜像仓库"><a href="#三、构建项目镜像并推送到镜像仓库" class="headerlink" title="三、构建项目镜像并推送到镜像仓库"></a>三、构建项目镜像并推送到镜像仓库</h3><p>构建镜像使用Docker和结合Dockerfile<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 product-service-biz]# cat Dockerfile</span><br><span class="line">FROM java:8-jdk-alpine</span><br><span class="line">RUN  apk add -U tzdata &amp;&amp; \</span><br><span class="line">     ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">COPY ./target/product-service-biz.jar ./</span><br><span class="line">EXPOSE 8010</span><br><span class="line">CMD java -jar /product-service-biz.jar</span><br></pre></td></tr></table></figure></p><p>打包：</p><p>如果不是https的Harbor需要在docker里面添加信任才能访问到镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master ~]# vim /etc/docker/daemon.json </span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">        &quot;registry-mirrors&quot;: [&quot;http://f1361db2.m.daocloud.io&quot;],</span><br><span class="line">        &quot;insecure-registries&quot;: [&quot;192.168.171.10&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build -t product .</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 product-service-biz]# docker images</span><br><span class="line">REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">product                                    latest              f8d58232cd1b        5 seconds ago       191MB</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 product-service-biz]# docker tag product 192.168.171.10/microservice/product:latest</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# docker login 192.168.171.10</span><br><span class="line">Username: admin</span><br><span class="line">Password: </span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line"></span><br><span class="line">推送镜像到Harbor仓库中：</span><br><span class="line">docker push 192.168.171.10/microservice/product:latest</span><br></pre></td></tr></table></figure><h3 id="四、推送镜像到harbor"><a href="#四、推送镜像到harbor" class="headerlink" title="四、推送镜像到harbor"></a>四、推送镜像到harbor</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 simple-microservice-dev3]# ll</span><br><span class="line">总用量 28</span><br><span class="line">drwxr-xr-x 4 root root    70 7月  28 11:50 basic-common</span><br><span class="line">drwxr-xr-x 2 root root    59 7月  28 11:50 db</span><br><span class="line">drwxr-xr-x 3 root root    50 7月  28 11:50 eureka-service</span><br><span class="line">drwxr-xr-x 3 root root    50 7月  28 11:50 gateway-service</span><br><span class="line">drwxr-xr-x 2 root root   143 7月  28 11:50 k8s</span><br><span class="line">-rw-r--r-- 1 root root 11357 7月  28 11:50 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root   420 7月  28 11:50 lombok.config</span><br><span class="line">drwxr-xr-x 4 root root    71 7月  28 11:50 order-service</span><br><span class="line">-rw-r--r-- 1 root root  5419 7月  28 11:50 pom.xml</span><br><span class="line">drwxr-xr-x 3 root root    50 7月  28 11:50 portal-service</span><br><span class="line">drwxr-xr-x 4 root root    75 7月  28 11:50 product-service</span><br><span class="line">-rw-r--r-- 1 root root    24 7月  28 11:50 README.md</span><br><span class="line">drwxr-xr-x 4 root root    71 7月  28 11:50 stock-service</span><br></pre></td></tr></table></figure><p>具体来看下批量将所有的微服务推送到镜像仓库并在K8S集群中发布：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">##创建拉取镜像的认证信息</span><br><span class="line">docker_registry=192.168.171.10</span><br><span class="line">kubectl create secret docker-registry registry-pull-secret --docker-server=$docker_registry --docker-username=admin --docker-password=XXXXXX --docker-email=zhdya@zhdya.cn -n ms</span><br><span class="line"></span><br><span class="line">##服务list</span><br><span class="line">service_list=&quot;eureka-service gateway-service order-service product-service stock-service portal-service&quot;</span><br><span class="line">service_list=$&#123;1:-$&#123;service_list&#125;&#125;</span><br><span class="line">work_dir=$(dirname $PWD)</span><br><span class="line">current_dir=$PWD</span><br><span class="line"></span><br><span class="line">##编译</span><br><span class="line">cd $work_dir</span><br><span class="line">mvn clean package -Dmaven.test.skip=true</span><br><span class="line"></span><br><span class="line">##推送镜像并在K8S中发布（此dev3先取消发布）</span><br><span class="line">for service in $service_list; do</span><br><span class="line">   cd $work_dir/$service</span><br><span class="line">   if ls |grep biz &amp;&gt;/dev/null; then</span><br><span class="line">      cd $&#123;service&#125;-biz</span><br><span class="line">   fi</span><br><span class="line">   service=$&#123;service%-*&#125;</span><br><span class="line">   image_name=$docker_registry/microservice/$&#123;service&#125;:$(date +%F-%H-%M-%S)</span><br><span class="line">   docker build -t $&#123;image_name&#125; .</span><br><span class="line">   docker push $&#123;image_name&#125;</span><br><span class="line">#   sed -i -r &quot;s#(image: )(.*)#\1$image_name#&quot; $&#123;current_dir&#125;/$&#123;service&#125;.yaml</span><br><span class="line">#   kubectl apply -f $&#123;current_dir&#125;/$&#123;service&#125;.yaml</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>其中一个service的dockerfile<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 product-service-biz]# vim Dockerfile</span><br><span class="line">FROM java:8-jdk-alpine</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">COPY ./target/product-service-biz.jar ./</span><br><span class="line">EXPOSE 8010</span><br><span class="line">CMD java -jar /product-service-biz.jar</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20200112/WoT92exlsnEm.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="五、部署springCloud项目"><a href="#五、部署springCloud项目" class="headerlink" title="五、部署springCloud项目"></a>五、部署springCloud项目</h3><ul><li>1、服务编排</li><li>2、在k8s平台部署Erueka</li><li>3、导入数据库文件到Mysql</li><li>4、部署网关gateway</li><li>5、部署业务程序（product、stock、order）</li><li>6、部署前端（portal）</li></ul><p>编译打成jar包，在<strong>dev3</strong>分支</p><h4 id="5-1、创建命名空间"><a href="#5-1、创建命名空间" class="headerlink" title="5.1、创建命名空间"></a>5.1、创建命名空间</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create ns ms</span><br></pre></td></tr></table></figure><p>部署环境要求：</p><ul><li>跨主机网络：使用flannel或者Calico ，需要网络来打通主机之间资源的通信</li><li>CoreDNS: k8s内部的DNS。 ,用于对pod对service做记录的，好让其他的pod做访问。</li><li>Harbor镜像仓库：这个我们已经准备好了，并将项目镜像推送上去了。</li><li>Ingress Controller：同一暴露我们的应用，写yaml文件实现。</li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20200112/F8hpWDRG2KCT.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这里portal是门户网站-前端，用户访问<a href="http://www.XXX.com的页面，通过域名访问之后，进行的一个页面展示，我们我们通过pod来进行实现，ingress来定义我们的域名，域名定义哪个service，来定义到某个pod上，来影响静态页面，下订单请求交给网关api,采用异步调用，暴露网关，进行来用户访问，ingress也来调用，service来实现pod副本gateway网关，通过一些前端页面的页面功能，同gateway来调用实现，用户点击某个功能gateway拿到这个请求之后,通过路由转发规则，到后端的业务程序，比如商品信息（product）库存，订单，他会根据不同的业务需要来处理，库存服务会根据订单的使用来和内部的调用接口来实现，pod直接调用，需要跨主机网络，怎么找到这个服务，就需要这个注册中心，应用间的互相调用就需要这个注册中心，所有的服务都会放在这里，来进行消息通信，现在比较流行的就是erueka,订单服务都会放入到我们的mysql数据库中的，mysql是部署在外部的，有状态应用，这个部署在k8s中是比较麻烦大的，erueka是部署在k8s集群内的，只需要保证他的id是唯一性就可以了,不需要考虑他的存储。" target="_blank" rel="noopener">www.XXX.com的页面，通过域名访问之后，进行的一个页面展示，我们我们通过pod来进行实现，ingress来定义我们的域名，域名定义哪个service，来定义到某个pod上，来影响静态页面，下订单请求交给网关api,采用异步调用，暴露网关，进行来用户访问，ingress也来调用，service来实现pod副本gateway网关，通过一些前端页面的页面功能，同gateway来调用实现，用户点击某个功能gateway拿到这个请求之后,通过路由转发规则，到后端的业务程序，比如商品信息（product）库存，订单，他会根据不同的业务需要来处理，库存服务会根据订单的使用来和内部的调用接口来实现，pod直接调用，需要跨主机网络，怎么找到这个服务，就需要这个注册中心，应用间的互相调用就需要这个注册中心，所有的服务都会放在这里，来进行消息通信，现在比较流行的就是erueka,订单服务都会放入到我们的mysql数据库中的，mysql是部署在外部的，有状态应用，这个部署在k8s中是比较麻烦大的，erueka是部署在k8s集群内的，只需要保证他的id是唯一性就可以了,不需要考虑他的存储。</a></p><p>先来看一个yaml（eureka，需要通过域名暴露服务）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat eureka.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: eureka.zhdya.cn</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            serviceName: eureka</span><br><span class="line">            servicePort: 8888</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: None       ##无头服务</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8888</span><br><span class="line">    name: eureka</span><br><span class="line">  selector:</span><br><span class="line">    project: ms</span><br><span class="line">    app: eureka</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      project: ms</span><br><span class="line">      app: eureka</span><br><span class="line">  serviceName: &quot;eureka&quot;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        project: ms</span><br><span class="line">        app: eureka</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: registry-pull-secret</span><br><span class="line">      containers:</span><br><span class="line">      - name: eureka</span><br><span class="line">        image: 192.168.171.10/microservice/eureka:2020-01-12-13-14-40</span><br><span class="line">        ports:</span><br><span class="line">          - protocol: TCP</span><br><span class="line">            containerPort: 8888</span><br><span class="line">        env:</span><br><span class="line">          - name: MY_POD_NAME</span><br><span class="line">            valueFrom:</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.name</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 0.5</span><br><span class="line">            memory: 256Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8888</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8888</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br></pre></td></tr></table></figure></p><p>注释：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如上名为eureka的service，其clusterIP为None，所以它是无头服务。下边为名eureka的StatefulSet，其.spec.serviceName的值为eureka，指明通过上边的eureka无头服务提供DNS解析功能功。</span><br></pre></td></tr></table></figure></p><p>Dockerfile：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat ../eureka-service/Dockerfile</span><br><span class="line">FROM java:8-jdk-alpine</span><br><span class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">COPY ./target/eureka-service.jar ./</span><br><span class="line">EXPOSE 8888</span><br><span class="line">CMD java -jar -Deureka.instance.hostname=$&#123;MY_POD_NAME&#125;.eureka.ms /eureka-service.jar</span><br></pre></td></tr></table></figure></p><p>再来看一个微服务（不需要暴露域名）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat product.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: product</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      project: ms</span><br><span class="line">      app: product</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        project: ms</span><br><span class="line">        app: product</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: registry-pull-secret</span><br><span class="line">      containers:</span><br><span class="line">      - name: product</span><br><span class="line">        image: 192.168.171.10/microservice/product:2020-01-12-13-14-56</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        ports:</span><br><span class="line">          - protocol: TCP</span><br><span class="line">            containerPort: 8010</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 0.5</span><br><span class="line">            memory: 256Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8010</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8010</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br></pre></td></tr></table></figure></p><p>创建k8s登录harbor信息认证：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# kubectl create secret docker-registry registry-pull-secret --docker-server=192.168.171.10 --docker-username=admin --docker-password=Zhang --docker-email=zhdya@zhdya.cn -n ms</span><br><span class="line">secret/registry-pull-secret created</span><br></pre></td></tr></table></figure></p><p>查看其中一个微服务的配置信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat ../stock-service/stock-service-biz/src/main/resources/application-fat.yml</span><br><span class="line">spring:</span><br><span class="line">  datasource:</span><br><span class="line">    url: jdbc:mysql://192.168.171.10:3306/tb_stock?characterEncoding=utf-8</span><br><span class="line">    username: root</span><br><span class="line">    password: XXXXXX</span><br><span class="line">    driver-class-name: com.mysql.jdbc.Driver</span><br><span class="line"></span><br><span class="line">eureka:</span><br><span class="line">  instance:</span><br><span class="line">    prefer-ip-address: true</span><br><span class="line">  client:</span><br><span class="line">    register-with-eureka: true</span><br><span class="line">    fetch-registry: true</span><br><span class="line">    service-url:</span><br><span class="line">      defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka</span><br></pre></td></tr></table></figure></p><p>部署eureka：</p><p>补充说明：<br>StatefulSet的应用特点:</p><ul><li>稳定且有唯一的网络标识符 当节点挂掉，既pod重新调度后其PodName和HostName不变，基于Headless Service来实现。</li></ul><p>我们知道kubernetes中的service是定义pod暴露外部访问的一种机制，例如：3个pod，我们可以定义一个service通过标签选择器选到这三个pod，然后让问这个service就可以访问这个pod。</p><p>Headless service是Service通过DNS访问的其中一种方式，只要我们访问”mypod.stsname.namespace.svc.cluster.local”，我们就会访问到stsname下的mypod。而Service DNS的方式下有两种处理方法：</p><ul><li>Normal Service 这里访问”mypod.stsname.namespace.svc.cluster.local”的时候会得到mypod的service的IP，既VIP。</li><li>Headless Service 这里访问”mypod.stsname.namespace.svc.cluster.local”的时候会得到mypod的IP，这里我们可以看到区别是，Headless Service 不需要分配一个VIP，而是通过DNS访问的方式可以解析出带代理的Pod的IP<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# kubectl apply -f eureka.yaml</span><br><span class="line">ingress.extensions/eureka created</span><br><span class="line">service/eureka created</span><br><span class="line">statefulset.apps/eureka created</span><br><span class="line"></span><br><span class="line">##因为是有状态应用，所以是一个一个的启动</span><br><span class="line">[root@k8s-master1 k8s]# kubectl get po -n ms</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE</span><br><span class="line">eureka-0   1/1     Running   0          5m37s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 k8s]# kubectl get po -n ms</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE</span><br><span class="line">eureka-0   1/1     Running   0          8m19s</span><br><span class="line">eureka-1   1/1     Running   0          3m7s</span><br><span class="line">eureka-2   1/1     Running   0          108s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 k8s]# kubectl exec -it eureka-1 sh -n ms</span><br><span class="line">/ # nslookup eureka</span><br><span class="line">nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolve</span><br><span class="line"></span><br><span class="line">Name:      eureka</span><br><span class="line">Address 1: 10.244.36.123 eureka-1.eureka.ms.svc.cluster.local       ##通过headless的stateful部署的应用 就算IP变了之后，eureka-1.eureka.ms.svc.cluster.local这个也会自动的绑定新的IP上，且不会变！</span><br><span class="line">Address 2: 10.244.169.176 eureka-2.eureka.ms.svc.cluster.local</span><br><span class="line">Address 3: 10.244.159.171 eureka-0.eureka.ms.svc.cluster.local</span><br><span class="line"></span><br><span class="line">StatefulSet+Headless DNS名称格式：</span><br><span class="line">&lt;statefulsetName-index&gt;.&lt;service-name&gt; .&lt;namespacename&gt;.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Eureka集群节点Pod名称：</span><br><span class="line">http://eureka-0.eureka.ms.svc.cluster.local</span><br><span class="line">http://eureka-1.eureka.ms.svc.cluster.local</span><br><span class="line">http://eureka-2.eureka.ms.svc.cluster.local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 k8s]# kubectl get ing -n ms</span><br><span class="line">NAME     HOSTS             ADDRESS   PORTS   AGE</span><br><span class="line">eureka   eureka.zhdya.cn             80      23m</span><br></pre></td></tr></table></figure></li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20200112/O2rftodcCRve.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><img src="http://myimage.okay686.cn/okay686cn/20200112/g6rnQqgIQIHc.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>部署gateway（<u>线上环境务必部署至少要2个副本，测试</u>）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat gateway.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: gateway</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: gateway.zhdya.cn</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            serviceName: gateway</span><br><span class="line">            servicePort: 9999</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: gateway</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9999</span><br><span class="line">    name: gateway</span><br><span class="line">  selector:</span><br><span class="line">    project: ms</span><br><span class="line">    app: gateway</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: gateway</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      project: ms</span><br><span class="line">      app: gateway</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        project: ms</span><br><span class="line">        app: gateway</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: registry-pull-secret</span><br><span class="line">      containers:</span><br><span class="line">      - name: gateway</span><br><span class="line">        image: 192.168.171.10/microservice/gateway:2020-01-12-13-14-44</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        ports:</span><br><span class="line">          - protocol: TCP</span><br><span class="line">            containerPort: 9999</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 0.5</span><br><span class="line">            memory: 256Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 9999</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 9999</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 k8s]# kubectl apply -f gateway.yaml</span><br><span class="line">ingress.extensions/gateway created</span><br><span class="line">service/gateway created</span><br><span class="line">deployment.apps/gateway created</span><br></pre></td></tr></table></figure></p><p>部署portal:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat portal.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: portal</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: portal.zhdya.cn</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            serviceName: portal</span><br><span class="line">            servicePort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: portal</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    name: portal</span><br><span class="line">  selector:</span><br><span class="line">    project: ms</span><br><span class="line">    app: portal</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: portal</span><br><span class="line">  namespace: ms</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      project: ms</span><br><span class="line">      app: portal</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        project: ms</span><br><span class="line">        app: portal</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: registry-pull-secret</span><br><span class="line">      containers:</span><br><span class="line">      - name: portal</span><br><span class="line">        image: 192.168.171.10/microservice/portal:2020-01-12-13-15-06</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        ports:</span><br><span class="line">          - protocol: TCP</span><br><span class="line">            containerPort: 8080</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 0.5</span><br><span class="line">            memory: 256Mi</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line"></span><br><span class="line"># kubectl apply -f portal.yaml</span><br><span class="line">ingress.extensions/portal created</span><br><span class="line">service/portal created</span><br><span class="line">deployment.apps/portal created</span><br></pre></td></tr></table></figure></p><p>部署其它微服务：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# kubectl get po -n ms</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">eureka-0                   1/1     Running   0          39m</span><br><span class="line">eureka-1                   1/1     Running   0          34m</span><br><span class="line">eureka-2                   1/1     Running   0          33m</span><br><span class="line">gateway-5b6b78b54c-vjq2l   1/1     Running   0          10m</span><br><span class="line">order-58cc95cf96-4t9pb     1/1     Running   0          2m35s</span><br><span class="line">portal-5574cbd9d6-56tzc    1/1     Running   0          8m53s</span><br><span class="line">product-74bb9d98d-8hz95    1/1     Running   0          2m42s</span><br><span class="line">stock-845f745db5-q868l     1/1     Running   0          2m39s</span><br></pre></td></tr></table></figure></p><p>查看部署情况：<br><img src="http://myimage.okay686.cn/okay686cn/20200112/wus4lmXQkbH6.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="六、总结："><a href="#六、总结：" class="headerlink" title="六、总结："></a>六、总结：</h3><ul><li>第一步：熟悉Spring Cloud微服务项目</li><li>第二步：源代码编译构建</li><li>第三步：构建项目镜像并推送到镜像仓库</li><li>第四步：在K8S中部署Spring Cloud微服务项目的逻辑架构</li><li>第五步： K8S服务编排</li><li>第六步：在K8S中部署Eureka集群（注册中心）</li><li>第七步：部署微服务网关服务</li><li>第八步：部署微服务业务程序</li><li>第九步：部署微服务前端</li><li>第十步：微服务扩容与发布</li></ul>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从运维角度看微服务架构</title>
    <link href="/2020/01/01/6.1%E3%80%81%E4%BB%8E%E8%BF%90%E7%BB%B4%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/"/>
    <url>/2020/01/01/6.1%E3%80%81%E4%BB%8E%E8%BF%90%E7%BB%B4%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h3 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h3><blockquote><p>微服务架构是一项在云中部署应用和服务的新技术。</p></blockquote><h4 id="1-1、微服务是什么？"><a href="#1-1、微服务是什么？" class="headerlink" title="1.1、微服务是什么？"></a>1.1、微服务是什么？</h4><p>微服务是一种软件设计风格，开发人员在开发项目时，是一种微服务这种标准去设计的，这种的设计风格是一种将单体的应用，拆分为多个小的组件去开发，那每个组件是独立的部署，独立的测试，服务之间采用轻量级的通信。</p><h4 id="1-2、微服务的特点"><a href="#1-2、微服务的特点" class="headerlink" title="1.2、微服务的特点"></a>1.2、微服务的特点</h4><ul><li>服务的组件化</li></ul><p>每个服务独立开发、部署、有效避免一个服务的修改引起整个系统重新部署。</p><ul><li>技术栈灵活</li></ul><p>约定通信方式，使得服务本身功能实现对技术要求不再那么敏感，可以根据不停语言进行开发。</p><ul><li>独立部署</li></ul><p>每个微服务独立部署，加快部署速度，方便扩展，比起单体应用来讲要小，轻量级的，方便快速部署，扩展。</p><ul><li>扩展性强</li></ul><p>每个微服务可以部署多个，没有多少依赖，并且有负载均衡能力，比如一个服务部署一个副本或5个副本，通过k8s可以更好的去扩展我们的应用。</p><ul><li>独立数据</li></ul><p>每个微服务有独立的基本组件，例如数据库、缓存等，可能有不同的开放人员，不依赖。</p><h4 id="1-3、微服务不足"><a href="#1-3、微服务不足" class="headerlink" title="1.3、微服务不足"></a>1.3、微服务不足</h4><ul><li><p>沟通成本：由于组件都是分开来开发的，不同的项目组，沟通起来不方便，单体应用就是集中起来开发的。</p></li><li><p>数据一致性：保证这个数据，独立的组件数据是一致性。</p></li><li><p>运维成本：虚拟机部署，需要考虑组件性，调用关系，监控，配置。</p></li><li><p>内部架构复杂性：分布式的，需要轻量级的通信，rbac,MQ,还有很多的数据库。</p></li></ul><h4 id="1-4、单体应用-vs-微服务"><a href="#1-4、单体应用-vs-微服务" class="headerlink" title="1.4、单体应用 vs 微服务"></a>1.4、单体应用 vs 微服务</h4><table><thead><tr><th>单体架构优势：</th><th>单体架构不足：</th></tr></thead><tbody><tr><td>易于部署</td><td>代码膨胀，难以维护</td></tr><tr><td>易于测试</td><td>构建、部署成本大、新人上手难</td></tr></tbody></table><p><img src="http://myimage.okay686.cn/okay686cn/20200107/UwHX0mtnrXyk.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>单体应用</strong>：适合于轻量级的应用，不提供复杂的应用。</p><p><strong>微服务适合</strong>：比较大的应用，复杂一些的。</p><p><img src="http://myimage.okay686.cn/okay686cn/20200107/JoFq5ueNFmkM.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="1-5、java微服务框架"><a href="#1-5、java微服务框架" class="headerlink" title="1.5、java微服务框架"></a>1.5、java微服务框架</h4><p>spring Boot 是独立的。</p><p>spring cloud ,基于spring boot的。</p><p>Dubbo 阿里巴巴的开源微服务框架，通过rbc实现组件之间的通信。</p><p>微服务架构图：</p><p><img src="http://myimage.okay686.cn/okay686cn/20200107/NElkGotI17eP.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>为什么要用注册中心？（主流注册中心： Eureka， Nacos）</strong></p><ul><li>1、需要记录一个或者多个微服务多个副本接口地址；</li><li>2、需要实现一个或者多个微服务多个副本负载均衡；</li><li>3、需要判断一个或者多个微服务的副本是否可用；</li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20200107/Cetm7OMlyfs2.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>不同环境如何区分配置文件？</strong></p><p><strong>第一种</strong>：java-jar –spring.profile.active=dev xxx.jar</p><p><strong>第二种</strong>：统一的配置中心，例如携程的Apollo，百度的Disconf,动态根据不同的环境进行配置，页面进行管理，需要二次开发。</p><h4 id="1-6、项目迁移到k8s平台是怎样的流程"><a href="#1-6、项目迁移到k8s平台是怎样的流程" class="headerlink" title="1.6、项目迁移到k8s平台是怎样的流程"></a>1.6、项目迁移到k8s平台是怎样的流程</h4><p>举个例子了解一下大概的一个怎样的流程：</p><p>1制作镜像 –&gt; 2控制管理pod –&gt; 3暴露应用 –&gt; 4对外发布应用 –&gt; 5日志/监控</p><p><strong>控制器管理Pod：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deployment :无状态部署</span><br><span class="line"></span><br><span class="line">statefulset ：有状态部署</span><br><span class="line"></span><br><span class="line">Daemonset :守护进程部署</span><br><span class="line"></span><br><span class="line">job &amp; cronjob：批处理</span><br></pre></td></tr></table></figure><p><strong>暴露应用Service</strong></p><p>service定义pod的逻辑集合，提供服务发现及负载均衡</p><p>支持cluster ip， nodeport， loadbalancer三种类型</p><p>底层实现iptables/ipvs两种网络模式</p><p>通过label关联pod</p><p>使用Coredns解析service名称</p><p><img src="http://myimage.okay686.cn/okay686cn/20200107/bHUzIC1b2Kjr.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>ingress</strong></p><p>通过service关联pod</p><p>基于域名访问</p><p>通过ingress controller实现pod的负载均衡</p><p>支持tcp、udp 4层和http7层</p><p><img src="http://myimage.okay686.cn/okay686cn/20200107/C3p1GVh7n9mi.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>pod数据持久化</strong></p><p>容器部署过程中一般有以下三种数据：</p><p>启动时需要的初始数据，可以是配置文件；</p><p>启动过程中产生的临时数据，该数据需要多个容器间共享；</p><p>启动过程中产生的持久化数据；</p><p><img src="http://myimage.okay686.cn/okay686cn/20200107/52xDEGsdYfbk.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>七、Ceph日常运维管理</title>
    <link href="/2019/12/29/%E4%B8%83%E3%80%81Ceph%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4%E7%AE%A1%E7%90%86/"/>
    <url>/2019/12/29/%E4%B8%83%E3%80%81Ceph%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4%E7%AE%A1%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="一、集群监控管理"><a href="#一、集群监控管理" class="headerlink" title="一、集群监控管理"></a>一、集群监控管理</h3><p>集群整体运行状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cephnode01 ~]# ceph -s </span><br><span class="line">cluster:</span><br><span class="line">    id:     8230a918-a0de-4784-9ab8-cd2a2b8671d0</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            application not enabled on 1 pool(s)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 27h)</span><br><span class="line">    mgr: cephnode01(active, since 53m), standbys: cephnode03, cephnode02</span><br><span class="line">    osd: 4 osds: 4 up (since 27h), 4 in (since 19h)</span><br><span class="line">    rgw: 1 daemon active (cephnode01)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   6 pools, 96 pgs</span><br><span class="line">    objects: 235 objects, 3.6 KiB</span><br><span class="line">    usage:   4.0 GiB used, 56 GiB / 60 GiB avail</span><br><span class="line">    pgs:     96 active+clean</span><br></pre></td></tr></table></figure></p><pre><code>id：集群IDhealth：集群运行状态，这里有一个警告，说明是有问题，意思是pg数大于pgp数，通常此数值相等。mon：Monitors运行状态。osd：OSDs运行状态。mgr：Managers运行状态。mds：Metadatas运行状态。pools：存储池与PGs的数量。objects：存储对象的数量。usage：存储的理论用量。pgs：PGs的运行状态</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph -w</span><br><span class="line">~]$ ceph health detail</span><br></pre></td></tr></table></figure><h4 id="PG状态"><a href="#PG状态" class="headerlink" title="PG状态"></a>PG状态</h4><p>查看pg状态查看通常使用下面两个命令即可，dump可以查看更详细信息，如。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph pg dump</span><br><span class="line">~]$ ceph pg stat</span><br></pre></td></tr></table></figure></p><h4 id="Pool状态"><a href="#Pool状态" class="headerlink" title="Pool状态"></a>Pool状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph osd pool stats</span><br><span class="line">~]$ ceph osd pool stats</span><br></pre></td></tr></table></figure><h4 id="OSD状态"><a href="#OSD状态" class="headerlink" title="OSD状态"></a>OSD状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph osd stat</span><br><span class="line">~]$ ceph osd dump</span><br><span class="line">~]$ ceph osd tree</span><br><span class="line">~]$ ceph osd df</span><br></pre></td></tr></table></figure><h4 id="Monitor状态和查看仲裁状态"><a href="#Monitor状态和查看仲裁状态" class="headerlink" title="Monitor状态和查看仲裁状态"></a>Monitor状态和查看仲裁状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph mon stat</span><br><span class="line">~]$ ceph mon dump</span><br><span class="line">~]$ ceph quorum_status</span><br></pre></td></tr></table></figure><h4 id="集群空间用量"><a href="#集群空间用量" class="headerlink" title="集群空间用量"></a>集群空间用量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]$ ceph df</span><br><span class="line">~]$ ceph df detail</span><br></pre></td></tr></table></figure><h3 id="二、集群配置管理-临时和全局，服务平滑重启"><a href="#二、集群配置管理-临时和全局，服务平滑重启" class="headerlink" title="二、集群配置管理(临时和全局，服务平滑重启)"></a>二、集群配置管理(临时和全局，服务平滑重启)</h3><p>有时候需要更改服务的配置，但不想重启服务，或者是临时修改。这时候就可以使用tell和daemon子命令来完成此需求。</p><h4 id="1、查看运行配置"><a href="#1、查看运行配置" class="headerlink" title="1、查看运行配置"></a>1、查看运行配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config show </span><br><span class="line"></span><br><span class="line">命令举例：</span><br><span class="line"># ceph daemon osd.0 config show</span><br></pre></td></tr></table></figure><h4 id="2、tell子命令格式"><a href="#2、tell子命令格式" class="headerlink" title="2、tell子命令格式"></a>2、tell子命令格式</h4><p>使用 tell 的方式适合对整个集群进行设置，使用 * 号进行匹配，就可以对整个集群的角色进行设置。而出现节点异常无法设置时候，只会在命令行当中进行报错，不太便于查找。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph tell &#123;daemon-type&#125;.&#123;daemon id or *&#125; injectargs --&#123;name&#125;=&#123;value&#125; [--&#123;name&#125;=&#123;value&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1</span><br></pre></td></tr></table></figure></p><ul><li>daemon-type：为要操作的对象类型如osd、mon、mds等。</li><li>daemon id：该对象的名称，osd通常为0、1等，mon为ceph -s显示的名称，这里可以输入*表示全部。</li><li>injectargs：表示参数注入，后面必须跟一个参数，也可以跟多个</li></ul><h4 id="3、daemon子命令"><a href="#3、daemon子命令" class="headerlink" title="3、daemon子命令"></a>3、daemon子命令</h4><p>使用 daemon 进行设置的方式就是一个个的去设置，这样可以比较好的反馈，此方法是需要在设置的角色所在的主机上进行设置。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config set &#123;name&#125;=&#123;value&#125;</span><br><span class="line">命令举例：</span><br><span class="line"># ceph daemon mon.ceph-monitor-1 config set mon_allow_pool_delete false</span><br></pre></td></tr></table></figure></p><h3 id="三、集群操作"><a href="#三、集群操作" class="headerlink" title="三、集群操作"></a>三、集群操作</h3><p>命令包含start、restart、status<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、启动所有守护进程</span><br><span class="line"># systemctl start ceph.target</span><br><span class="line">2、按类型启动守护进程</span><br><span class="line"># systemctl start ceph-mgr.target</span><br><span class="line"># systemctl start ceph-osd@id</span><br><span class="line"># systemctl start ceph-mon.target</span><br><span class="line"># systemctl start ceph-mds.target</span><br><span class="line"># systemctl start ceph-radosgw.target</span><br></pre></td></tr></table></figure></p><h3 id="四、添加和删除OSD"><a href="#四、添加和删除OSD" class="headerlink" title="四、添加和删除OSD"></a>四、添加和删除OSD</h3><h4 id="1、添加OSD"><a href="#1、添加OSD" class="headerlink" title="1、添加OSD"></a>1、添加OSD</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、格式化磁盘</span><br><span class="line">ceph-volume lvm zap /dev/sd&lt;id&gt;</span><br><span class="line">2、进入到ceph-deploy执行目录/my-cluster，添加OSD</span><br><span class="line"># ceph-deploy osd create --data /dev/sd&lt;id&gt; $hostname</span><br></pre></td></tr></table></figure><h4 id="2、删除OSD"><a href="#2、删除OSD" class="headerlink" title="2、删除OSD"></a>2、删除OSD</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、调整osd的crush weight为 0</span><br><span class="line">ceph osd crush reweight osd.&lt;ID&gt; 0.0</span><br><span class="line">2、将osd进程stop</span><br><span class="line">systemctl stop ceph-osd@&lt;ID&gt;</span><br><span class="line">3、将osd设置out</span><br><span class="line">ceph osd out &lt;ID&gt;</span><br><span class="line">4、立即执行删除OSD中数据</span><br><span class="line">ceph osd purge osd.&lt;ID&gt; --yes-i-really-mean-it</span><br><span class="line">5、卸载磁盘</span><br><span class="line">umount /var/lib/ceph/osd/ceph-？</span><br></pre></td></tr></table></figure><h3 id="五、扩容PG"><a href="#五、扩容PG" class="headerlink" title="五、扩容PG"></a>五、扩容PG</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_num 128</span><br><span class="line">ceph osd pool set &#123;pool-name&#125; pgp_num 128</span><br></pre></td></tr></table></figure><p>注：<br>1、扩容大小取跟它接近的2的N次方<br>2、在更改pool的PG数量时，需同时更改PGP的数量。PGP是为了管理placement而存在的专门的PG，它和PG的数量应该保持一致。如果你增加pool的pg_num，就需要同时增加pgp_num，保持它们大小一致，这样集群才能正常rebalancing。</p><h3 id="六、Pool操作"><a href="#六、Pool操作" class="headerlink" title="六、Pool操作"></a>六、Pool操作</h3><h4 id="列出存储池"><a href="#列出存储池" class="headerlink" title="列出存储池"></a>列出存储池</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure><h4 id="创建存储池"><a href="#创建存储池" class="headerlink" title="创建存储池"></a>创建存储池</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; [&#123;pgp-num&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph osd pool create rbd  32 32</span><br></pre></td></tr></table></figure><h4 id="设置存储池配额"><a href="#设置存储池配额" class="headerlink" title="设置存储池配额"></a>设置存储池配额</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph osd pool set-quota &#123;pool-name&#125; [max_objects &#123;obj-count&#125;] [max_bytes &#123;bytes&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph osd pool set-quota rbd max_objects 10000</span><br></pre></td></tr></table></figure><h4 id="删除存储池"><a href="#删除存储池" class="headerlink" title="删除存储池"></a>删除存储池</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool delete &#123;pool-name&#125; [&#123;pool-name&#125; --yes-i-really-really-mean-it]</span><br></pre></td></tr></table></figure><h4 id="重命名存储池"><a href="#重命名存储池" class="headerlink" title="重命名存储池"></a>重命名存储池</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool rename &#123;current-pool-name&#125; &#123;new-pool-name&#125;</span><br></pre></td></tr></table></figure><h4 id="查看存储池统计信息"><a href="#查看存储池统计信息" class="headerlink" title="查看存储池统计信息"></a>查看存储池统计信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rados df</span><br></pre></td></tr></table></figure><h4 id="给存储池做快照"><a href="#给存储池做快照" class="headerlink" title="给存储池做快照"></a>给存储池做快照</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool mksnap &#123;pool-name&#125; &#123;snap-name&#125;</span><br></pre></td></tr></table></figure><h4 id="删除存储池的快照"><a href="#删除存储池的快照" class="headerlink" title="删除存储池的快照"></a>删除存储池的快照</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool rmsnap &#123;pool-name&#125; &#123;snap-name&#125;</span><br></pre></td></tr></table></figure><h4 id="获取存储池选项值"><a href="#获取存储池选项值" class="headerlink" title="获取存储池选项值"></a>获取存储池选项值</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool get &#123;pool-name&#125; &#123;key&#125;</span><br></pre></td></tr></table></figure><h4 id="调整存储池选项值"><a href="#调整存储池选项值" class="headerlink" title="调整存储池选项值"></a>调整存储池选项值</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; &#123;key&#125; &#123;value&#125;</span><br><span class="line">size：设置存储池中的对象副本数，详情参见设置对象副本数。仅适用于副本存储池。</span><br><span class="line">min_size：设置 I/O 需要的最小副本数，详情参见设置对象副本数。仅适用于副本存储池。</span><br><span class="line">pg_num：计算数据分布时的有效 PG 数。只能大于当前 PG 数。</span><br><span class="line">pgp_num：计算数据分布时使用的有效 PGP 数量。小于等于存储池的 PG 数。</span><br><span class="line">hashpspool：给指定存储池设置/取消 HASHPSPOOL 标志。</span><br><span class="line">target_max_bytes：达到 max_bytes 阀值时会触发 Ceph 冲洗或驱逐对象。</span><br><span class="line">target_max_objects：达到 max_objects 阀值时会触发 Ceph 冲洗或驱逐对象。</span><br><span class="line">scrub_min_interval：在负载低时，洗刷存储池的最小间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_min_interval 。</span><br><span class="line">scrub_max_interval：不管集群负载如何，都要洗刷存储池的最大间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_max_interval 。</span><br><span class="line">deep_scrub_interval：“深度”洗刷存储池的间隔秒数。如果是 0 ，就按照配置文件里的 osd_deep_scrub_interval 。</span><br></pre></td></tr></table></figure><h4 id="获取对象副本数"><a href="#获取对象副本数" class="headerlink" title="获取对象副本数"></a>获取对象副本数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd dump | grep &apos;replicated size&apos;</span><br></pre></td></tr></table></figure><h3 id="七、用户管理"><a href="#七、用户管理" class="headerlink" title="七、用户管理"></a>七、用户管理</h3><p>Ceph 把数据以对象的形式存于各存储池中。Ceph 用户必须具有访问存储池的权限才能够读写数据。另外，Ceph 用户必须具有执行权限才能够使用 Ceph 的管理命令。</p><h4 id="1、查看用户信息"><a href="#1、查看用户信息" class="headerlink" title="1、查看用户信息"></a>1、查看用户信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看所有用户信息</span><br><span class="line"># ceph auth list</span><br><span class="line">获取所有用户的key与权限相关信息</span><br><span class="line"># ceph auth get client.admin</span><br><span class="line">如果只需要某个用户的key信息，可以使用pring-key子命令</span><br><span class="line"># ceph auth print-key client.admin</span><br></pre></td></tr></table></figure><h4 id="2、添加用户"><a href="#2、添加用户" class="headerlink" title="2、添加用户"></a>2、添加用户</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph auth add client.john mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;</span><br><span class="line"># ceph auth get-or-create client.paul mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;</span><br><span class="line"># ceph auth get-or-create client.george mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos; -o george.keyring</span><br><span class="line"># ceph auth get-or-create-key client.ringo mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos; -o ringo.key</span><br></pre></td></tr></table></figure><h4 id="3、修改用户权限"><a href="#3、修改用户权限" class="headerlink" title="3、修改用户权限"></a>3、修改用户权限</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph auth caps client.john mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;</span><br><span class="line"># ceph auth caps client.paul mon &apos;allow rw&apos; osd &apos;allow rwx pool=liverpool&apos;</span><br><span class="line"># ceph auth caps client.brian-manager mon &apos;allow *&apos; osd &apos;allow *&apos;</span><br><span class="line"># ceph auth caps client.ringo mon &apos; &apos; osd &apos; &apos;</span><br></pre></td></tr></table></figure><h4 id="4、删除用户"><a href="#4、删除用户" class="headerlink" title="4、删除用户"></a>4、删除用户</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph auth del &#123;TYPE&#125;.&#123;ID&#125;</span><br><span class="line">其中， &#123;TYPE&#125; 是 client，osd，mon 或 mds 的其中一种。&#123;ID&#125; 是用户的名字或守护进程的 ID 。</span><br></pre></td></tr></table></figure><h3 id="八、增加和删除Monitor"><a href="#八、增加和删除Monitor" class="headerlink" title="八、增加和删除Monitor"></a>八、增加和删除Monitor</h3><p>一个集群可以只有一个 monitor，推荐生产环境至少部署 3 个。 Ceph 使用 Paxos 算法的一个变种对各种 map 、以及其它对集群来说至关重要的信息达成共识。建议（但不是强制）部署奇数个 monitor 。Ceph 需要 mon 中的大多数在运行并能够互相通信，比如单个 mon，或 2 个中的 2 个，3 个中的 2 个，4 个中的 3 个等。初始部署时，建议部署 3 个 monitor。后续如果要增加，请一次增加 2 个。  </p><h4 id="1、新增一个monitor"><a href="#1、新增一个monitor" class="headerlink" title="1、新增一个monitor"></a>1、新增一个monitor</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mon create $hostname</span><br><span class="line">注意：执行ceph-deploy之前要进入之前安装时候配置的目录。/my-cluster</span><br></pre></td></tr></table></figure><h4 id="2、删除Monitor"><a href="#2、删除Monitor" class="headerlink" title="2、删除Monitor"></a>2、删除Monitor</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mon destroy $hostname</span><br><span class="line">注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个。</span><br></pre></td></tr></table></figure><h4 id="nearfull-osd-s-or-pool-s-nearfull"><a href="#nearfull-osd-s-or-pool-s-nearfull" class="headerlink" title="nearfull osd(s) or pool(s) nearfull"></a>nearfull osd(s) or pool(s) nearfull</h4><p>此时说明部分osd的存储已经超过阈值，mon会监控ceph集群中OSD空间使用情况。如果要消除WARN,可以修改这两个参数，提高阈值，但是通过实践发现并不能解决问题，可以通过观察osd的数据分布情况来分析原因。</p><h3 id="配置文件设置阈值"><a href="#配置文件设置阈值" class="headerlink" title="配置文件设置阈值"></a>配置文件设置阈值</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &quot;mon_osd_full_ratio&quot;: &quot;0.95&quot;,</span><br><span class="line">  &quot;mon_osd_nearfull_ratio&quot;: &quot;0.85&quot;</span><br><span class="line">，</span><br></pre></td></tr></table></figure><p>自动处理<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd reweight-by-utilization</span><br><span class="line">ceph osd reweight-by-pg 105 cephfs_data(pool_name)</span><br></pre></td></tr></table></figure></p><p>手动处理：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd reweight osd.2 0.8</span><br></pre></td></tr></table></figure></p><p>全局处理<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph mgr module ls</span><br><span class="line">ceph mgr module enable balancer</span><br><span class="line">ceph balancer on</span><br><span class="line">ceph balancer mode crush-compat</span><br><span class="line">ceph config-key set &quot;mgr/balancer/max_misplaced&quot;: &quot;0.01&quot;</span><br></pre></td></tr></table></figure></p><h4 id="PG-故障状态"><a href="#PG-故障状态" class="headerlink" title="PG 故障状态"></a>PG 故障状态</h4><p>PG状态概述<br>一个PG在它的生命周期的不同时刻可能会处于以下几种状态中:</p><p>Creating(创建中)<br>在创建POOL时,需要指定PG的数量,此时PG的状态便处于creating,意思是Ceph正在创建PG。</p><p>Peering(互联中)<br>peering的作用主要是在PG及其副本所在的OSD之间建立互联,并使得OSD之间就这些PG中的object及其元数据达成一致。</p><p>Active(活跃的)<br>处于该状态意味着数据已经完好的保存到了主PG及副本PG中,并且Ceph已经完成了peering工作。</p><p>Clean(整洁的)<br>当某个PG处于clean状态时,则说明对应的主OSD及副本OSD已经成功互联,并且没有偏离的PG。也意味着Ceph已经将该PG中的对象按照规定的副本数进行了复制操作。</p><p>Degraded(降级的)<br>当某个PG的副本数未达到规定个数时,该PG便处于degraded状态,例如:</p><p>在客户端向主OSD写入object的过程,object的副本是由主OSD负责向副本OSD写入的,直到副本OSD在创建object副本完成,并向主OSD发出完成信息前,该PG的状态都会一直处于degraded状态。又或者是某个OSD的状态变成了down,那么该OSD上的所有PG都会被标记为degraded。<br>当Ceph因为某些原因无法找到某个PG内的一个或多个object时,该PG也会被标记为degraded状态。此时客户端不能读写找不到的对象,但是仍然能访问位于该PG内的其他object。</p><p>Recovering(恢复中)<br>当某个OSD因为某些原因down了,该OSD内PG的object会落后于它所对应的PG副本。而在该OSD重新up之后,该OSD中的内容必须更新到当前状态,处于此过程中的PG状态便是recovering。</p><p>Backfilling(回填)<br>当有新的OSD加入集群时,CRUSH会把现有集群内的部分PG分配给它。这些被重新分配到新OSD的PG状态便处于backfilling。</p><p>Remapped(重映射)<br>当负责维护某个PG的acting set变更时,PG需要从原来的acting set迁移至新的acting set。这个过程需要一段时间,所以在此期间,相关PG的状态便会标记为remapped。</p><p>Stale(陈旧的)<br>默认情况下,OSD守护进程每半秒钟便会向Monitor报告其PG等相关状态,如果某个PG的主OSD所在acting set没能向Monitor发送报告,或者其他的Monitor已经报告该OSD为down时,该PG便会被标记为stale。</p><h3 id="OSD状态-1"><a href="#OSD状态-1" class="headerlink" title="OSD状态"></a>OSD状态</h3><p>单个OSD有两组状态需要关注,其中一组使用in/out标记该OSD是否在集群内,另一组使用up/down标记该OSD是否处于运行中状态。两组状态之间并不互斥,换句话说,当一个OSD处于“in”状态时,它仍然可以处于up或down的状态。</p><p>OSD状态为in且up<br>这是一个OSD正常的状态,说明该OSD处于集群内,并且运行正常。</p><p>OSD状态为in且down<br>此时该OSD尚处于集群中,但是守护进程状态已经不正常,默认在300秒后会被踢出集群,状态进而变为out且down,之后处于该OSD上的PG会迁移至其它OSD。</p><p>OSD状态为out且up<br>这种状态一般会出现在新增OSD时,意味着该OSD守护进程正常,但是尚未加入集群。</p><p>OSD状态为out且down<br>在该状态下的OSD不在集群内,并且守护进程运行不正常,CRUSH不会再分配PG到该OSD上。</p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>六、K8S 使用Ceph存储</title>
    <link href="/2019/12/28/%E5%85%AD%E3%80%81K8S%20%E4%BD%BF%E7%94%A8Ceph%E5%AD%98%E5%82%A8/"/>
    <url>/2019/12/28/%E5%85%AD%E3%80%81K8S%20%E4%BD%BF%E7%94%A8Ceph%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="一、PV、PVC概述"><a href="#一、PV、PVC概述" class="headerlink" title="一、PV、PVC概述"></a>一、PV、PVC概述</h3><p>管理存储是管理计算的一个明显问题。PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。于是引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim</p><blockquote><p>PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象包含存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 </p></blockquote><blockquote><p>PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。</p></blockquote><blockquote><p>虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 管理员需要能够提供多种不同于PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。</p></blockquote><blockquote><p>StorageClass为集群提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”</p></blockquote><h3 id="二、POD动态供给"><a href="#二、POD动态供给" class="headerlink" title="二、POD动态供给"></a>二、POD动态供给</h3><blockquote><p>动态供给主要是能够自动帮你创建pv，需要多大的空间就创建多大的pv。k8s帮助创建pv，创建pvc就直接api调用存储类来寻找pv。</p></blockquote><blockquote><p>如果是存储静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态。而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后帮你去连接，再帮你去自动创建pv。</p></blockquote><h3 id="三、POD使用RBD做为持久数据卷"><a href="#三、POD使用RBD做为持久数据卷" class="headerlink" title="三、POD使用RBD做为持久数据卷"></a>三、POD使用RBD做为持久数据卷</h3><h4 id="3-1、安装与配置"><a href="#3-1、安装与配置" class="headerlink" title="3.1、安装与配置"></a>3.1、安装与配置</h4><p>RBD支持ReadWriteOnce，ReadOnlyMany两种模式</p><p>3.1.1、配置rbd-provisioner<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果使用kubeadm部署的集群需要这些额外的步骤 # 由于使用动态存储时 controller-manager 需要使用 rbd 命令创建 image # 所以 controller-manager 需要使用 rbd 命令 # 由于官方controller-manager镜像里没有rbd命令 # 如果没使用如下方式会报错无法成功创建pvc # 相关 issue https://github.com/kubernetes/kubernetes/issues/38923</span><br><span class="line"></span><br><span class="line">cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    resourceNames: [&quot;kube-dns&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;, &quot;get&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: rbd-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: rbd-provisioner</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: rbd-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: rbd-provisioner</span><br><span class="line">        image: &quot;quay.io/external_storage/rbd-provisioner:v2.0.0-k8s1.11&quot;</span><br><span class="line">        env:</span><br><span class="line">        - name: PROVISIONER_NAME</span><br><span class="line">          value: ceph.com/rbd</span><br><span class="line">      serviceAccount: rbd-provisioner</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># kubectl apply -f external-storage-rbd-provisioner.yaml</span><br><span class="line"></span><br><span class="line"># 查看状态 等待running之后 再进行后续的操作</span><br><span class="line"># kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure></p><p>3.1.2、配置storageclass<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common。</span><br><span class="line">将ceph的ceph.client.admin.keyring和ceph.conf文件拷贝到master的/etc/ceph目录下</span><br><span class="line">yum -y install ceph-common</span><br><span class="line"></span><br><span class="line">如果K8S中是没有ceph客户端和配置文件，需要从ceph集群中copy下：</span><br><span class="line">scp /etc/ceph/ceph.c* root@192.168.171.11:/etc/ceph/</span><br><span class="line">scp /etc/ceph/ceph.c* root@192.168.171.12:/etc/ceph/</span><br><span class="line">scp /etc/ceph/ceph.c* root@192.168.171.13:/etc/ceph/</span><br><span class="line"></span><br><span class="line">这样就可以在K8S集群中查看集群的状态了！</span><br><span class="line"></span><br><span class="line">2、创建 osd pool 在ceph的mon或者admin节点</span><br><span class="line">ceph osd pool create kube 128 128 </span><br><span class="line">ceph osd pool ls</span><br><span class="line"></span><br><span class="line">3、创建k8s访问ceph的用户 在ceph的mon或者admin节点</span><br><span class="line">ceph auth get-or-create client.kube mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=kube&apos; -o ceph.client.kube.keyring</span><br><span class="line"></span><br><span class="line">4、查看key 在ceph的mon或者admin节点</span><br><span class="line">ceph auth get-key client.admin</span><br><span class="line">ceph auth get-key client.kube</span><br><span class="line"></span><br><span class="line">5、# 创建 admin secret # CEPH_ADMIN_SECRET 替换为 client.admin 获取到的key</span><br><span class="line">kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \</span><br><span class="line">--from-literal=key=AQCeEwpeo+I8HRAAnBphr8lyGc6+JBT7jU7rgA== \</span><br><span class="line">--namespace=kube-system</span><br><span class="line"></span><br><span class="line">6、# 在 default 命名空间创建pvc用于访问ceph的 secret # CEPH_KUBE_SECRET 替换为 client.kube 获取到的key</span><br><span class="line">kubectl create secret generic ceph-user-secret --type=&quot;kubernetes.io/rbd&quot; \</span><br><span class="line">--from-literal=key=AQC3OhNeYrGQLRAA8Xd/e1NUto/fXnGEk6hVMg== \</span><br><span class="line">--namespace=default</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看 secret</span><br><span class="line">kubectl get secret ceph-user-secret -o yaml</span><br><span class="line">kubectl get secret ceph-secret -n kube-system -o yaml</span><br></pre></td></tr></table></figure></p><p>3.1.3、配置StorageClass<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;storageclass-ceph-rdb.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: dynamic-ceph-rdb</span><br><span class="line">provisioner: ceph.com/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 192.168.171.135:6789,192.168.171.136:6789,192.168.171.137:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-secret</span><br><span class="line">  adminSecretNamespace: kube-system</span><br><span class="line">  pool: kube</span><br><span class="line">  userId: kube</span><br><span class="line">  userSecretName: ceph-user-secret</span><br><span class="line">  fsType: ext4</span><br><span class="line">  imageFormat: &quot;2&quot;</span><br><span class="line">  imageFeatures: &quot;layering&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><p>3.1.4、创建yaml<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f storageclass-ceph-rdb.yaml</span><br></pre></td></tr></table></figure></p><p>3.1.5、查看sc<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get sc</span><br></pre></td></tr></table></figure></p><h3 id="四、测试使用"><a href="#四、测试使用" class="headerlink" title="四、测试使用"></a>四、测试使用</h3><p>1、创建pvc测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-rdb-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:     </span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: dynamic-ceph-rdb</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f ceph-rdb-pvc-test.yaml</span><br></pre></td></tr></table></figure></p><p>2、查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get pvc</span><br><span class="line">NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE</span><br><span class="line">ceph-rdb-claim   Bound    pvc-e5f13194-67db-4d98-b69c-5a4272c2498d   2Gi        RWO            dynamic-ceph-rdb   7m10s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get pv</span><br><span class="line"></span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                      STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-e5f13194-67db-4d98-b69c-5a4272c2498d   2Gi        RWO            Delete           Bound         default/ceph-rdb-claim                     dynamic-ceph-rdb       48s</span><br></pre></td></tr></table></figure></p><p>3、创建 nginx pod 挂载测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;nginx-pod.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod1</span><br><span class="line">  labels:</span><br><span class="line">    name: nginx-pod1</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx-pod1</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    ports:</span><br><span class="line">    - name: web</span><br><span class="line">      containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: ceph-rdb</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  volumes:</span><br><span class="line">  - name: ceph-rdb</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: ceph-rdb-claim</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f nginx-pod.yaml</span><br></pre></td></tr></table></figure></p><p>4、查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure></p><p>5、修改文件内容<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl exec -ti nginx-pod1 -- /bin/sh -c &apos;echo Hello World from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html&apos; # 访问测试</span><br></pre></td></tr></table></figure></p><p>6、访问测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk &apos;&#123;print $6&#125;&apos;)</span><br><span class="line"></span><br><span class="line">curl http://$POD_ID #测试</span><br></pre></td></tr></table></figure></p><p>7、清理<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete -f nginx-pod.yaml</span><br><span class="line">kubectl delete -f ceph-rdb-pvc-test.yaml</span><br></pre></td></tr></table></figure></p><hr><h3 id="五、POD使用CephFS做为持久数据卷"><a href="#五、POD使用CephFS做为持久数据卷" class="headerlink" title="五、POD使用CephFS做为持久数据卷"></a>五、POD使用CephFS做为持久数据卷</h3><p>CephFS方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany</p><h4 id="5-1、Ceph端创建CephFS-pool"><a href="#5-1、Ceph端创建CephFS-pool" class="headerlink" title="5.1、Ceph端创建CephFS pool"></a>5.1、Ceph端创建CephFS pool</h4><p>1、如下操作在ceph的mon或者admin节点<br>CephFS需要使用两个Pool来分别存储数据和元数据<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph osd pool create fs_data 128</span><br><span class="line">ceph osd pool create fs_metadata 128</span><br><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure></p><p>2、创建一个CephFS<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph fs new cephfs fs_metadata fs_data</span><br></pre></td></tr></table></figure></p><p>3、查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph fs ls</span><br></pre></td></tr></table></figure></p><h4 id="5-2、部署-cephfs-provisioner"><a href="#5-2、部署-cephfs-provisioner" class="headerlink" title="5.2、部署 cephfs-provisioner"></a>5.2、部署 cephfs-provisioner</h4><p>1、使用社区提供的cephfs-provisioner<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">   name: cephfs</span><br><span class="line">   labels:</span><br><span class="line">     name: cephfs</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;get&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    resourceNames: [&quot;kube-dns&quot;,&quot;coredns&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;, &quot;get&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;policy&quot;]</span><br><span class="line">    resourceNames: [&quot;cephfs-provisioner&quot;]</span><br><span class="line">    resources: [&quot;podsecuritypolicies&quot;]</span><br><span class="line">    verbs: [&quot;use&quot;]</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: cephfs-provisioner</span><br><span class="line">    namespace: cephfs</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: cephfs-provisioner</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: cephfs-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: cephfs-provisioner</span><br><span class="line">        image: &quot;quay.io/external_storage/cephfs-provisioner:latest&quot;</span><br><span class="line">        env:</span><br><span class="line">        - name: PROVISIONER_NAME</span><br><span class="line">          value: ceph.com/cephfs</span><br><span class="line">        command:</span><br><span class="line">        - &quot;/usr/local/bin/cephfs-provisioner&quot;</span><br><span class="line">        args:</span><br><span class="line">        - &quot;-id=cephfs-provisioner-1&quot;</span><br><span class="line">        - &quot;-disable-ceph-namespace-isolation=true&quot;</span><br><span class="line">      serviceAccount: cephfs-provisioner</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f external-storage-cephfs-provisioner.yaml</span><br></pre></td></tr></table></figure></p><p>2、查看状态 等待running之后 再进行后续的操作<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n cephfs</span><br></pre></td></tr></table></figure></p><h4 id="5-3、配置-storageclass"><a href="#5-3、配置-storageclass" class="headerlink" title="5.3、配置 storageclass"></a>5.3、配置 storageclass</h4><p>1、查看key 在ceph的mon或者admin节点<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph auth get-key client.admin</span><br></pre></td></tr></table></figure></p><p>2、# 创建 admin secret # CEPH_ADMIN_SECRET 替换为 client.admin 获取到的key # 如果在测试 ceph rbd 方式已经添加 可以略过此步骤<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \</span><br><span class="line">--from-literal=key=AQCeEwpeo+I8HRAAnBphr8lyGc6+JBT7jU7rgA== \</span><br><span class="line">--namespace=kube-system</span><br></pre></td></tr></table></figure></p><p>3、查看 secret<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get secret ceph-secret -n kube-system -o yaml</span><br></pre></td></tr></table></figure></p><p>4、配置 StorageClass<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;storageclass-cephfs.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: dynamic-cephfs</span><br><span class="line">provisioner: ceph.com/cephfs</span><br><span class="line">parameters:</span><br><span class="line">    monitors: 192.168.171.135:6789,192.168.171.136:6789,192.168.171.137:6789</span><br><span class="line">    adminId: admin</span><br><span class="line">    adminSecretName: ceph-secret</span><br><span class="line">    adminSecretNamespace: &quot;kube-system&quot;</span><br><span class="line">    claimRoot: /volumes/kubernetes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><p>5、创建<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f storageclass-cephfs.yaml</span><br></pre></td></tr></table></figure></p><p>6、查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get sc</span><br></pre></td></tr></table></figure></p><h4 id="5-4、测试使用"><a href="#5-4、测试使用" class="headerlink" title="5.4、测试使用"></a>5.4、测试使用</h4><p>1、创建pvc测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:     </span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: dynamic-cephfs</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f cephfs-pvc-test.yaml</span><br></pre></td></tr></table></figure></p><p>2、查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ceph-all]# kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     AGE</span><br><span class="line">cephfs-claim   Bound    pvc-50ebdaab-c6ad-47ad-86cb-149327481a67   2Gi        RWO            dynamic-cephfs   4s</span><br><span class="line">[root@k8s-master1 ceph-all]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                    STORAGECLASS       REASON   AGE</span><br><span class="line">pvc-50ebdaab-c6ad-47ad-86cb-149327481a67   2Gi        RWO            Delete           Bound      default/cephfs-claim     dynamic-cephfs              6s</span><br></pre></td></tr></table></figure></p><p>3、创建 nginx pod 挂载测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;nginx-pod.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod2</span><br><span class="line">  labels:</span><br><span class="line">    name: nginx-pod2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx-pod2</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    ports:</span><br><span class="line">    - name: web</span><br><span class="line">      containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: cephfs</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cephfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: cephfs-claim</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f nginx-pod.yaml</span><br></pre></td></tr></table></figure></p><p> 4、查看<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure></p><p> 5、修改文件内容<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl exec -ti nginx-pod2 -- /bin/sh -c &apos;echo Hello World from CephFS!!! &gt; /usr/share/nginx/html/index.html&apos; # 访问测试</span><br></pre></td></tr></table></figure></p><p>6、访问pod测试<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POD_ID=$(kubectl get pods -o wide | grep nginx-pod2 | awk &apos;&#123;print $6&#125;&apos;)</span><br><span class="line">curl http://$POD_ID</span><br></pre></td></tr></table></figure></p><p>7、清理<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete -f nginx-pod.yaml</span><br><span class="line">kubectl delete -f cephfs-pvc-test.yaml</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五、Promethus+Grafana监控Ceph（2）</title>
    <link href="/2019/12/27/%E4%BA%94%E3%80%81Promethus+Grafana%E7%9B%91%E6%8E%A7Ceph%EF%BC%882%EF%BC%89/"/>
    <url>/2019/12/27/%E4%BA%94%E3%80%81Promethus+Grafana%E7%9B%91%E6%8E%A7Ceph%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="一、安装grafana"><a href="#一、安装grafana" class="headerlink" title="一、安装grafana"></a>一、安装grafana</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、配置yum源文件</span><br><span class="line"># vim /etc/yum.repos.d/grafana.repo</span><br><span class="line">[grafana]</span><br><span class="line">name=grafana</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/grafana/yum/rpm</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">2.通过yum命令安装grafana</span><br><span class="line"># yum -y install grafana</span><br><span class="line"></span><br><span class="line">3.启动grafana并设为开机自启</span><br><span class="line"># systemctl start grafana-server.service </span><br><span class="line"># systemctl enable grafana-server.service</span><br></pre></td></tr></table></figure><h3 id="二、安装promethus"><a href="#二、安装promethus" class="headerlink" title="二、安装promethus"></a>二、安装promethus</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、下载安装包，下载地址</span><br><span class="line">https://prometheus.io/download/</span><br><span class="line">2、解压压缩包</span><br><span class="line"># tar fvxz prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line">3、将解压后的目录改名</span><br><span class="line"># mv prometheus-2.13.1.linux-amd64 /opt/prometheus</span><br><span class="line">4、查看promethus版本</span><br><span class="line"># ./prometheus --version</span><br><span class="line">5、配置系统服务启动</span><br><span class="line"># vim /etc/systemd/system/prometheus.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Prometheus Monitoring System</span><br><span class="line">Documentation=Prometheus Monitoring System</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/opt/prometheus/prometheus \</span><br><span class="line">  --config.file /opt/prometheus/prometheus.yml \</span><br><span class="line">  --web.listen-address=:9090</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">6、加载系统服务</span><br><span class="line"># systemctl daemon-reload</span><br><span class="line">7、启动服务和添加开机自启动</span><br><span class="line"># systemctl start prometheus</span><br><span class="line"># systemctl enable prometheus</span><br></pre></td></tr></table></figure><h3 id="三、ceph-mgr-prometheus插件配置（回到cephnode01设置）"><a href="#三、ceph-mgr-prometheus插件配置（回到cephnode01设置）" class="headerlink" title="三、ceph mgr prometheus插件配置（回到cephnode01设置）"></a>三、ceph mgr prometheus插件配置（回到cephnode01设置）</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph mgr module enable prometheus     ##开启后就会吐ceph相关数据，然后prometheus去收集，grafana展示；</span><br><span class="line"># ceph mgr module ls | more 查看是否开启模块</span><br><span class="line"># netstat -nltp | grep mgr 检查端口</span><br><span class="line"># curl 127.0.0.1:9283/metrics  测试返回值</span><br></pre></td></tr></table></figure><h3 id="四、配置promethus"><a href="#四、配置promethus" class="headerlink" title="四、配置promethus"></a>四、配置promethus</h3><p>1、在 scrape_configs: 配置项下添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim prometheus.yml</span><br><span class="line">- job_name: &apos;ceph_cluster&apos;</span><br><span class="line">    honor_labels: true</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&apos;192.168.171.135:9283&apos;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: ceph</span><br></pre></td></tr></table></figure></p><p>2、重启promethus服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># systemctl restart prometheus</span><br></pre></td></tr></table></figure></p><p>3、检查prometheus服务器中是否添加成功<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 浏览器-》 http://x.x.x.x:9090 -》status -》Targets</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20200104/6TEBEnArzeRe.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="五、配置grafana"><a href="#五、配置grafana" class="headerlink" title="五、配置grafana"></a>五、配置grafana</h3><p>1、浏览器登录 grafana 管理界面<br>2、添加data sources，点击configuration–》data sources<br>3、添加dashboard，点击HOME–》find dashboard on grafana.com<br>4、搜索ceph的dashboard (编号:2842)<br>5、点击HOME–》Import dashboard, 选择合适的dashboard，记录编号</p><p><img src="http://myimage.okay686.cn/okay686cn/20200104/i7AdAr9kAWKE.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五、Ceph Dashboard（1）</title>
    <link href="/2019/12/26/%E4%BA%94%E3%80%81Ceph%20Dashboard%EF%BC%881%EF%BC%89/"/>
    <url>/2019/12/26/%E4%BA%94%E3%80%81Ceph%20Dashboard%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="一、Ceph-Dashboard介绍"><a href="#一、Ceph-Dashboard介绍" class="headerlink" title="一、Ceph Dashboard介绍"></a>一、Ceph Dashboard介绍</h3><p>Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。<br>mimic版  (nautilus版)  dashboard 安装。如果是  (nautilus版) 需要安装 ceph-mgr-dashboard </p><h3 id="二、配置Ceph-Dashboard"><a href="#二、配置Ceph-Dashboard" class="headerlink" title="二、配置Ceph Dashboard"></a>二、配置Ceph Dashboard</h3><p>配置yum源：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat &gt; /etc/yum.repos.d/cephbak.repo &lt;&lt; EOF</span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"> </span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"> </span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、在每个mgr节点安装</span><br><span class="line"># yum install ceph-mgr-dashboard -y</span><br><span class="line">2、开启mgr功能</span><br><span class="line"># ceph mgr module enable dashboard</span><br><span class="line">3、生成并安装自签名的证书</span><br><span class="line"># ceph dashboard create-self-signed-cert  </span><br><span class="line">4、创建一个dashboard登录用户名密码</span><br><span class="line"># ceph dashboard ac-user-create admin 1q2w3e4r administrator    ##创建用户admin 密码1q2w3e4r 权限是administrator</span><br><span class="line">5、查看服务访问方式</span><br><span class="line"># ceph mgr services</span><br><span class="line">6、查看已安装模块</span><br><span class="line"># ceph mgr module ls | more</span><br><span class="line">&#123;</span><br><span class="line">    &quot;enabled_modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/20200102/x9598uoU4TEt.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="三、修改默认配置命令"><a href="#三、修改默认配置命令" class="headerlink" title="三、修改默认配置命令"></a>三、修改默认配置命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">指定集群dashboard的访问端口</span><br><span class="line"># ceph config-key set mgr/dashboard/server_port 7000</span><br><span class="line">指定集群 dashboard的访问IP</span><br><span class="line"># ceph config-key set mgr/dashboard/server_addr $IP</span><br></pre></td></tr></table></figure><h3 id="四、开启Object-Gateway管理功能"><a href="#四、开启Object-Gateway管理功能" class="headerlink" title="四、开启Object Gateway管理功能"></a>四、开启Object Gateway管理功能</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、创建rgw用户</span><br><span class="line"># radosgw-admin user create --uid=user01 --display-name=user01</span><br><span class="line">1.1、查看rgw用户</span><br><span class="line"># radosgw-admin user info --uid=user01 --display-name=user01</span><br><span class="line">2、提供Dashboard证书</span><br><span class="line"># ceph dashboard set-rgw-api-access-key $access_key</span><br><span class="line"># ceph dashboard set-rgw-api-secret-key $secret_key</span><br><span class="line">3、配置rgw主机名和端口</span><br><span class="line"># ceph dashboard set-rgw-api-host 10.151.30.125</span><br><span class="line">4、刷新web页面</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>三、Ceph RBD介绍与使用</title>
    <link href="/2019/12/25/%E4%B8%89%E3%80%81Ceph%20RBD%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2019/12/25/%E4%B8%89%E3%80%81Ceph%20RBD%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="一、RBD介绍"><a href="#一、RBD介绍" class="headerlink" title="一、RBD介绍"></a>一、RBD介绍</h3><blockquote><p>RBD即RADOS Block Device的简称，<strong>RBD块存储是最稳定且最常用的存储类型</strong>。RBD块设备类似磁盘可以被挂载。<br>RBD块设备具有<strong>快照</strong>、<strong>多副本</strong>、<strong>克隆</strong>和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。如下是对Ceph RBD的理解。</p><ul><li>RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；</li><li>resizable：这个块可大可小；</li><li>data striped：这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；</li><li>thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间  ；</li></ul></blockquote><blockquote><p>块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。</p><blockquote><p>ceph可以通过内核模块和librbd库提供块设备支持。客户端可以通过内核模块挂在rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。</p></blockquote></blockquote><p>使用场景：</p><ul><li>云平台（OpenStack做为云的存储后端提供镜像存储）</li><li>K8s容器</li><li>map成块设备直接使用</li><li>ISCIS，安装Ceph客户端</li></ul><h3 id="二、RBD常用命令"><a href="#二、RBD常用命令" class="headerlink" title="二、RBD常用命令"></a>二、RBD常用命令</h3><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>rbd create</td><td>创建块设备映像</td></tr><tr><td>rbd ls</td><td>列出 rbd 存储池中的块设备</td></tr><tr><td>rbd info</td><td>查看块设备信息</td></tr><tr><td>rbd diff</td><td>可以统计 rbd 使用量</td></tr><tr><td>rbd map</td><td>映射块设备</td></tr><tr><td>rbd showmapped</td><td>查看已映射块设备</td></tr><tr><td>rbd remove</td><td>删除块设备</td></tr><tr><td>rbd resize</td><td>更改块设备的大小</td></tr></tbody></table><h3 id="三、RBD配置操作"><a href="#三、RBD配置操作" class="headerlink" title="三、RBD配置操作"></a>三、RBD配置操作</h3><h4 id="3-1、RBD挂载到本地操作系统"><a href="#3-1、RBD挂载到本地操作系统" class="headerlink" title="3.1、RBD挂载到本地操作系统"></a>3.1、RBD挂载到本地操作系统</h4><p>1、创建rbd使用的pool<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph osd pool create rbd  32 32</span><br><span class="line">pool &apos;rbd&apos; created</span><br><span class="line"></span><br><span class="line"># ceph osd pool ls detail   ##查看创建的详细信息</span><br><span class="line"></span><br><span class="line"># ceph osd pool application enable rbd rbd</span><br><span class="line">enabled application &apos;rbd&apos; on pool &apos;rbd&apos;</span><br></pre></td></tr></table></figure></p><p>2、创建一个块设备<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd create --size 10240 image01</span><br></pre></td></tr></table></figure></p><p>3、查看块设备<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd ls</span><br><span class="line">image01</span><br><span class="line"></span><br><span class="line"># rbd info image01</span><br><span class="line">rbd image &apos;image01&apos;:</span><br><span class="line">size 10GiB in 2560 objects</span><br><span class="line">order 22 (4MiB objects)</span><br><span class="line">block_name_prefix: rbd_data.10836b8b4567</span><br><span class="line">format: 2</span><br><span class="line">features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">flags:</span><br><span class="line">create_timestamp: Tue Dec 31 13:40:22 2019</span><br><span class="line"></span><br><span class="line"># rados -p rbd ls --all     ##查看底层存储格式</span><br><span class="line">rbd_object_map.10836b8b4567</span><br><span class="line">rbd_id.image01</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_info</span><br><span class="line">rbd_header.10836b8b4567</span><br></pre></td></tr></table></figure></p><p>4、将块设备映射到系统内核<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd map image01</span><br></pre></td></tr></table></figure></p><p>5、禁用当前系统内核不支持的feature<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd feature disable image01 exclusive-lock, object-map, fast-diff, deep-flatten</span><br></pre></td></tr></table></figure></p><p>6、再次映射<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd map image01</span><br><span class="line">/dev/rbd0</span><br><span class="line"></span><br><span class="line"># rbd info image01</span><br><span class="line">rbd image &apos;image01&apos;:</span><br><span class="line">size 10GiB in 2560 objects</span><br><span class="line">order 22 (4MiB objects)</span><br><span class="line">block_name_prefix: rbd_data.10836b8b4567</span><br><span class="line">format: 2</span><br><span class="line">features: layering</span><br><span class="line">flags:</span><br><span class="line">create_timestamp: Tue Dec 31 13:40:22 2019</span><br><span class="line"></span><br><span class="line"># fdisk -l</span><br><span class="line">...省略</span><br><span class="line">磁盘 /dev/rbd0：10.7 GB, 10737418240 字节，20971520 个扇区</span><br><span class="line">Units = 扇区 of 1 * 512 = 512 bytes</span><br><span class="line">扇区大小(逻辑/物理)：512 字节 / 512 字节</span><br><span class="line">I/O 大小(最小/最佳)：4194304 字节 / 4194304 字节</span><br></pre></td></tr></table></figure></p><p>7、格式化块设备镜像<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkfs.xfs /dev/rbd0</span><br></pre></td></tr></table></figure></p><p>8、mount到本地<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mount /dev/rbd0 /mnt</span><br><span class="line"></span><br><span class="line"># df -h</span><br><span class="line">文件系统        容量  已用  可用 已用% 挂载点</span><br><span class="line">...省略...</span><br><span class="line">/dev/rbd0        10G   33M   10G    1% /mnt</span><br><span class="line"></span><br><span class="line"># umount /mnt</span><br></pre></td></tr></table></figure></p><p>9、取消块设备和内核映射<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd unmap image01</span><br></pre></td></tr></table></figure></p><p>10、删除RBD块设备<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd rm image01</span><br><span class="line">Removing image: 100% complete...done.</span><br></pre></td></tr></table></figure></p><p>11、扩容<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd --image image02 resize --size=15240</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"># rbd info image02</span><br><span class="line">rbd image &apos;image02&apos;:</span><br><span class="line">size 14.9GiB in 3810 objects</span><br><span class="line">order 22 (4MiB objects)</span><br><span class="line">block_name_prefix: rbd_data.10c26b8b4567</span><br><span class="line">format: 2</span><br><span class="line">features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">flags:</span><br><span class="line">create_timestamp: Tue Dec 31 14:43:23 2019</span><br></pre></td></tr></table></figure></p><p>如上就是本地如何挂载rbd块设备的步骤；</p><hr><h3 id="四、快照配置"><a href="#四、快照配置" class="headerlink" title="四、快照配置"></a>四、快照配置</h3><p>1、创建快照（占用存储比较大）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd create --size 10240 image02</span><br><span class="line"></span><br><span class="line"># rbd info image02</span><br><span class="line"></span><br><span class="line"># rbd snap create image02@image02_snap01          ## 本地快照名@快照名字</span><br></pre></td></tr></table></figure></p><p>2、列出创建的快照<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap list image02</span><br><span class="line">或</span><br><span class="line"># rbd ls -l</span><br><span class="line">NAME                    SIZE PARENT FMT PROT LOCK</span><br><span class="line">image02                10GiB          2</span><br><span class="line">image02@image02_snap01 10GiB          2</span><br></pre></td></tr></table></figure></p><p>3、查看快照详细信息<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd info image02@image02_snap01</span><br></pre></td></tr></table></figure></p><p>4、克隆快照（快照必须处于被保护状态&lt;没有写入&gt;才能被克隆）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap protect image02@image02_snap01</span><br><span class="line"></span><br><span class="line"># ceph osd pool create kube 16 16</span><br><span class="line"></span><br><span class="line"># rbd clone rbd/image02@image02_snap01 kube/image02_clone01     ##将刚刚克隆的image02镜像克隆到 kube 资源池；</span><br><span class="line"></span><br><span class="line">## rbd ls -p kube</span><br></pre></td></tr></table></figure></p><p>5、查看快照的children<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd children image02</span><br></pre></td></tr></table></figure></p><p>6、去掉快照的parent<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd flatten kube/image02_clone01</span><br></pre></td></tr></table></figure></p><p>7、恢复快照<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap rollback image02@image02_snap01</span><br></pre></td></tr></table></figure></p><p>8、删除快照<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap unprotect image02@image02_snap01</span><br><span class="line"></span><br><span class="line"># rbd snap remove image02@image02_snap01</span><br><span class="line"></span><br><span class="line"># rbd snap ls image02</span><br></pre></td></tr></table></figure></p><h3 id="五、导出导入RBD镜像"><a href="#五、导出导入RBD镜像" class="headerlink" title="五、导出导入RBD镜像"></a>五、导出导入RBD镜像</h3><p>1、导出RBD镜像<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd export image02 /tmp/image02</span><br><span class="line"></span><br><span class="line"># ll /tmp/image02</span><br><span class="line">-rw-r--r-- 1 root root 10737418240 12月 31 14:41 /tmp/image02</span><br></pre></td></tr></table></figure></p><p>2、导入RBD镜像<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd remove image02    ##删除本地rbd设备</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"># rbd ls</span><br><span class="line"></span><br><span class="line"># rbd import /tmp/image02 rbd/image02 --image-format 2  ##导入</span><br><span class="line">Importing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"># rbd info image02</span><br><span class="line">rbd image &apos;image02&apos;:</span><br><span class="line">size 10GiB in 2560 objects</span><br><span class="line">order 22 (4MiB objects)</span><br><span class="line">block_name_prefix: rbd_data.10c26b8b4567</span><br><span class="line">format: 2</span><br><span class="line">features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">flags:</span><br><span class="line">create_timestamp: Tue Dec 31 14:43:23 2019</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二、部署Ceph集群</title>
    <link href="/2019/12/24/%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2Ceph%E9%9B%86%E7%BE%A4/"/>
    <url>/2019/12/24/%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2Ceph%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="一、Ceph版本选择"><a href="#一、Ceph版本选择" class="headerlink" title="一、Ceph版本选择"></a>一、Ceph版本选择</h3><h4 id="Ceph版本来源介绍"><a href="#Ceph版本来源介绍" class="headerlink" title="Ceph版本来源介绍"></a>Ceph版本来源介绍</h4><p>Ceph 社区最新版本是 14，而 Ceph 12 是市面用的最广的稳定版本。<br>第一个 Ceph 版本是 0.1 ，要回溯到 2008 年 1 月。多年来，版本号方案一直没变，直到 2015 年 4 月 0.94.1 （ Hammer 的第一个修正版）发布后，为了避免 0.99 （以及 0.100 或 1.00 ？），制定了新策略。</p><p>x.0.z - 开发版（给早期测试者和勇士们）</p><p>x.1.z - 候选版（用于测试集群、高手们）</p><p>x.2.z - 稳定、修正版（给用户们）</p><p>x 将从 9 算起，它代表 Infernalis （ I 是第九个字母），这样第九个发布周期的第一个开发版就是 9.0.0 ；后续的开发版依次是 9.0.1 、 9.0.2 等等。<br>| 版本名称 | 版本号 | 发布时间 |<br>| —— | —— | —— |<br>| Argonaut | 0.48版本(LTS) | 　　2012年6月3日 |<br>| Bobtail | 0.56版本(LTS) | 　2013年5月7日 |<br>| Cuttlefish | 0.61版本 | 　2013年1月1日 |<br>| Dumpling | 0.67版本(LTS) | 　2013年8月14日 |<br>| Emperor | 0.72版本 | 　　　 2013年11月9 |<br>| Firefly | 0.80版本(LTS) | 　2014年5月 |<br>| Giant | Giant | 　October 2014 - April 2015 |<br>| Hammer | Hammer | 　April 2015 - November 2016|<br>| Infernalis | Infernalis | 　November 2015 - June 2016 |<br>| Jewel | 10.2.9 | 　2016年4月 |<br>| Kraken | 11.2.1 | 　2017年10月 |<br>| Luminous |12.2.12  | 　2017年10月 |<br>| mimic | 13.2.7 | 　2018年5月 |<br>| nautilus | 14.2.5 | 　2019年2月 |</p><h6 id="Luminous新版本特性"><a href="#Luminous新版本特性" class="headerlink" title="Luminous新版本特性"></a>Luminous新版本特性</h6><ul><li>Bluestore<ul><li>ceph-osd的新后端存储BlueStore已经稳定，是新创建的OSD的默认设置。<br>BlueStore通过直接管理物理HDD或SSD而不使用诸如XFS的中间文件系统，来管理每个OSD存储的数据，这提供了更大的性能和功能。</li><li>BlueStore支持Ceph存储的所有的完整的数据和元数据校验。</li><li>BlueStore内嵌支持使用zlib，snappy或LZ4进行压缩。（Ceph还支持zstd进行RGW压缩，但由于性能原因，不为BlueStore推荐使用zstd）</li></ul></li><li>集群的总体可扩展性有所提高。我们已经成功测试了多达10,000个OSD的集群。</li><li>ceph-mgr<ul><li>ceph-mgr是一个新的后台进程，这是任何Ceph部署的必须部分。虽然当ceph-mgr停止时，IO可以继续，但是度量不会刷新，并且某些与度量相关的请求（例如，ceph df）可能会被阻止。我们建议您多部署ceph-mgr的几个实例来实现可靠性。</li><li>ceph-mgr守护进程daemon包括基于REST的API管理。注：API仍然是实验性质的，目前有一些限制，但未来会成为API管理的基础。</li><li>ceph-mgr还包括一个Prometheus插件。</li><li>ceph-mgr现在有一个Zabbix插件。使用zabbix_sender，它可以将集群故障事件发送到Zabbix Server主机。这样可以方便地监视Ceph群集的状态，并在发生故障时发送通知。</li></ul></li></ul><h3 id="二、安装前准备"><a href="#二、安装前准备" class="headerlink" title="二、安装前准备"></a>二、安装前准备</h3><ol><li>安装要求：</li></ol><ul><li>最少三台Centos7系统虚拟机用于部署Ceph集群。硬件配置：2C4G，另外每台机器最少挂载三块硬盘(每块盘5G)  </li></ul><table><thead><tr><th>主机名</th><th>IP</th></tr></thead><tbody><tr><td>cephnode01</td><td>192.168.171.135 </td></tr><tr><td>cephnode02</td><td>192.168.171.136 </td></tr><tr><td>cephnode03</td><td>192.168.171.137 </td></tr><tr><td>cephyumresource01</td><td>192.168.171.10（内网yum源服务器）</td></tr></tbody></table><ol start="2"><li>环境准备（在Ceph三台机器上操作）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（1）关闭防火墙：</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">（2）关闭selinux：</span><br><span class="line">sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">（3）关闭NetworkManager</span><br><span class="line">systemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager</span><br><span class="line">（4）添加主机名与IP对应关系：</span><br><span class="line">vim /etc/hosts</span><br><span class="line">192.168.171.135 cephnode01</span><br><span class="line">192.168.171.136 cephnode02</span><br><span class="line">192.168.171.137 cephnode03</span><br><span class="line">（5）设置主机名：</span><br><span class="line">hostnamectl set-hostname cephnode01</span><br><span class="line">hostnamectl set-hostname cephnode02</span><br><span class="line">hostnamectl set-hostname cephnode03</span><br><span class="line">（6）同步网络时间和修改时区</span><br><span class="line">systemctl restart chronyd.service &amp;&amp; systemctl enable chronyd.service</span><br><span class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">（7）设置文件描述符</span><br><span class="line">echo &quot;ulimit -SHn 102400&quot; &gt;&gt; /etc/rc.local</span><br><span class="line">cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF</span><br><span class="line">* soft nofile 65535</span><br><span class="line">* hard nofile 65535</span><br><span class="line">EOF</span><br><span class="line">（8）内核参数优化</span><br><span class="line">cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF</span><br><span class="line">kernel.pid_max = 4194303</span><br><span class="line">echo &quot;vm.swappiness = 0&quot; /etc/sysctl.conf </span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br><span class="line">（9）在cephnode01上配置免密登录到cephnode02、cephnode03</span><br><span class="line">ssh-copy-id root@cephnode02</span><br><span class="line">ssh-copy-id root@cephnode03</span><br><span class="line">(10)read_ahead,通过数据预读并且记载到随机访问内存方式提高磁盘读操作</span><br><span class="line">echo &quot;8192&quot; &gt; /sys/block/sda/queue/read_ahead_kb</span><br><span class="line">(11) I/O Scheduler，SSD要用noop，SATA/SAS使用deadline</span><br><span class="line">echo &quot;deadline&quot; &gt;/sys/block/sd[x]/queue/scheduler</span><br><span class="line">echo &quot;noop&quot; &gt;/sys/block/sd[x]/queue/scheduler</span><br></pre></td></tr></table></figure></li></ol><h3 id="三、安装内网yum源"><a href="#三、安装内网yum源" class="headerlink" title="三、安装内网yum源"></a>三、安装内网yum源</h3><p><u>仅在192.168.171.10操作</u></p><p>1、安装httpd、createrepo和epel源<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install httpd createrepo epel-release -y</span><br></pre></td></tr></table></figure></p><p>2、编辑yum源文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cephyumresource01 ~]# more /etc/yum.repos.d/ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">或者阿里云yum源：（推荐！！）</span><br><span class="line">[Ceph-SRPMS]</span><br><span class="line">name=Ceph SRPMS packagesno</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"> </span><br><span class="line">[Ceph-x86_64]</span><br><span class="line">name=Ceph x86_64 packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br></pre></td></tr></table></figure></p><p>3、下载Ceph安装包<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum --downloadonly --downloaddir=/var/www/html/ceph/rpm-nautilus/el7/x86_64/ install ceph ceph-radosgw</span><br></pre></td></tr></table></figure></p><p>4、下载Ceph依赖文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-14.2.4-0.el7.src.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-deploy-2.0.1-0.src.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-cloud-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-local-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-rook-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-ssh-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-release-1-1.el7.src.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-medic-1.0.4-16.g60cf7e9.el7.src.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/repomd.xml </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/repomd.xml</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/183278bb826f5b8853656a306258643384a1547c497dd8b601ed6af73907bb22-other.sqlite.bz2 </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/52bf459e39c76b2ea2cff2c5340ac1d7b5e17a105270f5f01b454d5a058adbd2-filelists.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/4f3141aec1132a9187ff5d1b4a017685e2f83a761880884d451a288fcedb154e-primary.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/0c554884aa5600b1311cd8f616aa40d036c1dfc0922e36bcce7fd84e297c5357-other.sqlite.bz2 </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/597468b64cddfc386937869f88c2930c8e5fda3dd54977c052bab068d7438fcb-primary.sqlite.bz2</span><br></pre></td></tr></table></figure></p><p>5、更新yum源<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">createrepo --update  /var/www/html/ceph/rpm-nautilus</span><br></pre></td></tr></table></figure></p><h3 id="四、安装Ceph集群"><a href="#四、安装Ceph集群" class="headerlink" title="四、安装Ceph集群"></a>四、安装Ceph集群</h3><p>1、编辑内网yum源,将yum源同步到其它节点并提前做好yum makecache</p><p>当然如果外网没有任何限制，也建议直接配置如上阿里云镜像源即可！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vim /etc/yum.repos.d/ceph.repo </span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/$basearch</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/srpms</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure></p><p>只在cephnode01上执行即可（如下标注：每个节点执行，需要在所有节点执行）<br>2、安装ceph-deploy(确认ceph-deploy版本是否为2.0.1)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum list | grep ceph</span><br><span class="line"># yum install -y ceph-deploy</span><br><span class="line"></span><br><span class="line"># ceph-deploy --version</span><br><span class="line">然后测试一下，发现报错：</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br><span class="line"></span><br><span class="line">原因是缺python-setuptools，安装它即可：</span><br><span class="line"># yum install -y python-setuptools</span><br><span class="line"></span><br><span class="line"># ceph-deploy --version</span><br><span class="line">2.0.1</span><br></pre></td></tr></table></figure></p><p>3、创建一个my-cluster目录，<strong>所有命令都需要在此目录下进行</strong>（文件位置和名字可以随意）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkdir /my-cluster</span><br><span class="line"># cd /my-cluster</span><br></pre></td></tr></table></figure></p><p>4、创建一个Ceph集群<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy new cephnode01 cephnode02 cephnode03</span><br><span class="line">...省略</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host cephnode03</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor cephnode03 at 192.168.171.137</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [&apos;cephnode01&apos;, &apos;cephnode02&apos;, &apos;cephnode03&apos;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [&apos;192.168.171.135&apos;, &apos;192.168.171.136&apos;, &apos;192.168.171.137&apos;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure></p><p>如果安装异常可以查看：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ls ceph-deploy-ceph.log</span><br></pre></td></tr></table></figure></p><p>5、安装Ceph软件（<strong>每个节点执行</strong>）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum -y install epel-release</span><br><span class="line"># yum install -y ceph</span><br><span class="line"># ceph -v</span><br><span class="line">ceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable)</span><br></pre></td></tr></table></figure></p><p>6、生成monitor检测集群所使用的的秘钥<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mon create-initial</span><br><span class="line"># ls</span><br><span class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log</span><br><span class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring</span><br></pre></td></tr></table></figure></p><p>7、<strong>安装Ceph CLI，方便执行一些管理命令</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy admin cephnode01 cephnode02 cephnode03</span><br></pre></td></tr></table></figure></p><p>8、<strong>配置mgr，用于管理集群</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mgr create cephnode01 cephnode02 cephnode03</span><br><span class="line"># more ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = b1f800c7-a4bc-4fc7-87e2-239291f2e4c7</span><br><span class="line">mon_initial_members = cephnode01, cephnode02, cephnode03</span><br><span class="line">mon_host = 192.168.171.135,192.168.171.136,192.168.171.137</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure></p><p>9、部署rgw（生产一般都是多台，然后nginx做负载均衡）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum install -y ceph-radosgw</span><br><span class="line"># ceph-deploy rgw create cephnode01</span><br></pre></td></tr></table></figure></p><p>10、部署MDS（CephFS）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mds create cephnode01 cephnode02 cephnode03</span><br></pre></td></tr></table></figure></p><p>11、添加osd<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda      8:0    0   30G  0 disk</span><br><span class="line">├─sda1   8:1    0    4G  0 part</span><br><span class="line">└─sda2   8:2    0   26G  0 part /</span><br><span class="line">sdb      8:16   0    8G  0 disk     ##磁盘仅仅为一块裸盘，没有做任何的初始化和处理；</span><br><span class="line">sr0     11:0    1  4.3G  0 rom</span><br><span class="line"></span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode01   ##会自动的格式化成ceph可以读取的格式；</span><br><span class="line">...省略</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephnode01 is now ready for osd use.</span><br><span class="line"></span><br><span class="line"># ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.00780 root default</span><br><span class="line">-3       0.00780     host cephnode01</span><br><span class="line"> 0   hdd 0.00780         osd.0           up  1.00000 1.00000</span><br><span class="line"></span><br><span class="line">##继续添加其它盘及其它node的盘</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode01   如有，修改sdX即可；</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode01</span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode03</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode03</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode03</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.02339 root default</span><br><span class="line">-3       0.00780     host cephnode01</span><br><span class="line"> 0   hdd 0.00780         osd.0           up  1.00000 1.00000</span><br><span class="line">-5       0.00780     host cephnode02</span><br><span class="line"> 1   hdd 0.00780         osd.1           up  1.00000 1.00000</span><br><span class="line">-7       0.00780     host cephnode03</span><br><span class="line"> 2   hdd 0.00780         osd.2           up  1.00000 1.00000</span><br></pre></td></tr></table></figure></p><p>查看集群状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     b1f800c7-a4bc-4fc7-87e2-239291f2e4c7</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03</span><br><span class="line">    mgr: cephnode01(active), standbys: cephnode02, cephnode03</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line">    rgw: 1 daemon active</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1.09KiB</span><br><span class="line">    usage:   3.01GiB used, 21.0GiB / 24.0GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph health detail</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd df   ##每块盘的使用量</span><br><span class="line">ID CLASS WEIGHT  REWEIGHT SIZE    USE     AVAIL   %USE  VAR  PGS</span><br><span class="line"> 0   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line"> 1   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line"> 2   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line">                    TOTAL 24.0GiB 3.01GiB 21.0GiB 12.55</span><br><span class="line">MIN/MAX VAR: 1.00/1.00  STDDEV: 0</span><br></pre></td></tr></table></figure></p><h3 id="五、ceph-conf"><a href="#五、ceph-conf" class="headerlink" title="五、ceph.conf"></a>五、ceph.conf</h3><p>1、该配置文件采用init文件语法，###和;为注释，ceph集群在启动的时候会按照顺序加载所有的conf配置文件。 配置文件分为以下几大块配置。</p><pre><code>global：全局配置。osd：osd专用配置，可以使用osd.N，来表示某一个OSD专用配置，N为osd的编号，如0、2、1等。mon：mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、ceph-monitor-1等。使用命令 ceph mon dump可以获取节点的名称。client：客户端专用配置。</code></pre><p>2、配置文件可以从多个地方进行顺序加载，如果冲突将使用最新加载的配置，其加载顺序为。</p><pre><code>$CEPH_CONF环境变量-c 指定的位置/etc/ceph/ceph.conf~/.ceph/ceph.conf./ceph.conf</code></pre><p>3、配置文件还可以使用一些元变量应用到配置文件，如。</p><pre><code>$cluster：当前集群名。$type：当前服务类型。$id：进程的标识符。$host：守护进程所在的主机名。$name：值为$type.$id。</code></pre><p>4、ceph.conf详细参数<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]#全局设置</span><br><span class="line">fsid = xxxxxxxxxxxxxxx                           #集群标识ID </span><br><span class="line">mon host = 10.0.1.1,10.0.1.2,10.0.1.3            #monitor IP 地址</span><br><span class="line">auth cluster required = cephx                    #集群认证</span><br><span class="line">auth service required = cephx                           #服务认证</span><br><span class="line">auth client required = cephx                            #客户端认证</span><br><span class="line">osd pool default size = 3                             #最小副本数 默认是3</span><br><span class="line">osd pool default min size = 1                           #PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数</span><br><span class="line">public network = 10.0.1.0/24                            #公共网络(monitorIP段) </span><br><span class="line">cluster network = 10.0.2.0/24                           #集群网络</span><br><span class="line">max open files = 131072                                 #默认0###如果设置了该选项，Ceph会设置系统的max open fds</span><br><span class="line">mon initial members = node1, node2, node3               #初始monitor (由创建monitor命令而定)</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[mon]</span><br><span class="line">mon data = /var/lib/ceph/mon/ceph-$id</span><br><span class="line">mon clock drift allowed = 1                             #默认值0.05###monitor间的clock drift</span><br><span class="line">mon osd min down reporters = 13                         #默认值1###向monitor报告down的最小OSD数</span><br><span class="line">mon osd down out interval = 600      #默认值300      #标记一个OSD状态为down和out之前ceph等待的秒数</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[osd]</span><br><span class="line">osd data = /var/lib/ceph/osd/ceph-$id</span><br><span class="line">osd mkfs type = xfs                                     #格式化系统类型</span><br><span class="line">osd max write size = 512 #默认值90                   #OSD一次可写入的最大值(MB)</span><br><span class="line">osd client message size cap = 2147483648 #默认值100    #客户端允许在内存中的最大数据(bytes)</span><br><span class="line">osd deep scrub stride = 131072 #默认值524288         #在Deep Scrub时候允许读取的字节数(bytes)</span><br><span class="line">osd op threads = 16 #默认值2                         #并发文件系统操作数</span><br><span class="line">osd disk threads = 4 #默认值1                        #OSD密集型操作例如恢复和Scrubbing时的线程</span><br><span class="line">osd map cache size = 1024 #默认值500                 #保留OSD Map的缓存(MB)</span><br><span class="line">osd map cache bl size = 128 #默认值50                #OSD进程在内存中的OSD Map缓存(MB)</span><br><span class="line">osd mount options xfs = &quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&quot; ###默认值rw,noatime,inode64  ###Ceph OSD xfs Mount选项</span><br><span class="line">osd recovery op priority = 2 #默认值10              #恢复操作优先级，取值1-63，值越高占用资源越高</span><br><span class="line">osd recovery max active = 10 #默认值15              #同一时间内活跃的恢复请求数 </span><br><span class="line">osd max backfills = 4  #默认值10                  #一个OSD允许的最大backfills数</span><br><span class="line">osd min pg log entries = 30000 #默认值3000           #修建PGLog是保留的最大PGLog数</span><br><span class="line">osd max pg log entries = 100000 #默认值10000         #修建PGLog是保留的最大PGLog数</span><br><span class="line">osd mon heartbeat interval = 40 #默认值30            #OSD ping一个monitor的时间间隔（默认30s）</span><br><span class="line">ms dispatch throttle bytes = 1048576000 #默认值 104857600 #等待派遣的最大消息数</span><br><span class="line">objecter inflight ops = 819200 #默认值1024           #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限</span><br><span class="line">osd op log threshold = 50 #默认值5                  #一次显示多少操作的log</span><br><span class="line">osd crush chooseleaf type = 0 #默认值为1              #CRUSH规则用到chooseleaf时的bucket的类型</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[client]</span><br><span class="line">rbd cache = true #默认值 true      #RBD缓存</span><br><span class="line">rbd cache size = 335544320 #默认值33554432           #RBD缓存大小(bytes)</span><br><span class="line">rbd cache max dirty = 134217728 #默认值25165824      #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</span><br><span class="line">rbd cache max dirty age = 30 #默认值1                #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</span><br><span class="line">rbd cache writethrough until flush = false #默认值true  #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写</span><br><span class="line">              #设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。</span><br><span class="line">rbd cache max dirty object = 2 #默认值0              #最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分</span><br><span class="line">      #每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能</span><br><span class="line">rbd cache target dirty = 235544320 #默认值16777216    #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一、Ceph介绍</title>
    <link href="/2019/12/23/%E4%B8%80%E3%80%81Ceph%E4%BB%8B%E7%BB%8D/"/>
    <url>/2019/12/23/%E4%B8%80%E3%80%81Ceph%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="为什么要用Ceph"><a href="#为什么要用Ceph" class="headerlink" title="为什么要用Ceph"></a>为什么要用Ceph</h3><blockquote><p>Ceph是当前非常流行的开源分布式存储系统，具有高扩展性、高性能、高可靠性等优点，同时提供块存储服务(rbd)、对象存储服务(rgw)以及文件系统存储服务(cephfs)，Ceph在存储的时候充分利用存储节点的计算能力，在存储每一个数据时都会通过计算得出该数据的位置，尽量的分布均衡。。目前也是OpenStack的主流后端存储，随着OpenStack在云计算领域的广泛使用，ceph也变得更加炙手可热。国内目前使用ceph搭建分布式存储系统较为成功的企业有x-sky,深圳元核云，上海UCloud等三家企业。</p></blockquote><p><img src="http://myimage.okay686.cn/okay686cn/20191229/mmd9867JuV5R.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="Ceph架构介绍"><a href="#Ceph架构介绍" class="headerlink" title="Ceph架构介绍"></a>Ceph架构介绍</h3><blockquote><p>Ceph使用RADOS提供对象存储，通过librados封装库提供多种存储方式的文件和对象转换。外层通过RGW（Object，有原生的API，而且也兼容Swift和S3的API，适合单客户端使用）、RBD（Block，支持精简配置、快照、克隆，适合多客户端有目录结构）、CephFS（File，Posix接口，支持快照，社会和更新变动少的数据，没有目录结构不能直接打开）将数据写入存储。</p><ul><li>高性能<br>a. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高<br>b.考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等<br>c. 能够支持上千个存储节点的规模，支持TB到PB级的数据  </li><li>高可扩展性<br>a. 去中心化<br>b. 扩展灵活<br>c. 随着节点增加而线性增长  </li><li>特性丰富<br>a. 支持三种存储接口：块存储、文件存储、对象存储<br>b. 支持自定义接口，支持多种语言驱动  </li></ul></blockquote><p><img src="http://myimage.okay686.cn/okay686cn/20191229/UzRMtUDwgiVm.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="Ceph核心概念"><a href="#Ceph核心概念" class="headerlink" title="Ceph核心概念"></a>Ceph核心概念</h3><p>ceph架构介绍：<br><img src="http://myimage.okay686.cn/okay686cn/20191229/V5moUrMU10wr.png?imageslim" srcset="/img/loading.gif" alt="mark"><br>目前多数公司选择的是RGW模式；</p><h5 id="RADOS"><a href="#RADOS" class="headerlink" title="RADOS"></a>RADOS</h5><blockquote><p>全称Reliable Autonomic Distributed Object Store，即可靠的、自动化的、分布式对象存储系统。RADOS是Ceph集群的精华，用户实现数据分配、Failover等集群操作。《场景：坏盘的数据迁移；新盘的数据一致性》</p></blockquote><h5 id="Librados"><a href="#Librados" class="headerlink" title="Librados"></a>Librados</h5><blockquote><p>Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。</p></blockquote><h5 id="Crush"><a href="#Crush" class="headerlink" title="Crush"></a>Crush</h5><blockquote><p>Crush算法是Ceph的两大创新之一，通过Crush算法的寻址操作，Ceph得以摒弃了传统的集中式存储元数据寻址方案。而Crush算法在一致性哈希基础上很好的考虑了容灾域的隔离，使得Ceph能够实现各类负载的副本放置规则，例如跨机房、机架感知等。同时，Crush算法有相当强大的扩展性，理论上可以支持数千个存储节点，这为Ceph在大规模云环境中的应用提供了先天的便利。</p></blockquote><h5 id="Pool"><a href="#Pool" class="headerlink" title="Pool"></a>Pool</h5><blockquote><p>Pool是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略（<strong>默认一份数据需要存三份</strong>，为的就是保证数据的强一致性，一旦md5不一致就会报错！！），支持两种类型：副本（replicated）和 纠删码（ Erasure Code）；</p></blockquote><h5 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h5><blockquote><p>PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略，简单点说就是<strong>相同PG内的对象都会放到相同的硬盘上</strong>，PG是 ceph的逻辑概念，服务端数据均衡和恢复的最小粒度就是PG，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据；</p></blockquote><h5 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h5><blockquote><p>简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。能否弄一个读写快，利 于共享的出来呢。于是就有了对象存储。最底层的存储单元，包含元数据和原始数据。</p></blockquote><p>ceph资源划分：<br><img src="http://myimage.okay686.cn/okay686cn/20191229/glU5lK6aJkVX.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><hr><p>ceph各层级架构：<br><img src="http://myimage.okay686.cn/okay686cn/20191229/04f3mpLC8uEf.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><hr><h3 id="Ceph核心组件"><a href="#Ceph核心组件" class="headerlink" title="Ceph核心组件"></a>Ceph核心组件</h3><h5 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h5><blockquote><p>OSD是负责物理存储的进程，一般配置成和磁盘一一对应，一块磁盘启动一个OSD进程。主要功能是：==存储数据、复制数据、平衡数据、恢复数据，以及与其它OSD间进行心跳检查，负责响应客户端请求返回具体数据的进程等==；  </p></blockquote><p><img src="http://myimage.okay686.cn/okay686cn/20191229/D9bwx8eRsRgz.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>Pool、PG和OSD的关系：</p><ul><li>一个Pool里有很多PG；  </li><li>一个PG里包含一堆对象，一个对象只能属于一个PG；  </li><li>PG有主从之分，一个PG分布在不同的OSD上（针对三副本类型）;  </li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20191229/pa1iR766mGfJ.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="Monitor（生产：至少要用3个monitor，使用奇数的monitor组成一个分布式高可用的monitor集群）"><a href="#Monitor（生产：至少要用3个monitor，使用奇数的monitor组成一个分布式高可用的monitor集群）" class="headerlink" title="Monitor（生产：至少要用3个monitor，使用奇数的monitor组成一个分布式高可用的monitor集群）"></a>Monitor（<strong>生产</strong>：至少要用3个monitor，使用奇数的monitor组成一个分布式高可用的monitor集群）</h5><blockquote><p>一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。负责==监视整个Ceph集群运行的Map视图（如OSD Map、Monitor Map、PG Map和CRUSH Map），维护集群的健康状态，维护展示集群状态的各种图表，管理集群客户端认证与授权==；</p></blockquote><h5 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h5><blockquote><p>MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。负责==保存文件系统的元数据，管理目录结构。对象存储和块设备存储不需要元数据服务==；</p></blockquote><h5 id="Mgr"><a href="#Mgr" class="headerlink" title="Mgr"></a>Mgr</h5><blockquote><p>ceph 官方开发了 ceph-mgr，主要目标==实现 ceph 集群的管理，为外界提供统一的入口==。例如cephmetrics、zabbix、calamari、promethus</p></blockquote><h5 id="RGW"><a href="#RGW" class="headerlink" title="RGW"></a>RGW</h5><blockquote><p>RGW全称RADOS gateway，是==Ceph对外提供的对象存储服务==，接口与S3和Swift兼容。</p></blockquote><h5 id="Admin"><a href="#Admin" class="headerlink" title="Admin"></a>Admin</h5><blockquote><p>Ceph常用管理接口通常都是命令行工具，如rados、ceph、rbd等命令，另外Ceph还有可以有一个专用的管理节点，在此节点上面部署专用的管理工具来实现近乎集群的一些管理工作，如集群部署，集群组件管理等。</p></blockquote><h3 id="Ceph三种存储类型"><a href="#Ceph三种存储类型" class="headerlink" title="Ceph三种存储类型"></a>Ceph三种存储类型</h3><h5 id="1、-块存储（RBD）"><a href="#1、-块存储（RBD）" class="headerlink" title="1、 块存储（RBD）"></a>1、 块存储（RBD）</h5><ul><li><p>优点：</p><ul><li>通过Raid与LVM等手段，对数据提供了保护；</li><li>多块廉价的硬盘组合起来，提高容量；</li><li>多块磁盘组合出来的逻辑盘，提升读写效率；  </li></ul></li><li><p>缺点：</p><ul><li>采用SAN架构组网时，光纤交换机，造价成本高；</li><li>主机之间无法共享数据；</li></ul></li><li>使用场景<ul><li>docker容器、虚拟机磁盘存储分配；</li><li>日志存储；</li><li>文件存储；</li></ul></li></ul><h5 id="2、文件存储（CephFS）"><a href="#2、文件存储（CephFS）" class="headerlink" title="2、文件存储（CephFS）"></a>2、文件存储（CephFS）</h5><ul><li><p>优点：</p><ul><li>造价低，随便一台机器就可以了；</li><li>方便文件共享；</li></ul></li><li><p>缺点：</p><ul><li>读写速率低；</li><li>传输速率慢；</li></ul></li><li>使用场景<ul><li>日志存储；</li><li>FTP、NFS；</li><li>其它有目录结构的文件存储<h5 id="3、对象存储（Object）-适合更新变动较少的数据"><a href="#3、对象存储（Object）-适合更新变动较少的数据" class="headerlink" title="3、对象存储（Object）(适合更新变动较少的数据)"></a>3、对象存储（Object）(适合更新变动较少的数据)</h5></li></ul></li><li>优点：<ul><li>具备块存储的读写高速；</li><li>具备文件存储的共享等特性；</li></ul></li></ul><ul><li>使用场景<ul><li>图片存储；</li><li>视频存储；</li></ul></li></ul><h4 id="文末补充："><a href="#文末补充：" class="headerlink" title="文末补充："></a>文末补充：</h4><ul><li>强一致性：</li></ul><p>当我们保存一份数据的时候，主副本保存完毕是不可以读取数据的，必须要所有的副本全部同步完成后，且数据一致才可以读取这部分数据！</p>]]></content>
    
    
    <categories>
      
      <category>K8s, ceph</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flannel网络组件实践(vxlan、host-gw)</title>
    <link href="/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9CFlannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(vxlan%E3%80%81host-gw)/"/>
    <url>/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9CFlannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(vxlan%E3%80%81host-gw)/</url>
    
    <content type="html"><![CDATA[<h4 id="Kubernetes网络组件之-Flannel"><a href="#Kubernetes网络组件之-Flannel" class="headerlink" title="Kubernetes网络组件之 Flannel"></a>Kubernetes网络组件之 Flannel</h4><p>Flannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供<strong>全局唯一的IP</strong>，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。</p><h5 id="1、Flannel-部署"><a href="#1、Flannel-部署" class="headerlink" title="1、Flannel 部署"></a>1、Flannel 部署</h5><p><a href="https://github.com/coreos/flannel" target="_blank" rel="noopener">https://github.com/coreos/flannel</a> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><h5 id="2、-Flannel工作模式及原理"><a href="#2、-Flannel工作模式及原理" class="headerlink" title="2、 Flannel工作模式及原理"></a>2、 Flannel工作模式及原理</h5><p>Flannel支持多种数据转发方式：</p><ul><li>UDP：最早支持的一种方式，由于性能最差，目前已经弃用。</li><li>VXLAN：Overlay Network方案，源数据包封装在另一种网络包里面进行路由转发和通信。==（100台左右node适用）==</li><li>Host-GW：Flannel通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机都有整个容器网络的路由数据了。==（node数量超过130台,性能瓶颈）==</li></ul><p>查看flannel 分配子网信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cat /var/run/flannel/subnet.env</span><br><span class="line">FLANNEL_NETWORK=10.244.0.0/16</span><br><span class="line">FLANNEL_SUBNET=10.244.0.1/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br></pre></td></tr></table></figure></p><p>查看flannel配置文件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# more /etc/cni/net.d/10-flannel.conflist</span><br></pre></td></tr></table></figure></p><h5 id="VXLAN"><a href="#VXLAN" class="headerlink" title="VXLAN"></a>VXLAN</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubeadm部署指定Pod网段</span><br><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16</span><br><span class="line"></span><br><span class="line"># 二进制部署指定（启用cni）</span><br><span class="line">cat /opt/kubernetes/cfg/kube-controller-manager.conf</span><br><span class="line">--allocate-node-cidrs=true \</span><br><span class="line">--cluster-cidr=10.244.0.0/16 \</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kube-flannel.yml</span><br><span class="line">net-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。下图flannel.1的设备就是VXLAN所需的VTEP设备。示意图如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191223/ogSxtUBuvNax.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>如果Pod 1访问Pod 2，源地址10.244.1.10，目的地址10.244.2.10 ，数据包传输流程如下：</p><ol><li><p><strong>容器路由</strong>：容器根据路由表从eth0发出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/ # ip route</span><br><span class="line">default via 10.244.0.1 dev eth0 </span><br><span class="line">10.244.0.0/24 dev eth0 scope link  src 10.244.0.45 </span><br><span class="line">10.244.0.0/16 via 10.244.0.1 dev eth0</span><br></pre></td></tr></table></figure><ol start="2"><li><p><strong>主机路由</strong>：数据包进入到宿主机虚拟网卡cni0，根据路由表转发到flannel.1虚拟网卡，也就是，来到了隧道的入口。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ip route</span><br><span class="line">default via 192.168.31.1 dev ens33 proto static metric 100 </span><br><span class="line">10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 </span><br><span class="line">10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink </span><br><span class="line">10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink</span><br></pre></td></tr></table></figure></li><li><p><strong>VXLAN封装</strong>：而这些VTEP设备（二层）之间组成二层网络必须要知道目的MAC地址。这个MAC地址从哪获取到呢？其实在flanneld进程启动后，就会自动添加其他节点ARP记录，可以通过ip命令查看，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ip neigh show dev flannel.1</span><br><span class="line">10.244.1.0 lladdr ca:2a:a4:59:b6:55 PERMANENT</span><br><span class="line">10.244.2.0 lladdr d2:d0:1b:a7:a9:cd PERMANENT</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>二次封包</strong>：知道了目的MAC地址，封装二层数据帧（容器源IP和目的IP）后，对于宿主机网络来说这个帧并没有什么实际意义。接下来，Linux内核还要把这个数据帧进一步封装成为宿主机网络的一个普通数据帧，好让它载着内部数据帧，通过宿主机的eth0网卡进行传输。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/vxlan-pkg.png" srcset="/img/loading.gif" alt=""></p></li><li><p><strong>封装到UDP包发出去</strong>：现在能直接发UDP包嘛？到目前为止，我们只知道另一端的flannel.1设备的MAC地址，却不知道对应的宿主机地址是什么。</p><p>flanneld进程也维护着一个叫做FDB的转发数据库，可以通过bridge fdb命令查看：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># bridge fdb show  dev flannel.1</span><br><span class="line"></span><br><span class="line">   d2:d0:1b:a7:a9:cd dst 192.168.31.61 self permanent</span><br><span class="line">   ca:2a:a4:59:b6:55 dst 192.168.31.63 self permanent</span><br></pre></td></tr></table></figure><p>可以看到，上面用的对方flannel.1的MAC地址对应宿主机IP，也就是UDP要发往的目的地。使用这个目的IP进行封装。</p></li><li><p><strong>数据包到达目的宿主机</strong>：Node1的eth0网卡发出去，发现是VXLAN数据包，把它交给flannel.1设备。flannel.1设备则会进一步拆包，取出原始二层数据帧包，发送ARP请求，经由cni0网桥转发给container。</p></li></ol><h5 id="Host-GW"><a href="#Host-GW" class="headerlink" title="Host-GW"></a>Host-GW</h5><p><u>host-gw模式相比vxlan简单了许多， 直接添加路由，将目的主机当做网关，直接路由原始封包。</u> </p><p>下面是示意图：</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/flanneld-hostgw.png" srcset="/img/loading.gif" alt=""></p><p>==线上更改网路模式一定要放在夜深人静去变更！！==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># more /tmp/k8s/kube-flannel.yaml</span><br><span class="line"></span><br><span class="line">net-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;host-gw&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">修改如上为host-gw然后应用：</span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yml</span><br><span class="line">podsecuritypolicy.policy/psp.flannel.unprivileged configured</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel unchanged</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel unchanged</span><br><span class="line">serviceaccount/flannel unchanged</span><br><span class="line">configmap/kube-flannel-cfg configured</span><br><span class="line">daemonset.apps/kube-flannel-ds-amd64 configured</span><br><span class="line">daemonset.apps/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.apps/kube-flannel-ds-arm created</span><br><span class="line">daemonset.apps/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.apps/kube-flannel-ds-s390x created</span><br><span class="line"></span><br><span class="line">###如上如果未生效，评估下风险，可删除后再次重建</span><br><span class="line">[root@k8s-master1 ~]# kubectl delete -f kube-flannel.yml</span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yml</span><br></pre></td></tr></table></figure></p><p>当你设置flannel使用host-gw模式,flanneld会在宿主机上创建节点的路由表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ip route  ##这样我们就看到了如上host-gw的特色，将目的主机当做网关。</span><br><span class="line"></span><br><span class="line">default via 192.168.31.1 dev ens33 proto static metric 100 </span><br><span class="line">10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 </span><br><span class="line">10.244.1.0/24 via 192.168.31.63 dev ens33   ##集群另外两台node的路由</span><br><span class="line">10.244.2.0/24 via 192.168.31.61 dev ens33   ##集群另外两台node的路由</span><br><span class="line">192.168.31.0/24 dev ens33 proto kernel scope link src 192.168.31.62 metric 100</span><br></pre></td></tr></table></figure><p>目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址是 192.168.31.63（即：via 192.168.31.63）。</p><p>一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。</p><p>而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.20，即 container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 container-2 当中。</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S集群网络生产级探究</title>
    <link href="/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E7%94%9F%E4%BA%A7%E7%BA%A7%E6%8E%A2%E7%A9%B6/"/>
    <url>/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E7%94%9F%E4%BA%A7%E7%BA%A7%E6%8E%A2%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="K8S集群网络"><a href="#K8S集群网络" class="headerlink" title="K8S集群网络"></a>K8S集群网络</h3><h4 id="网络基础知识"><a href="#网络基础知识" class="headerlink" title="网络基础知识"></a>网络基础知识</h4><h5 id="1、公司网络架构"><a href="#1、公司网络架构" class="headerlink" title="1、公司网络架构"></a>1、公司网络架构</h5><p><img src="http://myimage.okay686.cn/okay686cn/20191223/xOKekoVNmr2K.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><ul><li><strong>路由器</strong>：网络出口</li><li><strong>核心层</strong>：主要完成数据高效转发、链路备份等</li><li><strong>汇聚层</strong>：网络策略、安全、工作站交换机的接入、VLAN之间通信等功能</li><li><strong>接入层</strong>：工作站的接入</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、一个局域网内主机A（10）和主机B（20）之间通讯流程：</span><br><span class="line">网段：192.168.31.0/24</span><br><span class="line">源IP和目的IP均在同一子网下。</span><br><span class="line">四元组：源IP 源MAC 目的IP 目的MAC</span><br><span class="line"></span><br><span class="line">- 在本机查找ARP缓存表，ARP广播包询问20的MAC地址是多少。 </span><br><span class="line"></span><br><span class="line">2、主机A（10）和主机B（20）不在同一个局域网内之间通讯流程：</span><br><span class="line">VLAN1:192.168.31.0/24</span><br><span class="line">VLAN2:192.168.32.0/24</span><br><span class="line"></span><br><span class="line">- 主机A自顶向下将数据封装成链路层帧，缓存在主机A的适配器缓存中，主机A通过查看适配器中ARP表对应的MAC地址，将链路层帧发送到与其连接的第一跳路由器上，当链路层帧到达该路由器后，路由器根据自身的路由器转发表转发到指定的出口IP再一层层转向目的VLAN。</span><br></pre></td></tr></table></figure><h5 id="2、交换技术"><a href="#2、交换技术" class="headerlink" title="2、交换技术"></a>2、交换技术</h5><p>有想过局域网内主机怎么通信的？主机访问外网又是怎么通信的？</p><p>想要搞懂这些问题得从交换机、路由器讲起。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/switch.png" srcset="/img/loading.gif" alt=""></p><p>交换机工作在OSI参考模型的第二次，即数据链路层。交换机拥有一条高带宽的背部总线交换矩阵，在同一时间可进行多个端口对之间的数据传输。</p><p><strong>交换技术分为2层和3层：</strong></p><ul><li><p>2层：主要用于小型局域网，仅支持在数据链路层转发数据，对工作站接入。</p></li><li><p>3层：三层交换技术诞生，最初是为了解决广播域的问题，多年发展，三层交换机书已经成为构建中大型网络的主要力量。</p></li></ul><p><strong>广播域</strong></p><p>交换机在转发数据时会先进行广播，这个广播可以发送的区域就是一个广播域。交换机之间对广播帧是透明的，所以交换机之间组成的网络是一个广播域。</p><p>路由器的一个接口下的网络是一个广播域，所以路由器可以隔离广播域。</p><p><strong>ARP（地址解析协议</strong>，在IPV6中用NDP替代）</p><p>发送这个广播帧是由ARP协议实现，ARP是通过IP地址获取物理地址的一个TCP/IP协议。</p><p><strong>三层交换机</strong></p><p>前面讲的二层交换机只工作在数据链路层，路由器则工作在网络层。而功能强大的三层交换机可同时工作在数据链路层和网络层，并根据 MAC地址或IP地址转发数据包。</p><p><strong>VLAN（Virtual Local Area Network）：虚拟局域网</strong></p><p>VLAN是一种将局域网设备从逻辑上划分成一个个网段。</p><p>一个VLAN就是一个广播域，VLAN之间的通信是通过第3层的路由器来完成的。VLAN应用非常广泛，基本上大部分网络项目都会划分vlan。</p><p>VLAN的主要好处：</p><ul><li>分割广播域，减少广播风暴影响范围。</li><li>提高网络安全性，根据不同的部门、用途、应用划分不同网段</li></ul><h5 id="3、路由技术"><a href="#3、路由技术" class="headerlink" title="3、路由技术"></a>3、路由技术</h5><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/router.png" srcset="/img/loading.gif" alt=""></p><p>路由器主要分为两个端口类型：LAN口和WAN口</p><ul><li><p>WAN口：配置公网IP，接入到互联网，转发来自LAN口的IP数据包。</p></li><li><p>LAN口：配置内网IP（网关），连接内部交换机。</p></li></ul><p><strong>路由器是连接两个或多个网络的硬件设备，将从端口上接收的数据包，根据数据包的目的地址智能转发出去。</strong></p><p><strong>路由器的功能：</strong></p><ul><li>路由</li><li>转发</li><li>隔离子网</li><li>隔离广播域</li></ul><p>路由器是互联网的枢纽，是连接互联网中各个局域网、广域网的设备，相比交换机来说，路由器的数据转发很复杂，它会根据目的地址给出一条最优的路径。那么路径信息的来源有两种：<strong>动态路由和静态路由。</strong></p><p><strong>静态路由</strong>：指人工手动指定到目标主机的地址然后记录在路由表中，如果其中某个节点不可用则需要重新指定。</p><p><strong>动态路由</strong>：则是路由器根据动态路由协议自动计算出路径永久可用，能实时地<strong>适应网络结构</strong>的变化。</p><p>常用的动态路由协议：</p><ul><li><p>RIP（ Routing Information Protocol ，路由信息协议）</p></li><li><p>OSPF（Open Shortest Path First，开放式最短路径优先）</p></li><li><p>BGP（Border Gateway Protocol，边界网关协议）</p></li></ul><h5 id="4、OSI七层模型"><a href="#4、OSI七层模型" class="headerlink" title="4、OSI七层模型"></a>4、OSI七层模型</h5><p>OSI（Open System Interconnection）是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系，一般称为OSI参考模型或七层模型。 </p><table><thead><tr><th><strong>层次</strong></th><th><strong>名称</strong></th><th><strong>功能</strong></th><th><strong>协议数据单元（PDU）</strong></th><th><strong>常见协议</strong></th></tr></thead><tbody><tr><td>7</td><td>应用层</td><td>为用户的应用程序提供网络服务，提供一个接口。</td><td>数据</td><td>HTTP、FTP、Telnet</td></tr><tr><td>6</td><td>表示层</td><td>数据格式转换、数据加密/解密</td><td>数据单元</td><td>ASCII</td></tr><tr><td>5</td><td>会话层</td><td>建立、管理和维护会话</td><td>数据单元</td><td>SSH、RPC</td></tr><tr><td>4</td><td>传输层</td><td>建立、管理和维护端到端的连接</td><td>段/报文</td><td>TCP、UDP</td></tr><tr><td>3</td><td>网络层</td><td>IP选址及路由选择</td><td>分组/包</td><td>IP、ICMP、RIP、OSPF</td></tr><tr><td>2</td><td>数据链路层</td><td>硬件地址寻址，差错效验等。</td><td>帧</td><td>ARP、WIFI</td></tr><tr><td>1</td><td>物理层</td><td>利用物理传输介质提供物理连接，传送比特流。</td><td>比特流</td><td>RJ45、RJ11</td></tr></tbody></table><p><img src="http://myimage.okay686.cn/okay686cn/20191223/EAoHcLQudsD1.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="5、TCP-UDP协议"><a href="#5、TCP-UDP协议" class="headerlink" title="5、TCP/UDP协议"></a>5、TCP/UDP协议</h5><p>TCP（Transmission Control Protocol，传输控制协议），面向连接协议，双方先建立可靠的连接，再发送数据。适用于传输数据量大，可靠性要求高的应用场景。</p><p>UDP（User Data Protocol，用户数据报协议），面向非连接协议，不与对方建立连接，直接将数据包发送给对方。适用于一次只传输少量的数据，可靠性要求低的应用场景。相对TCP传输速度快。</p><h4 id="4-2-Kubernetes网络模型"><a href="#4-2-Kubernetes网络模型" class="headerlink" title="4.2 Kubernetes网络模型"></a>4.2 Kubernetes网络模型</h4><p>Kubernetes 要求所有的网络插件实现必须满足如下要求：</p><ul><li>一个Pod一个IP</li><li>所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射</li><li>所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射</li><li>Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个。</li></ul><h5 id="1、Docker容器网络模型"><a href="#1、Docker容器网络模型" class="headerlink" title="1、Docker容器网络模型"></a>1、Docker容器网络模型</h5><p>先看下Linux网络名词：</p><ul><li><p><strong>网络的命名空间：</strong>Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；Docker利用这一特性，实现不同容器间的网络隔离。</p></li><li><p><strong>Veth设备对</strong>：Veth设备对的引入是为了实现在不同网络命名空间的通信。</p></li><li><p><strong>Iptables/Netfilter</strong>：Docker使用Netfilter实现容器网络转发。</p></li><li><p><strong>网桥</strong>：网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来，并实现类似交换机那样的多对多的通信。</p></li><li><p><strong>路由</strong>：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里。</p></li></ul><p>Docker容器网络示意图如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191223/0Nq3u2ItmoPq.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="2、Pod-网络"><a href="#2、Pod-网络" class="headerlink" title="2、Pod 网络"></a>2、Pod 网络</h5><p><strong>问题</strong>：Pod是K8S最小调度单元，一个Pod由一个容器或多个容器组成，当多个容器时，怎么都用这一个Pod IP？</p><p><strong>实现</strong>：k8s会在每个Pod里先启动一个infra container小容器，然后让其他的容器连接进来这个网络命名空间，然后其他容器看到的网络试图就完全一样了。即网络设备、IP地址、Mac地址等。这就是解决网络共享的一种解法。在Pod的IP地址就是infra container的IP地址。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191223/ENnBVpuenmME.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。</p><p>Pod之间通信会有两种情况：</p><ul><li>两个Pod在同一个Node上</li><li>两个Pod在不同Node上</li></ul><p><strong>先看下第一种情况：两个Pod在同一个Node上</strong></p><p>同节点Pod之间通信道理与Docker网络一样的，如下图：</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/pod-to-pod-2.gif" srcset="/img/loading.gif" style="zoom:50%;"></p><ol><li>对 Pod1 来说，eth0 通过虚拟以太网设备（veth0）连接到 root namespace；</li><li>网桥 cbr0 中为 veth0 配置了一个网段。一旦数据包到达网桥，网桥使用ARP 协议解析出其正确的目标网段 veth1；</li><li>网桥 cbr0 将数据包发送到 veth1；</li><li>数据包到达 veth1 时，被直接转发到 Pod2 的 network namespace 中的 eth0 网络设备。</li></ol><p><strong>再看下第二种情况：两个Pod在不同Node上</strong></p><p>K8S网络模型要求Pod IP在整个网络中都可访问，这种需求是由第三方网络组件实现。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/pod-to-pod-3.gif" srcset="/img/loading.gif" style="zoom:50%;"></p><h5 id="3、CNI（容器网络接口）"><a href="#3、CNI（容器网络接口）" class="headerlink" title="3、CNI（容器网络接口）"></a>3、CNI（容器网络接口）</h5><p>CNI（Container Network Interface，容器网络接口)：是一个容器网络规范，Kubernetes网络采用的就是这个CNI规范，CNI实现依赖两种插件，一种CNI Plugin是负责容器连接到主机，另一种是IPAM负责配置容器网络命名空间的网络。</p><p>CNI插件默认路径：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ls /opt/cni/bin/</span><br></pre></td></tr></table></figure><p>地址：<a href="https://github.com/containernetworking/cni" target="_blank" rel="noopener">https://github.com/containernetworking/cni</a></p><p>当你在宿主机上部署Flanneld后，flanneld 启动后会在每台宿主机上生成它对应的CNI 配置文件（它其实是一个 ConfigMap），从而告诉Kubernetes，这个集群要使用 Flannel 作为容器网络方案。</p><p>CNI配置文件路径：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/etc/cni/net.d/10-flannel.conflist</span><br></pre></td></tr></table></figure><p>当 kubelet 组件需要创建 Pod 的时候，先调用dockershim它先创建一个 Infra 容器。然后调用 CNI 插件为 Infra 容器配置网络。</p><p>这两个路径在kubelet启动参数中定义： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--network-plugin=cni \</span><br><span class="line">--cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">--cni-bin-dir=/opt/cni/bin</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Calico网络组件实践(BGP、RR、IPIP)</title>
    <link href="/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E8%B7%AFCalico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(BGP%E3%80%81RR%E3%80%81IPIP)/"/>
    <url>/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E8%B7%AFCalico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(BGP%E3%80%81RR%E3%80%81IPIP)/</url>
    
    <content type="html"><![CDATA[<h4 id="Kubernetes网络方案之-Calico"><a href="#Kubernetes网络方案之-Calico" class="headerlink" title="Kubernetes网络方案之 Calico"></a>Kubernetes网络方案之 Calico</h4><p>Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。</p><p>Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。</p><p>此外，Calico  项目还实现了 Kubernetes 网络策略，提供ACL功能。</p><h5 id="1、BGP概述"><a href="#1、BGP概述" class="headerlink" title="1、BGP概述"></a>1、BGP概述</h5><p>实际上，Calico项目提供的网络解决方案，与Flannel的host-gw模式几乎一样。也就是说，Calico也是基于路由表实现容器数据包转发，但不同于Flannel使用flanneld进程来维护路由信息的做法，而Calico项目使用BGP协议来自动维护整个集群的路由信息。</p><p>BGP英文全称是Border Gateway Protocol，即边界网关协议，它是一种自治系统间的动态路由发现协议，与其他 BGP 系统交换网络可达信息。 </p><p>为了能让你更清楚理解BGP，举个例子：</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/bgp.png" srcset="/img/loading.gif" alt=""></p><p>在这个图中，有两个自治系统（autonomous system，简称为AS）：AS 1 和 AS 2。</p><p>在互联网中，一个自治系统(AS)是一个有权自主地决定在本系统中应采用何种路由协议的小型单位。这个网络单位可以是一个简单的网络也可以是一个由一个或多个普通的网络管理员来控制的网络群体，它是一个单独的可管理的网络单元（例如一所大学，一个企业或者一个公司个体）。一个自治系统有时也被称为是一个路由选择域（routing domain）。一个自治系统将会分配一个全局的唯一的16位号码，有时我们把这个号码叫做自治系统号（ASN）。</p><p>在正常情况下，自治系统之间不会有任何来往。如果两个自治系统里的主机，要通过 IP 地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。BGP协议就是让他们互联的一种方式。</p><h5 id="2、Calico-BGP实现"><a href="#2、Calico-BGP实现" class="headerlink" title="2、Calico BGP实现"></a>2、Calico BGP实现</h5><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/calico.png" srcset="/img/loading.gif" alt=""></p><p>在了解了 BGP 之后，Calico 项目的架构就非常容易理解了，Calico主要由三个部分组成：</p><ul><li>Felix：以DaemonSet方式部署，运行在每一个Node节点上，主要负责维护宿主机上路由规则以及ACL规则。</li><li>BGP Client（BIRD）：主要负责把 Felix 写入 Kernel 的路由信息分发到集群 Calico 网络。</li><li>Etcd：分布式键值存储，保存Calico的策略和网络配置状态。</li><li>calicoctl：允许您从简单的命令行界面实现高级策略和网络。</li></ul><h5 id="3、Calico-部署"><a href="#3、Calico-部署" class="headerlink" title="3、Calico 部署"></a>3、Calico 部署</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico.yaml</span><br></pre></td></tr></table></figure><p>下载完后还需要修改里面配置项：</p><p>具体步骤如下：</p><ul><li>配置连接etcd地址，如果使用https，还需要配置证书。（ConfigMap，Secret）</li><li>根据实际网络规划修改Pod CIDR（CALICO_IPV4POOL_CIDR）</li><li>选择工作模式（CALICO_IPV4POOL_IPIP），支持BGP，IPIP</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看证书所在位置：</span><br><span class="line">[root@k8s-master1 ~]# ls /opt/etcd/ssl/</span><br><span class="line">ca.pem          server-key.pem  server.pem</span><br><span class="line"></span><br><span class="line">拼接ca密钥, ca的key以及数字证书内容为1条字符串：</span><br><span class="line">[root@k8s-master1 ~]# cat /opt/etcd/ssl/ca.pem |base64 -w 0</span><br><span class="line">[root@k8s-master1 ~]# cat /opt/etcd/ssl/server-key.pem |base64 -w 0</span><br><span class="line">[root@k8s-master1 ~]# cat /opt/etcd/ssl/server.pem |base64 -w 0</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# vim calico.yaml</span><br><span class="line"></span><br><span class="line">###etcd证书：</span><br><span class="line">···</span><br><span class="line">###第一处：</span><br><span class="line">etcd-key: 对应如上server-key.pem生成的字符串</span><br><span class="line">etcd-cert: 对应如上server.pem生成的字符串</span><br><span class="line">etcd-ca: 对应如上ca.pem生成的字符串</span><br><span class="line"></span><br><span class="line">###第二处：（直接删除注释即可）</span><br><span class="line">  etcd_ca: &quot;/calico-secrets/etcd-ca&quot;</span><br><span class="line">  etcd_cert: &quot;/calico-secrets/etcd-cert&quot;</span><br><span class="line">  etcd_key: &quot;/calico-secrets/etcd-key&quot;</span><br><span class="line"></span><br><span class="line">###第三处：可查看此文件：[root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-apiserver.conf中etcd的配置：</span><br><span class="line">etcd_endpoints: &quot;https://192.168.171.11:2379,https://192.168.171.12:2379,https://192.168.171.13:2379&quot;</span><br><span class="line">···</span><br><span class="line"></span><br><span class="line">###修改Pod CIDR（查看：cat /opt/kubernetes/cfg/kube-controller-manager.conf 的 --cluster-cidr=10.244.0.0/16 \）</span><br><span class="line">···</span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">  value: &quot;10.244.0.0/16&quot;</span><br><span class="line">···</span><br><span class="line"></span><br><span class="line">###选择工作模式</span><br><span class="line">calico默认工作模式是IPIP，我们需要注释掉，然后默认就会改为BGP，Always改为Never即可</span><br><span class="line"># Enable IPIP</span><br><span class="line">- name: CALICO_IPV4POOL_IPIP</span><br><span class="line">  value: &quot;Nerver&quot;</span><br></pre></td></tr></table></figure><p>★★★<br>修改完成如上配置文件后，因为flannel升级calico，flannel默认的一些路由策略还会依旧保存，我们需要先删除，不然路由策略依然还会走之前的，导致问题！！<br>★★★<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# ip link delete cni0</span><br><span class="line">[root@k8s-master1 ~]# ip link delete flannel.1</span><br><span class="line"></span><br><span class="line">查看路由表再次确认（每个node都需要检查一下）：</span><br><span class="line">[root@k8s-node2 ~]# ip route</span><br><span class="line">default via 192.168.171.2 dev ens33 proto static metric 100</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.171.0/24 dev ens33 proto kernel scope link src 192.168.171.13 metric 100</span><br><span class="line">或者手动删除一些静态路由：</span><br><span class="line">[root@k8s-master1 ~]# ip route del 10.244.1.0/24 via 192.168.171.12 dev ens33</span><br><span class="line">[root@k8s-master1 ~]# ip route del 10.244.2.0/24 via 192.168.171.13 dev ens33</span><br><span class="line">[root@k8s-master1 ~]# ip route del 10.244.3.0/24 via 192.168.171.14 dev ens33</span><br></pre></td></tr></table></figure></p><p>修改完后应用清单：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl apply -f calico.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get pods -n kube-system</span><br><span class="line">NAME                                      READY   STATUS             RESTARTS   AGE</span><br><span class="line">calico-kube-controllers-f68c55884-tx2km   1/1     Running            0          4m26s</span><br><span class="line">calico-node-5kqcl                         1/1     Running            0          4m27s</span><br><span class="line">calico-node-ck4gf                         1/1     Running            0          4m27s</span><br><span class="line">calico-node-jp9kj                         1/1     Running            0          4m26s</span><br><span class="line">calico-node-mnslt                         1/1     Running            0          4m27s</span><br></pre></td></tr></table></figure></p><p>★★★</p><p>最后一点：当我们升级完毕，查看路由表时候，神奇的发现竟然没有任何路由！！</p><p>最终查找资料确认了其中的问题：现有容器需要重建！！</p><p>★★★</p><p>再次查看路由：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# ip route</span><br><span class="line">default via 192.168.171.2 dev ens33 proto static metric 100</span><br><span class="line">10.244.36.64/26 via 192.168.171.12 dev ens33 proto bird</span><br><span class="line">10.244.107.192/26 via 192.168.171.14 dev ens33 proto bird</span><br><span class="line">blackhole 10.244.159.128/26 proto bird</span><br><span class="line">10.244.159.132 dev calicc2e4908bc6 scope link</span><br><span class="line">10.244.159.133 dev calid85356b2213 scope link</span><br><span class="line">10.244.169.128/26 via 192.168.171.13 dev ens33 proto bird</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.171.0/24 dev ens33 proto kernel scope link src 192.168.171.11 metric 100</span><br></pre></td></tr></table></figure></p><h5 id="4、Calico-管理工具"><a href="#4、Calico-管理工具" class="headerlink" title="4、Calico 管理工具"></a>4、Calico 管理工具</h5><p>下载工具：<a href="https://github.com/projectcalico/calicoctl/releases" target="_blank" rel="noopener">https://github.com/projectcalico/calicoctl/releases</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># wget -O /usr/local/bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v3.9.1/calicoctl</span><br><span class="line"># chmod +x /usr/local/bin/calicoctl</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkdir /etc/calico</span><br><span class="line"># vim /etc/calico/calicoctl.cfg  </span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: CalicoAPIConfig</span><br><span class="line">metadata:</span><br><span class="line">spec:</span><br><span class="line">  datastoreType: &quot;etcdv3&quot;</span><br><span class="line">  etcdEndpoints: &quot;https://192.168.31.61:2379,https://192.168.31.62:2379,https://192.168.31.63:2379&quot;</span><br><span class="line">  etcdKeyFile: &quot;/opt/etcd/ssl/server-key.pem&quot;</span><br><span class="line">  etcdCertFile: &quot;/opt/etcd/ssl/server.pem&quot;</span><br><span class="line">  etcdCACertFile: &quot;/opt/etcd/ssl/ca.pem&quot;</span><br></pre></td></tr></table></figure><p>使用calicoctl查看服务状态：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">| 192.168.171.12 | node-to-node mesh | up    | 14:33:24 | Established |</span><br><span class="line">| 192.168.171.13 | node-to-node mesh | up    | 14:33:29 | Established |</span><br><span class="line">| 192.168.171.14 | node-to-node mesh | up    | 14:33:26 | Established |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl get nodes</span><br><span class="line">NAME</span><br><span class="line">k8s-master1</span><br><span class="line">k8s-node1</span><br><span class="line">k8s-node2</span><br><span class="line">k8s-node3</span><br></pre></td></tr></table></figure><p>查看 IPAM的IP地址池：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl get ippool -o wide</span><br><span class="line">NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR</span><br><span class="line">default-ipv4-ippool   10.244.0.0/16   true   Never      Never       false      all()</span><br></pre></td></tr></table></figure><h5 id="5、Calico-BGP-原理剖析"><a href="#5、Calico-BGP-原理剖析" class="headerlink" title="5、Calico BGP 原理剖析"></a>5、Calico BGP 原理剖析</h5><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/calico-bgp.png" srcset="/img/loading.gif" alt=""></p><p>Pod 1 访问 Pod 2大致流程如下：</p><ol><li><p>数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）；</p></li><li><p>宿主机根据路由规则，将数据包转发给下一跳（网关）；</p></li><li>到达Node2，根据路由规则将数据包转发给cali设备，从而到达容器2。</li></ol><p>路由表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># node1</span><br><span class="line">10.244.36.65 dev cali4f18ce2c9a1 scope link </span><br><span class="line">10.244.169.128/26 via 192.168.31.63 dev ens33 proto bird </span><br><span class="line">10.244.235.192/26 via 192.168.31.61 dev ens33 proto bird </span><br><span class="line"># node2</span><br><span class="line">10.244.169.129 dev calia4d5b2258bb scope link </span><br><span class="line">10.244.36.64/26 via 192.168.31.62 dev ens33 proto bird</span><br><span class="line">10.244.235.192/26 via 192.168.31.61 dev ens33 proto bird</span><br></pre></td></tr></table></figure><p>其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。</p><p>不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。</p><h5 id="6、Route-Reflector-模式（RR）"><a href="#6、Route-Reflector-模式（RR）" class="headerlink" title="6、Route Reflector 模式（RR）"></a>6、Route Reflector 模式（RR）</h5><p><a href="https://docs.projectcalico.org/master/networking/bgp" target="_blank" rel="noopener">https://docs.projectcalico.org/master/networking/bgp</a> </p><p>Calico 维护的网络在默认是（Node-to-Node Mesh）全互联模式，Calico集群中的节点之间都会相互建立连接，用于路由交换。但是随着集群规模的扩大，<u>mesh模式将形成一个巨大服务网格，连接数成倍增加。</u></p><p>例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">node1  node2  node3</span><br><span class="line"></span><br><span class="line">node1 ==&gt; node2 ==&gt; node3</span><br><span class="line">node2 ==&gt; node1 ==&gt; node3</span><br><span class="line">node3 ==&gt; node1 ==&gt; node2</span><br><span class="line">noden ...</span><br><span class="line"></span><br><span class="line">如上路由模式是node-node,这种模式下整个集群的node需要控制在100台左右！！</span><br><span class="line"></span><br><span class="line">也就是说node1上需要建立2条路由 node2上也需要建立node1和node3这2条路由，如集群很大，一旦增加1台node，就需要成倍的路由需要建立！！</span><br></pre></td></tr></table></figure></p><p>这时就需要使用 Route Reflector（路由器反射）模式解决这个问题。</p><p><u>这就类似一个nginx 后端的node作为负载，一旦有新的node只需要和路有反射的机器建立路由关系即可！！</u></p><p>确定一个或多个Calico节点充当路由反射器，让其他节点从这个RR节点获取路由信息。</p><p>具体步骤如下：</p><p><strong>1、关闭 node-to-node BGP网格</strong></p><p>★★★<br>一旦更改，整个集群网路就会断掉！！！线上一定要先评估！！<br>★★★</p><p>添加 default BGP配置，调整 nodeToNodeMeshEnabled和asNumber：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | calicoctl create -f -</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: default</span><br><span class="line">spec:</span><br><span class="line">  logSeverityScreen: Info</span><br><span class="line">  nodeToNodeMeshEnabled: false  </span><br><span class="line">  asNumber: 63400</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ASN号可以通过获取：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># calicoctl get nodes --output=wide</span><br></pre></td></tr></table></figure><p><strong>2、配置指定节点充当路由反射器</strong></p><p>为方便让BGPPeer轻松选择节点，通过标签选择器匹配。</p><p>给路由器反射器节点打标签：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl label node k8s-node2 route-reflector=true</span><br><span class="line">node/k8s-node2 labeled</span><br></pre></td></tr></table></figure><p>然后编辑刚打了tag的node节点：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl get node k8s-node2 -o yaml &gt; node2.yaml</span><br></pre></td></tr></table></figure></p><p>然后配置路由器反射器节点routeReflectorClusterID：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# vim node2.yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    projectcalico.org/kube-labels: &apos;&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;kubernetes.io/hostname&quot;:&quot;k8s-node2&quot;,&quot;kubernetes.io/os&quot;:&quot;linux&quot;,&quot;route-reflector&quot;:&quot;true&quot;&#125;&apos;</span><br><span class="line">  creationTimestamp: 2019-12-25T14:02:11Z</span><br><span class="line">  labels:</span><br><span class="line">    beta.kubernetes.io/arch: amd64</span><br><span class="line">    beta.kubernetes.io/os: linux</span><br><span class="line">    kubernetes.io/arch: amd64</span><br><span class="line">    kubernetes.io/hostname: k8s-node2</span><br><span class="line">    kubernetes.io/os: linux</span><br><span class="line">    route-reflector: &quot;true&quot;</span><br><span class="line">  name: k8s-node2</span><br><span class="line">  resourceVersion: &quot;349138&quot;</span><br><span class="line">  uid: 9387fc14-f6f4-4809-a8f2-418ed81ba6ca</span><br><span class="line">spec:</span><br><span class="line">  bgp:</span><br><span class="line">    ipv4Address: 192.168.171.13/24</span><br><span class="line">    routeReflectorClusterID: 244.0.0.1   # 集群ID，保证唯一性</span><br><span class="line">  orchRefs:</span><br><span class="line">  - nodeName: k8s-node2</span><br><span class="line">    orchestrator: k8s</span><br></pre></td></tr></table></figure><p>应用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl apply -f node2.yaml</span><br><span class="line">Successfully applied 1 &apos;Node&apos; resource(s)</span><br></pre></td></tr></table></figure></p><p>现在，很容易使用标签选择器将路由反射器节点与其他非路由反射器节点配置为对等：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# vim setbgprr.yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPPeer</span><br><span class="line">metadata:</span><br><span class="line">  name: peer-with-route-reflectors</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector: all()</span><br><span class="line">  peerSelector: route-reflector == &apos;true&apos;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">##创建规则：</span><br><span class="line">[root@k8s-master1 ~]# calicoctl apply -f setbgprr.yaml</span><br><span class="line">Successfully applied 1 &apos;BGPPeer&apos; resource(s)</span><br></pre></td></tr></table></figure><p>查看节点的BGP连接状态：（只与171.13建立了连接，再次ping网路就通了）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |   PEER TYPE   | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br><span class="line">| 192.168.171.13 | node specific | up    | 15:19:42 | Established |</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br></pre></td></tr></table></figure><p>查看规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl get bgppeer</span><br><span class="line">NAME                         PEERIP   NODE    ASN</span><br><span class="line">peer-with-route-reflectors            all()   0</span><br></pre></td></tr></table></figure></p><p>==当然rr和nginx也是一致的，也需要高可用，所以我们尽量配置2台以上的节点！==</p><p>条件允许的话，最好找2台单独的机器！</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl label node k8s-node1 route-reflector=true</span><br><span class="line">node/k8s-node1 labeled</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# calicoctl get node k8s-node1 -o yaml &gt; node1.yaml</span><br><span class="line">[root@k8s-master1 ~]# vim node1.yaml（增加和node2一致即可）</span><br><span class="line">···</span><br><span class="line">routeReflectorClusterID: 244.0.0.1   # 集群ID，保证唯一性</span><br><span class="line">···</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# calicoctl apply -f node1.yaml</span><br><span class="line">Successfully applied 1 &apos;Node&apos; resource(s)</span><br></pre></td></tr></table></figure><p>再次查看：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |   PEER TYPE   | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br><span class="line">| 192.168.171.13 | node specific | up    | 15:19:42 | Established |</span><br><span class="line">| 192.168.171.12 | node specific | up    | 15:26:13 | Established |</span><br><span class="line">+----------------+---------------+-------+----------+-------------+</span><br></pre></td></tr></table></figure></p><hr><h5 id="7、IPIP模式"><a href="#7、IPIP模式" class="headerlink" title="7、IPIP模式"></a>7、IPIP模式</h5><p><u>和flannel的vxlan差不多模式，也是基于二层数据的封装和解封；也会创建一个虚拟网卡 <strong>tunl0</strong> </u></p><p>在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。</p><p>修改为IPIP模式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># calicoctl get ipPool -o yaml &gt; ipip.yaml</span><br><span class="line"># vi ipip.yaml</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: IPPool</span><br><span class="line">metadata:</span><br><span class="line">  name: default-ipv4-ippool</span><br><span class="line">spec:</span><br><span class="line">  blockSize: 26</span><br><span class="line">  cidr: 10.244.0.0/16</span><br><span class="line">  ipipMode: Always</span><br><span class="line">  natOutgoing: true</span><br><span class="line"></span><br><span class="line"># calicoctl apply -f ipip.yaml</span><br><span class="line"># calicoctl get ippool -o wide</span><br></pre></td></tr></table></figure><p>IPIP示意图：</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/calico-ipip.png" srcset="/img/loading.gif" alt=""></p><p>Pod 1 访问 Pod 2大致流程如下：</p><ol><li>数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）；</li><li>进入IP隧道设备（tunl0），由Linux内核IPIP驱动封装在宿主机网络的IP包中（新的IP包目的地之是原IP包的下一跳地址，即192.168.31.63），这样，就成了Node1 到Node2的数据包；</li><li>数据包经过路由器三层转发到Node2；</li><li>Node2收到数据包后，网络协议栈会使用IPIP驱动进行解包，从中拿到原始IP包；</li><li>然后根据路由规则，根据路由规则将数据包转发给cali设备，从而到达容器2。</li></ol><p>路由表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># node1</span><br><span class="line">10.244.36.65 dev cali4f18ce2c9a1 scope link </span><br><span class="line">10.244.169.128/26 via 192.168.31.63 dev tunl0 proto bird onlink </span><br><span class="line"># node2</span><br><span class="line">10.244.169.129 dev calia4d5b2258bb scope link </span><br><span class="line">10.244.36.64/26 via 192.168.31.62 dev tunl0 proto bird onlink</span><br></pre></td></tr></table></figure><p>==<u>不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络<strong>性能会因为额外的封包和解包工作而下降</strong>。所以建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。</u>==</p><h5 id="8、CNI-网络方案优缺点及最终选择"><a href="#8、CNI-网络方案优缺点及最终选择" class="headerlink" title="8、CNI 网络方案优缺点及最终选择"></a>8、CNI 网络方案优缺点及最终选择</h5><p>先考虑几个问题：</p><ul><li>需要细粒度网络访问控制？ ==》 flannel不支持，calico支持（ACL）；</li><li>追求网络性能？  ==》  flannel（host-gw），calico（BGP）；</li><li>服务器之前是否可以跑BGP协议？ ==》 公有云有些不支持；</li><li>集群规模多大？  ==》 100台node左右推荐（flannel&lt;host-gw性能好些&gt;）维护方便；</li><li>是否有维护能力？ ==》 calico维护复杂，路由表！</li></ul><hr><h3 id="小话题：办公网络与K8S网络如何互通"><a href="#小话题：办公网络与K8S网络如何互通" class="headerlink" title="小话题：办公网络与K8S网络如何互通"></a>小话题：办公网络与K8S网络如何互通</h3><h4 id="4-5-网络策略"><a href="#4-5-网络策略" class="headerlink" title="4.5 网络策略"></a>4.5 网络策略</h4><h5 id="1、为什么需要网络隔离？"><a href="#1、为什么需要网络隔离？" class="headerlink" title="1、为什么需要网络隔离？"></a>1、为什么需要网络隔离？</h5><p>CNI插件插件解决了不同Node节点Pod互通问题，从而形成一个扁平化网络，默认情况下，Kubernetes 网络允许所有 Pod 到 Pod 的流量，在一些场景中，我们不希望Pod之间默认相互访问，例如：</p><ul><li>应用程序间的访问控制。例如微服务A允许访问微服务B，微服务C不能访问微服务A</li><li>开发环境命名空间不能访问测试环境命名空间Pod</li><li>当Pod暴露到外部时，需要做Pod白名单</li><li>多租户网络环境隔离</li></ul><p>所以，我们需要使用network policy对Pod网络进行隔离。支持对Pod级别和Namespace级别网络访问控制。</p><p>路由器层面解决：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、K8S集群测试环境在办公网路子网：</span><br><span class="line"># ip route add 10.244.0.0/16 via &lt;K8S-NODE1&gt; dev A</span><br><span class="line"></span><br><span class="line">2、K8S集群与办公网路不在同VLAN, 不同机房：</span><br><span class="line">前提：三层可达</span><br><span class="line">1）路由器添加路由表：10.244.0.0/16 &lt;K8S-NODE1&gt;</span><br><span class="line">2) 路由器BGP与路由反射器建立连接；</span><br></pre></td></tr></table></figure></p><p>Pod网络入口方向隔离</p><ul><li><strong>基于Pod级网络隔离</strong>：只允许特定对象访问Pod（使用标签定义），允许白名单上的IP地址或者IP段访问Pod</li><li><strong>基于Namespace级网络隔离</strong>：多个命名空间，A和B命名空间Pod完全隔离。</li></ul><p>Pod网络出口方向隔离</p><ul><li>拒绝某个Namespace上所有Pod访问外部</li><li>基于目的IP的网络隔离：只允许Pod访问白名单上的IP地址或者IP段</li><li>基于目标端口的网络隔离：只允许Pod访问白名单上的端口</li></ul><h5 id="2、网络策略概述"><a href="#2、网络策略概述" class="headerlink" title="2、网络策略概述"></a>2、网络策略概述</h5><p>一个NetworkPolicy例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: db      ##db的这个应用的容器</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  - Egress</span><br><span class="line">  ingress:  ##访问pod的流量（入方向）</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.17.0.0/16     ##此ip段可以访问</span><br><span class="line">        except: ##除了如下这个IP段不可以访问</span><br><span class="line">        - 172.17.1.0/24</span><br><span class="line">    - namespaceSelector:    ##namespace</span><br><span class="line">        matchLabels:</span><br><span class="line">          project: myproject</span><br><span class="line">    - podSelector:      ##pod</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: frontend</span><br><span class="line">    ports:      ##ports端口</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  egress:   ##出去的流量（出方向）</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:  ##只能访问如下此网段</span><br><span class="line">        cidr: 10.0.0.0/24</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP     ##且端口为5978</span><br><span class="line">      port: 5978</span><br></pre></td></tr></table></figure><p>配置解析：</p><ul><li><p>podSelector：用于选择策略应用到的Pod组。</p></li><li><p>policyTypes：其可以包括任一Ingress，Egress或两者。该policyTypes字段指示给定的策略用于Pod的入站流量、还是出站流量，或者两者都应用。如果未指定任何值，则默认值为Ingress，如果网络策略有出口规则，则设置egress。</p></li><li><p>Ingress：from是可以访问的白名单，可以来自于IP段、命名空间、Pod标签等，ports是可以访问的端口。</p></li><li><p>Egress：这个Pod组可以访问外部的IP段和端口。</p></li></ul><h5 id="3、入站、出站网络流量访问控制案例"><a href="#3、入站、出站网络流量访问控制案例" class="headerlink" title="3、入站、出站网络流量访问控制案例"></a>3、入站、出站网络流量访问控制案例</h5><p><strong>Pod访问限制</strong></p><p>准备测试环境，一个web pod，两个client pod</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl create deployment nginxweb --image=nginx</span><br><span class="line"></span><br><span class="line"># kubectl scale deployment nginxweb --replicas=3</span><br><span class="line"></span><br><span class="line"># kubectl get pods --show-labels</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE    LABELS</span><br><span class="line">nginxweb-c5d5747f8-4wff9    1/1     Running   1          24h    app=nginxweb,pod-template-hash=c5d5747f8</span><br><span class="line">nginxweb-c5d5747f8-6zbk6    1/1     Running   0          3m2s   app=nginxweb,pod-template-hash=c5d5747f8</span><br><span class="line">nginxweb-c5d5747f8-xxfc9    1/1     Running   1          24h    app=nginxweb,pod-template-hash=c5d5747f8</span><br></pre></td></tr></table></figure><p><strong>需求</strong>：<u>将default命名空间携带run=nginxweb标签的Pod隔离，只允许default命名空间携带run=client1标签的Pod访问80端口。</u></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginxweb</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          project: default </span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          run: client1</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##创建如上规则</span><br><span class="line"># kubectl apply -f networkpolicy.yaml</span><br><span class="line">networkpolicy.networking.k8s.io/test-network-policy created</span><br><span class="line"></span><br><span class="line"># kubectl run client1 --generator=run-pod/v1 --image=busybox --command -- sleep 36000      ##快速的启动一个测试容器</span><br><span class="line"></span><br><span class="line"># kubectl exec -it client1 sh</span><br><span class="line">/ # ping 10.244.169.143     ##从容器中测试访问另外的pod</span><br><span class="line">PING 10.244.169.143 (10.244.169.143): 56 data bytes</span><br><span class="line">64 bytes from 10.244.169.143: seq=0 ttl=62 time=0.639 ms</span><br><span class="line">完全可以访问；</span><br><span class="line"></span><br><span class="line"># kubectl run client2 --generator=run-pod/v1 --image=busybox --command -- sleep 36000      ##再次启动一个client2pod，按照规则client2是不允许访问80端口的</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it client2 sh</span><br><span class="line">/ # wget 10.244.36.73</span><br><span class="line">Connecting to 10.244.36.73 (10.244.36.73:80)</span><br><span class="line">^C      ##看来是不可以访问的</span><br><span class="line"></span><br><span class="line">##再来测试下client1</span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it client1 sh</span><br><span class="line">/ # wget 10.244.36.73</span><br><span class="line">Connecting to 10.244.36.73 (10.244.36.73:80)</span><br><span class="line">saving to &apos;index.html&apos;</span><br><span class="line">index.html           100% |***********************************************************************************************************************|   612  0:00:00 ETA</span><br><span class="line">&apos;index.html&apos; saved      </span><br><span class="line">/ # ping 10.244.36.73</span><br><span class="line">PING 10.244.36.73 (10.244.36.73): 56 data bytes</span><br><span class="line">^C</span><br><span class="line">--- 10.244.36.73 ping statistics ---</span><br><span class="line">3 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line">如上可以看得出：##可以访问！！但是不可以ping；</span><br><span class="line"></span><br><span class="line">看来如上策略就生效了，达到了预期的效果，只可以访问80, 其余均不可以！</span><br></pre></td></tr></table></figure><p>隔离策略配置：</p><p>Pod对象：default命名空间携带run=web标签的Pod</p><p>允许访问端口：80</p><p>允许访问对象：default命名空间携带run=client1标签的Pod</p><p>拒绝访问对象：除允许访问对象外的所有对象</p><p><strong>命名空间隔离</strong></p><p><strong>需求</strong>：<u>default命名空间下所有pod可以互相访问，但不能访问其他命名空间Pod，其他命名空间也不能访问default命名空间Pod。</u></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: deny-from-other-namespaces </span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - podSelector: &#123;&#125;</span><br></pre></td></tr></table></figure><p>podSelector: {}：<strong>default命名空间下所有Pod</strong></p><p>from.podSelector: {} : <strong>如果未配置具体的规则，默认不允许</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解除如上的规则：</span><br><span class="line">[root@k8s-master1 ~]# kubectl delete -f networkpolicy.yaml</span><br><span class="line">networkpolicy.networking.k8s.io &quot;test-network-policy&quot; deleted</span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it client2 sh</span><br><span class="line">/ # ping 10.244.36.73   ##现在已经可以ping了</span><br><span class="line">PING 10.244.36.73 (10.244.36.73): 56 data bytes</span><br><span class="line">64 bytes from 10.244.36.73: seq=0 ttl=63 time=0.313 ms</span><br><span class="line">64 bytes from 10.244.36.73: seq=1 ttl=63 time=0.083 ms</span><br><span class="line"></span><br><span class="line">如上我们的新需求是：default命名空间下所有pod可以互相访问，但不能访问其他命名空间Pod，其他命名空间也不能访问default命名空间Pod。</span><br><span class="line"></span><br><span class="line"># kubectl run client3 --generator=run-pod/v1 -n kube-system --image=busybox --command -- sleep 36000</span><br><span class="line">pod/client3 created     ##在kube-system环境中创建一个pod</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it client3 sh -n kube-system</span><br><span class="line">/ # ping 10.244.36.73       ##也是可以访问default空间的pod</span><br><span class="line">PING 10.244.36.73 (10.244.36.73): 56 data bytes</span><br><span class="line">64 bytes from 10.244.36.73: seq=0 ttl=62 time=1.394 ms</span><br><span class="line">64 bytes from 10.244.36.73: seq=1 ttl=62 time=0.425 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.244.36.73 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.425/0.909/1.394 ms</span><br><span class="line"></span><br><span class="line">##应用如上规则：</span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f ns22.yaml</span><br><span class="line">networkpolicy.networking.k8s.io/deny-from-other-namespaces created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it client3 sh -n kube-system</span><br><span class="line">/ # ping 10.244.36.73       ##再次访问就不可以了！！</span><br><span class="line">PING 10.244.36.73 (10.244.36.73): 56 data bytes</span><br><span class="line">^C</span><br><span class="line">--- 10.244.36.73 ping statistics ---</span><br><span class="line">3 packets transmitted, 0 packets received, 100% packet loss</span><br></pre></td></tr></table></figure><h4 id="文末彩蛋："><a href="#文末彩蛋：" class="headerlink" title="文末彩蛋："></a>文末彩蛋：</h4><p>来讲下我们目前公司的网路架构吧：</p><ul><li>calico（BGP）一个集群支持上千个节点问题不大；</li><li>将所有机房的网路打通；</li><li>3台路由反射器，对接每个节点的网关路由器进行BGP模式下的路由交换，这样就会将整个的pod_ip全部暴露在网路中；</li><li>LVS对pod做负载均衡；</li></ul><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8s Ingress Nginx使用</title>
    <link href="/2019/12/18/K8s%20Ingress%20Nginx%E4%BD%BF%E7%94%A8/"/>
    <url>/2019/12/18/K8s%20Ingress%20Nginx%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h4 id="案例1（基本转发，https配置与annotations基础使用）"><a href="#案例1（基本转发，https配置与annotations基础使用）" class="headerlink" title="案例1（基本转发，https配置与annotations基础使用）"></a>案例1（基本转发，https配置与annotations基础使用）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">  namespace: test</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - nginx-a.gogen.cn</span><br><span class="line">    secretName: gogen.cn</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx-a.gogen.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-a</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /.*.(txt|css|doc)</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-b</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /(api|app)/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-c</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /api</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-d</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure><p>上面我们定义了一个ingress，并指定运行在test名称空间（此名名称空间需要自行创建）。后端我们定义了四组服务，分别为：nginx-a，nginx-b，nginx-c和nginx-d，并指定服务的port为80（这四组服务也需要自行定义）。</p><p>然后我们ingress的主要配置里面我们定义了tls证书，并指定可使用的host和需要使用的secret。我们是将证书先导入进secret，然后直接引用secret，导入方法如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret tls gogen.cn --cert=1592339__gogen.cn.pem --key=1592339__gogen.cn.key -n test</span><br></pre></td></tr></table></figure></p><p>tls详解：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tls:</span><br><span class="line">- hosts:                        #此为固定项，是一个列表，我们可以有另外的证书对应其它域名</span><br><span class="line">  - nginx-a.gogen.cn            #此为列表，必须为一个域名，一个secret可以对多个域名</span><br><span class="line">  secretName: gogen.cn          #创建secret时指定的名称</span><br></pre></td></tr></table></figure></p><p>annotations配置：作用于server<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 指定了我们使用后端ingress controller的类别，如果后端有多个ingress controller的时候很重要</span><br><span class="line">kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line"> </span><br><span class="line"># 指定我们的rules的path可以使用正则表达式，如果我们没有使用正则表达式，此项则可不使用</span><br><span class="line">nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;</span><br></pre></td></tr></table></figure></p><p>rules配置：作用于location<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rules:</span><br><span class="line">- host: nginx-a.gogen.cn            #相当于定义了nginx的一个server_name</span><br><span class="line">  http:</span><br><span class="line">    paths:</span><br><span class="line">    - path: /                       #一个path就相当于一个location，path的值必须为“/”。这里为匹配的规则，根表示默认请求转发规则</span><br><span class="line">      backend:</span><br><span class="line">        serviceName: nginx-a        #定义后端的service</span><br><span class="line">        servicePort: 80             #定义后端service的访问端口，也就是service port指定的端口</span><br><span class="line">    - path: /.*.(txt|css|doc)        #这里使用的正则（低版本不支持）,默认情况下都是不区分大小写，可以进入到ingress controller查看nginx的配置,这里相当于把结尾为txt,css,doc的url请求转发到nginx-b service</span><br><span class="line">      backend:</span><br><span class="line">        serviceName: nginx-b</span><br><span class="line">        servicePort: 80</span><br><span class="line">    - path: /(api|app)/              #这里相当于将api和app开头的目录语法转发至nginx-c service</span><br><span class="line">      backend:</span><br><span class="line">        serviceName: nginx-c</span><br><span class="line">        servicePort: 80</span><br><span class="line">    - path: /api                    #这里相当于将api开头的url（可以是一个文件，也可以是一个目录）的请求，转发到nginx-d</span><br><span class="line">      backend:</span><br><span class="line">        serviceName: nginx-d</span><br><span class="line">        servicePort: 80</span><br></pre></td></tr></table></figure></p><p><strong>说明</strong>：<u>上面定义的所有path到ingress controller都将会转换成nginx location规则，那么关于location的优先级与nginx一样。path转换到nginx后，会将path规则最长的排在最前面，最短的排在最后面。</u></p><hr><h4 id="案例2（通过annotations对nginx做个性化配置）"><a href="#案例2（通过annotations对nginx做个性化配置）" class="headerlink" title="案例2（通过annotations对nginx做个性化配置）"></a>案例2（通过annotations对nginx做个性化配置）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">  namespace: test</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;600&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/proxy-body-size: &quot;10m&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - nginx-a.gogen.cn</span><br><span class="line">    secretName: gogen.cn</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx-a.gogen.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-a</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /.*.(txt|css|doc)</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-b</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /(api|app)/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-c</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /api</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-d</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure><p>在案例1的基础上面我们增加了annotations的一些配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;</span><br><span class="line"> </span><br><span class="line">#连接超时时间，默认为5s</span><br><span class="line">nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;600&quot;</span><br><span class="line"> </span><br><span class="line">#后端服务器回转数据超时时间，默认为60s</span><br><span class="line">nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot;</span><br><span class="line"> </span><br><span class="line">#后端服务器响应超时时间，默认为60s</span><br><span class="line">nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot;</span><br><span class="line"> </span><br><span class="line">#客户端上传文件，最大大小，默认为20m</span><br><span class="line">nginx.ingress.kubernetes.io/proxy-body-size: &quot;10m&quot;</span><br></pre></td></tr></table></figure></p><p>上面我们自定义了四项基本配置，我们还可以定义更多的基本配置，可参考nginx-configuration annotations相关文档</p><hr><h4 id="案例3（通过annotations做rewrite基本配置）"><a href="#案例3（通过annotations做rewrite基本配置）" class="headerlink" title="案例3（通过annotations做rewrite基本配置）"></a>案例3（通过annotations做rewrite基本配置）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-rewrite-tfs</span><br><span class="line">  namespace: test</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/rewrite-target: https://gogen-test.oss-cn-hangzhou.aliyuncs.com</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - nginx-a.gogen.cn</span><br><span class="line">    secretName: gogen.cn</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx-a.gogen.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /v1/tfs</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-a</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure><p>上面的方法也是官方的方法：</p><ul><li>使用“nginx.ingress.kubernetes.io/rewrite-target”来定义。</li></ul><p>通过上面的的定义代表如果访问<a href="https://nginx-a.gogen.cn/v1/tfs/(.*)，则会rewrite为https://gogen-test.oss-cn-hangzhou.aliyuncs.com/$1，如果有多个path，每个都会被rewrite，所以如果只需要替换单个path（也就是location）的时候单独使用个manifests写。" target="_blank" rel="noopener">https://nginx-a.gogen.cn/v1/tfs/(.*)，则会rewrite为https://gogen-test.oss-cn-hangzhou.aliyuncs.com/$1，如果有多个path，每个都会被rewrite，所以如果只需要替换单个path（也就是location）的时候单独使用个manifests写。</a></p><hr><h4 id="案例4（介绍-nginx-ingress-kubernetes-io-server-snippet-使用方法）"><a href="#案例4（介绍-nginx-ingress-kubernetes-io-server-snippet-使用方法）" class="headerlink" title="案例4（介绍 nginx.ingress.kubernetes.io/server-snippet 使用方法）"></a>案例4（介绍 nginx.ingress.kubernetes.io/server-snippet 使用方法）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">  namespace: test</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/server-snippet: |</span><br><span class="line">        if ($uri ~* &quot;/v1/tfs/.*&quot;) &#123;</span><br><span class="line">            rewrite ^/v1/tfs/(.*)$ https://gogen-test.oss-cn-hangzhou.aliyuncs.com/$1 permanent;</span><br><span class="line">        &#125;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - nginx-a.gogen.cn</span><br><span class="line">    secretName: gogen.cn</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx-a.gogen.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-a</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /.*.(txt|css|doc)</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-b</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /(api|app)/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-c</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /api</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nginx-d</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure><p>这里直接使用了“nginx.ingress.kubernetes.io/server-snippet”来指定配置，这里可以直接写nginx的配置，通过这里可以不止是实现rewrite重写，还可以实现更多的功能需求，只要是作用于server的都可以。</p><hr><h4 id="案例5（使用configMap做更多个性化配置）"><a href="#案例5（使用configMap做更多个性化配置）" class="headerlink" title="案例5（使用configMap做更多个性化配置）"></a>案例5（使用configMap做更多个性化配置）</h4><p>有些时候我们使用annotations并不能全完实现我们nginx灵活个性化配置，这时候就需要使用借助configMap配置。官方configMap使用文档，annotations与configMap对照关系表。</p><p>首先创建一个configMap文件，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  use-http2: &quot;false&quot;</span><br><span class="line">  ssl-protocols: &quot;TLSv1 TLSv1.1 TLSv1.2&quot;</span><br><span class="line">  ssl-ciphers: &quot;HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM&quot;</span><br></pre></td></tr></table></figure></p><p>data里面内容参考上面给出的【官方configMap使用文档】，metadata内的 name和namespace不可随意写，需要参考nginx-ingress-controller YAML配置文件的配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">containers:</span><br><span class="line">- args:</span><br><span class="line">  - /nginx-ingress-controller</span><br><span class="line">  - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">  - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">  - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">  - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">  - --publish-service=$(POD_NAMESPACE)/nginx-ingress-lb</span><br><span class="line">  - --v=2</span><br></pre></td></tr></table></figure></p><p>参考”- –configmap=$(POD_NAMESPACE)/nginx-configuration)“，其中configmap的名称空间需要与nginx-ingress-controller在同一个名称空间，名称为”/“后面的名称</p><p>完成配置后apply配置清单，通过configMap配置的配置apply无法直接生效，需要重启pods，最简单的方法使用edit编辑nginx-ingress-controller的controller，改个不影响pods运行的参数，来触发pods升级，从而让我们的配置生效，如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">livenessProbe:</span><br><span class="line">  failureThreshold: 3</span><br><span class="line">  httpGet:</span><br><span class="line">    path: /healthz</span><br><span class="line">    port: 10254</span><br><span class="line">    scheme: HTTP</span><br><span class="line">  initialDelaySeconds: 10</span><br><span class="line">  periodSeconds: 10</span><br><span class="line">  successThreshold: 1</span><br><span class="line">  timeoutSeconds: 1</span><br></pre></td></tr></table></figure></p><p>可以将”initialDelaySeconds“改为11或者其它的，该值的含义为pod启动多少时候后开始执行健康检查，单位为秒。</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s二进制安装环境下证书过期问题</title>
    <link href="/2019/12/17/k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/"/>
    <url>/2019/12/17/k8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>k8s配置信息的工作目录一般为/etc/kubernetes，证书目录一般为/etc/kubernetes/ssl</p><h3 id="一、重新生成证书"><a href="#一、重新生成证书" class="headerlink" title="一、重新生成证书"></a>一、重新生成证书</h3><p>当你的kubernetes报错：certificate has expired or is not yet valid，可以通过命令：openssl x509 -in [证书全路径] -noout -text查看证书详情。</p><h4 id="1、备份"><a href="#1、备份" class="headerlink" title="1、备份"></a>1、备份</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhdya]# cd /etc/kubernetes;</span><br><span class="line">[root@zhdya]# cp kubelet.kubeconfig kubelet.kubeconfig.bak;</span><br><span class="line">[root@zhdya]# mkdir sslbak &amp;&amp; cp ssl/ sslbak;</span><br></pre></td></tr></table></figure><h4 id="2、清理"><a href="#2、清理" class="headerlink" title="2、清理"></a>2、清理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhdya]# rm -f kubelet.kubeconfig;</span><br><span class="line">[root@zhdya]# rm ssl/kubelet.*;</span><br></pre></td></tr></table></figure><h4 id="3-重启kubelete"><a href="#3-重启kubelete" class="headerlink" title="3. 重启kubelete"></a>3. 重启kubelete</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhdya]# systemctl  restart kubelet &amp;&amp; systemctl  status  kubelet</span><br></pre></td></tr></table></figure><h4 id="4-使用CSR重新生产证书"><a href="#4-使用CSR重新生产证书" class="headerlink" title="4. 使用CSR重新生产证书"></a>4. 使用CSR重新生产证书</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhdya]# kubectl get csr</span><br><span class="line">NAME                        AGE       REQUESTOR           CONDITION</span><br><span class="line">node-csr-NsKZSz-myrv   11s       kubelet-bootstrap   Pending</span><br><span class="line"></span><br><span class="line">[root@zhdya]# kubectl certificate approve node-csr-NsKZSz-myrv</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-NsKZSz-myrv approved</span><br></pre></td></tr></table></figure><p>备注：CSR授权完成后，kubelete会自动发起CSR请求更新证书。</p><h4 id="5、查看集群状态"><a href="#5、查看集群状态" class="headerlink" title="5、查看集群状态"></a>5、查看集群状态</h4><p>大概等待10秒，运行如下命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@zhdya]# kubectl get node</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Helm应用包管理器（上）</title>
    <link href="/2019/12/16/Helm%E5%BA%94%E7%94%A8%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <url>/2019/12/16/Helm%E5%BA%94%E7%94%A8%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="三、Helm应用包管理器"><a href="#三、Helm应用包管理器" class="headerlink" title="三、Helm应用包管理器"></a>三、Helm应用包管理器</h3><h4 id="3-1-为什么需要Helm？"><a href="#3-1-为什么需要Helm？" class="headerlink" title="3.1 为什么需要Helm？"></a>3.1 为什么需要Helm？</h4><p>K8S上的应用对象，都是由特定的资源描述组成，包括deployment、service等。都保存各自文件中或者集中写到一个配置文件。然后kubectl apply –f 部署。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/yaml-all.png" srcset="/img/loading.gif" alt=""></p><p>如果应用只由一个或几个这样的服务组成，上面部署方式足够了。</p><p>而对于一个复杂的应用，会有很多类似上面的资源描述文件，例如微服务架构应用，组成应用的服务可能多达十个，几十个。如果有更新或回滚应用的需求，可能要修改和维护所涉及的大量资源文件，而这种组织和管理应用的方式就显得力不从心了。</p><p>且由于缺少对发布过的应用版本管理和控制，使Kubernetes上的应用维护和更新等面临诸多的挑战，主要面临以下问题：</p><ol><li><strong>如何将这些服务作为一个整体管理</strong></li><li><strong>这些资源文件如何高效复用</strong></li><li><strong>不支持应用级别的版本管理</strong></li></ol><h4 id="3-2-Helm-介绍"><a href="#3-2-Helm-介绍" class="headerlink" title="3.2 Helm 介绍"></a>3.2 Helm 介绍</h4><p>Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。</p><p>Helm有两个重要概念：</p><ul><li><p><strong>helm</strong>：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。</p></li><li><p><strong>Chart</strong>：应用描述，一系列用于描述 k8s 资源相关文件的集合。</p></li><li><strong>Release</strong>：基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在k8s中创建出真实运行的资源对象。</li></ul><h4 id="3-3-Helm-v3-变化"><a href="#3-3-Helm-v3-变化" class="headerlink" title="3.3 Helm v3 变化"></a>3.3 Helm v3 变化</h4><p><strong>2019年11月13日，</strong> Helm团队发布 <code>Helm v3</code>的第一个稳定版本。</p><p><strong>该版本主要变化如下：</strong></p><h5 id="1、-架构变化"><a href="#1、-架构变化" class="headerlink" title="1、 架构变化"></a>1、 架构变化</h5><p><strong>最明显的变化是 <code>Tiller</code>的删除</strong></p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/helm-arch.png" srcset="/img/loading.gif" alt=""></p><h5 id="2、Release名称可以在不同命名空间重用"><a href="#2、Release名称可以在不同命名空间重用" class="headerlink" title="2、Release名称可以在不同命名空间重用"></a>2、<code>Release</code>名称可以在不同命名空间重用</h5><h5 id="3、支持将-Chart-推送至-Docker-镜像仓库中"><a href="#3、支持将-Chart-推送至-Docker-镜像仓库中" class="headerlink" title="3、支持将 Chart 推送至 Docker 镜像仓库中"></a>3、支持将 Chart 推送至 Docker 镜像仓库中</h5><h5 id="4、使用JSONSchema验证chart-values"><a href="#4、使用JSONSchema验证chart-values" class="headerlink" title="4、使用JSONSchema验证chart values"></a>4、使用JSONSchema验证chart values</h5><h5 id="5、其他"><a href="#5、其他" class="headerlink" title="5、其他"></a>5、其他</h5><p>1）为了更好地协调其他包管理者的措辞 <code>Helm CLI</code>个别更名</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm delete` 更名为 `helm uninstall</span><br><span class="line">helm inspect` 更名为 `helm show</span><br><span class="line">helm fetch` 更名为 `helm pull</span><br></pre></td></tr></table></figure><p>但以上旧的命令当前仍能使用。</p><p>2）移除了用于本地临时搭建 <code>Chart Repository</code>的 <code>helm serve</code> 命令。</p><p>3）自动创建名称空间</p><p>在不存在的命名空间中创建发行版时，Helm 2创建了命名空间。Helm 3遵循其他Kubernetes对象的行为，如果命名空间不存在则返回错误。</p><p>4） 不再需要<code>requirements.yaml</code>, 依赖关系是直接在<code>chart.yaml</code>中定义。 </p><h4 id="3-4-Helm客户端"><a href="#3-4-Helm客户端" class="headerlink" title="3.4 Helm客户端"></a>3.4 Helm客户端</h4><h5 id="1、部署Helm客户端"><a href="#1、部署Helm客户端" class="headerlink" title="1、部署Helm客户端"></a>1、部署Helm客户端</h5><p>Helm客户端下载地址：<a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">https://github.com/helm/helm/releases</a></p><p>解压移动到/usr/bin/目录即可。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz</span><br><span class="line">tar zxvf helm-v3.0.0-linux-amd64.tar.gz </span><br><span class="line">mv linux-amd64/helm /usr/bin/</span><br></pre></td></tr></table></figure><h5 id="2、Helm常用命令"><a href="#2、Helm常用命令" class="headerlink" title="2、Helm常用命令"></a>2、Helm常用命令</h5><table><thead><tr><th><strong>命令</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>create</td><td>创建一个chart并指定名字</td></tr><tr><td>dependency</td><td>管理chart依赖</td></tr><tr><td>get</td><td>下载一个release。可用子命令：all、hooks、manifest、notes、values</td></tr><tr><td>history</td><td>获取release历史</td></tr><tr><td>install</td><td>安装一个chart</td></tr><tr><td>list</td><td>列出release</td></tr><tr><td>package</td><td>将chart目录打包到chart存档文件中</td></tr><tr><td>pull</td><td>从远程仓库中下载chart并解压到本地  # helm pull stable/mysql –untar</td></tr><tr><td>repo</td><td>添加，列出，移除，更新和索引chart仓库。可用子命令：add、index、list、remove、update</td></tr><tr><td>rollback</td><td>从之前版本回滚</td></tr><tr><td>search</td><td>根据关键字搜索chart。可用子命令：hub、repo</td></tr><tr><td>show</td><td>查看chart详细信息。可用子命令：all、chart、readme、values</td></tr><tr><td>status</td><td>显示已命名版本的状态</td></tr><tr><td>template</td><td>本地呈现模板</td></tr><tr><td>uninstall</td><td>卸载一个release</td></tr><tr><td>upgrade</td><td>更新一个release</td></tr><tr><td>version</td><td>查看helm客户端版本</td></tr></tbody></table><h5 id="3、配置国内Chart仓库"><a href="#3、配置国内Chart仓库" class="headerlink" title="3、配置国内Chart仓库"></a>3、配置国内Chart仓库</h5><ul><li>微软仓库（<a href="http://mirror.azure.cn/kubernetes/charts/）这个仓库强烈推荐，基本上官网有的chart这里都有。" target="_blank" rel="noopener">http://mirror.azure.cn/kubernetes/charts/）这个仓库强烈推荐，基本上官网有的chart这里都有。</a></li><li>阿里云仓库（<a href="https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts" target="_blank" rel="noopener">https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</a>  ）</li><li>官方仓库（<a href="https://hub.kubeapps.com/charts/incubator）官方chart仓库，国内有点不好使。" target="_blank" rel="noopener">https://hub.kubeapps.com/charts/incubator）官方chart仓库，国内有点不好使。</a></li></ul><p>添加存储库：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts </span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><p>查看配置的存储库：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm repo list</span><br><span class="line">helm search repo stable</span><br></pre></td></tr></table></figure><p>一直在stable存储库中安装charts，你可以配置其他存储库。</p><p>删除存储库：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm repo remove aliyun</span><br></pre></td></tr></table></figure><h4 id="3-5-Helm基本使用"><a href="#3-5-Helm基本使用" class="headerlink" title="3.5 Helm基本使用"></a>3.5 Helm基本使用</h4><p>主要介绍三个命令：</p><ul><li><p>chart install</p></li><li><p>chart update</p></li><li><p>chart rollback</p></li></ul><h5 id="1、使用chart部署一个应用"><a href="#1、使用chart部署一个应用" class="headerlink" title="1、使用chart部署一个应用"></a>1、使用chart部署一个应用</h5><p>查找chart：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm search repo</span><br><span class="line"># helm search repo mysql</span><br></pre></td></tr></table></figure><p>为什么mariadb也在列表中？因为他和mysql有关。</p><p>查看chart信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm show chart stable/mysql</span><br></pre></td></tr></table></figure><p>安装包：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm install mysql(自定义name) stable/mysql</span><br></pre></td></tr></table></figure><p>查看发布状态：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm status mysql</span><br></pre></td></tr></table></figure><p>查看安装的应用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# helm list</span><br><span class="line">NAME NAMESPACEREVISIONUPDATED                                STATUS  CHART      APP VERSION</span><br><span class="line">mysqldefault  1       2019-12-14 13:25:51.506486337 +0800 CSTdeployedmysql-1.6.15.7.27</span><br></pre></td></tr></table></figure><h5 id="2、安装前自定义chart配置选项"><a href="#2、安装前自定义chart配置选项" class="headerlink" title="2、安装前自定义chart配置选项"></a>2、安装前自定义chart配置选项</h5><p>上面部署的mysql并没有成功，这是因为并不是所有的chart都能按照默认配置运行成功，且helm仅仅只是一个包管理器，并不能判断可能会需要一些环境依赖，例如有状态应用的PV。</p><p>所以我们需要自定义chart配置选项，安装过程中有两种方法可以传递配置数据：</p><ul><li>–values（或-f）：指定带有覆盖的YAML文件。这可以多次指定，最右边的文件优先</li><li>–set：在命令行上指定替代。如果两者都用，–set优先级高</li></ul><p>–values使用，先将修改的变量写到一个文件中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm show values stable/mysql</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get pvc</span><br><span class="line">NAME    STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql   Pending                                                     6m34s</span><br><span class="line"></span><br><span class="line">通过如上命令我们可以看到mysql已经自动创建了一个pvc，pod没有正常启动就是因为没有pv与之绑定！</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# cat pv.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv0003</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 8Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  nfs:</span><br><span class="line">    path: /opt/sharedata/mysql/</span><br><span class="line">    server: 192.168.171.12</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f pv.yaml</span><br><span class="line">persistentvolume/pv0003 created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get pv,pvc    ##已经绑定了mysql</span><br><span class="line">NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                      STORAGECLASS          REASON   AGE</span><br><span class="line">persistentvolume/pv0003                                     8Gi        RWO            Retain           Bound    default/mysql                   14s</span><br><span class="line">NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim/mysql   Bound    pv0003   8Gi        RWO                           18m</span><br><span class="line"></span><br><span class="line">再次查看下是否已经启动：</span><br><span class="line">[root@k8s-master1 ~]# kubectl get pod</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running   1          38h</span><br><span class="line">metrics-app-7674cfb699-btch5             1/1     Running   1          38h</span><br><span class="line">mysql-8558fcfbb4-98wzl                   0/1     Running   0          18m</span><br><span class="line">nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running   2          37h</span><br><span class="line"></span><br><span class="line">然后就可以登录到容器的mysql中；</span><br><span class="line">## 但是有时候我们不需要mysql默认的一些配置，我们需要自定义的配置一些简单的参数随着容器一起启动：</span><br><span class="line"># cat config.yaml </span><br><span class="line">persistence:</span><br><span class="line">  enabled: true</span><br><span class="line">  storageClass: &quot;managed-nfs-storage&quot;   ##pv的自动供给 通过kubectl get sc 获取</span><br><span class="line">  accessMode: ReadWriteOnce</span><br><span class="line">  size: 8Gi     ##容量</span><br><span class="line">mysqlUser: &quot;k8s&quot;    ##创建个k8s的用户</span><br><span class="line">mysqlPassword: &quot;123456&quot;     ##k8s的密码</span><br><span class="line">mysqlDatabase: &quot;k8s&quot;        ##创建个k8s的database</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# helm install mysql2 -f mysql2conf.yaml stable/mysql</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">mysql-8558fcfbb4-98wzl                   1/1     Running   0          3h32m</span><br><span class="line">mysql2-7899bb48c9-w85gf                  1/1     Running   0          47s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it mysql2-7899bb48c9-w85gf bash</span><br><span class="line">root@mysql2-7899bb48c9-w85gf:/# mysql -uk8s -p123456</span><br><span class="line">Enter password:</span><br><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| k8s                |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">5 rows in set (0.02 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select user from mysql.user;</span><br><span class="line">+------+</span><br><span class="line">| user |</span><br><span class="line">+------+</span><br><span class="line">| k8s  |</span><br><span class="line">| root |</span><br><span class="line">+------+</span><br><span class="line">2 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure><p>以上将创建具有名称的默认MySQL用户k8s，并授予此用户访问新创建的k8s数据库的权限，但将接受该图表的所有其余默认值。</p><h4 id="命令行替代变量："><a href="#命令行替代变量：" class="headerlink" title="命令行替代变量："></a>命令行替代变量：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm install mysql3 --set persistence.storageClass=&quot;managed-nfs-storage&quot; stable/mysql</span><br></pre></td></tr></table></figure><p>也可以把chart包下载下来查看详情：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm pull stable/mysql --untar</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# cd mysql</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 mysql]# ls</span><br><span class="line">Chart.yaml  README.md  templates  values.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 mysql]# cd templates/</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 templates]# ls</span><br><span class="line">configurationFiles-configmap.yaml  _helpers.tpl                        NOTES.txt  secrets.yaml         servicemonitor.yaml  tests</span><br><span class="line">deployment.yaml                    initializationFiles-configmap.yaml  pvc.yaml   serviceaccount.yaml  svc.yaml</span><br></pre></td></tr></table></figure><hr><h4 id="values-yaml与set使用："><a href="#values-yaml与set使用：" class="headerlink" title="values yaml与set使用："></a>values yaml与set使用：</h4><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/yaml-set.png" srcset="/img/loading.gif" alt=""></p><p><strong>该helm install命令可以从多个来源安装：</strong></p><ul><li>chart存储库</li><li>本地chart存档（helm install foo-0.1.1.tgz）</li><li>chart目录（helm install path/to/foo）</li><li>完整的URL（helm install <a href="https://example.com/charts/foo-1.2.3.tgz）" target="_blank" rel="noopener">https://example.com/charts/foo-1.2.3.tgz）</a></li></ul><h5 id="3、构建一个Helm-Chart"><a href="#3、构建一个Helm-Chart" class="headerlink" title="3、构建一个Helm Chart"></a>3、构建一个Helm Chart</h5><p>创建目录和各个文件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm create mychart</span><br><span class="line">Creating mychart</span><br><span class="line"></span><br><span class="line"># tree mychart/</span><br><span class="line">mychart/</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br></pre></td></tr></table></figure><ul><li>Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。</li><li>values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。</li><li>Templates： 目录里面存放所有yaml模板文件。</li><li>charts：目录里存放这个chart依赖的所有子chart。</li><li>NOTES.txt ：用于介绍Chart帮助信息， helm install 部署后展示给用户。例如：如何使用这个 Chart、列出缺省的设置等。</li><li>_helpers.tpl：放置模板助手的地方，可以在整个 chart 中重复使用</li></ul><h4 id="接下来我来手动创建一个简单的helm应用包："><a href="#接下来我来手动创建一个简单的helm应用包：" class="headerlink" title="接下来我来手动创建一个简单的helm应用包："></a>接下来我来手动创建一个简单的helm应用包：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# helm create mytestchart</span><br><span class="line">Creating mytestchart</span><br><span class="line"></span><br><span class="line">##修改后剩余：</span><br><span class="line">[root@k8s-master1 ~]# tree mytestchart/</span><br><span class="line">mytestchart/</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── README.md</span><br><span class="line">├── templates</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 templates]# kubectl create deployment zhdyaweb --image=nginx:1.17 --dry-run -o yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: zhdyaweb</span><br><span class="line">  name: zhdyaweb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zhdyaweb</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: zhdyaweb</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.17</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 templates]# kubectl create deployment helloWeb --image=nginx:1.17 --dry-run -o yaml &gt; deployment.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 templates]# cat deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Values.name &#125;&#125;</span><br><span class="line">spec:</span><br><span class="line">  replicas: &#123;&#123; .Values.replicas &#125;&#125;</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: &#123;&#123; .Values.replicas &#125;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: &#123;&#123; .Values.replicas &#125;&#125;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag &#125;&#125;</span><br><span class="line">        name: nginx</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 mytestchart]# cat values.yaml</span><br><span class="line">name: zhdyaweb</span><br><span class="line">replicas: 3</span><br><span class="line">image: nginx</span><br><span class="line">imageTag: 1.17</span><br><span class="line"></span><br><span class="line">##如下成功安装</span><br><span class="line">[root@k8s-master1 ~]# helm install zhdyaweb zhdyacc/</span><br><span class="line">NAME: zhdyaweb</span><br><span class="line">LAST DEPLOYED: Sat Dec 14 18:53:32 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">zhdyacc-6bd459478b-5lvzd                 1/1     Running   0          2m3s</span><br><span class="line">zhdyacc-6bd459478b-cpv8g                 1/1     Running   0          2m3s</span><br><span class="line">zhdyacc-6bd459478b-qbqwt                 1/1     Running   0          2m3s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# helm list     ##查看安装的服务</span><br><span class="line">NAME    NAMESPACEREVISIONUPDATED                                STATUS  CHART        APP VERSION</span><br><span class="line">mysql2  default  1       2019-12-14 16:57:16.387584786 +0800 CSTdeployedmysql-1.6.1  5.7.27</span><br><span class="line">zhdyawebdefault  1       2019-12-14 18:53:32.545438231 +0800 CSTdeployedzhdyacc-0.1.01.16.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# helm get manifest zhdyaweb        ##查看渲染后的yaml</span><br><span class="line">---</span><br><span class="line"># Source: zhdyacc/templates/deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: zhdyacc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zhdyacc</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: zhdyacc</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.17</span><br><span class="line">        name: nginx</span><br></pre></td></tr></table></figure><p>也可以打包推送的charts仓库共享别人使用。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm package mychart/</span><br><span class="line">mychart-0.1.0.tgz</span><br></pre></td></tr></table></figure><h5 id="4、升级、回滚和删除"><a href="#4、升级、回滚和删除" class="headerlink" title="4、升级、回滚和删除"></a>4、升级、回滚和删除</h5><p>发布新版本的chart时，或者当您要更改发布的配置时，可以使用该<code>helm upgrade</code> 命令。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##升级nginx版本为1.16</span><br><span class="line">[root@k8s-master1 ~]# helm upgrade --set imageTag=1.16 zhdyaweb zhdyacc/</span><br><span class="line">Release &quot;zhdyaweb&quot; has been upgraded. Happy Helming!</span><br><span class="line">NAME: zhdyaweb</span><br><span class="line">LAST DEPLOYED: Sat Dec 14 19:04:22 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 2</span><br><span class="line">TEST SUITE: None</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# helm get manifest zhdyaweb</span><br><span class="line">---</span><br><span class="line"># Source: zhdyacc/templates/deployment.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: zhdyacc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zhdyacc</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: zhdyacc</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.16       ##版本已经更改</span><br><span class="line">        name: nginx</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# curl 10.244.2.26 -I</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.16.1</span><br><span class="line"></span><br><span class="line">##或者另外一种方法：</span><br><span class="line"># helm upgrade -f values.yaml zhdyaweb zhdyacc/</span><br></pre></td></tr></table></figure><h5 id="回滚："><a href="#回滚：" class="headerlink" title="回滚："></a>回滚：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# helm rollback zhdyaweb</span><br><span class="line">Rollback was a success! Happy Helming!</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running   1          44h</span><br><span class="line">metrics-app-7674cfb699-btch5             1/1     Running   1          44h</span><br><span class="line">mysql2-7899bb48c9-w85gf                  1/1     Running   0          130m</span><br><span class="line">nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running   2          43h</span><br><span class="line">zhdyacc-6bd459478b-9mcpt                 1/1     Running   0          9s</span><br><span class="line">zhdyacc-6bd459478b-9ntcg                 1/1     Running   0          6s</span><br><span class="line">zhdyacc-6bd459478b-jmjlh                 1/1     Running   0          7s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po -o wide</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running   1          44h    10.244.1.19   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-btch5             1/1     Running   1          44h    10.244.2.21   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">mysql2-7899bb48c9-w85gf                  1/1     Running   0          130m   10.244.0.22   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running   2          43h    10.244.2.22   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zhdyacc-6bd459478b-9mcpt                 1/1     Running   0          11s    10.244.0.25   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zhdyacc-6bd459478b-9ntcg                 1/1     Running   0          8s     10.244.3.19   k8s-node3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zhdyacc-6bd459478b-jmjlh                 1/1     Running   0          9s     10.244.2.27   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# curl 10.244.2.27 -I</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.17.6</span><br><span class="line">Date: Sat, 14 Dec 2019 11:08:34 GMT</span><br><span class="line"></span><br><span class="line">如上我们看到再次回滚到了1.17的版本！！</span><br></pre></td></tr></table></figure><p>如果在发布后没有达到预期的效果，则可以使用<code>helm rollback</code>回滚到之前的版本。</p><p>例如将应用回滚到第一个版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm rollback web 2</span><br></pre></td></tr></table></figure><p>卸载发行版，请使用以下<code>helm uninstall</code>命令：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm uninstall web</span><br></pre></td></tr></table></figure><p>查看历史版本配置信息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm get --revision 1 web</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次java_MetaspaceSize设置不当导致的问题</title>
    <link href="/2019/12/15/Java%E5%8F%82%E6%95%B0%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/"/>
    <url>/2019/12/15/Java%E5%8F%82%E6%95%B0%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h3 id="先来了解下："><a href="#先来了解下：" class="headerlink" title="先来了解下："></a>先来了解下：</h3><p><img src="http://myimage.okay686.cn/okay686cn/20191217/5zSS59967ry9.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="内存的分配及回收–堆中的新生代和老年代："><a href="#内存的分配及回收–堆中的新生代和老年代：" class="headerlink" title="内存的分配及回收–堆中的新生代和老年代："></a>内存的分配及回收–堆中的新生代和老年代：</h4><p>java堆被分为两部分， 一部分被称为新生代， 一部分称为老年代， 他们的<strong>比例通常为 1：2</strong></p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/pWuA7RQLJGVA.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="一、堆（新生代、老年代）与元空间。"><a href="#一、堆（新生代、老年代）与元空间。" class="headerlink" title="一、堆（新生代、老年代）与元空间。"></a>一、堆（新生代、老年代）与元空间。</h3><h4 id="1-1、新生代："><a href="#1-1、新生代：" class="headerlink" title="1.1、新生代："></a>1.1、新生代：</h4><p>主要是用来存放新生的对象。一般占据堆空间的1/3，由于频繁创建对象，所以新生代会频繁触发MinorGC进行垃圾回收。<br>新生代分为<strong>Eden</strong>区、<strong>ServivorFrom</strong>、<strong>ServivorTo</strong>三个区，==比例为 8：1：1==。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191217/aSadpXMNvKXL.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>Eden区</strong>：Java新对象的出生地(如果新创建的对象占用内存很大则直接分配给老年代)。</p><h5 id="1-1-1、MinorGC-触发机制："><a href="#1-1-1、MinorGC-触发机制：" class="headerlink" title="1.1.1、MinorGC 触发机制："></a>1.1.1、MinorGC 触发机制：</h5><ul><li>当Eden区内存不够的时候就会触发一次MinorGc，对新生代区进行一次垃圾回收。</li></ul><p><strong>ServivorTo</strong>：保留了一次MinorGc过程中的幸存者。</p><p><strong>ServivorFrom</strong>: 上一次GC的幸存者，作为这一次GC的被扫描者。<br>当JVM无法为新建对象分配内存空间的时候(Eden区满的时候)，JVM触发MinorGc。因此新生代空间占用越低，MinorGc越频繁。</p><p><u>每次对象会存在于Eden区和一个Survivor区， 当内存不够时，触发GC，然后把依然存活的对象复制到另一个空白的Survivor区， 然后直接清空其余新生代内存，然后如此循环。总有一个Survivor区是空白的。每进行一次GC，存活对象的年龄+1， 默认情况下，对象年龄达到15时，就会移动到老年代中。</u></p><p><u>默认的，Edem : from : to = 8 :1 : 1 ( 可以通过参数–XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。</u></p><h4 id="1-2、老年代："><a href="#1-2、老年代：" class="headerlink" title="1.2、老年代："></a>1.2、老年代：</h4><p>老年代的对象比较稳定，所以MajorGC不会频繁执行。</p><h5 id="1-2-1、触发MinorGC的条件："><a href="#1-2-1、触发MinorGC的条件：" class="headerlink" title="1.2.1、触发MinorGC的条件："></a>1.2.1、触发MinorGC的条件：</h5><ul><li><p>在进行MajorGC之前，一般都先进行了一次MinorGC，使得有新生代的对象进入老年代，当老年代空间不足时就会触发MajorGC。</p></li><li><p>当无法找到足够大的连续空间分配给新创建的较大对象时，也会触发MajorGC进行垃圾回收腾出空间。</p></li></ul><p><strong><em>当老年代也满了装不下的时候，就会抛出OOM。</em></strong></p><h4 id="1-3、元空间："><a href="#1-3、元空间：" class="headerlink" title="1.3、元空间："></a>1.3、元空间：</h4><p>JVM 使用本地内存来存储存放Class和Meta（元数据）的信息并称之为：元空间（Metaspace）</p><ul><li><p><strong>Metaspace 垃圾回收</strong><br>对于僵死的类及类加载器的垃圾回收将在元数据使用达到<code>MaxMetaspaceSize</code>参数的设定值时进行。</p><p>适时地监控和调整元空间对于减小垃圾回收频率和减少延时是很有必要的。持续的元空间垃圾回收说明，可能存在类、类加载器导致的内存泄漏或是大小设置不合适。</p></li></ul><h3 id="二、GC-堆"><a href="#二、GC-堆" class="headerlink" title="二、GC 堆"></a>二、GC 堆</h3><p>Java 中的堆是 GC 收集垃圾的主要区域。堆内GC 分为：Minor GC、FullGC ( 或称为 Major GC )。</p><h4 id="2-1、Minor-GC"><a href="#2-1、Minor-GC" class="headerlink" title="2.1、Minor GC"></a>2.1、Minor GC</h4><p><strong>Minor GC</strong> 是发生在新生代中的垃圾收集动作，所采用的是<strong>复制算法</strong>。</p><blockquote><p>简要概括如下：</p></blockquote><p>当对象在 Eden ( 包括一个 Survivor 区域，这里假设是 from 区域 ) 出生后，<strong>在经过一次 Minor GC 后，如</strong></p><p><strong>果对象还存活，并且能够被另外一块 Survivor 区域所容纳</strong>( 上面已经假设为 from 区域，这里应为 to 区域，</p><p>即 to 区域有足够的内存空间来存储 Eden 和 from 区域中存活的对象 )，<strong>则使用复制算法将这些仍然还存活的对</strong></p><p><strong>象复制到另外一块 Survivor 区域 ( 即 to 区域 ) 中</strong>，然后清理所使用过的 Eden 以及 Survivor 区域 ( 即</p><p>from 区域 )，<strong>并且将这些对象的年龄设置为1，以后对象在 Survivor 区每熬过一次 Minor GC，就将对象的年龄 + 1，当对象的年龄达到某个值时 ( 默认是 15 岁，可以通过参数 <code>-XX:MaxTenuringThreshold</code> 来设定)，这些对象就会成为老年代。</strong></p><h4 id="2-2、Full-GC"><a href="#2-2、Full-GC" class="headerlink" title="2.2、Full GC"></a>2.2、Full GC</h4><p><strong>Full GC</strong> 是<strong>发生在老年代</strong>的垃圾收集动作，所采用的是<strong>标记-清除-整理算法</strong>。</p><p>现实的生活中，老年代的人通常会比新生代的人”早死”。堆内存中的老年代(Old)不同于这个，老年代里面的对象</p><p>几乎个个都是在 Survivor 区域中熬过来的，它们是不会那么容易就 “死掉” 了的。因此，<strong>Full GC 发生的次数不</strong></p><p><strong>会有 Minor GC 那么频繁，并且做一次 Full GC 要比进行一次 Minor GC 的时间更长。</strong></p><p>另外，标记-清除算法收集垃圾的时候会产生许多的内存碎片 ( 即不连续的内存空间 )，此后需要为较大的对象</p><p>分配内存空间时，若无法找到足够的连续的内存空间，就会提前触发一次 GC 的收集动作。</p><h3 id="三、故障重现"><a href="#三、故障重现" class="headerlink" title="三、故障重现"></a>三、故障重现</h3><p>某开发负责的其中一个业务其中1台容器无故gg了，已知不是核心应用，且目前线上1台虚机也可以扛住目前的量，赶紧一起同开发分析问题：<br>- 先看虚机和宿主机的负载，正常；<br>- 进入容器看下进程还在；<br>- 业务日志已经停止写入；<br>- 查看是否有gclog；</p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/PsY75q3oyKLg.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>再次看下大盘：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/zEU4HwRh09jm.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>gc问题，有异常了，节点被下掉了；</p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/iDq0cFbYf9aO.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>先问下开发此业务当然有没有什么其它动作？经查看业务日志发现，刚刚那一阵在跑job，为什么metadata区会爆掉呢？</p><p>开发给的答案：</p><p>类加载器创建过多，带来的一个问题是，在类加载器第一次加载类的时候，会在Metaspace里会给它分配内存块，为了分配高效，每个类加载器用来存放类信息的内存块都是独立的，所以哪怕你这个类加载器只加载一个类，也会为之分配一块空的内存给这个类加载器，其实是至少两个内存块，于是你有可能会发现Metaspace的内存使用率非常低，但是committed的内存已经达到了阈值，从而触发了Full GC，如果这种只加载很少类的类加载器非常多，那造成的后果就是很多碎片化的内存！</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##先来看下目前的JVM参数：</span><br><span class="line">[root@xhy-18-175-61 dsf]# ps aux | grep java</span><br><span class="line">root         136 13.7  0.7 18846112 1968564 ?    Sl+  18:28   4:08 java -Duser.timezone=GMT+08 -server -Xms3840m -Xmx3840m -XX:NewSize=1024m -XX:MaxNewSize=1024m -XX:MaxDirectMemorySize=256m -XX:MetaspaceSize=128-XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_heapDump.hprof -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_gc.log -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Dapm.applicationName=cxgc.java.dsf.zkt.service -Dapm.agentId=172.18.175.61-17851 -Dapm.env=product -classpath -DDSF_HOME=/usr/local/dsf -DDio.netty.leakDetectionLevel=PARANOID -jar /usr/local/dsf/dsfapps/dsf.zkt.service.jar</span><br><span class="line"></span><br><span class="line">##分析下：</span><br><span class="line">通过jstat -gcutil pid查看M的值为97.37，即Meta区使用率也达到了97.37%：</span><br><span class="line">[root@xhy-18-175-61 dsf]# jstat -gcutil 136</span><br><span class="line">  S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  0.00  89.03  35.00   3.79  97.37  97.18     19    4.312     0    0.000    4.312</span><br><span class="line"></span><br><span class="line">那么-XX:MetaspaceSize=256m的含义到底是什么呢？其实，这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。</span><br><span class="line"></span><br><span class="line">先来解决下：</span><br><span class="line">- MetaspaceSize改成256m；</span><br><span class="line">增加 -XX:MinMetaspaceFreeRatio=40；</span><br></pre></td></tr></table></figure><p>这里有几个要点需要明确：</p><ol><li>无论<code>-XX:MetaspaceSize</code>配置什么值，Metaspace的初始容量一定是<code>21807104</code>（约20.8m）；</li><li>Metaspace由于使用不断扩容到<code>-XX:MetaspaceSize</code>参数指定的量，就会发生FGC；且之后每次Metaspace扩容都会发生FGC；</li><li>如果Old区配置CMS垃圾回收，那么第2点的FGC也会使用CMS算法进行回收；</li><li>Meta区容量范围为(20.8m, MaxMetaspaceSize)；</li><li>如果MaxMetaspaceSize设置太小，可能会导致频繁FGC，甚至OOM；</li></ol><h4 id="最后建议："><a href="#最后建议：" class="headerlink" title="最后建议："></a>最后建议：</h4><ul><li>MetaspaceSize和MaxMetaspaceSize设置一样大；</li><li>具体设置多大，建议稳定运行一段时间后通过jstat -gc pid确认且这个值大一些，对于大部分项目256m即可。</li></ul><p>写这篇日志已经是第二天了 暂时没有发现异常：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/AHxpwnr3vKBy.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><img src="http://myimage.okay686.cn/okay686cn/20191218/SLo0txoUUzBJ.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>JAVA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S弹性伸缩之Pods 资源扩容方案</title>
    <link href="/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A92/"/>
    <url>/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A92/</url>
    
    <content type="html"><![CDATA[<h3 id="Pod自动扩容-缩容（HPA）"><a href="#Pod自动扩容-缩容（HPA）" class="headerlink" title="Pod自动扩容/缩容（HPA）"></a>Pod自动扩容/缩容（HPA）</h3><p>Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据资源利用率或者自定义指标自动调整replication controller, deployment 或 replica set，实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适于无法缩放的对象，例如DaemonSet。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/q08Axxe9Bt3p.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="1、HPA基本原理"><a href="#1、HPA基本原理" class="headerlink" title="1、HPA基本原理"></a>1、HPA基本原理</h5><p>Kubernetes 中的 Metrics Server 持续采集所有 Pod 副本的指标数据。HPA 控制器通过 Metrics Server 的 API（Heapster 的 API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod 副本数量。当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。如图所示。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/jIal8CEvNnLr.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在弹性伸缩中，冷却周期是不能逃避的一个话题， 由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸， 所以在每次做出扩容缩容后，冷却时间是多少。</p><p>在 HPA 中，<strong>默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。</strong></p><p>可以通过调整kube-controller-manager组件启动参数设置冷却时间：</p><ul><li>–horizontal-pod-autoscaler-downscale-delay ：扩容冷却</li><li>–horizontal-pod-autoscaler-upscale-delay ：缩容冷却</li></ul><h5 id="2、HPA的演进历程"><a href="#2、HPA的演进历程" class="headerlink" title="2、HPA的演进历程"></a>2、HPA的演进历程</h5><p>目前 HPA 已经支持了 autoscaling/v1、autoscaling/v2beta1和autoscaling/v2beta2  三个大版本 。</p><p>目前大多数人比较熟悉是autoscaling/v1，这个版本只支持CPU一个指标的弹性伸缩。</p><p>而autoscaling/v2beta1增加了支持自定义指标，autoscaling/v2beta2又额外增加了外部指标支持。</p><p>而产生这些变化不得不提的是Kubernetes社区对监控与监控指标的认识与转变。从早期Heapster到Metrics Server再到将指标边界进行划分，一直在丰富监控生态。</p><p>示例：</p><p>v1版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: php-apache    ##指定对象</span><br><span class="line">  minReplicas: 1        ##指定副本范围</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  targetCPUUtilizationPercentage: 50        ##指定阈值</span><br></pre></td></tr></table></figure><p>v2beta2版本：（可以基于resource，pod，object，external等）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: php-apache    ##选择对象</span><br><span class="line">  minReplicas: 1        ##范围</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu     ##基于cpu</span><br><span class="line">      target:</span><br><span class="line">        type: Utilization</span><br><span class="line">        averageUtilization: 50</span><br><span class="line">  - type: Pods      ##基于pod</span><br><span class="line">    pods:</span><br><span class="line">      metric:</span><br><span class="line">        name: packets-per-second</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 1k</span><br><span class="line">  - type: Object    ##基于qps</span><br><span class="line">    object:</span><br><span class="line">      metric:</span><br><span class="line">        name: requests-per-second</span><br><span class="line">      describedObject:</span><br><span class="line">        apiVersion: networking.k8s.io/v1beta1</span><br><span class="line">        kind: Ingress</span><br><span class="line">        name: main-route</span><br><span class="line">      target:</span><br><span class="line">        type: Value</span><br><span class="line">        value: 10k</span><br><span class="line">  - type: External</span><br><span class="line">    external:</span><br><span class="line">      metric:</span><br><span class="line">        name: queue_messages_ready</span><br><span class="line">        selector: &quot;queue=worker_tasks&quot;</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 30</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S弹性伸缩之基于Metrics Server CPU指标的HPA</title>
    <link href="/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A93/"/>
    <url>/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A93/</url>
    
    <content type="html"><![CDATA[<h3 id="基于CPU指标缩放"><a href="#基于CPU指标缩放" class="headerlink" title="基于CPU指标缩放"></a>基于CPU指标缩放</h3><h4 id="1、-Kubernetes-API-Aggregation"><a href="#1、-Kubernetes-API-Aggregation" class="headerlink" title="1、 Kubernetes API Aggregation"></a>1、 Kubernetes API Aggregation</h4><p>什么是k8sapi聚合？</p><pre><code>它是允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。</code></pre><p>当然看如下图↓：你也可以把agg当作一个nginx的反代，它就是代理层。</p><p>在 Kubernetes 1.7 版本引入了聚合层，允许第三方应用程序通过将自己注册到kube-apiserver上，仍然通过 API Server 的 HTTP URL 对新的 API 进行访问和操作。为了实现这个机制，Kubernetes 在 kube-apiserver 服务中引入了一个 API 聚合层（API Aggregation Layer），用于将扩展 API 的访问请求转发到用户服务的功能。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/4ltOHHqMSpAC.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>当你访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端 。通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。</p><p>如果你使用kubeadm部署的，默认已开启。<strong>如果你使用==二进制方式部署==的话，需要在kube-APIServer中添加启动参数</strong>，增加以下配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi /opt/kubernetes/cfg/kube-apiserver.conf</span><br><span class="line">...</span><br><span class="line">--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \</span><br><span class="line">--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \</span><br><span class="line">--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \</span><br><span class="line">--requestheader-allowed-names=kubernetes \</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">--requestheader-group-headers=X-Remote-Group \</span><br><span class="line">--requestheader-username-headers=X-Remote-User \</span><br><span class="line">--enable-aggregator-routing=true \</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>  在设置完成重启 kube-apiserver 服务，就启用 API 聚合功能了。  </p><h5 id="2、部署-Metrics-Server"><a href="#2、部署-Metrics-Server" class="headerlink" title="2、部署 Metrics Server"></a>2、部署 Metrics Server</h5><p>Metrics Server是一个集群范围的资源使用情况的数据聚合器。作为一个应用部署在集群中。</p><p>Metric server从每个节点上Kubelet公开的摘要API收集指标。 </p><p>Metrics server通过Kubernetes聚合器注册在Master APIServer中。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/zvXGP2D8wtDk.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># git clone https://github.com/kubernetes-incubator/metrics-server</span><br><span class="line"># cd metrics-server/deploy/1.8+/</span><br><span class="line"># vi metrics-server-deployment.yaml   # 添加2条启动参数   </span><br><span class="line">...</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: zhdya/metrics-server-amd64:v0.3.1</span><br><span class="line">        command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --kubelet-insecure-tls        ##忽略证书的验证</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP      ##一般node都是用主机名注册的，但是metricserver是通过pod启动的，解析不到hostname，所以需要采集节点的IP</span><br><span class="line">...</span><br><span class="line"># kubectl create -f .</span><br></pre></td></tr></table></figure><p>可通过Metrics API在Kubernetes中获得资源使用率指标，例如容器CPU和内存使用率。这些度量标准既可以由用户直接访问（例如，通过使用<code>kubectl top</code>命令），也可以由集群中的控制器（例如，Horizontal Pod Autoscaler）用于进行决策。 </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 metrics-server]# kubectl get po -n kube-system -o wide</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6d8cfdd59d-pbbbc         1/1     Running   0          131m   10.244.2.2       k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-4gj8p      1/1     Running   1          142m   192.168.171.11   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-k5lfh      1/1     Running   1          75m    192.168.171.14   k8s-node3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-kddhv      1/1     Running   1          142m   192.168.171.12   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-q8g25      1/1     Running   1          141m   192.168.171.13   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-7dbbcf4c7-v5zpm   1/1     Running   0          49s    10.244.2.4       k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>查看是否注册到apiservice<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 metrics-server]# kubectl get apiservice</span><br><span class="line">v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        3m22s</span><br></pre></td></tr></table></figure></p><p>测试：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 metrics-server]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes</span><br><span class="line">&#123;&quot;kind&quot;:&quot;NodeMetricsList&quot;,&quot;apiVersion&quot;:&quot;metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;&#125;,&quot;items&quot;:[&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-master1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-master1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:34Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;104718141n&quot;,&quot;memory&quot;:&quot;1158508Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:26Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;61460193n&quot;,&quot;memory&quot;:&quot;556328Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node2&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node2&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:32Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;57896643n&quot;,&quot;memory&quot;:&quot;570056Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node3&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node3&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:30Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;30890872n&quot;,&quot;memory&quot;:&quot;403264Ki&quot;&#125;&#125;]&#125;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 metrics-server]# kubectl top node</span><br><span class="line">NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">k8s-master1   105m         5%     1131Mi          65%</span><br><span class="line">k8s-node1     62m          3%     543Mi           31%</span><br><span class="line">k8s-node2     58m          2%     556Mi           32%</span><br><span class="line">k8s-node3     31m          1%     393Mi           22%</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 metrics-server]# kubectl top pod</span><br><span class="line">NAME                  CPU(cores)   MEMORY(bytes)</span><br><span class="line">web-944cddf48-4x6qr   0m           3Mi</span><br><span class="line">web-944cddf48-64d7r   0m           2Mi</span><br><span class="line">web-944cddf48-hjwng   0m           3Mi</span><br><span class="line">web-944cddf48-skh82   0m           3Mi</span><br></pre></td></tr></table></figure><h5 id="3、autoscaling-v1（CPU指标实践）"><a href="#3、autoscaling-v1（CPU指标实践）" class="headerlink" title="3、autoscaling/v1（CPU指标实践）"></a>3、autoscaling/v1（CPU指标实践）</h5><p>autoscaling/v1版本只支持CPU一个指标。</p><p>首先部署一个应用：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl run web --image=nginx --replicas=8 --requests=&quot;cpu=100m,memory=100Mi&quot; --expose 80 --port 80 --dry-run -o yaml &gt;app.yaml</span><br></pre></td></tr></table></figure><p>创建HPA策略：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl autoscale deployment web --min=2 --max=8 -o yaml --dry-run &gt;hpav1.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 hpa]# kubectl apply -f hpav1.yaml</span><br><span class="line">horizontalpodautoscaler.autoscaling/web created</span><br><span class="line"></span><br><span class="line">###cat hpav1.yaml</span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  maxReplicas: 8</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: web</span><br><span class="line">  targetCPUUtilizationPercentage: 60</span><br></pre></td></tr></table></figure><p>scaleTargetRef：表示当前要伸缩对象是谁</p><p>targetCPUUtilizationPercentage：当整体的资源利用率超过50%的时候，会进行扩容。</p><p>查看当前状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl get hpa</span><br><span class="line">NAME   REFERENCE        TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">web    Deployment/web   0%/60%    2         8         3          24s</span><br></pre></td></tr></table></figure></p><p>开启压测：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum install httpd-tools</span><br><span class="line"># ab -n 100000 -c 100  http://10.1.206.176/status.php</span><br></pre></td></tr></table></figure><p>10.0.0.147 为ClusterIP。</p><p>检查扩容状态：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl get hpa</span><br><span class="line"># kubectl top pods</span><br><span class="line"># kubectl get pods</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 hpa]# ab -n 1000000 -c 1000  http://10.0.0.201/index.html    ##压测</span><br><span class="line"></span><br><span class="line">关闭压测，过一会检查缩容状态。</span><br><span class="line">[root@k8s-master1 ~]# kubectl top po</span><br><span class="line">NAME                   CPU(cores)   MEMORY(bytes)</span><br><span class="line">web-77cfdb7c6c-dpm5t   116m         5Mi</span><br><span class="line">web-77cfdb7c6c-lwkbj   343m         5Mi</span><br><span class="line">[root@k8s-master1 ~]# kubectl get po</span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">web-77cfdb7c6c-94p6x   1/1     Running   0          7s</span><br><span class="line">web-77cfdb7c6c-9xwbj   1/1     Running   0          23s</span><br><span class="line">web-77cfdb7c6c-dpm5t   1/1     Running   0          33m</span><br><span class="line">web-77cfdb7c6c-gpk6d   1/1     Running   0          7s</span><br><span class="line">web-77cfdb7c6c-l7b4r   1/1     Running   0          23s</span><br><span class="line">web-77cfdb7c6c-lwkbj   1/1     Running   0          33m</span><br><span class="line">web-77cfdb7c6c-w6lz6   1/1     Running   0          7s</span><br><span class="line">web-77cfdb7c6c-wpzb5   1/1     Running   0          7s</span><br></pre></td></tr></table></figure><p><strong>==工作流程==</strong>：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor)</p><h5 id="4、autoscaling-v2beta2（多指标）"><a href="#4、autoscaling-v2beta2（多指标）" class="headerlink" title="4、autoscaling/v2beta2（多指标）"></a>4、autoscaling/v2beta2（多指标）</h5><p>为满足更多的需求， HPA 还有 autoscaling/v2beta1和 autoscaling/v2beta2两个版本。</p><p>这两个版本的区别是 autoscaling/v1beta1支持了 Resource Metrics（CPU）和 Custom Metrics（应用程序指标），而在 autoscaling/v2beta2的版本中额外增加了 External Metrics的支持。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl get hpa.v2beta2.autoscaling -o yaml &gt; /tmp/hpa-v2.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: web</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - resource:</span><br><span class="line">    type: Resource</span><br><span class="line">      name: cpu</span><br><span class="line">      target:</span><br><span class="line">        averageUtilization: 60</span><br><span class="line">        type: Utilization</span><br></pre></td></tr></table></figure><p>与上面v1版本效果一样，只不过这里格式有所变化。</p><p>v2还支持其他另种类型的度量指标，：Pods和Object。  </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type: Pods</span><br><span class="line">pods:</span><br><span class="line">  metric:</span><br><span class="line">    name: packets-per-second</span><br><span class="line">  target:</span><br><span class="line">    type: AverageValue</span><br><span class="line">    averageValue: 1k</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type: Object</span><br><span class="line">object:</span><br><span class="line">  metric:</span><br><span class="line">    name: requests-per-second</span><br><span class="line">  describedObject:</span><br><span class="line">    apiVersion: networking.k8s.io/v1beta1</span><br><span class="line">    kind: Ingress</span><br><span class="line">    name: main-route</span><br><span class="line">  target:</span><br><span class="line">    type: Value</span><br><span class="line">    value: 2k</span><br></pre></td></tr></table></figure><p>metrics中的type字段有四种类型的值：Object、Pods、Resource、External。 </p><ul><li><p>Resource：指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。</p></li><li><p>Object：指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。</p></li><li><p>Pods：指的是伸缩对象Pods的指标，数据需要第三方的adapter提供，只允许AverageValue类型的目标值。</p></li><li><p>External：指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。</p></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hpa-v2.yaml</span><br><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: web</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu</span><br><span class="line">      target:</span><br><span class="line">        type: Utilization</span><br><span class="line">        averageUtilization: 50</span><br><span class="line">  - type: Pods</span><br><span class="line">    pods:</span><br><span class="line">      metric:</span><br><span class="line">        name: packets-per-second</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 1k</span><br><span class="line">  - type: Object</span><br><span class="line">    object:</span><br><span class="line">      metric:</span><br><span class="line">        name: requests-per-second</span><br><span class="line">      describedObject:</span><br><span class="line">        apiVersion: networking.k8s.io/v1beta1</span><br><span class="line">        kind: Ingress</span><br><span class="line">        name: main-route</span><br><span class="line">      target:</span><br><span class="line">        type: Value</span><br><span class="line">        value: 10k</span><br></pre></td></tr></table></figure><p><strong>==工作流程==</strong>：hpa -&gt; apiserver -&gt; kube aggregation  -&gt; prometheus-adapter -&gt; prometheus -&gt; pods</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S弹性伸缩之基于Prometheus QPS指标的HPA</title>
    <link href="/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A94/"/>
    <url>/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A94/</url>
    
    <content type="html"><![CDATA[<h3 id="基于Prometheus自定义指标缩放"><a href="#基于Prometheus自定义指标缩放" class="headerlink" title="基于Prometheus自定义指标缩放"></a>基于Prometheus自定义指标缩放</h3><p>资源指标只包含CPU、内存，一般来说也够了。但如果想根据自定义指标:如请求qps/5xx错误数来实现HPA，就需要使用自定义指标了，目前比较成熟的实现是 Prometheus Custom Metrics。自定义指标由Prometheus来提供，再利用k8s-prometheus-adpater聚合到apiserver，实现和核心指标（metric-server)同样的效果。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/spITPJoVOwoH.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="1、部署Prometheus"><a href="#1、部署Prometheus" class="headerlink" title="1、部署Prometheus"></a>1、部署Prometheus</h5><p>Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。</p><p><strong>Prometheus</strong> <strong>特点：</strong></p><ul><li><p>自动采集，服务发现；</p></li><li><p>多维数据模型：由度量名称和键值对标识的时间序列数据；</p></li><li><p>PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询；</p></li><li><p>不依赖分布式存储，单个服务器节点可直接工作；</p></li><li><p>基于HTTP的pull方式采集时间序列数据；</p></li><li><p>推送时间序列数据通过PushGateway组件支持；</p></li><li><p>通过服务发现或静态配置发现目标；</p></li><li><p>多种图形模式及仪表盘支持（grafana）；</p></li></ul><p><strong>Prometheus组成及架构：</strong></p><p><img src="http://myimage.okay686.cn/okay686cn/20191213/JotK1OzGMJpT.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><ul><li><p>Prometheus Server：收集指标和存储时间序列数据，并提供查询接口</p></li><li><p>ClientLibrary：客户端库</p></li><li><p>Push Gateway：短期存储指标数据。主要用于临时性的任务</p></li><li><p>Exporters：采集已有的第三方服务监控指标并暴露metrics</p></li><li><p>Alertmanager：告警</p></li><li><p>Web UI：简单的Web控制台</p></li></ul><p><strong>部署：</strong></p><p>现在node上安装：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# yum install -y nfs-utils</span><br><span class="line"></span><br><span class="line">NFS 配置及使用</span><br><span class="line">我们在服务端创建一个共享目录 /data/share ，作为客户端挂载的远端入口，然后设置权限。</span><br><span class="line"></span><br><span class="line">$ mkdir -p /opt/sharedata/</span><br><span class="line">$ chmod 666 /opt/sharedata/</span><br><span class="line"></span><br><span class="line">然后，修改 NFS 配置文件 /etc/exports</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 ~]# cat /etc/exports</span><br><span class="line">/opt/sharedata 192.168.171.0/24(rw,sync,insecure,no_subtree_check,no_root_squash)</span><br><span class="line"></span><br><span class="line">说明一下，这里配置后边有很多参数，每个参数有不同的含义，具体可以参考下边。此处，我配置了将 /data/share 文件目录设置为允许 IP 为该 192.168.171.0/24 区间的客户端挂载，当然，如果客户端 IP 不在该区间也想要挂载的话，可以设置 IP 区间更大或者设置为 * 即允许所有客户端挂载，例如：/home *(ro,sync,insecure,no_root_squash) 设置 /home 目录允许所有客户端只读挂载。</span><br><span class="line"></span><br><span class="line"># 启动 NFS 服务</span><br><span class="line">$ service nfs start</span><br><span class="line"># 或者使用如下命令亦可</span><br><span class="line">/bin/systemctl start nfs.service</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 ~]# showmount -e localhost</span><br><span class="line">Export list for localhost:</span><br><span class="line">/opt/sharedata 192.168.171.0/24</span><br><span class="line"></span><br><span class="line">示例：</span><br><span class="line">挂载远端目录到本地 /share 目录。</span><br><span class="line"></span><br><span class="line">$ mount 192.168.171.11:/opt/sharedata /share</span><br><span class="line">$ df -h | grep 192.168.171.11</span><br><span class="line">Filesystem                 Size  Used  Avail Use% Mounted on</span><br><span class="line">192.168.171.11:/opt/sharedata   27G   11G   17G   40%  /share</span><br><span class="line"></span><br><span class="line">客户端要卸载 NFS 挂载的话，使用如下命令即可。</span><br><span class="line">$ umount /share</span><br></pre></td></tr></table></figure></p><p>现在master上安装：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg  提取码：7l3z</span><br><span class="line">从分享包中导入nfs-client.zip</span><br><span class="line"></span><br><span class="line"># cd nfs-client</span><br><span class="line"># [root@k8s-master1 nfs-client]# cat deployment.yaml</span><br><span class="line">...省略</span><br><span class="line">serviceAccountName: nfs-client-provisioner</span><br><span class="line">containers:</span><br><span class="line">  - name: nfs-client-provisioner</span><br><span class="line">    image: quay.io/external_storage/nfs-client-provisioner:latest</span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: nfs-client-root</span><br><span class="line">        mountPath: /persistentvolumes</span><br><span class="line">    env:</span><br><span class="line">      - name: PROVISIONER_NAME</span><br><span class="line">        value: fuseim.pri/ifs</span><br><span class="line">      - name: NFS_SERVER</span><br><span class="line">        value: 192.168.171.12   ##nfs的server地址</span><br><span class="line">      - name: NFS_PATH</span><br><span class="line">        value: /opt/sharedata  ##暴露的目录</span><br><span class="line">volumes:</span><br><span class="line">  - name: nfs-client-root</span><br><span class="line">    nfs:</span><br><span class="line">      server: 192.168.171.12</span><br><span class="line">      path: /opt/sharedata</span><br><span class="line">...省略</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 nfs-client]# kubectl apply -f .</span><br><span class="line">storageclass.storage.k8s.io/managed-nfs-storage created</span><br><span class="line">serviceaccount/nfs-client-provisioner created</span><br><span class="line">deployment.apps/nfs-client-provisioner created</span><br><span class="line">serviceaccount/nfs-client-provisioner unchanged</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created</span><br><span class="line">role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 nfs-client]# kubectl get po</span><br><span class="line">NAME                                    READY   STATUS              RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner-9c784f97-cqzhb   1/1     running   0          2m16s</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg  提取码：7l3z</span><br><span class="line"># cd prometheus</span><br><span class="line"># kubectl apply -f .</span><br><span class="line">[root@k8s-master1 nfs-client]# kubectl get po -o wide -n kube-system</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6d8cfdd59d-pbbbc         1/1     Running   2          2d1h    10.244.2.15      k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-q8g25      1/1     Running   3          2d1h    192.168.171.13   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-7dbbcf4c7-v5zpm   1/1     Running   3          47h     10.244.2.14      k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prometheus-0                     2/2     Running   0          6m48s   10.244.3.15      k8s-node3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 nfs-client]# kubectl get svc -n kube-system</span><br><span class="line">NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kube-dns         ClusterIP   10.0.0.2     &lt;none&gt;        53/UDP,53/TCP    2d1h</span><br><span class="line">metrics-server   ClusterIP   10.0.0.5     &lt;none&gt;        443/TCP          47h</span><br><span class="line">prometheus       NodePort    10.0.0.147   &lt;none&gt;        9090:30090/TCP   7m46s</span><br></pre></td></tr></table></figure><p>访问Prometheus UI：<a href="http://NdeIP:30090" target="_blank" rel="noopener">http://NdeIP:30090</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20191212/fo1z9VXaJWiH.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="2、-部署-Custom-Metrics-Adapter"><a href="#2、-部署-Custom-Metrics-Adapter" class="headerlink" title="2、 部署 Custom Metrics Adapter"></a>2、 部署 Custom Metrics Adapter</h5><p>但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(k8s-prometheus-adpater)，将prometheus的metrics 数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主APIServer中注册，以便直接通过/apis/来访问。</p><p> <a href="https://github.com/DirectXMan12/k8s-prometheus-adapter" target="_blank" rel="noopener">https://github.com/DirectXMan12/k8s-prometheus-adapter</a> </p><p>该 PrometheusAdapter 有一个稳定的Helm Charts，我们直接使用。</p><p>先准备下helm环境：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz</span><br><span class="line">tar zxvf helm-v3.0.0-linux-amd64.tar.gz </span><br><span class="line">mv linux-amd64/helm /usr/bin/</span><br><span class="line">helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">helm repo update</span><br><span class="line">helm repo list</span><br></pre></td></tr></table></figure><p>部署prometheus-adapter，指定prometheus地址：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># helm install prometheus-adapter stable/prometheus-adapter --namespace kube-system --set prometheus.url=http://prometheus.kube-system,prometheus.port=9090</span><br><span class="line"># helm list -n kube-system</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl get pods -n kube-system</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">prometheus-adapter-77b7b4dd8b-ktsvx   1/1     Running   0          9m</span><br></pre></td></tr></table></figure><p>确保适配器注册到APIServer：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get apiservices |grep custom</span><br><span class="line">v1beta1.custom.metrics.k8s.io          kube-system/prometheus-adapter   True        87s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot;</span><br><span class="line">&#123;&quot;kind&quot;:&quot;APIResourceList&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;groupVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;resources&quot;:[&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;pods/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure><h4 id="基于QPS指标实践"><a href="#基于QPS指标实践" class="headerlink" title="基于QPS指标实践"></a>基于QPS指标实践</h4><p>部署一个应用：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: metrics-app</span><br><span class="line">  name: metrics-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: metrics-app</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: metrics-app</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;    ##是否可以被采集数据</span><br><span class="line">        prometheus.io/port: &quot;80&quot;        ##采集访问的端口</span><br><span class="line">        prometheus.io/path: &quot;/metrics&quot;  ##采集访问的URL</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: zhdya/metrics-app</span><br><span class="line">        name: metrics-app</span><br><span class="line">        ports:</span><br><span class="line">        - name: web</span><br><span class="line">          containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 200m</span><br><span class="line">            memory: 256Mi</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /</span><br><span class="line">            port: 80</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /</span><br><span class="line">            port: 80</span><br><span class="line">          initialDelaySeconds: 3</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-app</span><br><span class="line">  labels:</span><br><span class="line">    app: metrics-app</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: metrics-app</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl get po -o wide</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running   0          19s   10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-btch5             0/1     Running   0          19s   10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-kksjr             0/1     Running   0          19s   10.244.0.15   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 hpa]# kubectl get svc</span><br><span class="line">NAME          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes    ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP   2d1h</span><br><span class="line">metrics-app   ClusterIP   10.0.0.163   &lt;none&gt;        80/TCP    39s</span><br></pre></td></tr></table></figure><p>该metrics-app暴露了一个Prometheus指标接口，可以通过访问service看到：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# curl 10.0.0.163/metrics</span><br><span class="line"># HELP http_requests_total The amount of requests in total</span><br><span class="line"># TYPE http_requests_total counter</span><br><span class="line">http_requests_total 20</span><br><span class="line"># HELP http_requests_per_second The amount of requests per second the latest ten seconds</span><br><span class="line"># TYPE http_requests_per_second gauge</span><br><span class="line">http_requests_per_second 0.5</span><br><span class="line"></span><br><span class="line">##顺带测试下负载均衡：</span><br><span class="line">[root@k8s-master1 hpa]# curl 10.0.0.163</span><br><span class="line">Hello! My name is metrics-app-7674cfb699-btch5. The last 10 seconds, the average QPS has been 0.5. Total requests served: 35</span><br><span class="line">[root@k8s-master1 hpa]# curl 10.0.0.163</span><br><span class="line">Hello! My name is metrics-app-7674cfb699-5l72f. The last 10 seconds, the average QPS has been 0.5. Total requests served: 38</span><br><span class="line">[root@k8s-master1 hpa]# curl 10.0.0.163</span><br><span class="line">Hello! My name is metrics-app-7674cfb699-kksjr. The last 10 seconds, the average QPS has been 0.5. Total requests served: 37</span><br></pre></td></tr></table></figure><p>收集到的每个容器被访问的次数：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191212/9W3JdYyaEVcC.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>创建HPA策略：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi app-hpa-v2.yml</span><br><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-app-hpa </span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: metrics-app</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 8</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Pods</span><br><span class="line">    pods:</span><br><span class="line">      metric:</span><br><span class="line">        name: http_requests_per_second</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 800m   # 800m 即0.8个/秒</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl get hpa</span><br><span class="line">NAME              REFERENCE                TARGETS          MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   &lt;unknown&gt;/800m   1         8         3          36s</span><br></pre></td></tr></table></figure><p>这里使用Prometheus提供的指标测试来测试自定义指标（QPS）的自动缩放。</p><h5 id="4、配置适配器收集特定的指标"><a href="#4、配置适配器收集特定的指标" class="headerlink" title="4、配置适配器收集特定的指标"></a>4、配置适配器收集特定的指标</h5><p>当创建好HPA还没结束，因为适配器还不知道你要什么指标（http_requests_per_second），HPA也就获取不到Pod提供指标。</p><p> ConfigMap在default名称空间中编辑prometheus-adapter ，并seriesQuery在该rules: 部分的顶部添加一个新的： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl edit cm prometheus-adapter -n kube-system</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus-adapter</span><br><span class="line">    chart: prometheus-adapter-v0.1.2</span><br><span class="line">    heritage: Tiller</span><br><span class="line">    release: prometheus-adapter</span><br><span class="line">  name: prometheus-adapter</span><br><span class="line">data:</span><br><span class="line">  config.yaml: |</span><br><span class="line">    rules:      ##增加如下一段：</span><br><span class="line">    - seriesQuery: &apos;http_requests_total&#123;kubernetes_namespace!=&quot;&quot;,kubernetes_pod_name!=&quot;&quot;&#125;&apos;      ##在prometheus中就可以直接查询到这部分数据</span><br><span class="line">      resources:</span><br><span class="line">        overrides:</span><br><span class="line">          kubernetes_namespace: &#123;resource: &quot;namespace&quot;&#125;</span><br><span class="line">          kubernetes_pod_name: &#123;resource: &quot;pod&quot;&#125;</span><br><span class="line">      name:</span><br><span class="line">        matches: &quot;^(.*)_total&quot;</span><br><span class="line">        as: &quot;$&#123;1&#125;_per_second&quot;</span><br><span class="line">      metricsQuery: &apos;sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&apos;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>该规则将http_requests在2分钟的间隔内收集该服务的所有Pod的平均速率。</p><p>测试API：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot;</span><br><span class="line">&#123;&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;apiVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;&#125;,&quot;items&quot;:[&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-5l72f&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-btch5&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-kksjr&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 hpa]# kubectl get hpa</span><br><span class="line">NAME              REFERENCE                TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   416m/800m   1         8         2          20m</span><br></pre></td></tr></table></figure><p>压测：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ab -n 100000 -c 100  http://10.0.0.163/metrics</span><br></pre></td></tr></table></figure><p>查看容器扩容的情况：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get po -o wide</span><br><span class="line">NAME                                     READY   STATUS              RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running             0          48m    10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-6rht6             1/1     Running             0          16s    10.244.0.16   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-9ltvr             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-btch5             1/1     Running             0          48m    10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-kft7p             1/1     Running             0          16s    10.244.3.16   k8s-node3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-plhrp             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-sgvln             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-wr56r             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running             0          8m7s   10.244.2.17   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@k8s-master1 ~]# kubectl get po -o wide</span><br><span class="line">NAME                                     READY   STATUS    RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">metrics-app-7674cfb699-5l72f             1/1     Running   0          48m    10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-6rht6             1/1     Running   0          18s    10.244.0.16   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-9ltvr             0/1     Running   0          3s     10.244.0.17   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-btch5             1/1     Running   0          48m    10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-kft7p             1/1     Running   0          18s    10.244.3.16   k8s-node3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-plhrp             0/1     Running   0          3s     10.244.2.18   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-sgvln             0/1     Running   0          3s     10.244.1.16   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-app-7674cfb699-wr56r             0/1     Running   0          3s     10.244.1.17   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running   0          8m9s   10.244.2.17   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><p>查看HPA状态：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get hpa</span><br><span class="line">NAME              REFERENCE                TARGETS        MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   414345m/800m   1         8         8          21m</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl describe hpa metrics-app-hpa</span><br><span class="line">...省略</span><br><span class="line">Metrics:                               ( current / target )</span><br><span class="line">  &quot;http_requests_per_second&quot; on pods:  818994m / 800m</span><br><span class="line">Min replicas:                          1</span><br><span class="line">Max replicas:                          8</span><br><span class="line">Deployment pods:                       8 current / 8 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason            Message</span><br><span class="line">  ----            ------  ------            -------</span><br><span class="line">  AbleToScale     True    ReadyForNewScale  recommended size matches current size</span><br><span class="line">  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second</span><br><span class="line">  ScalingLimited  True    TooManyReplicas   the desired replica count is more than the maximum replica count</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                        Age                   From                       Message</span><br><span class="line">  ----     ------                        ----                  ----                       -------</span><br><span class="line">  Warning  FailedComputeMetricsReplicas  19m (x12 over 22m)    horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get pods metric value: unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods</span><br><span class="line">  Warning  FailedGetPodsMetric           7m18s (x61 over 22m)  horizontal-pod-autoscaler  unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods</span><br><span class="line">  Normal   SuccessfulRescale             88s                   horizontal-pod-autoscaler  New size: 4; reason: pods metric http_requests_per_second above target</span><br></pre></td></tr></table></figure><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><img src="http://myimage.okay686.cn/okay686cn/20191213/doMHytVNy9eS.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、应用程序暴露/metrics监控指标并且是prometheus数据格式；</span><br><span class="line"></span><br><span class="line">2、通过/metrics收集每个Pod的http_request_total指标；</span><br><span class="line"></span><br><span class="line">3、prometheus将收集到的信息汇总；</span><br><span class="line"></span><br><span class="line">4、APIServer定时从Prometheus查询，获取request_per_second的数据；</span><br><span class="line"></span><br><span class="line">5、HPA定期向APIServer查询以判断是否符合配置的autoscaler规则；</span><br><span class="line"></span><br><span class="line">6、如果符合autoscaler规则，则修改Deployment的ReplicaSet副本数量进行伸缩。</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S弹性伸缩之Nodes 资源扩容（CA，Ansible）</title>
    <link href="/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/"/>
    <url>/2019/12/13/K8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/</url>
    
    <content type="html"><![CDATA[<h3 id="弹性伸缩"><a href="#弹性伸缩" class="headerlink" title="弹性伸缩"></a>弹性伸缩</h3><h4 id="1-传统弹性伸缩的困境"><a href="#1-传统弹性伸缩的困境" class="headerlink" title="1 传统弹性伸缩的困境"></a>1 传统弹性伸缩的困境</h4><p>从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-vs.png" srcset="/img/loading.gif" alt=""></p><p>蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。</p><p>弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。</p><h3 id="1、Kubernetes中弹性伸缩存在的问题"><a href="#1、Kubernetes中弹性伸缩存在的问题" class="headerlink" title="1、Kubernetes中弹性伸缩存在的问题"></a>1、Kubernetes中弹性伸缩存在的问题</h3><p>常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kubernetes中，就会发现如下2个问题。</p><ol><li><p><strong>机器规格不统一造成机器利用率百分比碎片化</strong></p><p>在一个Kubernetes集群中，通常不只包含一种规格的机器，假设集群中存在4C8G与16C32G两种规格的机器，对于10%的资源预留，这两种规格代表的意义是完全不同的。</p></li></ol><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-machine-config.png" srcset="/img/loading.gif" alt=""></p><p>特别是在缩容的场景下，为了保证缩容后集群稳定性，我们一般会一个节点一个节点从集群中摘除，那么如何判断节点是否可以摘除其利用率百分比就是重要的指标。此时如果大规格机器有较低的利用率被判断缩容，那么很有可能会造成节点缩容后，容器重新调度后的争抢。如果优先缩容小规格机器，则可能造成缩容后资源的大量冗余。</p><ol start="2"><li><p><strong>机器利用率不单纯依靠宿主机计算</strong></p><p>在大部分生产环境中，资源利用率都不会保持一个高的水位，但从调度来讲，调度应该保持一个比较高的水位，这样才能保障集群稳定性，又不过多浪费资源。</p></li></ol><h3 id="2、弹性伸缩概念的延伸"><a href="#2、弹性伸缩概念的延伸" class="headerlink" title="2、弹性伸缩概念的延伸"></a>2、弹性伸缩概念的延伸</h3><p>不是所有的业务都存在峰值流量，越来越细分的业务形态带来更多成本节省和可用性之间的跳转。</p><ol><li>在线负载型：微服务、网站、API</li><li>离线任务型：离线计算、机器学习</li><li>定时任务型：定时批量计算</li></ol><p>不同类型的负载对于弹性伸缩的要求有所不同，在线负载对弹出时间敏感，离线任务对价格敏感，定时任务对调度敏感。</p><h4 id="2-2-kubernetes-弹性伸缩布局"><a href="#2-2-kubernetes-弹性伸缩布局" class="headerlink" title="2.2 kubernetes 弹性伸缩布局"></a>2.2 kubernetes 弹性伸缩布局</h4><p>在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。</p><p>有三种弹性伸缩：</p><ul><li><p>CA（Cluster Autoscaler）：Node级别自动扩/缩容</p><p>cluster-autoscaler组件</p></li><li><p>HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容</p></li><li><p>VPA（Vertical Pod Autoscaler）：Pod配置自动扩/缩容，主要是CPU、内存</p><p>addon-resizer组件</p></li></ul><p>如果在云上建议 HPA 结合 cluster-autoscaler 的方式进行集群的弹性伸缩管理。</p><h4 id="2-3-Node-自动扩容-缩容"><a href="#2-3-Node-自动扩容-缩容" class="headerlink" title="2.3 Node 自动扩容/缩容"></a>2.3 Node 自动扩容/缩容</h4><h5 id="1、Cluster-AutoScaler"><a href="#1、Cluster-AutoScaler" class="headerlink" title="1、Cluster AutoScaler"></a>1、Cluster AutoScaler</h5><p><strong>扩容：</strong>Cluster AutoScaler 定期检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-up.png" srcset="/img/loading.gif" alt=""></p><p><strong>缩容：</strong>Cluster AutoScaler 也会定期监测 Node 的资源使用情况，当一个 Node 长时间资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除。此时，原来的 Pod 会自动调度到其他 Node 上面。</p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-down.png" srcset="/img/loading.gif" alt=""></p><p>支持的云提供商：</p><ul><li>阿里云：<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md</a></li><li>AWS： <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li><li>Azure： <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md</a></li></ul><h5 id="2、Ansible扩容Node"><a href="#2、Ansible扩容Node" class="headerlink" title="2、Ansible扩容Node"></a>2、Ansible扩容Node</h5><p><strong>自动化流程：</strong></p><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/ansible-node-scaler.png" srcset="/img/loading.gif" alt=""></p><ol><li>触发新增Node</li><li>调用Ansible脚本部署组件</li><li>检查服务是否可用</li><li>调用API将新Node加入集群或者启用Node自动加入</li><li>观察新Node状态</li><li>完成Node扩容，接收新Pod</li></ol><p><strong>扩容</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat hosts </span><br><span class="line">...</span><br><span class="line">[newnode]</span><br><span class="line">192.168.31.71 node_name=k8s-node3</span><br><span class="line"># ansible-playbook -i hosts add-node.yml -k</span><br></pre></td></tr></table></figure><p><strong>缩容</strong></p><p>如果你想从Kubernetes集群中删除节点，正确流程如下：</p><p><strong>1、获取节点列表</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure><p><strong>2、设置不可调度</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl cordon $node_name</span><br></pre></td></tr></table></figure><p><strong>3、驱逐节点上的Pod</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl drain $node_name --ignore-daemonsets</span><br></pre></td></tr></table></figure><p><strong>4、移除节点</strong></p><p>该节点上已经没有任何资源了，可以直接移除节点：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete node $node_name</span><br></pre></td></tr></table></figure><p>这样，我们平滑移除了一个 k8s 节点。</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S之Ingress-Nginx实现高可用</title>
    <link href="/2019/12/10/K8S%E4%B9%8BIngress-Nginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <url>/2019/12/10/K8S%E4%B9%8BIngress-Nginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>承接上文，我们部署好了ingress，为了达到生产级的阈值，我们必须要要配置ingress的高可用：</p><p>假定我们在Kubernetes 指定两个worker节点中部署了ingress nginx来为后端的pod做proxy，这时候我们就需要通过keepalived实现高可用，提供对外的VIP，也就是externalLB的upstream只需要绑定此VIP即可。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191210/QiuFxjwJq4x7.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>首先我们要先确保有两个worker节点部署了ingress nginx</p><p>在本实验中，环境如下：</p><table><thead><tr><th>IP地址</th><th>主机名</th><th>描述</th></tr></thead><tbody><tr><td>10.0.0.31</td><td>k8s-master01    </td></tr><tr><td>10.0.0.34</td><td>k8s-node02</td><td>ingress nginx、keepalived</td></tr><tr><td>10.0.0.35</td><td>k8s-node03</td><td>ingress nginx、keepalived</td></tr></tbody></table><h4 id="1、查看ingress-nginx状态"><a href="#1、查看ingress-nginx状态" class="headerlink" title="1、查看ingress nginx状态"></a>1、查看ingress nginx状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master01 Ingress]# kubectl get pod -n ingress-nginx -o wide</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-ingress-controller-85bd8789cd-8c4xh   1/1     Running   0          62s     10.0.0.34   k8s-node02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ingress-controller-85ff8dfd88-vqkhx   1/1     Running   0          3m56s   10.0.0.35   k8s-node03   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>创建一个用于测试环境的namespace</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl  create namespace test</span><br></pre></td></tr></table></figure><h4 id="2、部署一个Deployment（用于测试）"><a href="#2、部署一个Deployment（用于测试）" class="headerlink" title="2、部署一个Deployment（用于测试）"></a>2、部署一个Deployment（用于测试）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: myweb-deploy</span><br><span class="line">  # 部署在测试环境</span><br><span class="line">  namespace: test</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: myweb</span><br><span class="line">      type: test</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: myweb</span><br><span class="line">        type: test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">          - containerPort: 80</span><br><span class="line">---</span><br><span class="line"># service</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: myweb-svc</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    name: myweb</span><br><span class="line">    type: test</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">---</span><br><span class="line"># ingress</span><br></pre></td></tr></table></figure><p>执行kubectl create 创建deployment</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl  create -f myweb-demo.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看deployment是否部署成功</span><br><span class="line"></span><br><span class="line">[root@k8s-master01 Project]# kubectl get pods -n test -o wide | grep &quot;myweb&quot;</span><br><span class="line">myweb-deploy-6d586d7db4-2g5ll   1/1     Running   0          23s     10.244.3.240   k8s-node02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">myweb-deploy-6d586d7db4-cf7w7   1/1     Running   0          4m2s    10.244.1.132   k8s-node01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">myweb-deploy-6d586d7db4-rp5zc   1/1     Running   0          3m59s   10.244.2.5     k8s-node03   &lt;none&gt;</span><br></pre></td></tr></table></figure><h4 id="3、在两个worker节点部署keepalived"><a href="#3、在两个worker节点部署keepalived" class="headerlink" title="3、在两个worker节点部署keepalived"></a>3、在两个worker节点部署keepalived</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">VIP：10.0.0.130，接口：eth0</span><br></pre></td></tr></table></figure><h5 id="3-1、安装keepalived"><a href="#3-1、安装keepalived" class="headerlink" title="3.1、安装keepalived"></a>3.1、安装keepalived</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install keepalived</span><br></pre></td></tr></table></figure><p>3.1.1、k8s-node03节点作为MASTER配置keepalived<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node03 ~]# cat /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   router_id k8s-node03</span><br><span class="line">   vrrp_skip_check_adv_addr</span><br><span class="line">   vrrp_strict</span><br><span class="line">   vrrp_garp_interval 0</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface eth0</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 110</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.0.0.130/24 dev eth0 label eth0:1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>3.1.2、k8s-node02节点作为BACKUP配置keeplived<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node02 ~]# cat /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id k8s-node02</span><br><span class="line">   vrrp_skip_check_adv_addr</span><br><span class="line">   vrrp_strict</span><br><span class="line">   vrrp_garp_interval 0</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface eth0</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 100</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      10.0.0.130/24 dev eth0 label eth0:1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>两个节点启动keepalived并加入开机启动</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start keepalived.service</span><br><span class="line">systemctl enable keepalived.service</span><br></pre></td></tr></table></figure><p>启动完成后检查k8s-node03的IP地址是否已有VIP</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node03 ~]# ip add | grep &quot;130&quot;</span><br><span class="line">    inet 10.0.0.130/24 scope global secondary eth0:1</span><br></pre></td></tr></table></figure><p>然后我们在externalLB上的后端配置此VIP，即可实现预期的效果！！</p><p>效果不展示了！自己实验便是最好的展示！！</p><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s之存储卷及pvc</title>
    <link href="/2019/12/09/k8s%E4%B9%8B%E5%AD%98%E5%82%A8%E5%8D%B7%E5%8F%8Apvc/"/>
    <url>/2019/12/09/k8s%E4%B9%8B%E5%AD%98%E5%82%A8%E5%8D%B7%E5%8F%8Apvc/</url>
    
    <content type="html"><![CDATA[<h4 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h4><p>因为pod是有生命周期的,pod一重启,里面的数据就没了,所以我们需要数据持久化存储,在k8s中,存储卷不属于容器,而是属于pod,也就是说同一个pod中的容器可以共享一个存储卷,存储卷可以是宿主机上的目录,也可以是挂载在宿主机上的外部设备。</p><h5 id="1-1、存储卷类型："><a href="#1-1、存储卷类型：" class="headerlink" title="1.1、存储卷类型："></a>1.1、存储卷类型：</h5><ul><li><p>emptyDIR存储卷：pod一重启,存储卷也删除,这叫emptyDir存储卷,一般用于当做临时空间或缓存关系;</p></li><li><p>hostPath存储卷：宿主机上目录作为存储卷,这种也不是真正意义实现了数据持久性;</p></li><li><p>SAN(iscsi)或NAS(nfs、cifs)：网络存储设备;</p></li><li><p>分布式存储：ceph,glusterfs,cephfs,rbd；</p></li><li><p>云存储：亚马逊的EBS,Azure Disk,阿里云,关键数据一定要有异地备份；</p></li></ul><h5 id="emptyDIR存储卷："><a href="#emptyDIR存储卷：" class="headerlink" title="emptyDIR存储卷："></a>emptyDIR存储卷：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim podtest/pod-vol-demo.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-demo</span><br><span class="line">  namespace: default</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">    tier: frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp</span><br><span class="line">    image: ikubernetes/myapp:v2</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    ports:</span><br><span class="line">    - name: http</span><br><span class="line">      containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  - name: busybox</span><br><span class="line">    image: busybox:latest</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /data/</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br><span class="line">    args: [&quot;-c&quot;,&quot;while true;do echo $(date) &gt;&gt; /data/index.html; sleep 10;done&quot;]</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">volumeMounts:把哪个存储卷挂到pod中的哪个目录下；</span><br><span class="line">emptyDir:不设置意味着对这个参数下的两个选项不做限制；</span><br></pre></td></tr></table></figure><h5 id="hostPath-使用宿主机上目录作为存储卷"><a href="#hostPath-使用宿主机上目录作为存储卷" class="headerlink" title="hostPath:使用宿主机上目录作为存储卷"></a>hostPath:使用宿主机上目录作为存储卷</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl explain pods.spec.volumes.hostPath.type</span><br><span class="line">DirectoryOrCreate:要挂载的路径是一个目录,不存在就创建目录;</span><br><span class="line">Directory:宿主机上必须实现存在目录,如果不存在就报错;</span><br><span class="line">FileOrCreate:表示挂载的是文件,如果不存在就创建;</span><br><span class="line">File:表示要挂载的文件必须事先存在,否则就报错.</span><br><span class="line"> </span><br><span class="line">cat pod-hostpath-vol.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-vol-hostpath</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp</span><br><span class="line">    image: ikubernetes/myapp:v2</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /usr/share/nginx/html/</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /data/pod/volume1</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line"> </span><br><span class="line">hostPath:宿主机上的目录；</span><br><span class="line">volumes的名字可以随便取,这是存储卷的名字,但是上面的volumeMounts指定时,</span><br><span class="line">name必须和存储卷的名字一致,这样两者才建立了联系；</span><br></pre></td></tr></table></figure><h5 id="nfs做共享存储"><a href="#nfs做共享存储" class="headerlink" title="nfs做共享存储"></a>nfs做共享存储</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这里为了方便,把master节点当做nfs存储,三个节点均执行：</span><br><span class="line"></span><br><span class="line">yum -y install nfs-utils  # 然后在master上启动nfs</span><br><span class="line">mkdir /data/volumes</span><br><span class="line">cat /etc/exports</span><br><span class="line">/data/volumes 10.0.0.0/16(rw,no_root_squash)</span><br><span class="line">systemctl start nfs</span><br><span class="line"></span><br><span class="line">在node1和node2上试挂载</span><br><span class="line">mount -t nfs k8s-master:/data/volumes /mnt</span><br><span class="line">cat pod-vol-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-vol-nfs</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp</span><br><span class="line">    image: ikubernetes/myapp:v2</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /usr/share/nginx/html/</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    nfs:</span><br><span class="line">      path: /data/volumes</span><br><span class="line">      server: k8s-master</span><br><span class="line"> </span><br><span class="line">kubectl apply -f pod-vol-nfs.yaml</span><br><span class="line">此时不管pod被建立在哪个节点上,对应节点上是不存放数据的,数据都在nfs主机上</span><br></pre></td></tr></table></figure><h5 id="pvc和pv"><a href="#pvc和pv" class="headerlink" title="pvc和pv"></a>pvc和pv</h5><p>用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。<strong>pvc和pv的关系与pod和node关系类似，前者消耗后者的资源，pvc可以向pv申请指定大小的存储资源并设置访问模式。</strong></p><p><img src="http://myimage.okay686.cn/okay686cn/20191209/gtefBKlEgvB6.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在定义pod时,我们只需要说明我们要一个多大的存储卷就行了,pvc存储卷必须与当前namespace的pvc建立直接绑定关系,pvc必须与pv建立绑定关系,而pv是真正的某个存储设备上的空间.</p><p><strong>一个pvc和pv是一一对应关系,一旦一个pv被一个pvc绑定了,那么这个pv就不能被其他pvc绑定了,一个pvc是可以被多个pod所访问的,pvc在名称空间中,pv是集群级别的</strong>。</p><p><img src="http://myimage.okay686.cn/okay686cn/20191209/h4b5mB22jgnC.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>将master作为存储节点,创建挂载目录<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /data/volumes &amp;&amp; mkdir v&#123;1,2,3,4,5&#125;</span><br><span class="line">cat  /etc/exports</span><br><span class="line">/data/volumes/v1 10.0.0.0/16(rw,no_root_squash)</span><br><span class="line">/data/volumes/v2 10.0.0.0/16(rw,no_root_squash)</span><br><span class="line">/data/volumes/v3 10.0.0.0/16(rw,no_root_squash)</span><br><span class="line">exportfs -arv</span><br><span class="line">showmount -e</span><br><span class="line"></span><br><span class="line">kubectl explain pv.spec.nfs</span><br><span class="line">accessModes模式有:</span><br><span class="line">ReadWriteOnce:单路读写,可以简写为RWO;</span><br><span class="line">ReadOnlyMany:多路只读,可以简写为ROX;</span><br><span class="line">ReadWriteMany:多路读写,可以简写为RWX</span><br><span class="line"> </span><br><span class="line"># 先将存储设备定义为pv</span><br><span class="line">cat pv-demo.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv001 # 定义pv时不用加名称空间,因为pv是集群级别</span><br><span class="line">  labels:</span><br><span class="line">    name: pv001</span><br><span class="line">spec:</span><br><span class="line">  nfs:</span><br><span class="line">    path: /data/volumes/v1</span><br><span class="line">    server: k8s-master</span><br><span class="line">  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;]</span><br><span class="line">  capacity: # 分配磁盘空间大小</span><br><span class="line">    storage: 3Gi</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv002</span><br><span class="line">  labels:</span><br><span class="line">    name: pv002</span><br><span class="line">spec:</span><br><span class="line">  nfs:</span><br><span class="line">    path: /data/volumes/v2</span><br><span class="line">    server: k8s-master</span><br><span class="line">  accessModes: [&quot;ReadWriteOnce&quot;]</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 5Gi</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv003</span><br><span class="line">  labels:</span><br><span class="line">    name: pv003</span><br><span class="line">spec:</span><br><span class="line">  nfs:</span><br><span class="line">    path: /data/volumes/v3</span><br><span class="line">    server: k8s-master</span><br><span class="line">  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;]</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 8Gi</span><br><span class="line"> </span><br><span class="line">kubectl apply -f pv-demo.yaml</span><br><span class="line"></span><br><span class="line">kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS  </span><br><span class="line">pv001     3Gi        RWO,RWX        Retain           Available</span><br><span class="line">pv002     5Gi        RWO            Retain           Available</span><br><span class="line">pv003     8Gi        RWO,RWX        Retain           Available</span><br></pre></td></tr></table></figure></p><h6 id="回收策略："><a href="#回收策略：" class="headerlink" title="回收策略："></a>回收策略：</h6><p>如果某个pvc在pv里面存数据了,后来pvc删了,那么pv里面的数据怎么处理？</p><ul><li>reclaim_policy：即pvc删了,但pv里面的数据不删除,还保留着；</li><li>recycle：即pvc删了,那么就把pv里面的数据也删了；</li><li>delete：即pvc删了,那么就把pv也删了；</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建pvc的清单文件</span><br><span class="line">kubectl explain pods.spec.volumes.persistentVolumeClaim</span><br><span class="line">cat pod-vol-pvc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim # 简称pvc</span><br><span class="line">metadata:</span><br><span class="line">  name: mypvc</span><br><span class="line">  namespace: default # pvc和pod在同一个名称空间</span><br><span class="line">spec:</span><br><span class="line">  accessModes: [&quot;ReadWriteMany&quot;] # 一定是pv策略的子集</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 7Gi # 申请一个大小至少为7G的pv</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-vol-pvc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp</span><br><span class="line">    image: ikubernetes/myapp:v1</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html # 使用的存储卷的名字</span><br><span class="line">      mountPath: /usr/share/nginx/html/ #挂载路径</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: mypvc # 表示要使用哪个pvc</span><br></pre></td></tr></table></figure><p>所以pod的存储卷类型如果是pvc,则:pod指定的pvc需要先匹配一个pv,才能被pod所挂载,在k8s 1.10之后,不能手工从底层删除pv。</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S负载均衡之nginx-ingress</title>
    <link href="/2019/12/07/K8S%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E4%B9%8Bnginx-ingress/"/>
    <url>/2019/12/07/K8S%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E4%B9%8Bnginx-ingress/</url>
    
    <content type="html"><![CDATA[<p>++本篇引自多篇大佬文档整合而来，加上自己所理解整理如下：++</p><h3 id="一、目前状况："><a href="#一、目前状况：" class="headerlink" title="一、目前状况："></a>一、目前状况：</h3><pre><code>k8s有了coreDNS解决了k8s集群内部通过dns域名的方式相互访问容器服务，但是集群内部的域名无法在外部被访问，也没有解决域名7层负载均衡的问题，而nginx-ingress就是为了解决基于k8s的7层负载均衡，nginx-ingress也是已addon方式加入k8s集群，以pod的方式运行，多个副本，高可用。</code></pre><h3 id="二、Nginx-Ingress-一般有三个组件组成："><a href="#二、Nginx-Ingress-一般有三个组件组成：" class="headerlink" title="二、Nginx Ingress 一般有三个组件组成："></a>二、Nginx Ingress 一般有三个组件组成：</h3><ul><li>1）ingress是kubernetes的一个资源对象，用于编写定义规则。</li><li>2）反向代理负载均衡器，通常以Service的Port方式运行，接收并按照ingress定义的规则进行转发，通常为nginx，haproxy，traefik等，本文使用nginx。</li><li>3）ingress-controller，监听apiserver，获取服务新增，删除等变化，并结合ingress规则动态更新到反向代理负载均衡器上，并重载配置使其生效。</li></ul><p>以上三者有机的协调配合起来，就可以完成 Kubernetes 集群服务的暴露。</p><p>来看个图例：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191205/7PokLaEqGsMN.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>Nginx 对后端运行的服务（Service1、Service2）提供反向代理，在配置文件中配置了域名与后端服务 Endpoints 的对应关系。客户端通过使用 DNS 服务或者直接配置本地的 hosts 文件，将域名都映射到 Nginx 代理服务器。当客户端访问 service1.com 时，浏览器会把包含域名的请求发送给 nginx 服务器，nginx 服务器根据传来的域名，选择对应的 Service，这里就是选择 Service 1 后端服务，然后根据一定的负载均衡策略，选择 Service1 中的某个容器接收来自客户端的请求并作出响应。过程很简单，nginx 在整个过程中仿佛是一台根据域名进行请求转发的“路由器”，这也就是7层代理的整体工作流程了！</p><p>对于 Nginx 反向代理做了什么，我们已经大概了解了。在 k8s 系统中，后端服务的变化是十分频繁的，单纯依靠人工来更新nginx 的配置文件几乎不可能，nginx-ingress 由此应运而生。Nginx-ingress 通过监视 k8s 的资源状态变化实现对 nginx 配置文件的自动更新，下面本文就来分析下其工作原理。</p><h4 id="2-1、nginx-ingress-工作流程分析"><a href="#2-1、nginx-ingress-工作流程分析" class="headerlink" title="2.1、nginx-ingress 工作流程分析"></a>2.1、nginx-ingress 工作流程分析</h4><p>首先，上一张整体工作模式架构图（只关注配置同步更新）：</p><p><img src="http://myimage.okay686.cn/okay686cn/20191205/WUbRrLEOqYWx.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p> 不考虑 nginx 状态收集等附件功能，nginx-ingress 模块在运行时主要包括三个主体：<strong>NginxController</strong>、<strong>Store</strong>、<strong>SyncQueue</strong>。</p><ul><li>Store 主要负责从 kubernetes APIServer 收集运行时信息，感知各类资源（如 ingress、service等）的变化，并及时将更新事件消息（event）写入一个环形管道；</li><li>SyncQueue 协程定期扫描 syncQueue 队列，发现有任务就执行更新操作，即借助 Store 完成最新运行数据的拉取，然后根据一定的规则产生新的 nginx 配置，（有些更新必须reload，就本地写入新配置，执行 reload），然后执行动态更新操作，即构造 POST 数据，向本地 Nginx Lua 服务模块发送 post 请求，实现配置更新；</li><li>NginxController 作为中间的联系者，监听 updateChannel，一旦收到配置更新事件，就向同步队列 syncQueue 里写入一个更新请求。</li></ul><p>大白话描述下：</p><ul><li>1、ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置；</li><li>2、再写到nginx-ingress-control的pod里，这个Ingress； controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中；</li><li>3、然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。</li></ul><h4 id="2-2、Ingress-可以解决什么问题"><a href="#2-2、Ingress-可以解决什么问题" class="headerlink" title="2.2、Ingress 可以解决什么问题"></a>2.2、Ingress 可以解决什么问题</h4><h5 id="1、动态配置服务"><a href="#1、动态配置服务" class="headerlink" title="1、动态配置服务"></a>1、动态配置服务</h5><p>　　如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作。　　</p><h5 id="2、减少不必要的端口暴露"><a href="#2、减少不必要的端口暴露" class="headerlink" title="2、减少不必要的端口暴露"></a>2、减少不必要的端口暴露</h5><p>　　配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式。</p><h4 id="2-3、Pod与Ingress的关系"><a href="#2-3、Pod与Ingress的关系" class="headerlink" title="2.3、Pod与Ingress的关系"></a>2.3、Pod与Ingress的关系</h4><ul><li>通过Ingress Controller实现Pod的负载均衡, 支持TCP/UDP 4层和HTTP 7层<br>Ingress 只是定义规则，具体的负载均衡服务是由Ingress controller控制器完成。</li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20191209/LznJshJLip1I.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><img src="http://myimage.okay686.cn/okay686cn/20191209/ChWibWNlOiH1.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><strong>访问流程</strong>：用户—&gt; Ingress Controller(Node) —&gt;service —&gt; Pod</p><h3 id="三、部署nginx-ingress-controller以及定义ingress策略"><a href="#三、部署nginx-ingress-controller以及定义ingress策略" class="headerlink" title="三、部署nginx-ingress-controller以及定义ingress策略"></a>三、部署nginx-ingress-controller以及定义ingress策略</h3><p>20191205最新版目前：</p><p>获取配置文件位置: <a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.26.1/deploy" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/tree/nginx-0.26.1/deploy</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20191205/DNO7ddhKvUXT.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="3-1、下载部署文件"><a href="#3-1、下载部署文件" class="headerlink" title="3.1、下载部署文件"></a>3.1、下载部署文件</h4><p>提供了两种方式 ：</p><ul><li><p>默认下载最新的yaml：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</span><br></pre></td></tr></table></figure></li><li><p>指定版本号下载对应的yaml；</p></li></ul><p>修改镜像路径image</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost src]# grep image mandatory.yaml</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1</span><br></pre></td></tr></table></figure><p>上面的镜像我没办法pull下来，改成使用阿里的google库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1</span><br></pre></td></tr></table></figure><p>修改后的yaml：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line"> </span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-serviceaccount</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">      - &quot;networking.k8s.io&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">      - &quot;networking.k8s.io&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    resourceNames:</span><br><span class="line">      # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot;</span><br><span class="line">      # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot;</span><br><span class="line">      # This has to be adapted if you change either parameter</span><br><span class="line">      # when launching the nginx-ingress-controller.</span><br><span class="line">      - &quot;ingress-controller-leader-nginx&quot;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role-nisa-binding</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole-nisa-binding</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line"> </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      # wait up to five minutes for the drain of connections</span><br><span class="line">      terminationGracePeriodSeconds: 300</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            allowPrivilegeEscalation: true</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 10</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 10</span><br><span class="line">          lifecycle:</span><br><span class="line">            preStop:</span><br><span class="line">              exec:</span><br><span class="line">                command:</span><br><span class="line">                  - /wait-shutdown</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>mandatory.yaml这一个yaml中包含了很多资源的创建，包括namespace、ConfigMap、role，ServiceAccount等等所有部署ingress-controller需要的资源，配置太多就不粘出来了，我们重点看下如上deployment部分↑</p><ul><li><p>可以看到主要使用了“registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1”这个镜像</p></li><li><p>指定了一些启动参数。同时开放了80与443两个端口，并在10254端口做了健康检查。</p></li></ul><p>然后修改上面mandatory.yaml的<strong>deployment部分</strong>配置为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改api版本及kind</span><br><span class="line"># apiVersion: apps/v1</span><br><span class="line"># kind: Deployment</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line"># 删除Replicas</span><br><span class="line"># replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: &quot;10254&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      # 选择对应标签的node</span><br><span class="line">      nodeSelector:</span><br><span class="line">        isIngress: &quot;true&quot;</span><br><span class="line">      # 使用hostNetwork暴露服务</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">          securityContext:</span><br><span class="line">            allowPrivilegeEscalation: true</span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            # www-data -&gt; 33</span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 10</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 10</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>kind: DaemonSet：官方原始文件使用的是deployment，replicate 为 1，这样将会在某一台节点上启动对应的nginx-ingress-controller pod。外部流量访问至该节点，由该节点负载分担至内部的service。测试环境考虑防止单点故障，改为DaemonSet然后删掉replicate ，配合亲和性部署在制定节点上启动nginx-ingress-controller pod，确保有多个节点启动nginx-ingress-controller pod，后续将这些节点加入到外部硬件负载均衡组实现高可用性。</p><p>hostNetwork: true：添加该字段，暴露nginx-ingress-controller pod的服务端口（80）</p><p>nodeSelector: 增加亲和性部署，有isIngress=”true” 标签的节点才会部署该DaemonSet</p><p>为需要部署nginx-ingress-controller的节点设置lable，这里测试部署在”k8s-node1，k8s-node2，k8s-node3”这个节点。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl label node k8s-node1 isIngress=&quot;true&quot;</span><br><span class="line">$ kubectl label node k8s-node2 isIngress=&quot;true&quot;</span><br><span class="line">$ kubectl label node k8s-node3 isIngress=&quot;true&quot;</span><br></pre></td></tr></table></figure><p>执行yaml文件部署</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 src]# kubectl apply -f mandatory.yaml</span><br><span class="line"> </span><br><span class="line"># 执行结果 </span><br><span class="line">namespace/ingress-nginx created</span><br><span class="line">configmap/nginx-configuration created</span><br><span class="line">configmap/tcp-services created</span><br><span class="line">configmap/udp-services created</span><br><span class="line">serviceaccount/nginx-ingress-serviceaccount created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created</span><br><span class="line">role.rbac.authorization.k8s.io/nginx-ingress-role created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created</span><br><span class="line">daemonset.apps/nginx-ingress-controller created</span><br></pre></td></tr></table></figure><h4 id="3-2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）"><a href="#3-2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）" class="headerlink" title="3.2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）"></a>3.2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 src]# kubectl get daemonset -n ingress-nginx</span><br><span class="line">NAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR    AGE</span><br><span class="line">nginx-ingress-controller   1         1         1       1            1           isIngress=true   3m24s</span><br><span class="line">[root@k8s-master1 src]# kubectl get po -n ingress-nginx -o wide</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE     IP                NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-ingress-controller-ql4x5   1/1     Running   0          3m39s   192.168.171.136   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>可以看到，nginx-controller的pod已经部署在在k8s-node1上了。</p><p>到node-1上看下本地端口：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# netstat -lntp | grep nginx</span><br><span class="line">tcp        0      0 127.0.0.1:10247         0.0.0.0:*               LISTEN      34132/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      34132/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:8181            0.0.0.0:*               LISTEN      34132/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      34132/nginx: master</span><br><span class="line">tcp        0      0 127.0.0.1:10245         0.0.0.0:*               LISTEN      34078/nginx-ingress</span><br><span class="line">tcp        0      0 127.0.0.1:10246         0.0.0.0:*               LISTEN      34132/nginx: master</span><br><span class="line">tcp6       0      0 :::10254                :::*                    LISTEN      34078/nginx-ingress</span><br><span class="line">tcp6       0      0 :::80                   :::*                    LISTEN      34132/nginx: master</span><br><span class="line">tcp6       0      0 :::8181                 :::*                    LISTEN      34132/nginx: master</span><br><span class="line">tcp6       0      0 :::443                  :::*                    LISTEN      34132/nginx: master</span><br></pre></td></tr></table></figure></p><p>由于配置了hostnetwork，nginx已经在node主机本地监听80/443/8181端口。其中8181是nginx-controller默认配置的一个default backend。这样，只要访问node主机有公网IP，就可以直接映射域名来对外网暴露服务了。如果要nginx高可用的话，可以在多个node<br>上部署，并在前面再搭建一套LVS+keepalive做负载均衡。用hostnetwork的另一个好处是，如果lvs用DR模式的话，是不支持端口映射的，这时候如果用nodeport，暴露非标准的端口，管理起来会很麻烦。</p><p><strong>划重点：生产须知</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将keepalived与ingress关联</span><br><span class="line"></span><br><span class="line">现状：</span><br><span class="line"></span><br><span class="line">因为pod可以分配在很多node上，若域名与一个node节点绑定，这一个node服务器出现问题，则这个域名就挂了，不能实现高可用</span><br><span class="line"></span><br><span class="line">解决：</span><br><span class="line"></span><br><span class="line">将每个node上装上keepalived服务，设置vip，主master，备用的backup，然后域名 绑定到 vip上就实现高可用（假如10台node，其中1台设置为master，其余9台设置为backup，一旦master挂了，其余9台马上顶替，到时候域名直接绑定虚拟vip即可）</span><br></pre></td></tr></table></figure></p><h4 id="3-3、部署service用于对外提供服务"><a href="#3-3、部署service用于对外提供服务" class="headerlink" title="3.3、部署service用于对外提供服务"></a>3.3、部署service用于对外提供服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml</span><br><span class="line"></span><br><span class="line">修改service文件，指定一下nodePort，使用30080端口和30443端口作为nodePort</span><br></pre></td></tr></table></figure><p>修改后的配置文件如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: http</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">      nodePort: 30080   # http请求对外映射30080端口</span><br><span class="line">    - name: https</span><br><span class="line">      port: 443</span><br><span class="line">      targetPort: 443</span><br><span class="line">      protocol: TCP</span><br><span class="line">      nodePort: 30443  # https请求对外映射30443端口</span><br><span class="line">  selector:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure><h4 id="3-4、部署一个tomcat用于测试ingress转发功能"><a href="#3-4、部署一个tomcat用于测试ingress转发功能" class="headerlink" title="3.4、部署一个tomcat用于测试ingress转发功能"></a>3.4、部署一个tomcat用于测试ingress转发功能</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim k8s-tomcat-test.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">   app: tomcat</span><br><span class="line">   release: canary</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    targetPort: 8080</span><br><span class="line">    port: 8080</span><br><span class="line">  - name: ajp</span><br><span class="line">    targetPort: 8009</span><br><span class="line">    port: 8009</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line"> </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-deploy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: tomcat</span><br><span class="line">     release: canary</span><br><span class="line">  template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: tomcat</span><br><span class="line">       release: canary</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - name: tomcat</span><br><span class="line">       image: tomcat</span><br><span class="line">       ports:</span><br><span class="line">       - name: http</span><br><span class="line">         containerPort: 8080</span><br></pre></td></tr></table></figure><h4 id="3-5、定义ingress策略"><a href="#3-5、定义ingress策略" class="headerlink" title="3.5、定义ingress策略"></a>3.5、定义ingress策略</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim k8s-tomcat-test-ingress.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-tomcat</span><br><span class="line">  annotations:</span><br><span class="line">    kubernets.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: myapp.zhdya.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path:</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat</span><br><span class="line">          servicePort: 8080</span><br></pre></td></tr></table></figure><p>手动绑定hosts测试：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.171.188 myapp.zhdya.com</span><br></pre></td></tr></table></figure></p><p>把tomcat service通过ingress发布出去：<br><img src="http://myimage.okay686.cn/okay686cn/20191209/9Xaas32hkhyg.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在浏览器输入：<a href="http://myapp.zhdya.com:30080/" target="_blank" rel="noopener">http://myapp.zhdya.com:30080/</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20191207/ImaETDuVh832.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="3-6、下面我们对tomcat服务添加https服务"><a href="#3-6、下面我们对tomcat服务添加https服务" class="headerlink" title="3.6、下面我们对tomcat服务添加https服务"></a>3.6、下面我们对tomcat服务添加https服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master ingress-nginx]# openssl genrsa -out tls.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">.......+++</span><br><span class="line">..............................+++</span><br><span class="line">e is 65537 (0x10001)</span><br><span class="line">[root@k8s-master ingress-nginx]# openssl req -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=myapp.zhdya.com #注意域名要和服务的域名一致 </span><br><span class="line">[root@k8s-master ingress-nginx]# kubectl create secret tls tomcat-ingress-secret --cert=tls.crt --key=tls.key #创建secret</span><br><span class="line">secret &quot;tomcat-ingress-secret&quot; created</span><br><span class="line">[root@k8s-master ingress-nginx]# kubectl get secret</span><br><span class="line">NAME                    TYPE                                  DATA      AGE</span><br><span class="line">default-token-bf52l     kubernetes.io/service-account-token   3         9d</span><br><span class="line">tomcat-ingress-secret   kubernetes.io/tls                     2         7s</span><br><span class="line">[root@k8s-master ingress-nginx]# kubectl describe secret tomcat-ingress-secret</span><br><span class="line">Name:         tomcat-ingress-secret</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/tls</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">tls.crt:  1294 bytes  #base64加密</span><br><span class="line">tls.key:  1679 bytes</span><br></pre></td></tr></table></figure><p>将证书应用至tomcat服务中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master01 ingress]# vim k8s-tomcat-test-ingress-tls.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-tomcat-tls</span><br><span class="line">  annotations: </span><br><span class="line">    kubernets.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - myapp.zhdya.com        #与secret证书的域名需要保持一致</span><br><span class="line">    secretName: tomcat-ingress-secret   #secret证书的名称</span><br><span class="line">  rules:</span><br><span class="line">  - host: myapp.zhdya.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: </span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat</span><br><span class="line">          servicePort: 8080</span><br><span class="line">[root@k8s-master01 ingress]#  kubectl apply -f k8s-tomcat-test-ingress-tls.yaml</span><br></pre></td></tr></table></figure></p><p>再次访问服务：</p><p><a href="https://myapp.zhdya.com:30443/" target="_blank" rel="noopener">https://myapp.zhdya.com:30443/</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20191207/6z3WmSMi3hMK.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="文末彩蛋："><a href="#文末彩蛋：" class="headerlink" title="文末彩蛋："></a>文末彩蛋：</h4><p>从3.3-3.6大家有没有发现，我在测试tomcat的容器的时候手动创建了一个service-nodeport.yaml 这个文件，这个文件刚刚也讲到了，就是为了把容器内部的服务暴露出来，当然我们自己也测试了手动绑定了tomcat pod容器所在node节点的IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.171.188 myapp.zhdya.com</span><br></pre></td></tr></table></figure></p><p>之前我是为了给大家证明，内部pod是如何把service从pod中暴露出来，但是细心的人肯定发现了，在3.1章节，我们明明已经创建了一个nginx-ingress-controller 这个就完全可以帮我们完成服务暴露啊。对的，非常对！！！</p><p>nginx-ingress-controller这个重要的组件具体实现了什么功能看文首！！</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get po,svc,ep --all-namespaces -o wide</span><br><span class="line">NAMESPACE              NAME                                             READY   STATUS    RESTARTS   AGE    IP                NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">default                pod/busybox                                      1/1     Running   7          8d     10.244.0.16       k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                pod/tomcat-deploy-758b795dcc-69gjz               1/1     Running   1          2d6h   10.244.0.17       k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                pod/tomcat-deploy-758b795dcc-llcp5               1/1     Running   2          2d6h   10.244.1.17       k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                pod/web-d86c95cc9-k9vnf                          1/1     Running   4          9d     10.244.1.20       k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                pod/web-d86c95cc9-x2wn6                          1/1     Running   4          8d     10.244.0.18       k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-ql4x5               1/1     Running   0          47m    192.168.171.136   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/coredns-6d8cfdd59d-gbd2m                     1/1     Running   5          8d     10.244.0.19       k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-d2gzx                  1/1     Running   3          9d     192.168.171.136   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-lwsnd                  1/1     Running   4          9d     192.168.171.137   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   pod/dashboard-metrics-scraper-566cddb686-wrkfl   1/1     Running   3          9d     10.244.1.19       k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   pod/kubernetes-dashboard-7b5bf5d559-csfwm        1/1     Running   4          9d     10.244.1.18       k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 src]# kubectl exec -it nginx-ingress-controller-ql4x5 -n ingress-nginx -- cat nginx.conf</span><br><span class="line"></span><br><span class="line">...start省略不重要的配置...</span><br><span class="line">## start server myapp.zhdya.com</span><br><span class="line">server &#123;</span><br><span class="line">server_name myapp.zhdya.com ;</span><br><span class="line"></span><br><span class="line">listen 80  ;</span><br><span class="line">listen [::]:80  ;</span><br><span class="line">listen 443  ssl http2 ;</span><br><span class="line">listen [::]:443  ssl http2 ;</span><br><span class="line"></span><br><span class="line">set $proxy_upstream_name &quot;-&quot;;</span><br><span class="line"></span><br><span class="line">ssl_certificate_by_lua_block &#123;</span><br><span class="line">certificate.call()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location / &#123;</span><br><span class="line"></span><br><span class="line">set $namespace      &quot;default&quot;;</span><br><span class="line">set $ingress_name   &quot;ingress-tomcat&quot;;</span><br><span class="line">set $service_name   &quot;tomcat&quot;;</span><br><span class="line">set $service_port   &quot;8080&quot;;</span><br><span class="line">set $location_path  &quot;/&quot;;</span><br><span class="line"></span><br><span class="line">rewrite_by_lua_block &#123;</span><br><span class="line">lua_ingress.rewrite(&#123;</span><br><span class="line">force_ssl_redirect = false,</span><br><span class="line">ssl_redirect = true,</span><br><span class="line">force_no_ssl_redirect = false,</span><br><span class="line">use_port_in_redirects = false,</span><br><span class="line">&#125;)</span><br><span class="line">balancer.rewrite()</span><br><span class="line">plugins.run()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header_filter_by_lua_block &#123;</span><br><span class="line"></span><br><span class="line">plugins.run()</span><br><span class="line">&#125;</span><br><span class="line">body_filter_by_lua_block &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log_by_lua_block &#123;</span><br><span class="line"></span><br><span class="line">balancer.log()</span><br><span class="line"></span><br><span class="line">monitor.call()</span><br><span class="line"></span><br><span class="line">plugins.run()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">port_in_redirect off;</span><br><span class="line"></span><br><span class="line">set $balancer_ewma_score -1;</span><br><span class="line">set $proxy_upstream_name &quot;default-tomcat-8080&quot;;</span><br><span class="line">set $proxy_host          $proxy_upstream_name;</span><br><span class="line">set $pass_access_scheme  $scheme;</span><br><span class="line">set $pass_server_port    $server_port;</span><br><span class="line">set $best_http_host      $http_host;</span><br><span class="line">set $pass_port           $pass_server_port;</span><br><span class="line"></span><br><span class="line">set $proxy_alternative_upstream_name &quot;&quot;;</span><br><span class="line"></span><br><span class="line">client_max_body_size                    1m;</span><br><span class="line"></span><br><span class="line">proxy_set_header Host                   $best_http_host;</span><br><span class="line"></span><br><span class="line"># Pass the extracted client certificate to the backend</span><br><span class="line"></span><br><span class="line"># Allow websocket connections</span><br><span class="line">proxy_set_header                        Upgrade           $http_upgrade;</span><br><span class="line"></span><br><span class="line">proxy_set_header                        Connection        $connection_upgrade;</span><br><span class="line"></span><br><span class="line">proxy_set_header X-Request-ID           $req_id;</span><br><span class="line">proxy_set_header X-Real-IP              $remote_addr;</span><br><span class="line"></span><br><span class="line">proxy_set_header X-Forwarded-For        $remote_addr;</span><br><span class="line"></span><br><span class="line">proxy_set_header X-Forwarded-Host       $best_http_host;</span><br><span class="line">proxy_set_header X-Forwarded-Port       $pass_port;</span><br><span class="line">proxy_set_header X-Forwarded-Proto      $pass_access_scheme;</span><br><span class="line"></span><br><span class="line">proxy_set_header X-Scheme               $pass_access_scheme;</span><br><span class="line"></span><br><span class="line"># Pass the original X-Forwarded-For</span><br><span class="line">proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;</span><br><span class="line"></span><br><span class="line"># mitigate HTTPoxy Vulnerability</span><br><span class="line"># https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/</span><br><span class="line">proxy_set_header Proxy                  &quot;&quot;;</span><br><span class="line"></span><br><span class="line"># Custom headers to proxied server</span><br><span class="line"></span><br><span class="line">proxy_connect_timeout                   5s;</span><br><span class="line">proxy_send_timeout                      60s;</span><br><span class="line">proxy_read_timeout                      60s;</span><br><span class="line"></span><br><span class="line">proxy_buffering                         off;</span><br><span class="line">proxy_buffer_size                       4k;</span><br><span class="line">proxy_buffers                           4 4k;</span><br><span class="line"></span><br><span class="line">proxy_max_temp_file_size                1024m;</span><br><span class="line"></span><br><span class="line">proxy_request_buffering                 on;</span><br><span class="line">proxy_http_version                      1.1;</span><br><span class="line"></span><br><span class="line">proxy_cookie_domain                     off;</span><br><span class="line">proxy_cookie_path                       off;</span><br><span class="line"></span><br><span class="line"># In case of errors try the next upstream server before returning an error</span><br><span class="line">proxy_next_upstream                     error timeout;</span><br><span class="line">proxy_next_upstream_timeout             0;</span><br><span class="line">proxy_next_upstream_tries               3;</span><br><span class="line"></span><br><span class="line">proxy_pass http://upstream_balancer;</span><br><span class="line"></span><br><span class="line">proxy_redirect                          off;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">## end server myapp.zhdya.com</span><br><span class="line">...end省略不重要的配置...</span><br></pre></td></tr></table></figure><p>建议大家一定要把这个nginx.conf文件细细的看下你就会证实nginx-ingress-controller 这个组件的功劳是多么的强大！！</p><p>然后我们换掉之前手动绑定的hosts，变更为 ingress-nginx pod所在的节点：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.171.136 myapp.zhdya.com</span><br></pre></td></tr></table></figure></p><p>再次访问 是不是就不需要所谓的 30443端口了呢？？？</p><p><img src="http://myimage.okay686.cn/okay686cn/20191209/GRSGRKWqWgYA.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>再然后，小伙伴们也知道了外层的externalLB改如何操作和绑定了吧？当然还有我文中的重点！！</p><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S集群优化之路由转发：使用IPVS替代iptables</title>
    <link href="/2019/12/06/K8S%E9%9B%86%E7%BE%A4%E4%BC%98%E5%8C%96%E4%B9%8B%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91%EF%BC%9A%E4%BD%BF%E7%94%A8IPVS%E6%9B%BF%E4%BB%A3iptables/"/>
    <url>/2019/12/06/K8S%E9%9B%86%E7%BE%A4%E4%BC%98%E5%8C%96%E4%B9%8B%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91%EF%BC%9A%E4%BD%BF%E7%94%A8IPVS%E6%9B%BF%E4%BB%A3iptables/</url>
    
    <content type="html"><![CDATA[<h4 id="一、为什么要使用IPVS"><a href="#一、为什么要使用IPVS" class="headerlink" title="一、为什么要使用IPVS"></a>一、为什么要使用IPVS</h4><p>从k8s的1.8版本开始，kube-proxy引入了IPVS模式，IPVS模式与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。</p><h4 id="二、具体步骤"><a href="#二、具体步骤" class="headerlink" title="二、具体步骤"></a>二、具体步骤</h4><h5 id="2-1、开启内核参数"><a href="#2-1、开启内核参数" class="headerlink" title="2.1、开启内核参数"></a>2.1、开启内核参数</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h5 id="2-2、开启ipvs支持"><a href="#2-2、开启ipvs支持" class="headerlink" title="2.2、开启ipvs支持"></a>2.2、开启ipvs支持</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install ipvsadm  ipset</span><br><span class="line"></span><br><span class="line"># 临时生效</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line"></span><br><span class="line"># 永久生效</span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h5 id="2-3、配置kube-proxy"><a href="#2-3、配置kube-proxy" class="headerlink" title="2.3、配置kube-proxy"></a>2.3、配置kube-proxy</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加下面两行</span><br><span class="line">  --proxy-mode=ipvs  \</span><br><span class="line">  --masquerade-all=true \</span><br><span class="line"></span><br><span class="line"># 修改服务文件</span><br><span class="line">vim /usr/lib/systemd/system/kube-proxy.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kube-Proxy Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/data/k8s/kube-proxy</span><br><span class="line">ExecStart=/data/k8s/bin/kube-proxy \</span><br><span class="line">  --bind-address=192.168.1.145 \</span><br><span class="line">  --hostname-override=192.168.1.145 \</span><br><span class="line">  --cluster-cidr=10.254.0.0/16 \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><br><span class="line">  --logtostderr=true \</span><br><span class="line">  --proxy-mode=ipvs  \</span><br><span class="line">  --masquerade-all=true \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h5 id="2-4、重启kube-proxy"><a href="#2-4、重启kube-proxy" class="headerlink" title="2.4、重启kube-proxy"></a>2.4、重启kube-proxy</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kube-proxy</span><br><span class="line">systemctl status kube-proxy</span><br></pre></td></tr></table></figure><h3 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h3><p>测试是否生效<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8sNode01 docker]# ipvsadm -Ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.1.142:6443           Masq    1      0          0         </span><br><span class="line">  -&gt; 192.168.1.143:6443           Masq    1      1          0         </span><br><span class="line">  -&gt; 192.168.1.144:6443           Masq    1      1          0         </span><br><span class="line">TCP  10.254.27.38:80 rr</span><br><span class="line">  -&gt; 172.30.36.4:9090             Masq    1      0          0         </span><br><span class="line">TCP  10.254.72.60:80 rr</span><br><span class="line">  -&gt; 172.30.90.4:8080             Masq    1      0          0         </span><br><span class="line">TCP  10.254.72.247:80 rr</span><br><span class="line">  -&gt; 172.30.36.5:3000             Masq    1      0          0         </span><br><span class="line">TCP  127.0.0.1:27841 rr</span><br><span class="line">  -&gt; 172.30.36.2:80               Masq    1      0          0         </span><br><span class="line">  -&gt; 172.30.90.2:80               Masq    1      0          0         </span><br><span class="line">TCP  127.0.0.1:28453 rr</span><br><span class="line">  -&gt; 172.30.36.5:3000             Masq    1      0          0         </span><br><span class="line">TCP  127.0.0.1:36018 rr</span><br><span class="line">  -&gt; 172.30.36.4:9090             Masq    1      0          0         </span><br><span class="line">TCP  172.30.90.0:27841 rr</span><br><span class="line">  -&gt; 172.30.36.2:80               Masq    1      0          0         </span><br><span class="line">  -&gt; 172.30.90.2:80               Masq    1      0          0</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ansible自动化部署kubernetes-1.16</title>
    <link href="/2019/12/05/ansible%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2kubernetes-1.16/"/>
    <url>/2019/12/05/ansible%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2kubernetes-1.16/</url>
    
    <content type="html"><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>集群包含coreDNS、cni、nginx-ingress、HA、flanneld</p><p>百度网盘链接：<a href="https://pan.baidu.com/s/1KYbpshhpTu62DnQwF1LUnQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1KYbpshhpTu62DnQwF1LUnQ</a> 提取码：vi5e</p><h3 id="一、单master部署"><a href="#一、单master部署" class="headerlink" title="一、单master部署"></a>一、单master部署</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ~]# tree ansible-install-k8s-master</span><br><span class="line">ansible-install-k8s-master</span><br><span class="line">├── add-node.yml</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── group_vars</span><br><span class="line">│   └── all.yml</span><br><span class="line">├── hosts</span><br><span class="line">├── multi-master-deploy.yml</span><br><span class="line">├── multi-master.jpg</span><br><span class="line">├── README.md</span><br><span class="line">├── roles</span><br><span class="line">│   ├── addons</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   ├── coredns.yaml</span><br><span class="line">│   │   │   ├── ingress-controller.yaml</span><br><span class="line">│   │   │   ├── kube-flannel.yaml</span><br><span class="line">│   │   │   └── kubernetes-dashboard.yaml</span><br><span class="line">│   │   └── tasks</span><br><span class="line">│   │       └── main.yml</span><br><span class="line">│   ├── common</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yml</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   │       └── hosts.j2</span><br><span class="line">│   ├── docker</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   ├── daemon.json</span><br><span class="line">│   │   │   └── docker.service</span><br><span class="line">│   │   └── tasks</span><br><span class="line">│   │       └── main.yml</span><br><span class="line">│   ├── etcd</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   └── etcd_cert</span><br><span class="line">│   │   │       ├── ca-key.pem</span><br><span class="line">│   │   │       ├── ca.pem</span><br><span class="line">│   │   │       ├── server-key.pem</span><br><span class="line">│   │   │       └── server.pem</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yml</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   │       ├── etcd.conf.j2</span><br><span class="line">│   │       ├── etcd.service.j2</span><br><span class="line">│   │       └── etcd.sh.j2</span><br><span class="line">│   ├── ha</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   └── check_nginx.sh</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yml</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   │       ├── keepalived.conf.j2</span><br><span class="line">│   │       └── nginx.conf.j2</span><br><span class="line">│   ├── master</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   ├── apiserver-to-kubelet-rbac.yaml</span><br><span class="line">│   │   │   ├── etcd_cert</span><br><span class="line">│   │   │   │   ├── ca.pem</span><br><span class="line">│   │   │   │   ├── server-key.pem</span><br><span class="line">│   │   │   │   └── server.pem</span><br><span class="line">│   │   │   ├── k8s_cert</span><br><span class="line">│   │   │   │   ├── admin-key.pem</span><br><span class="line">│   │   │   │   ├── admin.pem</span><br><span class="line">│   │   │   │   ├── ca-key.pem</span><br><span class="line">│   │   │   │   ├── ca.pem</span><br><span class="line">│   │   │   │   ├── kube-proxy-key.pem</span><br><span class="line">│   │   │   │   ├── kube-proxy.pem</span><br><span class="line">│   │   │   │   ├── server-key.pem</span><br><span class="line">│   │   │   │   └── server.pem</span><br><span class="line">│   │   │   ├── kubelet-bootstrap-rbac.yaml</span><br><span class="line">│   │   │   └── token.csv</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yml</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   │       ├── kube-apiserver.conf.j2</span><br><span class="line">│   │       ├── kube-apiserver.service.j2</span><br><span class="line">│   │       ├── kube-controller-manager.conf.j2</span><br><span class="line">│   │       ├── kube-controller-manager.service.j2</span><br><span class="line">│   │       ├── kube-scheduler.conf.j2</span><br><span class="line">│   │       └── kube-scheduler.service.j2</span><br><span class="line">│   ├── node</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   │   └── k8s_cert</span><br><span class="line">│   │   │       ├── ca.pem</span><br><span class="line">│   │   │       ├── kube-proxy-key.pem</span><br><span class="line">│   │   │       └── kube-proxy.pem</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yml</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   │       ├── bootstrap.kubeconfig.j2</span><br><span class="line">│   │       ├── kubelet-config.yml.j2</span><br><span class="line">│   │       ├── kubelet.conf.j2</span><br><span class="line">│   │       ├── kubelet.service.j2</span><br><span class="line">│   │       ├── kube-proxy-config.yml.j2</span><br><span class="line">│   │       ├── kube-proxy.conf.j2</span><br><span class="line">│   │       ├── kube-proxy.kubeconfig.j2</span><br><span class="line">│   │       └── kube-proxy.service.j2</span><br><span class="line">│   └── tls</span><br><span class="line">│       ├── files</span><br><span class="line">│       │   ├── generate_etcd_cert.sh</span><br><span class="line">│       │   └── generate_k8s_cert.sh</span><br><span class="line">│       ├── tasks</span><br><span class="line">│       │   └── main.yml</span><br><span class="line">│       └── templates</span><br><span class="line">│           ├── etcd</span><br><span class="line">│           │   ├── ca-config.json.j2</span><br><span class="line">│           │   ├── ca-csr.json.j2</span><br><span class="line">│           │   └── server-csr.json.j2</span><br><span class="line">│           └── k8s</span><br><span class="line">│               ├── admin-csr.json.j2</span><br><span class="line">│               ├── ca-config.json.j2</span><br><span class="line">│               ├── ca-csr.json.j2</span><br><span class="line">│               ├── kube-proxy-csr.json.j2</span><br><span class="line">│               └── server-csr.json.j2</span><br><span class="line">├── single-master-deploy.yml</span><br><span class="line">└── single-master.jpg</span><br></pre></td></tr></table></figure><h4 id="1-1、解压缩binary-pkg-tar-gz"><a href="#1-1、解压缩binary-pkg-tar-gz" class="headerlink" title="1.1、解压缩binary_pkg.tar.gz"></a>1.1、解压缩binary_pkg.tar.gz</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ~]# ls</span><br><span class="line">anaconda-ks.cfg  ansible-install-k8s-master  ansible-install-k8s-master.zip  binary_pkg.tar.gz</span><br><span class="line">[root@k8s-ansible1 ~]# tar zxvf binary_pkg.tar.gz</span><br></pre></td></tr></table></figure><h4 id="1-2、修改所以节点hosts-及ansible内的hosts"><a href="#1-2、修改所以节点hosts-及ansible内的hosts" class="headerlink" title="1.2、修改所以节点hosts 及ansible内的hosts"></a>1.2、修改所以节点hosts 及ansible内的hosts</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ~]# vim /etc/hosts</span><br><span class="line"></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.171.11 k8s-ansible1     ##ansible机器</span><br><span class="line">192.168.171.12 k8s-ansible2     ##master</span><br><span class="line">192.168.171.13 k8s-ansible3     ##node1</span><br><span class="line">192.168.171.14 k8s-ansible4     ##node2</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible1 ~]# cd ansible-install-k8s-master</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts</span><br><span class="line">[master]</span><br><span class="line"># 如果部署单Master，只保留一个Master节点</span><br><span class="line">192.168.171.12 node_name=k8s-ansible2</span><br><span class="line">#192.168.171.111 node_name=k8s-master2</span><br><span class="line"></span><br><span class="line">[node]</span><br><span class="line">192.168.171.13 node_name=k8s-ansible3</span><br><span class="line">192.168.171.14 node_name=k8s-ansible4</span><br><span class="line"></span><br><span class="line">[etcd]</span><br><span class="line">192.168.171.12 etcd_name=k8s-ansible2</span><br><span class="line">192.168.171.13 etcd_name=k8s-ansible3</span><br><span class="line">192.168.171.14 etcd_name=k8s-ansible4</span><br><span class="line"></span><br><span class="line">[lb]</span><br><span class="line"># 如果部署单Master，该项忽略</span><br><span class="line">192.168.31.63 lb_name=lb-master</span><br><span class="line">192.168.31.71 lb_name=lb-backup</span><br><span class="line"></span><br><span class="line">[k8s:children]</span><br><span class="line">master</span><br><span class="line">node</span><br><span class="line"></span><br><span class="line">[newnode]</span><br><span class="line">#192.168.31.91 node_name=k8s-node3</span><br></pre></td></tr></table></figure><h4 id="1-3、更改全局环境配置"><a href="#1-3、更改全局环境配置" class="headerlink" title="1.3、更改全局环境配置"></a>1.3、更改全局环境配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml</span><br><span class="line"># 安装目录</span><br><span class="line">software_dir: &apos;/root/binary_pkg&apos;</span><br><span class="line">k8s_work_dir: &apos;/opt/kubernetes&apos;</span><br><span class="line">etcd_work_dir: &apos;/opt/etcd&apos;</span><br><span class="line">tmp_dir: &apos;/tmp/k8s&apos;</span><br><span class="line"></span><br><span class="line"># 集群网络</span><br><span class="line">service_cidr: &apos;10.0.0.0/24&apos;</span><br><span class="line">cluster_dns: &apos;10.0.0.2&apos;   # 与roles/addons/files/coredns.yaml中IP一致</span><br><span class="line">pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致</span><br><span class="line">service_nodeport_range: &apos;30000-32767&apos;</span><br><span class="line">cluster_domain: &apos;cluster.local&apos;</span><br><span class="line"></span><br><span class="line"># 高可用，如果部署单Master，该项忽略</span><br><span class="line">vip: &apos;192.168.31.88&apos;</span><br><span class="line">nic: &apos;ens33&apos;</span><br><span class="line"></span><br><span class="line"># 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span><br><span class="line">cert_hosts:</span><br><span class="line">  # 包含所有LB、VIP、Master（多多益善，可以多余出来几个后期扩展用） IP和service_cidr的第一个IP</span><br><span class="line">  k8s:</span><br><span class="line">    - 10.0.0.1</span><br><span class="line">    - 192.168.171.11</span><br><span class="line">    - 192.168.171.12</span><br><span class="line">    - 192.168.171.13</span><br><span class="line">    - 192.168.171.14</span><br><span class="line">    - 192.168.171.15</span><br><span class="line">    - 192.168.171.16</span><br><span class="line">    - 192.168.171.17</span><br><span class="line">    - 192.168.171.18</span><br><span class="line">    - 192.168.171.19</span><br><span class="line">    - 192.168.171.111</span><br><span class="line">  # 包含所有etcd节点IP</span><br><span class="line">  etcd:</span><br><span class="line">    - 192.168.171.12</span><br><span class="line">    - 192.168.171.13</span><br><span class="line">    - 192.168.171.14</span><br></pre></td></tr></table></figure><h3 id="二、准备部署"><a href="#二、准备部署" class="headerlink" title="二、准备部署"></a>二、准备部署</h3><h4 id="2-1、单Master版："><a href="#2-1、单Master版：" class="headerlink" title="2.1、单Master版："></a>2.1、单Master版：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible-playbook -i hosts single-master-deploy.yml -uroot -k</span><br></pre></td></tr></table></figure><h4 id="2-2、历史记录"><a href="#2-2、历史记录" class="headerlink" title="2.2、历史记录"></a>2.2、历史记录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ansible-install-k8s-master]# ansible-playbook -i hosts single-master-deploy.yml -uroot -k</span><br><span class="line">SSH password:</span><br><span class="line"></span><br><span class="line">PLAY [0.系统初始化] ********************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [common : 关闭firewalld] *******************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">TASK [common : 关闭selinux] *********************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [common : 关闭swap] ************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [common : 即时生效] **************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [common : 拷贝时区] **************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">TASK [common : 添加hosts] ***********************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">PLAY [1.自签证书] *********************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [tls : 获取Ansible工作目录] ********************************************************************************************************************************************</span><br><span class="line">changed: [localhost]</span><br><span class="line"></span><br><span class="line">TASK [tls : 创建工作目录] ***************************************************************************************************************************************************</span><br><span class="line">ok: [localhost] =&gt; (item=etcd)</span><br><span class="line">ok: [localhost] =&gt; (item=k8s)</span><br><span class="line"></span><br><span class="line">TASK [tls : 准备cfssl工具] ************************************************************************************************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line">TASK [tls : 准备etcd证书请求文件] *********************************************************************************************************************************************</span><br><span class="line">ok: [localhost] =&gt; (item=ca-config.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=ca-csr.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=server-csr.json.j2)</span><br><span class="line"></span><br><span class="line">TASK [tls : 准备生成etcd证书脚本] *********************************************************************************************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line">TASK [tls : 生成etcd证书] *************************************************************************************************************************************************</span><br><span class="line">changed: [localhost]</span><br><span class="line"></span><br><span class="line">TASK [tls : 准备k8s证书请求文件] **********************************************************************************************************************************************</span><br><span class="line">ok: [localhost] =&gt; (item=ca-config.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=ca-csr.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=server-csr.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=admin-csr.json.j2)</span><br><span class="line">ok: [localhost] =&gt; (item=kube-proxy-csr.json.j2)</span><br><span class="line"></span><br><span class="line">TASK [tls : 准备生成k8s证书脚本] **********************************************************************************************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line">TASK [tls : 生成k8s证书] **************************************************************************************************************************************************</span><br><span class="line">changed: [localhost]</span><br><span class="line"></span><br><span class="line">PLAY [2.部署Docker] *****************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [docker : 创建临时目录] ************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 分发并解压docker二进制包] ***************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)</span><br><span class="line">ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)</span><br><span class="line"></span><br><span class="line">TASK [docker : 移动docker二进制文件] *****************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 分发service文件] *******************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 创建目录] **************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 配置docker] **********************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 启动docker] **********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : 查看状态] **************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [docker : debug] *************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.13] =&gt; &#123;</span><br><span class="line">    &quot;docker.stdout_lines&quot;: [</span><br><span class="line">        &quot;Containers: 0&quot;,</span><br><span class="line">        &quot; Running: 0&quot;,</span><br><span class="line">        &quot; Paused: 0&quot;,</span><br><span class="line">        &quot; Stopped: 0&quot;,</span><br><span class="line">        &quot;Images: 0&quot;,</span><br><span class="line">        &quot;Server Version: 18.09.6&quot;,</span><br><span class="line">        &quot;Storage Driver: overlay2&quot;,</span><br><span class="line">        &quot; Backing Filesystem: xfs&quot;,</span><br><span class="line">        &quot; Supports d_type: true&quot;,</span><br><span class="line">        &quot; Native Overlay Diff: true&quot;,</span><br><span class="line">        &quot;Logging Driver: json-file&quot;,</span><br><span class="line">        &quot;Cgroup Driver: cgroupfs&quot;,</span><br><span class="line">        &quot;Plugins:&quot;,</span><br><span class="line">        &quot; Volume: local&quot;,</span><br><span class="line">        &quot; Network: bridge host macvlan null overlay&quot;,</span><br><span class="line">        &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;,</span><br><span class="line">        &quot;Swarm: inactive&quot;,</span><br><span class="line">        &quot;Runtimes: runc&quot;,</span><br><span class="line">        &quot;Default Runtime: runc&quot;,</span><br><span class="line">        &quot;Init Binary: docker-init&quot;,</span><br><span class="line">        &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;,</span><br><span class="line">        &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;,</span><br><span class="line">        &quot;init version: fec3683&quot;,</span><br><span class="line">        &quot;Security Options:&quot;,</span><br><span class="line">        &quot; seccomp&quot;,</span><br><span class="line">        &quot;  Profile: default&quot;,</span><br><span class="line">        &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;,</span><br><span class="line">        &quot;Operating System: CentOS Linux 7 (Core)&quot;,</span><br><span class="line">        &quot;OSType: linux&quot;,</span><br><span class="line">        &quot;Architecture: x86_64&quot;,</span><br><span class="line">        &quot;CPUs: 2&quot;,</span><br><span class="line">        &quot;Total Memory: 1.777GiB&quot;,</span><br><span class="line">        &quot;Name: k8s-ansible3&quot;,</span><br><span class="line">        &quot;ID: O3LF:KDZ3:CXD6:MU6T:3DKL:PS42:6ATX:R4QE:GMI7:QHNO:CVQO:7ZW6&quot;,</span><br><span class="line">        &quot;Docker Root Dir: /var/lib/docker&quot;,</span><br><span class="line">        &quot;Debug Mode (client): false&quot;,</span><br><span class="line">        &quot;Debug Mode (server): false&quot;,</span><br><span class="line">        &quot;Registry: https://index.docker.io/v1/&quot;,</span><br><span class="line">        &quot;Labels:&quot;,</span><br><span class="line">        &quot;Experimental: false&quot;,</span><br><span class="line">        &quot;Insecure Registries:&quot;,</span><br><span class="line">        &quot; 192.168.31.70&quot;,</span><br><span class="line">        &quot; 127.0.0.0/8&quot;,</span><br><span class="line">        &quot;Registry Mirrors:&quot;,</span><br><span class="line">        &quot; http://bc437cce.m.daocloud.io/&quot;,</span><br><span class="line">        &quot;Live Restore Enabled: false&quot;,</span><br><span class="line">        &quot;Product License: Community Engine&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">ok: [192.168.171.12] =&gt; &#123;</span><br><span class="line">    &quot;docker.stdout_lines&quot;: [</span><br><span class="line">        &quot;Containers: 0&quot;,</span><br><span class="line">        &quot; Running: 0&quot;,</span><br><span class="line">        &quot; Paused: 0&quot;,</span><br><span class="line">        &quot; Stopped: 0&quot;,</span><br><span class="line">        &quot;Images: 0&quot;,</span><br><span class="line">        &quot;Server Version: 18.09.6&quot;,</span><br><span class="line">        &quot;Storage Driver: overlay2&quot;,</span><br><span class="line">        &quot; Backing Filesystem: xfs&quot;,</span><br><span class="line">        &quot; Supports d_type: true&quot;,</span><br><span class="line">        &quot; Native Overlay Diff: true&quot;,</span><br><span class="line">        &quot;Logging Driver: json-file&quot;,</span><br><span class="line">        &quot;Cgroup Driver: cgroupfs&quot;,</span><br><span class="line">        &quot;Plugins:&quot;,</span><br><span class="line">        &quot; Volume: local&quot;,</span><br><span class="line">        &quot; Network: bridge host macvlan null overlay&quot;,</span><br><span class="line">        &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;,</span><br><span class="line">        &quot;Swarm: inactive&quot;,</span><br><span class="line">        &quot;Runtimes: runc&quot;,</span><br><span class="line">        &quot;Default Runtime: runc&quot;,</span><br><span class="line">        &quot;Init Binary: docker-init&quot;,</span><br><span class="line">        &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;,</span><br><span class="line">        &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;,</span><br><span class="line">        &quot;init version: fec3683&quot;,</span><br><span class="line">        &quot;Security Options:&quot;,</span><br><span class="line">        &quot; seccomp&quot;,</span><br><span class="line">        &quot;  Profile: default&quot;,</span><br><span class="line">        &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;,</span><br><span class="line">        &quot;Operating System: CentOS Linux 7 (Core)&quot;,</span><br><span class="line">        &quot;OSType: linux&quot;,</span><br><span class="line">        &quot;Architecture: x86_64&quot;,</span><br><span class="line">        &quot;CPUs: 2&quot;,</span><br><span class="line">        &quot;Total Memory: 1.777GiB&quot;,</span><br><span class="line">        &quot;Name: k8s-ansible2&quot;,</span><br><span class="line">        &quot;ID: DFQT:2YYV:YWY5:IXSS:U6VS:BW7R:6MPH:WCLC:QKOW:Y63I:5TV6:C3HT&quot;,</span><br><span class="line">        &quot;Docker Root Dir: /var/lib/docker&quot;,</span><br><span class="line">        &quot;Debug Mode (client): false&quot;,</span><br><span class="line">        &quot;Debug Mode (server): false&quot;,</span><br><span class="line">        &quot;Registry: https://index.docker.io/v1/&quot;,</span><br><span class="line">        &quot;Labels:&quot;,</span><br><span class="line">        &quot;Experimental: false&quot;,</span><br><span class="line">        &quot;Insecure Registries:&quot;,</span><br><span class="line">        &quot; 192.168.31.70&quot;,</span><br><span class="line">        &quot; 127.0.0.0/8&quot;,</span><br><span class="line">        &quot;Registry Mirrors:&quot;,</span><br><span class="line">        &quot; http://bc437cce.m.daocloud.io/&quot;,</span><br><span class="line">        &quot;Live Restore Enabled: false&quot;,</span><br><span class="line">        &quot;Product License: Community Engine&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">ok: [192.168.171.14] =&gt; &#123;</span><br><span class="line">    &quot;docker.stdout_lines&quot;: [</span><br><span class="line">        &quot;Containers: 0&quot;,</span><br><span class="line">        &quot; Running: 0&quot;,</span><br><span class="line">        &quot; Paused: 0&quot;,</span><br><span class="line">        &quot; Stopped: 0&quot;,</span><br><span class="line">        &quot;Images: 0&quot;,</span><br><span class="line">        &quot;Server Version: 18.09.6&quot;,</span><br><span class="line">        &quot;Storage Driver: overlay2&quot;,</span><br><span class="line">        &quot; Backing Filesystem: xfs&quot;,</span><br><span class="line">        &quot; Supports d_type: true&quot;,</span><br><span class="line">        &quot; Native Overlay Diff: true&quot;,</span><br><span class="line">        &quot;Logging Driver: json-file&quot;,</span><br><span class="line">        &quot;Cgroup Driver: cgroupfs&quot;,</span><br><span class="line">        &quot;Plugins:&quot;,</span><br><span class="line">        &quot; Volume: local&quot;,</span><br><span class="line">        &quot; Network: bridge host macvlan null overlay&quot;,</span><br><span class="line">        &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;,</span><br><span class="line">        &quot;Swarm: inactive&quot;,</span><br><span class="line">        &quot;Runtimes: runc&quot;,</span><br><span class="line">        &quot;Default Runtime: runc&quot;,</span><br><span class="line">        &quot;Init Binary: docker-init&quot;,</span><br><span class="line">        &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;,</span><br><span class="line">        &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;,</span><br><span class="line">        &quot;init version: fec3683&quot;,</span><br><span class="line">        &quot;Security Options:&quot;,</span><br><span class="line">        &quot; seccomp&quot;,</span><br><span class="line">        &quot;  Profile: default&quot;,</span><br><span class="line">        &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;,</span><br><span class="line">        &quot;Operating System: CentOS Linux 7 (Core)&quot;,</span><br><span class="line">        &quot;OSType: linux&quot;,</span><br><span class="line">        &quot;Architecture: x86_64&quot;,</span><br><span class="line">        &quot;CPUs: 2&quot;,</span><br><span class="line">        &quot;Total Memory: 1.777GiB&quot;,</span><br><span class="line">        &quot;Name: k8s-ansible4&quot;,</span><br><span class="line">        &quot;ID: M3EP:OAKQ:6AMI:RDC6:QX2H:U34M:5GTT:Q2E7:AFP7:C4M3:FUO2:UHS3&quot;,</span><br><span class="line">        &quot;Docker Root Dir: /var/lib/docker&quot;,</span><br><span class="line">        &quot;Debug Mode (client): false&quot;,</span><br><span class="line">        &quot;Debug Mode (server): false&quot;,</span><br><span class="line">        &quot;Registry: https://index.docker.io/v1/&quot;,</span><br><span class="line">        &quot;Labels:&quot;,</span><br><span class="line">        &quot;Experimental: false&quot;,</span><br><span class="line">        &quot;Insecure Registries:&quot;,</span><br><span class="line">        &quot; 192.168.31.70&quot;,</span><br><span class="line">        &quot; 127.0.0.0/8&quot;,</span><br><span class="line">        &quot;Registry Mirrors:&quot;,</span><br><span class="line">        &quot; http://bc437cce.m.daocloud.io/&quot;,</span><br><span class="line">        &quot;Live Restore Enabled: false&quot;,</span><br><span class="line">        &quot;Product License: Community Engine&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY [3.部署ETCD集群] *****************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [etcd : 创建工作目录] **************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=bin)</span><br><span class="line">ok: [192.168.171.13] =&gt; (item=bin)</span><br><span class="line">ok: [192.168.171.14] =&gt; (item=bin)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=cfg)</span><br><span class="line">ok: [192.168.171.13] =&gt; (item=cfg)</span><br><span class="line">ok: [192.168.171.14] =&gt; (item=cfg)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=ssl)</span><br><span class="line">ok: [192.168.171.13] =&gt; (item=ssl)</span><br><span class="line">ok: [192.168.171.14] =&gt; (item=ssl)</span><br><span class="line"></span><br><span class="line">TASK [etcd : 创建临时目录] **************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 分发并解压etcd二进制包] *******************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)</span><br><span class="line">ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)</span><br><span class="line">ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)</span><br><span class="line"></span><br><span class="line">TASK [etcd : 移动etcd二进制文件] *********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 分发证书] ****************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=server.pem)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=server.pem)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=server-key.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=server-key.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server-key.pem)</span><br><span class="line"></span><br><span class="line">TASK [etcd : 分发etcd配置文件] **********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 分发service文件] *********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 启动etcd] **************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 分发etcd脚本] ************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">TASK [etcd : 获取etcd集群状态] **********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [etcd : debug] ***************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.13] =&gt; &#123;</span><br><span class="line">    &quot;status.stdout_lines&quot;: [</span><br><span class="line">        &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;,</span><br><span class="line">        &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;,</span><br><span class="line">        &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;,</span><br><span class="line">        &quot;cluster is healthy&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">ok: [192.168.171.12] =&gt; &#123;</span><br><span class="line">    &quot;status.stdout_lines&quot;: [</span><br><span class="line">        &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;,</span><br><span class="line">        &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;,</span><br><span class="line">        &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;,</span><br><span class="line">        &quot;cluster is healthy&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">ok: [192.168.171.14] =&gt; &#123;</span><br><span class="line">    &quot;status.stdout_lines&quot;: [</span><br><span class="line">        &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;,</span><br><span class="line">        &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;,</span><br><span class="line">        &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;,</span><br><span class="line">        &quot;cluster is healthy&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY [4.部署K8S Master] *************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [master : 创建工作目录] ************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=bin)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=cfg)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=ssl)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=logs)</span><br><span class="line"></span><br><span class="line">TASK [master : 创建临时目录] ************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [master : 分发并解压k8s二进制包] ******************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)</span><br><span class="line"></span><br><span class="line">TASK [master : 移动k8s master二进制文件] *************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [master : 分发k8s证书] ***********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=ca-key.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server-key.pem)</span><br><span class="line"></span><br><span class="line">TASK [master : 分发etcd证书] **********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=server-key.pem)</span><br><span class="line"></span><br><span class="line">TASK [master : 分发token文件] *********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [master : 分发k8s配置文件] *********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-apiserver.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-controller-manager.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-scheduler.conf.j2)</span><br><span class="line"></span><br><span class="line">TASK [master : 分发service文件] *******************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-apiserver.service.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-controller-manager.service.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-scheduler.service.j2)</span><br><span class="line"></span><br><span class="line">TASK [master : 启动k8s master组件] ****************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-apiserver)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-controller-manager)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-scheduler)</span><br><span class="line"></span><br><span class="line">TASK [master : 查看集群状态] ************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [master : debug] *************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; &#123;</span><br><span class="line">    &quot;cs.stdout_lines&quot;: [</span><br><span class="line">        &quot;NAME                 AGE&quot;,</span><br><span class="line">        &quot;scheduler            &lt;unknown&gt;&quot;,</span><br><span class="line">        &quot;controller-manager   &lt;unknown&gt;&quot;,</span><br><span class="line">        &quot;etcd-0               &lt;unknown&gt;&quot;,</span><br><span class="line">        &quot;etcd-1               &lt;unknown&gt;&quot;,</span><br><span class="line">        &quot;etcd-2               &lt;unknown&gt;&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TASK [master : 拷贝RBAC文件] **********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kubelet-bootstrap-rbac.yaml)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=apiserver-to-kubelet-rbac.yaml)</span><br><span class="line"></span><br><span class="line">TASK [master : 授权APIServer访问Kubelet与授权kubelet bootstrap] **************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">PLAY [5.部署K8S Node] ***************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [node : 创建工作目录] **************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=bin)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=bin)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=bin)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=cfg)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=cfg)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=cfg)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=ssl)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=ssl)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=ssl)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=logs)</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=logs)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=logs)</span><br><span class="line"></span><br><span class="line">TASK [node : 创建cni插件目录] ***********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=/opt/cni/bin)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=/opt/cni/bin)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/opt/cni/bin)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=/etc/cni/net.d)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=/etc/cni/net.d)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/etc/cni/net.d)</span><br><span class="line"></span><br><span class="line">TASK [node : 创建临时目录] **************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12]</span><br><span class="line">ok: [192.168.171.13]</span><br><span class="line">ok: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [node : 分发并解压k8s二进制包] ********************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)</span><br><span class="line"></span><br><span class="line">TASK [node : 分发并解压cni插件二进制包] ******************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)</span><br><span class="line"></span><br><span class="line">TASK [node : 移动k8s node二进制文件] *****************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line"></span><br><span class="line">TASK [node : 分发k8s证书] *************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=ca.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy.pem)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy.pem)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy-key.pem)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy-key.pem)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy-key.pem)</span><br><span class="line"></span><br><span class="line">TASK [node : 分发k8s配置文件] ***********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=bootstrap.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=bootstrap.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=bootstrap.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kubelet.conf.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kubelet.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kubelet.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kubelet-config.yml.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kubelet-config.yml.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kubelet-config.yml.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy.kubeconfig.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy.conf.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy.conf.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy-config.yml.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy-config.yml.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy-config.yml.j2)</span><br><span class="line"></span><br><span class="line">TASK [node : 分发service文件] *********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kubelet.service.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kubelet.service.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kubelet.service.j2)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy.service.j2)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy.service.j2)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy.service.j2)</span><br><span class="line"></span><br><span class="line">TASK [node : 启动k8s node组件] ********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kubelet)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kubelet)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kubelet)</span><br><span class="line">changed: [192.168.171.14] =&gt; (item=kube-proxy)</span><br><span class="line">changed: [192.168.171.13] =&gt; (item=kube-proxy)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=kube-proxy)</span><br><span class="line"></span><br><span class="line">TASK [node : 分发预准备镜像] *************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [node : 导入镜像] ****************************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line">changed: [192.168.171.14]</span><br><span class="line">changed: [192.168.171.13]</span><br><span class="line"></span><br><span class="line">PLAY [6.部署插件] *********************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [addons : 允许Node加入集群] ********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : 拷贝YAML文件到Master] ***************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/coredns.yaml)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/ingress-controller.yaml)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kube-flannel.yaml)</span><br><span class="line">changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kubernetes-dashboard.yaml)</span><br><span class="line"></span><br><span class="line">TASK [addons : 部署Flannel,Dashboard,CoreDNS,Ingress] *******************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : 替换Dashboard证书] *****************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : 查看Pod状态] ***********************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : debug] *************************************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; &#123;</span><br><span class="line">    &quot;getall.stdout_lines&quot;: [</span><br><span class="line">        &quot;NAMESPACE              NAME                                             READY   STATUS    RESTARTS   AGE&quot;,</span><br><span class="line">        &quot;kube-system            pod/coredns-6d8cfdd59d-hcfw5                     0/1     Pending   0          2s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   pod/dashboard-metrics-scraper-566cddb686-nk7t8   0/1     Pending   0          1s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   pod/kubernetes-dashboard-c4bc5bd44-cxgb6         0/1     Pending   0          1s&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">        &quot;NAMESPACE              NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE&quot;,</span><br><span class="line">        &quot;default                service/kubernetes                  ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP          2m19s&quot;,</span><br><span class="line">        &quot;ingress-nginx          service/ingress-nginx               ClusterIP   10.0.0.158   &lt;none&gt;        80/TCP,443/TCP   2s&quot;,</span><br><span class="line">        &quot;kube-system            service/kube-dns                    ClusterIP   10.0.0.2     &lt;none&gt;        53/UDP,53/TCP    2s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.0.0.38    &lt;none&gt;        8000/TCP         1s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   service/kubernetes-dashboard        NodePort    10.0.0.180   &lt;none&gt;        443:30001/TCP    1s&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">        &quot;NAMESPACE       NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE&quot;,</span><br><span class="line">        &quot;ingress-nginx   daemonset.apps/nginx-ingress-controller   0         0         0       0            0           &lt;none&gt;          2s&quot;,</span><br><span class="line">        &quot;kube-system     daemonset.apps/kube-flannel-ds-amd64      0         0         0       0            0           &lt;none&gt;          2s&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">        &quot;NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE&quot;,</span><br><span class="line">        &quot;kube-system            deployment.apps/coredns                     0/1     1            0           2s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   0/1     1            0           1s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   deployment.apps/kubernetes-dashboard        0/1     1            0           1s&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">        &quot;NAMESPACE              NAME                                                   DESIRED   CURRENT   READY   AGE&quot;,</span><br><span class="line">        &quot;kube-system            replicaset.apps/coredns-6d8cfdd59d                     1         1         0       2s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-566cddb686   1         1         0       1s&quot;,</span><br><span class="line">        &quot;kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-c4bc5bd44         1         1         0       1s&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TASK [addons : 创建Dashboard管理员令牌] **************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : 获取Dashboard管理员令牌] **************************************************************************************************************************************</span><br><span class="line">changed: [192.168.171.12]</span><br><span class="line"></span><br><span class="line">TASK [addons : Kubernetes Dashboard登录信息] ******************************************************************************************************************************</span><br><span class="line">ok: [192.168.171.12] =&gt; &#123;</span><br><span class="line">    &quot;ui.stdout_lines&quot;: [</span><br><span class="line">        &quot;访问地址---&gt;https://NodeIP:30001&quot;,</span><br><span class="line">        &quot;令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IlhOV0FZU1ZXRU80MU5oRUlYeGsxbExFcVB1R1k0bEEzMDhQQWdWVE5oZG8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tenQ2Y3ciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTU3YjEyYmYtNjNjNC00NzU1LWI4YTAtN2IyY2ZkZmRmNmE3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fKCHvmsmZmIErB9YLHtQrqWQBL_b89W0i_gDa4rwgV9x4UfzVAXskUiZiQs_yAHNmyUaIqPpBdUI64pvAoXilr-6wIk8-R8hpp4BJXLL4OsTtPXxrhIQF4_NP0D-4flg9sHba-I9X9A_2RWskcY53PAPTOjlyOQuldUyTdIT9tXi6jeSgj8CrDBc9O_A3xYWZ1f7RvrdEdU4Kkotc1rsBeGg-OzabU1nNLxWAaDHZJFciYeABtbPoY2fTkdz0JGoIxLpAqcQKoFp9ztGPcoOboCOqeb_hc-caBAmyvVIfbPvBiywdtuidjvb1IazETt_GQlzg7FMBoUpHhJYOTvnAA&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP ************************************************************************************************************************************************************</span><br><span class="line">192.168.171.12             : ok=61   changed=40   unreachable=0    failed=0</span><br><span class="line">192.168.171.13             : ok=38   changed=23   unreachable=0    failed=0</span><br><span class="line">192.168.171.14             : ok=38   changed=23   unreachable=0    failed=0</span><br><span class="line">localhost                  : ok=9    changed=3    unreachable=0    failed=0</span><br></pre></td></tr></table></figure><h4 id="2-3、测试"><a href="#2-3、测试" class="headerlink" title="2.3、测试"></a>2.3、测试</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible2 ~]# kubectl get nodes</span><br><span class="line">NAME           STATUS   ROLES    AGE    VERSION</span><br><span class="line">k8s-ansible2   Ready    &lt;none&gt;   117s   v1.16.0</span><br><span class="line">k8s-ansible3   Ready    &lt;none&gt;   117s   v1.16.0</span><br><span class="line">k8s-ansible4   Ready    &lt;none&gt;   117s   v1.16.0</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible2 ~]# kubectl get po --all-namespaces -o wide</span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">ingress-nginx          nginx-ingress-controller-7g9fh               1/1     Running   0          2m41s   192.168.171.13   k8s-ansible3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          nginx-ingress-controller-hc492               1/1     Running   0          2m41s   192.168.171.14   k8s-ansible4   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          nginx-ingress-controller-s4slm               1/1     Running   0          2m41s   192.168.171.12   k8s-ansible2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            coredns-6d8cfdd59d-hcfw5                     1/1     Running   0          3m8s    10.244.1.2       k8s-ansible2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-flannel-ds-amd64-6nbt4                  1/1     Running   0          2m51s   192.168.171.12   k8s-ansible2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-flannel-ds-amd64-mfksz                  1/1     Running   0          2m51s   192.168.171.14   k8s-ansible4   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-flannel-ds-amd64-vclgg                  1/1     Running   0          2m51s   192.168.171.13   k8s-ansible3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-566cddb686-nk7t8   1/1     Running   0          3m7s    10.244.2.2       k8s-ansible4   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-c4bc5bd44-cxgb6         1/1     Running   0          3m7s    10.244.0.2       k8s-ansible3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible2 ~]# kubectl get csr</span><br><span class="line">NAME                                                   AGE    REQUESTOR           CONDITION</span><br><span class="line">node-csr-96KwqYYgTQ0ajISzq2pUc5Dzu07UzjaKRZqRWs31yUk   5m5s   kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-9PduNyHHpXtDmuNFj3fCkpoNkGcDkO2NPEk3uGQ3kIk   5m6s   kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-WLFKgflHlDK2f0RFvuTYKlkHcr8hz0iOrzYcp2V50JE   5m6s   kubelet-bootstrap   Approved,Issued</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/20191204/RlnwtcB5dboO.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><hr><h3 id="三、多master部署"><a href="#三、多master部署" class="headerlink" title="三、多master部署"></a>三、多master部署</h3><h4 id="3-1、hosts"><a href="#3-1、hosts" class="headerlink" title="3.1、hosts"></a>3.1、hosts</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts</span><br><span class="line">[master]</span><br><span class="line"># 如果部署单Master，只保留一个Master节点</span><br><span class="line">192.168.171.11 node_name=k8s-master1</span><br><span class="line">192.168.171.12 node_name=k8s-master2</span><br><span class="line"></span><br><span class="line">[node]</span><br><span class="line">192.168.171.13 node_name=k8s-node1</span><br><span class="line">192.168.171.14 node_name=k8s-node2</span><br><span class="line"></span><br><span class="line">[etcd]</span><br><span class="line">192.168.171.11 etcd_name=etcd-1</span><br><span class="line">192.168.171.12 etcd_name=etcd-2</span><br><span class="line">192.168.171.13 etcd_name=etcd-3</span><br><span class="line"></span><br><span class="line">[lb]</span><br><span class="line"># 如果部署单Master，该项忽略</span><br><span class="line">192.168.171.15 lb_name=lb-master</span><br><span class="line">192.168.171.16 lb_name=lb-backup</span><br><span class="line"></span><br><span class="line">[k8s:children]</span><br><span class="line">master</span><br><span class="line">node</span><br><span class="line"></span><br><span class="line">[newnode]</span><br><span class="line">#192.168.31.91 node_name=k8s-node3</span><br></pre></td></tr></table></figure><h4 id="3-2、全局参数配置"><a href="#3-2、全局参数配置" class="headerlink" title="3.2、全局参数配置"></a>3.2、全局参数配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml</span><br><span class="line"># 安装目录</span><br><span class="line">software_dir: &apos;/root/binary_pkg&apos;</span><br><span class="line">k8s_work_dir: &apos;/opt/kubernetes&apos;</span><br><span class="line">etcd_work_dir: &apos;/opt/etcd&apos;</span><br><span class="line">tmp_dir: &apos;/tmp/k8s&apos;</span><br><span class="line"></span><br><span class="line"># 集群网络</span><br><span class="line">service_cidr: &apos;10.0.0.0/24&apos;</span><br><span class="line">cluster_dns: &apos;10.0.0.2&apos;   # 与roles/addons/files/coredns.yaml中IP一致</span><br><span class="line">pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致</span><br><span class="line">service_nodeport_range: &apos;30000-32767&apos;</span><br><span class="line">cluster_domain: &apos;cluster.local&apos;</span><br><span class="line"></span><br><span class="line"># 高可用，如果部署单Master，该项忽略</span><br><span class="line">vip: &apos;192.168.171.88&apos;</span><br><span class="line">nic: &apos;ens33&apos;</span><br><span class="line"></span><br><span class="line"># 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span><br><span class="line">cert_hosts:</span><br><span class="line">  # 包含所有LB、VIP、Master IP和service_cidr的第一个IP（多多益善，可以多余出来几个后期扩展用）</span><br><span class="line">  k8s:</span><br><span class="line">    - 10.0.0.1</span><br><span class="line">    - 192.168.171.11</span><br><span class="line">    - 192.168.171.12</span><br><span class="line">    - 192.168.171.13</span><br><span class="line">    - 192.168.171.14</span><br><span class="line">    - 192.168.171.15</span><br><span class="line">    - 192.168.171.16</span><br><span class="line">    - 192.168.171.17</span><br><span class="line">    - 192.168.171.18</span><br><span class="line">    - 192.168.171.19</span><br><span class="line">    - 192.168.171.10</span><br><span class="line">    - 192.168.171.21</span><br><span class="line">    - 192.168.171.88</span><br><span class="line">  # 包含所有etcd节点IP</span><br><span class="line">  etcd:</span><br><span class="line">    - 192.168.171.11</span><br><span class="line">    - 192.168.171.12</span><br><span class="line">    - 192.168.171.13</span><br></pre></td></tr></table></figure><h4 id="3-3、准备部署"><a href="#3-3、准备部署" class="headerlink" title="3.3、准备部署"></a>3.3、准备部署</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多Master版：</span><br><span class="line"></span><br><span class="line">ansible-playbook -i hosts multi-master-deploy.yml -uroot -k</span><br></pre></td></tr></table></figure><h4 id="3-4、输出历史（重点是结果）"><a href="#3-4、输出历史（重点是结果）" class="headerlink" title="3.4、输出历史（重点是结果）"></a>3.4、输出历史（重点是结果）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">省略...</span><br></pre></td></tr></table></figure><h4 id="3-5、测试"><a href="#3-5、测试" class="headerlink" title="3.5、测试"></a>3.5、测试</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-ansible1 ~]# kubectl get node</span><br><span class="line">NAME          STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-master1   Ready    &lt;none&gt;   85s   v1.16.0</span><br><span class="line">k8s-master2   Ready    &lt;none&gt;   85s   v1.16.0</span><br><span class="line">k8s-node1     Ready    &lt;none&gt;   85s   v1.16.0</span><br><span class="line">k8s-node2     Ready    &lt;none&gt;   85s   v1.16.0</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespaces</span><br><span class="line">NAMESPACE              NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-92b8v               1/1     Running   0          90s</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-dfkp5               1/1     Running   0          90s</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-hckvr               1/1     Running   0          91s</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-qckdd               1/1     Running   0          90s</span><br><span class="line">kube-system            pod/coredns-6d8cfdd59d-lsdps                     1/1     Running   0          117s</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-2mc74                  1/1     Running   1          100s</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-4hqq7                  1/1     Running   0          101s</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-dgzrb                  1/1     Running   0          100s</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-zjtpq                  1/1     Running   0          100s</span><br><span class="line">kubernetes-dashboard   pod/dashboard-metrics-scraper-566cddb686-9xh7b   1/1     Running   0          116s</span><br><span class="line">kubernetes-dashboard   pod/kubernetes-dashboard-c4bc5bd44-4f45q         1/1     Running   0          116s</span><br><span class="line"></span><br><span class="line">NAMESPACE              NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">default                service/kubernetes                  ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP          5m40s</span><br><span class="line">ingress-nginx          service/ingress-nginx               ClusterIP   10.0.0.170   &lt;none&gt;        80/TCP,443/TCP   117s</span><br><span class="line">kube-system            service/kube-dns                    ClusterIP   10.0.0.2     &lt;none&gt;        53/UDP,53/TCP    117s</span><br><span class="line">kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.0.0.172   &lt;none&gt;        8000/TCP         116s</span><br><span class="line">kubernetes-dashboard   service/kubernetes-dashboard        NodePort    10.0.0.57    &lt;none&gt;        443:30001/TCP    116s</span><br><span class="line">[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespaces -o wide</span><br><span class="line">NAMESPACE              NAME                                             READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-92b8v               1/1     Running   0          96s    192.168.171.11   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-dfkp5               1/1     Running   0          96s    192.168.171.12   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-hckvr               1/1     Running   0          97s    192.168.171.13   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">ingress-nginx          pod/nginx-ingress-controller-qckdd               1/1     Running   0          96s    192.168.171.14   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/coredns-6d8cfdd59d-lsdps                     1/1     Running   0          2m3s   10.244.1.2       k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-2mc74                  1/1     Running   1          106s   192.168.171.12   k8s-master2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-4hqq7                  1/1     Running   0          107s   192.168.171.13   k8s-node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-dgzrb                  1/1     Running   0          106s   192.168.171.14   k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            pod/kube-flannel-ds-amd64-zjtpq                  1/1     Running   0          106s   192.168.171.11   k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   pod/dashboard-metrics-scraper-566cddb686-9xh7b   1/1     Running   0          2m2s   10.244.2.2       k8s-master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   pod/kubernetes-dashboard-c4bc5bd44-4f45q         1/1     Running   0          2m2s   10.244.3.2       k8s-node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAMESPACE              NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE     SELECTOR</span><br><span class="line">default                service/kubernetes                  ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP          5m46s   &lt;none&gt;</span><br><span class="line">ingress-nginx          service/ingress-nginx               ClusterIP   10.0.0.170   &lt;none&gt;        80/TCP,443/TCP   2m3s    app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx</span><br><span class="line">kube-system            service/kube-dns                    ClusterIP   10.0.0.2     &lt;none&gt;        53/UDP,53/TCP    2m3s    k8s-app=kube-dns</span><br><span class="line">kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.0.0.172   &lt;none&gt;        8000/TCP         2m2s    k8s-app=dashboard-metrics-scraper</span><br><span class="line">kubernetes-dashboard   service/kubernetes-dashboard        NodePort    10.0.0.57    &lt;none&gt;        443:30001/TCP    2m2s    k8s-app=kubernetes-dashboard</span><br><span class="line"></span><br><span class="line">### 检查高可用的两台机器：</span><br><span class="line"></span><br><span class="line">[root@k8s-ansible5 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:85:37:e0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.171.15/24 brd 192.168.171.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.171.88/24 scope global secondary ens33     ##虚拟VIP</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::d20b:b903:7edd:b18b/64 scope link tentative noprefixroute dadfailed</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::5d9e:cf1f:ea7f:801f/64 scope link tentative noprefixroute dadfailed</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::5c0:8885:2874:a77b/64 scope link tentative noprefixroute dadfailed</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">[root@k8s-ansible5 ~]# ps aux | grep nginx</span><br><span class="line">root       7895  0.0  0.0  46356  1168 ?        Ss   22:57   0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf</span><br><span class="line">nginx      7896  0.1  0.1  47184  2308 ?        S    22:57   0:00 nginx: worker process</span><br><span class="line">nginx      7897  0.0  0.1  46780  1980 ?        S    22:57   0:00 nginx: worker process</span><br><span class="line">nginx      7898  0.0  0.1  46924  2216 ?        S    22:57   0:00 nginx: worker process</span><br><span class="line">nginx      7899  0.0  0.1  46876  2220 ?        S    22:57   0:00 nginx: worker process</span><br><span class="line">root      11331  0.0  0.0 112728   988 pts/0    S+   23:07   0:00 grep --color=auto nginx</span><br><span class="line">[root@k8s-ansible5 ~]# ps aux | grep keepalived</span><br><span class="line">root       7975  0.0  0.0 122884  1404 ?        Ss   22:57   0:00 /usr/sbin/keepalived -D</span><br><span class="line">root       7976  0.0  0.1 133844  3336 ?        S    22:57   0:00 /usr/sbin/keepalived -D</span><br><span class="line">root       7977  0.0  0.1 133784  2892 ?        S    22:57   0:00 /usr/sbin/keepalived -D</span><br><span class="line">root      11369  0.0  0.0 112724   992 pts/0    R+   23:07   0:00 grep --color=auto keepalived</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/20191204/7GzoI4FprwXv.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建一个生产级K8S高可用集群（2）</title>
    <link href="/2019/12/01/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%882%EF%BC%89/"/>
    <url>/2019/12/01/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="写在前面："><a href="#写在前面：" class="headerlink" title="写在前面："></a>写在前面：</h3><p>此次为了贴合线上的真实情况，此次K8S搭建将不会和咱们网路上的一气呵成相媲美，更多的表现在：</p><ul><li>最新版K8S_1.16；</li><li>完全基于离线模式的二进制HA搭建（政企）《链接：<a href="https://pan.baidu.com/s/1aCmiYdfn5gujnyMutVdaVw" target="_blank" rel="noopener">https://pan.baidu.com/s/1aCmiYdfn5gujnyMutVdaVw</a><br>提取码：m39k》；</li><li>全部组件均采用二进制部署（包含Docker）；</li><li>逐一摸索每个组件的配置文件，做到线上有故障能清楚的定位到问题；</li><li>既然是分布式，本次安装完全基于：<ul><li><strong>先单Master到双Master高可用</strong>；</li><li><strong>新Node如何加到集群</strong>；</li></ul></li></ul><h4 id="服务器硬件配置推荐："><a href="#服务器硬件配置推荐：" class="headerlink" title="服务器硬件配置推荐："></a>服务器硬件配置推荐：</h4><p><img src="http://myimage.okay686.cn/okay686cn/20191129/BvPYq5hwuEMm.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="生产环境K8S平台规划-–-单Master集群"><a href="#生产环境K8S平台规划-–-单Master集群" class="headerlink" title="生产环境K8S平台规划 – 单Master集群"></a>生产环境K8S平台规划 – 单Master集群</h4><p><img src="http://myimage.okay686.cn/okay686cn/20191201/nQwlixWkIRt4.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="生产环境K8S平台规划-–-多Master集群（HA）"><a href="#生产环境K8S平台规划-–-多Master集群（HA）" class="headerlink" title="生产环境K8S平台规划 – 多Master集群（HA）"></a>生产环境K8S平台规划 – 多Master集群（HA）</h4><p><img src="http://myimage.okay686.cn/okay686cn/20191201/FYKl7EO1goUl.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="一、服务器规划"><a href="#一、服务器规划" class="headerlink" title="一、服务器规划"></a>一、服务器规划</h3><table><thead><tr><th>角色</th><th>IP</th><th>组件</th></tr></thead><tbody><tr><td>k8s-master1</td><td>192.168.171.134</td><td>kube-apiserver，kube-controller-manager，kube-scheduler，etcd</td></tr><tr><td>k8s-master2</td><td>192.168.171.135</td><td>kube-apiserver，kube-controller-manager，kube-scheduler，etcd</td></tr><tr><td>k8s-node1</td><td>192.168.171.136</td><td>kubelet，kube-proxy，docker，etcd</td></tr><tr><td>k8s-node2</td><td>192.168.171.137</td><td>kubelet，kube-proxy，docker</td></tr><tr><td>Load Balancer（Master）</td><td>192.168.171.138，192.168.171.188 (VIP)</td><td>Nginx L4，Keepalived</td></tr><tr><td>Load Balancer（Backup）</td><td>192.168.171.139</td><td>Nginx L4，Keepalived</td></tr></tbody></table><h4 id="1-1、系统初始化"><a href="#1-1、系统初始化" class="headerlink" title="1.1、系统初始化"></a>1.1、系统初始化</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">关闭防火墙：</span><br><span class="line"># systemctl stop firewalld</span><br><span class="line"># systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">关闭selinux：</span><br><span class="line"># setenforce 0 # 临时</span><br><span class="line"># sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/config # 永久</span><br><span class="line"></span><br><span class="line">关闭swap：</span><br><span class="line"># swapoff -a  # 临时</span><br><span class="line"># vim /etc/fstab  # 永久</span><br><span class="line"></span><br><span class="line">同步系统时间：</span><br><span class="line"># ntpdate time.windows.com</span><br><span class="line"></span><br><span class="line">添加hosts：</span><br><span class="line"># vim /etc/hosts</span><br><span class="line">192.168.171.134 k8s-master1</span><br><span class="line">192.168.171.135 k8s-master2</span><br><span class="line">192.168.171.136 k8s-node1</span><br><span class="line">192.168.171.137 k8s-node2</span><br><span class="line"></span><br><span class="line">修改主机名：</span><br><span class="line">hostnamectl set-hostname k8s-master1</span><br><span class="line"></span><br><span class="line">##开启转发</span><br><span class="line">cat /etc/sysctl.d/kubernetes.conf</span><br><span class="line"></span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">fs.inotify.max_user_watches=89100</span><br><span class="line"></span><br><span class="line">sysctl -p  /etc/sysctl.d/kubernetes.conf</span><br></pre></td></tr></table></figure><h3 id="二、ETCD集群"><a href="#二、ETCD集群" class="headerlink" title="二、ETCD集群"></a>二、ETCD集群</h3><p>整个集群中所有的组件均是走的https协议进行交互，所以我们需要配置自签证书到各个服务中；</p><p><img src="http://myimage.okay686.cn/okay686cn/20191129/QDRHjUnB4bab.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="2-1、将下载好的证书文件上传到K8s-master1中，并解压"><a href="#2-1、将下载好的证书文件上传到K8s-master1中，并解压" class="headerlink" title="2.1、将下载好的证书文件上传到K8s-master1中，并解压"></a>2.1、将下载好的证书文件上传到K8s-master1中，并解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# ls</span><br><span class="line">anaconda-ks.cfg  TLS.tar.gz</span><br><span class="line">[root@k8s-master1 ~]# tar zxvf TLS.tar.gz</span><br><span class="line">TLS/</span><br><span class="line">TLS/cfssl</span><br><span class="line">TLS/cfssl-certinfo</span><br><span class="line">TLS/cfssljson</span><br><span class="line">TLS/etcd/</span><br><span class="line">TLS/etcd/ca-config.json</span><br><span class="line">TLS/etcd/ca-csr.json</span><br><span class="line">TLS/etcd/generate_etcd_cert.sh</span><br><span class="line">TLS/etcd/server-csr.json</span><br><span class="line">TLS/k8s/</span><br><span class="line">TLS/k8s/ca-config.json</span><br><span class="line">TLS/k8s/ca-csr.json</span><br><span class="line">TLS/k8s/kube-proxy-csr.json</span><br><span class="line">TLS/k8s/server-csr.json</span><br><span class="line">TLS/k8s/generate_k8s_cert.sh</span><br><span class="line">TLS/cfssl.sh</span><br><span class="line">[root@k8s-master1 ~]# cd TLS</span><br><span class="line">[root@k8s-master1 TLS]# ls</span><br><span class="line">cfssl  cfssl-certinfo  cfssljson  cfssl.sh  etcd  k8s</span><br></pre></td></tr></table></figure><p>将超cfssl移动到可执行目录中：<br>运行脚本：（cfssl.sh）《注意脚本中curl原始是被注释掉了》<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 TLS]# cat cfssl.sh</span><br><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl</span><br><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson</span><br><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo</span><br><span class="line">cp -rf cfssl cfssl-certinfo cfssljson /usr/local/bin</span><br><span class="line">chmod +x /usr/local/bin/cfssl*</span><br></pre></td></tr></table></figure></p><p>执行完成脚本后：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 TLS]# ls /usr/local/bin/</span><br><span class="line">cfssl  cfssl-certinfo  cfssljson</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 TLS]# ls</span><br><span class="line">cfssl  cfssl-certinfo  cfssljson  cfssl.sh  etcd  k8s</span><br><span class="line">[root@k8s-master1 TLS]# cd etcd/</span><br><span class="line">[root@k8s-master1 etcd]# ls</span><br><span class="line">ca-config.json  ca-csr.json  generate_etcd_cert.sh  server-csr.json</span><br><span class="line">[root@k8s-master1 etcd]# vim server-csr.json</span><br><span class="line">[root@k8s-master1 etcd]# cat server-csr.json    ###修改如下hosts中的host</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;192.168.171.134&quot;,</span><br><span class="line">        &quot;192.168.171.135&quot;,</span><br><span class="line">        &quot;192.168.171.136&quot;</span><br><span class="line">        ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行脚本：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 etcd]# sh generate_etcd_cert.sh</span><br><span class="line">2019/11/29 20:15:53 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/11/29 20:15:53 [INFO] generate received request</span><br><span class="line">2019/11/29 20:15:53 [INFO] received CSR</span><br><span class="line">2019/11/29 20:15:53 [INFO] generating key: rsa-2048</span><br><span class="line">2019/11/29 20:15:53 [INFO] encoded CSR</span><br><span class="line">2019/11/29 20:15:53 [INFO] signed certificate with serial number 24102972475512203247000931916818116185424147280</span><br><span class="line">2019/11/29 20:15:53 [INFO] generate received request</span><br><span class="line">2019/11/29 20:15:53 [INFO] received CSR</span><br><span class="line">2019/11/29 20:15:53 [INFO] generating key: rsa-2048</span><br><span class="line">2019/11/29 20:15:53 [INFO] encoded CSR</span><br><span class="line">2019/11/29 20:15:53 [INFO] signed certificate with serial number 12936195516565485048517952341546410494181088290</span><br><span class="line">2019/11/29 20:15:53 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@k8s-master1 etcd]# ls server*</span><br><span class="line">server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p><p>至此etcd密钥和证书生成完毕！！</p><p>上传etcd.tar.gz 并解压到k8s-master1中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# tar zxvf etcd.tar.gz</span><br><span class="line">etcd/</span><br><span class="line">etcd/bin/</span><br><span class="line">etcd/bin/etcd</span><br><span class="line">etcd/bin/etcdctl</span><br><span class="line">etcd/cfg/</span><br><span class="line">etcd/cfg/etcd.conf</span><br><span class="line">etcd/ssl/</span><br><span class="line">etcd/ssl/ca.pem</span><br><span class="line">etcd/ssl/server.pem</span><br><span class="line">etcd/ssl/server-key.pem</span><br><span class="line">etcd.service</span><br></pre></td></tr></table></figure></p><p>先来了解下etcd.service<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cat etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/opt/etcd/cfg/etcd.conf     ##etcd配置文件目录</span><br><span class="line">ExecStart=/opt/etcd/bin/etcd \      ##etcd执行文件所在的目录</span><br><span class="line">        --name=$&#123;ETCD_NAME&#125; \</span><br><span class="line">        --data-dir=$&#123;ETCD_DATA_DIR&#125; \</span><br><span class="line">        --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \</span><br><span class="line">        --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \</span><br><span class="line">        --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \</span><br><span class="line">        --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \</span><br><span class="line">        --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \</span><br><span class="line">        --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \</span><br><span class="line">        --initial-cluster-state=new \</span><br><span class="line">        --cert-file=/opt/etcd/ssl/server.pem \</span><br><span class="line">        --key-file=/opt/etcd/ssl/server-key.pem \</span><br><span class="line">        --peer-cert-file=/opt/etcd/ssl/server.pem \</span><br><span class="line">        --peer-key-file=/opt/etcd/ssl/server-key.pem \</span><br><span class="line">        --trusted-ca-file=/opt/etcd/ssl/ca.pem \</span><br><span class="line">        --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 etcd]# ls</span><br><span class="line">bin  cfg  ssl</span><br><span class="line">[root@k8s-master1 etcd]# cd bin/        ##此目录为etcd的执行文件目录（后期升级可直接下载二进制的可执行文件覆盖升级即可）</span><br><span class="line">[root@k8s-master1 bin]# ls</span><br><span class="line">etcd  etcdctl</span><br></pre></td></tr></table></figure><p>再来看下etcd的配置文件目录：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 cfg]# cat etcd.conf</span><br><span class="line"></span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd-1&quot;      ##集群节点的name</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;  ##数据存放位置</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.134:2380&quot;    ##etcd集群内部通讯url</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot;  ##etcd客户端通讯url</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.134:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;  ##集群节点的配置信息</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;   ##集群简单认证的TOKEN</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;    ##集群的状态（新增的节点要改为existing）</span><br></pre></td></tr></table></figure></p><p>copy刚刚生成的etcd证书文件到指定的目录（/root/etcd/ssl）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 etcd]# cp /root/TLS/etcd/&#123;ca,server,server-key&#125;.pem ssl/</span><br><span class="line">[root@k8s-master1 etcd]# ls ssl/</span><br><span class="line">ca.pem  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p><p>然后下发配置etcd和etcd.service到三台集群机器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# ls</span><br><span class="line">anaconda-ks.cfg  etcd  etcd.service  etcd.tar.gz  TLS  TLS.tar.gz</span><br><span class="line">[root@k8s-master1 ~]# scp -r etcd root@192.168.171.134:/opt/</span><br><span class="line">etcd                                                                                                                                100%   16MB  51.2MB/s   00:00</span><br><span class="line">etcdctl                                                                                                                             100%   13MB  58.8MB/s   00:00</span><br><span class="line">.etcd.conf.swp                                                                                                                      100%   12KB  11.8MB/s   00:00</span><br><span class="line">etcd.conf                                                                                                                           100%  523   634.0KB/s   00:00</span><br><span class="line">ca.pem                                                                                                                              100% 1265   788.8KB/s   00:00</span><br><span class="line">server.pem                                                                                                                          100% 1338     1.8MB/s   00:00</span><br><span class="line">server-key.pem                                                                                                                      100% 1675     1.5MB/s   00:00</span><br><span class="line">[root@k8s-master1 ~]# scp -r etcd root@192.168.171.135:/opt/</span><br><span class="line">root@192.168.171.135&apos;s password:</span><br><span class="line">etcd                                                                                                                                100%   16MB  82.4MB/s   00:00</span><br><span class="line">etcdctl                                                                                                                             100%   13MB  92.3MB/s   00:00</span><br><span class="line">.etcd.conf.swp                                                                                                                      100%   12KB   7.7MB/s   00:00</span><br><span class="line">etcd.conf                                                                                                                           100%  523   169.7KB/s   00:00</span><br><span class="line">ca.pem                                                                                                                              100% 1265     1.3MB/s   00:00</span><br><span class="line">server.pem                                                                                                                          100% 1338     1.4MB/s   00:00</span><br><span class="line">server-key.pem                                                                                                                      100% 1675     1.5MB/s   00:00</span><br><span class="line">[root@k8s-master1 ~]# scp -r etcd root@192.168.171.136:/opt/</span><br><span class="line">etcd                                                                                                                                100%   16MB  68.7MB/s   00:00</span><br><span class="line">etcdctl                                                                                                                             100%   13MB  80.8MB/s   00:00</span><br><span class="line">.etcd.conf.swp                                                                                                                      100%   12KB  12.5MB/s   00:00</span><br><span class="line">etcd.conf                                                                                                                           100%  523   385.2KB/s   00:00</span><br><span class="line">ca.pem                                                                                                                              100% 1265     1.5MB/s   00:00</span><br><span class="line">server.pem                                                                                                                          100% 1338     2.0MB/s   00:00</span><br><span class="line">server-key.pem                                                                                                                      100% 1675     2.2MB/s   00</span><br></pre></td></tr></table></figure></p><p>同理copyetcd.service文件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# scp etcd.service root@192.168.171.134:/usr/lib/systemd/system/</span><br><span class="line">etcd.service                                                                                                                        100% 1078   577.1KB/s   00:00</span><br><span class="line">[root@k8s-master1 ~]# scp etcd.service root@192.168.171.135:/usr/lib/systemd/system/</span><br><span class="line">etcd.service                                                                                                                        100% 1078   780.0KB/s   00:00</span><br><span class="line">[root@k8s-master1 ~]# scp etcd.service root@192.168.171.136:/usr/lib/systemd/system/</span><br><span class="line">etcd.service</span><br></pre></td></tr></table></figure></p><p>修改另外2台etcd的配置文件：</p><p>192.168.171.135中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master2 ~]# cat /opt/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd-2&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.135:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;</span><br><span class="line"></span><br><span class="line">##[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.135:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br></pre></td></tr></table></figure></p><p>192.168.171.136中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cat /opt/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd-3&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.136:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.136:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br></pre></td></tr></table></figure></p><p>启动etcd（第一台启动的时候有些慢是因为在侦听其它节点）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# systemctl daemon-reload</span><br><span class="line">[root@k8s-master1 ~]# systemctl start etcd</span><br><span class="line">[root@k8s-master1 ~]# systemctl enable etcd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.</span><br></pre></td></tr></table></figure></p><p>查看etcd集群的日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# tail /var/log/messages -f</span><br><span class="line">Nov 29 21:06:20 localhost etcd: set the initial cluster version to 3.0</span><br><span class="line">Nov 29 21:06:20 localhost etcd: enabled capabilities for version 3.0</span><br><span class="line">Nov 29 21:06:24 localhost etcd: peer 92fcf2aa055d676f became active</span><br><span class="line">Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message reader)</span><br><span class="line">Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 reader)</span><br><span class="line">Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message writer)</span><br><span class="line">Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 writer)</span><br><span class="line">Nov 29 21:06:24 localhost etcd: updating the cluster version from 3.0 to 3.3</span><br><span class="line">Nov 29 21:06:24 localhost etcd: updated the cluster version from 3.0 to 3.3</span><br><span class="line">Nov 29 21:06:24 localhost etcd: enabled capabilities for version 3.3</span><br></pre></td></tr></table></figure></p><h5 id="查看etcd集群的状态："><a href="#查看etcd集群的状态：" class="headerlink" title="查看etcd集群的状态："></a>查看etcd集群的状态：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># /opt/etcd/bin/etcdctl \</span><br><span class="line">--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \</span><br><span class="line">--endpoints=&quot;https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379&quot; \</span><br><span class="line">cluster-health</span><br><span class="line"></span><br><span class="line">member 3530acf25e9921b5 is healthy: got healthy result from https://192.168.171.134:2379</span><br><span class="line">member 833528c821fcdcd2 is healthy: got healthy result from https://192.168.171.135:2379</span><br><span class="line">member 92fcf2aa055d676f is healthy: got healthy result from https://192.168.171.136:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure><h3 id="二、部署Master节点"><a href="#二、部署Master节点" class="headerlink" title="二、部署Master节点"></a>二、部署Master节点</h3><h4 id="2-1、自签证书"><a href="#2-1、自签证书" class="headerlink" title="2.1、自签证书"></a>2.1、自签证书</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cd TLS/k8s/</span><br><span class="line">[root@k8s-master1 k8s]# pwd</span><br><span class="line">/root/TLS/k8s</span><br><span class="line">[root@k8s-master1 k8s]# ls</span><br><span class="line">ca-config.json  ca-csr.json  generate_k8s_cert.sh  kube-proxy-csr.json  server-csr.json</span><br><span class="line"></span><br><span class="line">kube-proxy-csr.json：为kube-proxy服务自签的证书</span><br><span class="line">ca-config.json，ca-csr.json，server-csr.json：为Api-server服务自签的证书</span><br></pre></td></tr></table></figure><h4 id="2-2、划重点（K8S集群内部是用证书进行校验通信）"><a href="#2-2、划重点（K8S集群内部是用证书进行校验通信）" class="headerlink" title="2.2、划重点（K8S集群内部是用证书进行校验通信）"></a>2.2、划重点（K8S集群内部是用证书进行校验通信）</h4><ul><li>一定要把和API-SERVER 通信服务的IP写到如下hosts中（master节点，LB，etcd，keepalived，VIP）；</li><li>当然这个也是我之前的疑问，如果后期扩展了master 如何加入到当前集群？<ul><li><strong>目前得到的验证是先提前多增加IP</strong>； <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# cat server-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;10.0.0.1&quot;,</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;kubernetes&quot;,</span><br><span class="line">      &quot;kubernetes.default&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster.local&quot;,</span><br><span class="line">      &quot;192.168.171.134&quot;,</span><br><span class="line">      &quot;192.168.171.135&quot;,</span><br><span class="line">      &quot;192.168.171.136&quot;,</span><br><span class="line">      &quot;192.168.171.137&quot;,</span><br><span class="line">      &quot;192.168.171.138&quot;,</span><br><span class="line">      &quot;192.168.171.139&quot;,</span><br><span class="line">      &quot;192.168.171.188&quot;,</span><br><span class="line">      &quot;192.168.171.140&quot;,</span><br><span class="line">      &quot;192.168.171.141&quot;,</span><br><span class="line">      &quot;192.168.171.142&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h5 id="执行脚本生成证书："><a href="#执行脚本生成证书：" class="headerlink" title="执行脚本生成证书："></a>执行脚本生成证书：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# sh generate_k8s_cert.sh</span><br><span class="line">2019/11/30 16:08:18 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] generate received request</span><br><span class="line">2019/11/30 16:08:18 [INFO] received CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] generating key: rsa-2048</span><br><span class="line">2019/11/30 16:08:18 [INFO] encoded CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] signed certificate with serial number 341826322118494245750742070723426886230473381959</span><br><span class="line">2019/11/30 16:08:18 [INFO] generate received request</span><br><span class="line">2019/11/30 16:08:18 [INFO] received CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] generating key: rsa-2048</span><br><span class="line">2019/11/30 16:08:18 [INFO] encoded CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] signed certificate with serial number 298916502664941699479785933454138161410913060966</span><br><span class="line">2019/11/30 16:08:18 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">2019/11/30 16:08:18 [INFO] generate received request</span><br><span class="line">2019/11/30 16:08:18 [INFO] received CSR</span><br><span class="line">2019/11/30 16:08:18 [INFO] generating key: rsa-2048</span><br><span class="line">2019/11/30 16:08:19 [INFO] encoded CSR</span><br><span class="line">2019/11/30 16:08:19 [INFO] signed certificate with serial number 11454632622297749262296986610747834462011118952</span><br><span class="line">2019/11/30 16:08:19 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br></pre></td></tr></table></figure><p>查看生成的证书：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# ls *.pem</span><br><span class="line">ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p><h4 id="准备部署master组件："><a href="#准备部署master组件：" class="headerlink" title="准备部署master组件："></a>准备部署master组件：</h4><p>二进制包下载地址：<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#v1161" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#v1161</a></p><p>上传包中的k8s-master.tar.gz到/目录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# tar zxvf k8s-master.tar.gz</span><br><span class="line">kubernetes/</span><br><span class="line">kubernetes/bin/</span><br><span class="line">kubernetes/bin/kubectl</span><br><span class="line">kubernetes/bin/kube-apiserver</span><br><span class="line">kubernetes/bin/kube-controller-manager</span><br><span class="line">kubernetes/bin/kube-scheduler</span><br><span class="line">kubernetes/cfg/</span><br><span class="line">kubernetes/cfg/token.csv</span><br><span class="line">kubernetes/cfg/kube-apiserver.conf</span><br><span class="line">kubernetes/cfg/kube-controller-manager.conf</span><br><span class="line">kubernetes/cfg/kube-scheduler.conf</span><br><span class="line">kubernetes/ssl/</span><br><span class="line">kubernetes/logs/</span><br><span class="line">kube-apiserver.service</span><br><span class="line">kube-controller-manager.service</span><br><span class="line">kube-scheduler.service</span><br></pre></td></tr></table></figure><p>copy刚刚生成的证书文件放到当前ssl中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 kubernetes]# cp /root/TLS/k8s/*pem ssl/</span><br><span class="line">[root@k8s-master1 kubernetes]# ls</span><br><span class="line">bin  cfg  logs  ssl</span><br><span class="line">[root@k8s-master1 kubernetes]# ls ssl/</span><br><span class="line">ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p><h5 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 cfg]# cat kube-apiserver.conf</span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=false \      ##输出日志</span><br><span class="line">--v=2 \     ##日志级别</span><br><span class="line">--log-dir=/opt/kubernetes/logs \    ##日志存放目录</span><br><span class="line">--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \   ##etcd集群IP</span><br><span class="line">--bind-address=192.168.171.134 \    ##绑定IP（可以为外网IP）</span><br><span class="line">--secure-port=6443 \    ##安全端口</span><br><span class="line">--advertise-address=192.168.171.134 \   ##集群内部通讯地址</span><br><span class="line">--allow-privileged=true \   ##允许pod有超级权限</span><br><span class="line">--service-cluster-ip-range=10.0.0.0/24 \    ##service的IP范围</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \      ##启动准入控制插件</span><br><span class="line">--authorization-mode=RBAC,Node \    ##授权模式</span><br><span class="line">--enable-bootstrap-token-auth=true \    ##bootstrap-token认证，自动颁发证书</span><br><span class="line">--token-auth-file=/opt/kubernetes/cfg/token.csv \   ##token文件</span><br><span class="line">--service-node-port-range=30000-32767 \     ##service的ip范围</span><br><span class="line">--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \</span><br><span class="line">--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \</span><br><span class="line">--tls-cert-file=/opt/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/opt/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/opt/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/opt/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/opt/etcd/ssl/server-key.pem \</span><br><span class="line">--audit-log-maxage=30 \     ##如下均为日志的一些策略</span><br><span class="line">--audit-log-maxbackup=3 \</span><br><span class="line">--audit-log-maxsize=100 \</span><br><span class="line">--audit-log-path=/opt/kubernetes/logs/k8s-audit.log&quot;</span><br></pre></td></tr></table></figure><h5 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 cfg]# cat kube-controller-manager.conf</span><br><span class="line">KUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \</span><br><span class="line">--v=2 \</span><br><span class="line">--log-dir=/opt/kubernetes/logs \    ##日志存放路径</span><br><span class="line">--leader-elect=true \       ##选举模式</span><br><span class="line">--master=127.0.0.1:8080 \   ##连接本地api-server</span><br><span class="line">--address=127.0.0.1 \   ##监听地址</span><br><span class="line">--allocate-node-cidrs=true \    ##cni组件</span><br><span class="line">--cluster-cidr=10.244.0.0/16 \  ##cni组件IP段</span><br><span class="line">--service-cluster-ip-range=10.0.0.0/24 \    ##service范围和api-server中一致</span><br><span class="line">--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \</span><br><span class="line">--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">--root-ca-file=/opt/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--experimental-cluster-signing-duration=87600h0m0s&quot;</span><br></pre></td></tr></table></figure><h5 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 cfg]# cat kube-scheduler.conf</span><br><span class="line">KUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \</span><br><span class="line">--v=2 \</span><br><span class="line">--log-dir=/opt/kubernetes/logs \</span><br><span class="line">--leader-elect \</span><br><span class="line">--master=127.0.0.1:8080 \</span><br><span class="line">--address=127.0.0.1&quot;</span><br></pre></td></tr></table></figure><h4 id="启动apiserver"><a href="#启动apiserver" class="headerlink" title="启动apiserver"></a>启动apiserver</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 cfg]# cd</span><br><span class="line">[root@k8s-master1 ~]# mv kubernetes/ /opt/</span><br><span class="line">[root@k8s-master1 ~]# mv *.service /usr/lib/systemd/system/</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# systemctl daemon-reload</span><br><span class="line">[root@k8s-master1 ~]# systemctl start kube-apiserver</span><br><span class="line">[root@k8s-master1 ~]# less /opt/kubernetes/logs/kube-apiserver.INFO    ##查看启动日志</span><br><span class="line">[root@k8s-master1 ~]# ps aux | grep kube</span><br><span class="line">root      17717 24.2 18.0 549604 336048 ?       Ssl  16:39   0:06 /opt/kubernetes/bin/kube-apiserver --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 --bind-address=192.168.171.134 --secure-port=6443 --advertise-address=192.168.171.134 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth=true --token-auth-file=/opt/kubernetes/cfg/token.csv --service-node-port-range=30000-32767--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem --kubelet-client-key=/opt/kubernetes/ssl/server-key.pem --tls-cert-file=/opt/kubernetes/ssl/server.pem --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem --client-ca-file=/opt/kubernetes/ssl/ca.pem --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem --etcd-cafile=/opt/etcd/ssl/ca.pem --etcd-certfile=/opt/etcd/ssl/server.pem --etcd-keyfile=/opt/etcd/ssl/server-key.pem --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/opt/kubernetes/logs/k8s-audit.log</span><br><span class="line">root      17731  0.0  0.0 112724   988 pts/1    S+   16:39   0:00 grep --color=auto kube</span><br></pre></td></tr></table></figure><p>再次启动kube-controller-manager 及 kube-scheduler<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# systemctl start kube-controller-manager</span><br><span class="line">[root@k8s-master1 ~]# systemctl start kube-scheduler</span><br><span class="line">[root@k8s-master1 ~]# systemctl enable kube-apiserver</span><br><span class="line">[root@k8s-master1 ~]# systemctl enable kube-controller-manager</span><br><span class="line">[root@k8s-master1 ~]# systemctl enable kube-scheduler</span><br></pre></td></tr></table></figure></p><p>移动kubectl到可执行目录<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# mv /opt/kubernetes/bin/kubectl /usr/local/bin/</span><br><span class="line">[root@k8s-master1 ~]# kubectl get node</span><br><span class="line">No resources found in default namespace.</span><br><span class="line">[root@k8s-master1 ~]# kubectl get cs    ##经过查看发现了此版本的bug</span><br><span class="line">NAME                 AGE</span><br><span class="line">controller-manager   &lt;unknown&gt;</span><br><span class="line">scheduler            &lt;unknown&gt;</span><br><span class="line">etcd-2               &lt;unknown&gt;</span><br><span class="line">etcd-0               &lt;unknown&gt;</span><br><span class="line">etcd-1               &lt;unknown&gt;</span><br></pre></td></tr></table></figure></p><p>如上bug：<a href="https://segmentfault.com/a/1190000020912684" target="_blank" rel="noopener">https://segmentfault.com/a/1190000020912684</a></p><h4 id="启用TLS-Bootstrapping"><a href="#启用TLS-Bootstrapping" class="headerlink" title="启用TLS Bootstrapping"></a>启用TLS Bootstrapping</h4><p>为kubelet TLS Bootstrapping 授权：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /opt/kubernetes/cfg/token.csv </span><br><span class="line">c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;</span><br><span class="line"></span><br><span class="line">格式：token,用户,uid,用户组</span><br></pre></td></tr></table></figure></p><p>给kubelet-bootstrap授权：</p><p>自动的给kubelet创建证书</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kubelet-bootstrap \</span><br><span class="line">--clusterrole=system:node-bootstrapper \</span><br><span class="line">--user=kubelet-bootstrap</span><br></pre></td></tr></table></figure><p>token也可自行生成替换：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;</span><br></pre></td></tr></table></figure><p>==但apiserver配置的token必须要与node节点bootstrap.kubeconfig配置里一致。==</p><h3 id="三、部署Worker-Node"><a href="#三、部署Worker-Node" class="headerlink" title="三、部署Worker Node"></a>三、部署Worker Node</h3><p>二进制包下载地址：<a href="https://download.docker.com/linux/static/stable/x86_64/" target="_blank" rel="noopener">https://download.docker.com/linux/static/stable/x86_64/</a></p><p>上传k8s-node.tar.gz到node节点<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# tar zxvf k8s-node.tar.gz</span><br><span class="line">cni-plugins-linux-amd64-v0.8.2.tgz</span><br><span class="line">daemon.json</span><br><span class="line">docker-18.09.6.tgz</span><br><span class="line">docker.service</span><br><span class="line">kubelet.service</span><br><span class="line">kube-proxy.service</span><br><span class="line">kubernetes/</span><br><span class="line">kubernetes/bin/</span><br><span class="line">kubernetes/bin/kubelet</span><br><span class="line">kubernetes/bin/kube-proxy</span><br><span class="line">kubernetes/cfg/</span><br><span class="line">kubernetes/cfg/kubelet-config.yml</span><br><span class="line">kubernetes/cfg/bootstrap.kubeconfig</span><br><span class="line">kubernetes/cfg/kube-proxy.kubeconfig</span><br><span class="line">kubernetes/cfg/kube-proxy.conf</span><br><span class="line">kubernetes/cfg/kubelet.conf</span><br><span class="line">kubernetes/cfg/kube-proxy-config.yml</span><br><span class="line">kubernetes/ssl/</span><br><span class="line">kubernetes/logs/</span><br></pre></td></tr></table></figure></p><h4 id="3-1、配置并启动Docker"><a href="#3-1、配置并启动Docker" class="headerlink" title="3.1、配置并启动Docker"></a>3.1、配置并启动Docker</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tar zxvf docker-18.09.6.tgz</span><br><span class="line"># mv docker/* /usr/bin</span><br><span class="line">[root@k8s-node1 ~]# ls /usr/bin/</span><br><span class="line">docker        dockerd       docker-init   docker-proxy  domainname</span><br><span class="line"># mkdir /etc/docker</span><br><span class="line">[root@k8s-node1 ~]# cat daemon.json     ##配置镜像加速器</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;http://bc437cce.m.daocloud.io&quot;],</span><br><span class="line">    &quot;insecure-registries&quot;: [&quot;192.168.171.170&quot;]</span><br><span class="line">&#125;</span><br><span class="line"># mv daemon.json /etc/docker</span><br><span class="line"># mv docker.service /usr/lib/systemd/system</span><br><span class="line"># systemctl start docker</span><br><span class="line"># systemctl enable docker</span><br><span class="line">[root@k8s-node1 ~]# ps aux | grep docker</span><br><span class="line">root      17326  2.1  1.5 405704 28404 ?        Ssl  17:05   0:00 /usr/bin/dockerd</span><br><span class="line">root      17333  1.2  0.8 316224 15048 ?        Ssl  17:05   0:00 containerd --config /var/run/docker/containerd/containerd.toml --log-level info</span><br><span class="line">root      17534  0.0  0.0 112724   988 pts/2    R+   17:05   0:00 grep --color=auto docker</span><br></pre></td></tr></table></figure><p>在查看docker info的时候发现了：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARNING: bridge-nf-call-iptables is disabled</span><br><span class="line">WARNING: bridge-nf-call-ip6tables is disabled</span><br></pre></td></tr></table></figure></p><p>解决方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line">添加以下内容</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"></span><br><span class="line">最后再执行</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure></p><h4 id="3-2、部署kubelet和kube-proxy"><a href="#3-2、部署kubelet和kube-proxy" class="headerlink" title="3.2、部署kubelet和kube-proxy"></a>3.2、部署kubelet和kube-proxy</h4><p>在master上拷贝证书到Node（有多少node节点就需要scp到多少节点）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cd TLS/k8s/</span><br><span class="line">[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.136:/opt/kubernetes/ssl/</span><br><span class="line">[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.137:/opt/kubernetes/ssl/</span><br></pre></td></tr></table></figure></p><p>node节点目录<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cd kubernetes/</span><br><span class="line">[root@k8s-node1 kubernetes]# tree .</span><br><span class="line">.</span><br><span class="line">├── bin</span><br><span class="line">│   ├── kubelet</span><br><span class="line">│   └── kube-proxy</span><br><span class="line">├── cfg</span><br><span class="line">│   ├── bootstrap.kubeconfig</span><br><span class="line">│   ├── kubelet.conf</span><br><span class="line">│   ├── kubelet-config.yml</span><br><span class="line">│   ├── kube-proxy.conf</span><br><span class="line">│   ├── kube-proxy-config.yml</span><br><span class="line">│   └── kube-proxy.kubeconfig</span><br><span class="line">├── logs</span><br><span class="line">└── ssl</span><br></pre></td></tr></table></figure></p><p>先来看下几个主要的配置文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# ls</span><br><span class="line">bootstrap.kubeconfig  kubelet.conf  kubelet-config.yml  kube-proxy.conf  kube-proxy-config.yml  kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">conf：基本配置文件</span><br><span class="line">kubeconfig：连接apiserver的配置文件</span><br><span class="line">yml：主要配置文件</span><br></pre></td></tr></table></figure></p><h5 id="kubelet-conf"><a href="#kubelet-conf" class="headerlink" title="kubelet.conf"></a>kubelet.conf</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# cat kubelet.conf</span><br><span class="line">KUBELET_OPTS=&quot;--logtostderr=false \</span><br><span class="line">--v=2 \</span><br><span class="line">--log-dir=/opt/kubernetes/logs \</span><br><span class="line">--hostname-override=k8s-node1 \     ##每个node的name（必须要唯一）</span><br><span class="line">--network-plugin=cni \      ##指定网路组件</span><br><span class="line">--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \    ##配置文件</span><br><span class="line">--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \</span><br><span class="line">--config=/opt/kubernetes/cfg/kubelet-config.yml \</span><br><span class="line">--cert-dir=/opt/kubernetes/ssl \</span><br><span class="line">--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;     ##镜像</span><br></pre></td></tr></table></figure><h5 id="bootstrap-kubeconfig（自动为即将要加入集群的node颁发证书）"><a href="#bootstrap-kubeconfig（自动为即将要加入集群的node颁发证书）" class="headerlink" title="bootstrap.kubeconfig（自动为即将要加入集群的node颁发证书）"></a>bootstrap.kubeconfig（自动为即将要加入集群的node颁发证书）</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# cat bootstrap.kubeconfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: /opt/kubernetes/ssl/ca.pem  ##拿着master的ca证书</span><br><span class="line">    server: https://192.168.171.134:6443    ##master的地址</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubelet-bootstrap</span><br><span class="line">  name: default</span><br><span class="line">current-context: default</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kubelet-bootstrap</span><br><span class="line">  user:</span><br><span class="line">    token: c47ffb939f5ca36231d9e3121a252940     ## 这个token一定要和如上master上token一致</span><br></pre></td></tr></table></figure><p>我们也来了解下启动kubelet后如何和apiserver通信的：<br><img src="http://myimage.okay686.cn/okay686cn/20191130/6PTbR2yAxSTP.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>kubelet 启动带着bootstrap.kubeconfig请求apiserver，apiserver首先会校验所携带的token是否正确，正确则会颁发证书，不正确则会启动失败。</p><h5 id="kubelet-config-yml"><a href="#kubelet-config-yml" class="headerlink" title="kubelet-config.yml"></a>kubelet-config.yml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# cat kubelet-config.yml</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 0.0.0.0</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs  ##底层驱动（和docker一致）</span><br><span class="line">clusterDNS:     ##dns</span><br><span class="line">- 10.0.0.2</span><br><span class="line">clusterDomain: cluster.local    ##域</span><br><span class="line">failSwapOn: false   ##swap关闭</span><br><span class="line">authentication:     ##认证信息</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    cacheTTL: 2m0s</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: /opt/kubernetes/ssl/ca.pem</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">  webhook:</span><br><span class="line">    cacheAuthorizedTTL: 5m0s</span><br><span class="line">    cacheUnauthorizedTTL: 30s</span><br><span class="line">evictionHard:   ##资源配置</span><br><span class="line">  imagefs.available: 15%</span><br><span class="line">  memory.available: 100Mi</span><br><span class="line">  nodefs.available: 10%</span><br><span class="line">  nodefs.inodesFree: 5%</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">maxPods: 110</span><br></pre></td></tr></table></figure><h5 id="kube-proxy-kubeconfig"><a href="#kube-proxy-kubeconfig" class="headerlink" title="kube-proxy.kubeconfig"></a>kube-proxy.kubeconfig</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# cat kube-proxy.kubeconfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: /opt/kubernetes/ssl/ca.pem</span><br><span class="line">    server: https://192.168.171.134:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kube-proxy</span><br><span class="line">  name: default</span><br><span class="line">current-context: default</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kube-proxy</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: /opt/kubernetes/ssl/kube-proxy.pem</span><br><span class="line">    client-key: /opt/kubernetes/ssl/kube-proxy-key.pem</span><br></pre></td></tr></table></figure><h5 id="kube-proxy-config-yml"><a href="#kube-proxy-config-yml" class="headerlink" title="kube-proxy-config.yml"></a>kube-proxy-config.yml</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 cfg]# vim kube-proxy-config.yml</span><br><span class="line"></span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">address: 0.0.0.0</span><br><span class="line">metricsBindAddress: 0.0.0.0:10249</span><br><span class="line">clientConnection:</span><br><span class="line">  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig</span><br><span class="line">hostnameOverride: k8s-node1     ##全node唯一</span><br><span class="line">clusterCIDR: 10.0.0.0/24</span><br><span class="line">mode: ipvs      ##模式</span><br><span class="line">ipvs:</span><br><span class="line">  scheduler: &quot;rr&quot;</span><br><span class="line">iptables:</span><br><span class="line">  masqueradeAll: true</span><br></pre></td></tr></table></figure><h4 id="启动kubelet、kube-proxy服务"><a href="#启动kubelet、kube-proxy服务" class="headerlink" title="启动kubelet、kube-proxy服务"></a>启动kubelet、kube-proxy服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mv kubernetes /opt</span><br><span class="line"># cp kubelet.service kube-proxy.service /usr/lib/systemd/system</span><br><span class="line"></span><br><span class="line">修改以下三个文件中IP地址：</span><br><span class="line"># grep 192 *</span><br><span class="line">bootstrap.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line">kubelet.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line">kube-proxy.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line"></span><br><span class="line">修改以下两个文件中主机名：</span><br><span class="line"># grep hostname *</span><br><span class="line">kubelet.conf:--hostname-override=k8s-node1 \</span><br><span class="line">kube-proxy-config.yml:hostnameOverride: k8s-node1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# systemctl start kubelet</span><br><span class="line">[root@k8s-node1 ~]# systemctl status kubelet</span><br><span class="line">● kubelet.service - Kubernetes Kubelet</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 六 2019-11-30 19:10:01 CST; 11s ago</span><br><span class="line"> Main PID: 17702 (kubelet)</span><br><span class="line">    Tasks: 9</span><br><span class="line">   Memory: 17.2M</span><br><span class="line">   CGroup: /system.slice/kubelet.service</span><br><span class="line">           └─17702 /opt/kubernetes/bin/kubelet --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --hostname-override=k8s-node1 --network-plugin=cni --kubeco...</span><br><span class="line"></span><br><span class="line">11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a</span><br><span class="line">11月 30 19:10:01 k8s-node1 systemd[1]: Stopped Kubernetes Kubelet.</span><br><span class="line">11月 30 19:10:01 k8s-node1 systemd[1]: Unit kubelet.service entered failed state.</span><br><span class="line">11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service failed.</span><br><span class="line">11月 30 19:10:01 k8s-node1 systemd[1]: Started Kubernetes Kubelet.</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 ~]# systemctl enable kubelet</span><br></pre></td></tr></table></figure><p>查看kubelet日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">less /opt/kubernetes/logs/kubelet.INFO</span><br><span class="line"></span><br><span class="line">其中我们会看到：</span><br><span class="line">W1130 19:27:08.379468   17702 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d</span><br><span class="line">E1130 19:27:08.929388   17702 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin isnot ready: cni config uninitialized</span><br><span class="line"></span><br><span class="line">如上是因为cni的组件没有安装，稍后安装后即可恢复；</span><br></pre></td></tr></table></figure></p><p>然后我们再次回到master节点 查看是否有node节点：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# kubectl get csr</span><br><span class="line">NAME                                                   AGE    REQUESTOR           CONDITION</span><br><span class="line">node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo   2m1s   kubelet-bootstrap   Pending</span><br><span class="line">[root@k8s-master1 k8s]# kubectl certificate approve node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo approved</span><br><span class="line">[root@k8s-master1 k8s]# kubectl get csr</span><br><span class="line">NAME                                                   AGE   REQUESTOR           CONDITION</span><br><span class="line">node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo   14m   kubelet-bootstrap   Approved,Issued</span><br><span class="line">[root@k8s-master1 k8s]# kubectl get node        ##等待配置完毕cni则会ready</span><br><span class="line">NAME        STATUS     ROLES    AGE   VERSION</span><br><span class="line">k8s-node1   NotReady   &lt;none&gt;   25s   v1.16.0</span><br></pre></td></tr></table></figure></p><p>启动kube-proxy服务：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# systemctl start kube-proxy</span><br><span class="line">[root@k8s-node1 ~]# systemctl status kube-proxy</span><br></pre></td></tr></table></figure></p><p>查看kube-proxy日志：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# tailf /opt/kubernetes/logs/kube-proxy.INFO</span><br><span class="line"></span><br><span class="line">I1130 19:32:23.692156   18623 proxier.go:1729] Not using `--random-fully` in the MASQUERADE rule for iptables because the local version of iptables does not support it</span><br><span class="line"></span><br><span class="line">解决方案：https://blog.51cto.com/juestnow/2440260</span><br></pre></td></tr></table></figure></p><h4 id="3-3、部署CNI网络"><a href="#3-3、部署CNI网络" class="headerlink" title="3.3、部署CNI网络"></a>3.3、部署CNI网络</h4><p>二进制包下载地址：<a href="https://github.com/containernetworking/plugins/releases" target="_blank" rel="noopener">https://github.com/containernetworking/plugins/releases</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkdir /opt/cni/bin /etc/cni/net.d</span><br><span class="line"># tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz –C /opt/cni/bin</span><br><span class="line"></span><br><span class="line">确保kubelet启用CNI：</span><br><span class="line"></span><br><span class="line"># cat /opt/kubernetes/cfg/kubelet.conf </span><br><span class="line">--network-plugin=cni</span><br></pre></td></tr></table></figure><h4 id="3-4、同理增加另外一个node节点"><a href="#3-4、同理增加另外一个node节点" class="headerlink" title="3.4、同理增加另外一个node节点"></a>3.4、同理增加另外一个node节点</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">57  tar zxvf k8s-node.tar.gz</span><br><span class="line">58  mv *.service /usr/lib/systemd/system/</span><br><span class="line">59  tar zxvf docker-18.09.6.tgz</span><br><span class="line">60  mv docker/* /usr/bin/</span><br><span class="line">61  mkdir /etc/docker</span><br><span class="line">62  vim daemon.json</span><br><span class="line">63  mv daemon.json /etc/docker/</span><br><span class="line">64  systemctl start docker</span><br><span class="line">65  systemctl enable docker</span><br><span class="line">66  systemctl status docker</span><br><span class="line">67  mv kubernetes/ /opt/</span><br><span class="line">68  cd /opt/kubernetes/</span><br><span class="line">69  ls</span><br><span class="line">70  cd cfg/</span><br><span class="line">71  ls</span><br><span class="line">72  vim bootstrap.kubeconfig</span><br><span class="line">73  vim kubelet.conf</span><br><span class="line">74  vim kubelet-config.yml</span><br><span class="line">75  vim kube-proxy.conf</span><br><span class="line">76  vim kube-proxy-config.yml</span><br><span class="line">77  vim kube-proxy.kubeconfig</span><br><span class="line">78  grep 192 *</span><br><span class="line">79  grep hostname *</span><br><span class="line">80  systemctl start kubelet</span><br><span class="line">81  systemctl start kube-proxy</span><br><span class="line">82  systemctl enable kubelet</span><br><span class="line">83  systemctl enable kube-proxy</span><br><span class="line">84   systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy</span><br><span class="line">85  mkdir /opt/cni/bin /etc/cni/net.d -p</span><br><span class="line">86  cd</span><br><span class="line">87  tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/</span><br></pre></td></tr></table></figure><p>虽然如上我只是把node2节点上的操作历史copy了一下，但是足以证明正确的操作步骤就是如上这些步骤，唯一需要注意的地方就是 如上的 kubelet和kube-proxy的配置文件。</p><h4 id="3-5、部署flannel组件"><a href="#3-5、部署flannel组件" class="headerlink" title="3.5、部署flannel组件"></a>3.5、部署flannel组件</h4><p>如要实现cni网路覆盖，我们就必须部署实现这个组件的flannel服务。</p><p>在master上操作：</p><p>上传kube-flannel.yaml到/目录<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cat kube-flannel.yaml     ##来看几个主要的信息：</span><br><span class="line">1、</span><br><span class="line">  net-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">如上的网路信息要和：</span><br><span class="line">[root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-controller-manager.conf</span><br><span class="line">--cluster-cidr=10.244.0.0/16 \  一致</span><br><span class="line"></span><br><span class="line">2、（DaemonSet模式：每个node节点都会自动部署这个服务）</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br></pre></td></tr></table></figure></p><p>在Master执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yaml</span><br><span class="line">[root@k8s-master1 ~]# kubectl get po -n kube-system</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-flannel-ds-amd64-d2gzx   1/1     Running   0          51s</span><br><span class="line">kube-flannel-ds-amd64-lwsnd   1/1     Running   0          51s</span><br><span class="line">[root@k8s-master1 ~]# kubectl get node</span><br><span class="line">NAME        STATUS   ROLES    AGE   VERSION</span><br><span class="line">k8s-node1   Ready    &lt;none&gt;   67m   v1.16.0</span><br><span class="line">k8s-node2   Ready    &lt;none&gt;   30m   v1.16.0</span><br></pre></td></tr></table></figure></p><h5 id="授权apiserver访问kubelet"><a href="#授权apiserver访问kubelet" class="headerlink" title="授权apiserver访问kubelet"></a>授权apiserver访问kubelet</h5><p>为提供安全性，kubelet禁止匿名访问，必须授权才可以。</p><p>上传apiserver-to-kubelet-rbac.yaml到/目录<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# cat apiserver-to-kubelet-rbac.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: system:kube-apiserver-to-kubelet</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:      ##允许直接在master上操作如下的权限</span><br><span class="line">      - nodes/proxy</span><br><span class="line">      - nodes/stats</span><br><span class="line">      - nodes/log</span><br><span class="line">      - nodes/spec</span><br><span class="line">      - nodes/metrics</span><br><span class="line">      - pods/log</span><br><span class="line">    verbs:</span><br><span class="line">      - &quot;*&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:kube-apiserver</span><br><span class="line">  namespace: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:kube-apiserver-to-kubelet</span><br><span class="line">subjects:</span><br><span class="line">  - apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: User</span><br><span class="line">    name: kubernetes</span><br></pre></td></tr></table></figure></p><p>测试：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system     ##没有权限查看</span><br><span class="line">Error from server (Forbidden): Forbidden (user=kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-flannel-ds-amd64-d2gzx)</span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f apiserver-to-kubelet-rbac.yaml</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created</span><br><span class="line">[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system     ##现在可以查看了</span><br><span class="line">I1130 12:30:26.695707       1 main.go:514] Determining IP address of default interface</span><br><span class="line">I1130 12:30:26.698072       1 main.go:527] Using interface with name ens33 and address 192.168.171.136</span><br><span class="line">I1130 12:30:26.698106       1 main.go:544] Defaulting external address to interface address (192.168.171.136)</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl create deployment web --image=nginx       ##创建测试deployment</span><br><span class="line">deployment.apps/web created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po -o wide</span><br><span class="line">NAME                  READY   STATUS    RESTARTS   AGE     IP           NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">web-d86c95cc9-ztx9n   1/1     Running   0          2m49s   10.244.0.2   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl expose deployment web --port=80 --type=NodePort     ##创建一个port测试下nginx是否OK</span><br><span class="line">service/web exposed</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl get po,svc</span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/web-d86c95cc9-k9vnf   1/1     Running   0          2m34s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">service/kubernetes   ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP        4h49m</span><br><span class="line">service/web          NodePort    10.0.0.34    &lt;none&gt;        80:32762/TCP   17m</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20191130/jqilbTLci6Wt.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>至此单master节点的K8S集群搭建完毕！</p><hr><h3 id="四、部署Web-UI和DNS"><a href="#四、部署Web-UI和DNS" class="headerlink" title="四、部署Web UI和DNS"></a>四、部署Web UI和DNS</h3><p>上传yaml/dashboard.yaml<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi dashboard.yaml</span><br><span class="line">…</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 443</span><br><span class="line">      targetPort: 8443</span><br><span class="line">      nodePort: 30001</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">…</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl apply -f dashboard.yaml</span><br><span class="line">namespace/kubernetes-dashboard created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">secret/kubernetes-dashboard-csrf created</span><br><span class="line">secret/kubernetes-dashboard-key-holder created</span><br><span class="line">configmap/kubernetes-dashboard-settings created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/dashboard-metrics-scraper created</span><br><span class="line">deployment.apps/dashboard-metrics-scraper created</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20191130/Ru0ulhAqqBEf.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>创建token登录：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##在创建token之前我们需要先创建service account</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# cat dashboard-adminuser.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br></pre></td></tr></table></figure></p><p>创建service account并绑定默认cluster-admin管理员集群角色：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl apply -f dashboard-adminuser.yaml</span><br><span class="line">serviceaccount/admin-user created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/admin-user created</span><br></pre></td></tr></table></figure></p><p>获取token<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">Name:         admin-user-token-bccww</span><br><span class="line">Namespace:    kubernetes-dashboard</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name: admin-user</span><br><span class="line">              kubernetes.io/service-account.uid: 6e6e1b2d-a0a3-4150-a611-98ce1653b79c</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1359 bytes</span><br><span class="line">namespace:  20 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IktJYmdRdDdkbW1US0dnOHRKemdPMjJ6eUEzTXEtMGQyS0h6cWRpRUVLRE0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWJjY3d3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2ZTZlMWIyZC1hMGEzLTQxNTAtYTYxMS05OGNlMTY1M2I3OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YusDtTl_glNewEO0kMaiZDqOcbSMkRNY6sRT9BQYbzTjdmediGHcEB49wHepo_mXsW0isBnu4Mgpb4KL5y27OkE2hFICwQwQBX5gvHQI2CxuoHaVVi7G8eZn85fR7aKmKi7Uxppv6qOL5icZyl_74_-iQVIm3U59B-x2zoyoUa3tsFgQEpUWvkmbCajD-4sANU-UMyisR3uMdXvnyvz2oCUQBjuqJ5ZqqAupqrvtoJ1L27vHK1t7i_sLgVR_2X8MARrwgynHatEYAODVEsVRMJCBzR4ZW09xcCSbeQ1CopNyGbyPi7o9re_9FyGK18y3q7EmjaEOr2NJ3Yk0MesIyw</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20191130/nauU73qDEA3I.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="部署coreDNS"><a href="#部署coreDNS" class="headerlink" title="部署coreDNS"></a>部署coreDNS</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl apply -f coredns.yaml</span><br><span class="line">serviceaccount/coredns created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">configmap/coredns created</span><br><span class="line">deployment.apps/coredns created</span><br><span class="line">service/kube-dns created</span><br></pre></td></tr></table></figure><h4 id="测试是否dns正常"><a href="#测试是否dns正常" class="headerlink" title="测试是否dns正常"></a>测试是否dns正常</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 k8s]# kubectl apply -f bs.yaml</span><br><span class="line">pod/busybox created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# kubectl exec -it busybox sh</span><br><span class="line">/ # ping 10.0.0.34  ##测试内网IP是否通过</span><br><span class="line">PING 10.0.0.34 (10.0.0.34): 56 data bytes</span><br><span class="line">64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.086 ms</span><br><span class="line">64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.068 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.0.0.34 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.068/0.077/0.086 ms</span><br><span class="line">/ # ping web    ##测试dns是否可以解析</span><br><span class="line">PING web (10.0.0.34): 56 data bytes</span><br><span class="line">64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.049 ms</span><br><span class="line">64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.065 ms</span><br><span class="line"></span><br><span class="line">/ # nslookup kubernetes （均可以解析）</span><br><span class="line">Server:    10.0.0.2</span><br><span class="line">Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes</span><br><span class="line">Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h3 id="五、Master高可用"><a href="#五、Master高可用" class="headerlink" title="五、Master高可用"></a>五、Master高可用</h3><h4 id="5-1、部署Master组件（与Master1一致）"><a href="#5-1、部署Master组件（与Master1一致）" class="headerlink" title="5.1、部署Master组件（与Master1一致）"></a>5.1、部署Master组件（与Master1一致）</h4><p>拷贝master1/opt/kubernetes和service文件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># scp –r /opt/kubernetes root@192.168.171.135:/opt</span><br><span class="line"># scp -r /opt/etcd/ssl/ root@192.168.171.135:/opt/etcd/</span><br><span class="line"># scp /usr/lib/systemd/system/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125;.service root@192.168.171.135:/usr/lib/systemd/system</span><br><span class="line"># scp /usr/local/bin/kubectl root@192.168.171.135:/usr/local/bin/</span><br></pre></td></tr></table></figure></p><p>修改apiserver配置文件为本地IP：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /opt/kubernetes/cfg/kube-apiserver.conf </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=false \</span><br><span class="line">--v=2 \</span><br><span class="line">--log-dir=/opt/kubernetes/logs \</span><br><span class="line">--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \</span><br><span class="line">--bind-address=192.168.171.135 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=192.168.171.135 \</span><br><span class="line">……</span><br></pre></td></tr></table></figure></p><p>启动kube-apiserver，kube-controller-manager，kube-scheduler<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master2 cfg]# systemctl start kube-apiserver</span><br><span class="line">[root@k8s-master2 cfg]# systemctl start kube-controller-manager</span><br><span class="line">[root@k8s-master2 cfg]# systemctl start kube-scheduler</span><br><span class="line">[root@k8s-master2 cfg]# systemctl enable kube-apiserver</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /usr/lib/systemd/system/kube-apiserver.service.</span><br><span class="line">[root@k8s-master2 cfg]# systemctl enable kube-controller-manager</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.</span><br><span class="line">[root@k8s-master2 cfg]# systemctl enable kube-scheduler</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service.</span><br></pre></td></tr></table></figure></p><p>在master2上面查看node节点的po<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master2 cfg]# kubectl get node -o wide</span><br><span class="line">NAME        STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME</span><br><span class="line">k8s-node1   Ready    &lt;none&gt;   25h   v1.16.0   192.168.171.136   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.el7.x86_64   docker://18.9.6</span><br><span class="line">k8s-node2   Ready    &lt;none&gt;   25h   v1.16.0   192.168.171.137   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.el7.x86_64   docker://18.9.6</span><br><span class="line">[root@k8s-master2 cfg]# kubectl get po -n kube-system -o wide</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6d8cfdd59d-gbd2m      1/1     Running   2          21h   10.244.0.9        k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-d2gzx   1/1     Running   1          24h   192.168.171.136   k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-lwsnd   1/1     Running   2          24h   192.168.171.137   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@k8s-master2 cfg]# kubectl get po -n kubernetes-dashboard -o wide</span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">dashboard-metrics-scraper-566cddb686-wrkfl   1/1     Running   1          23h   10.244.1.8   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-7b5bf5d559-csfwm        1/1     Running   1          23h   10.244.1.6   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><h4 id="5-2、部署Nginx负载均衡"><a href="#5-2、部署Nginx负载均衡" class="headerlink" title="5.2、部署Nginx负载均衡"></a>5.2、部署Nginx负载均衡</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx rpm包：http://nginx.org/packages/rhel/7/x86_64/RPMS/</span><br><span class="line"></span><br><span class="line"># rpm -vih http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.0-1.el7.ngx.x86_64.rpm</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# cat /etc/nginx/nginx.conf</span><br><span class="line"></span><br><span class="line">user  nginx;</span><br><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">error_log  /var/log/nginx/error.log warn;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">####此处↓</span><br><span class="line">stream &#123;</span><br><span class="line"></span><br><span class="line">    log_format  main  &apos;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent&apos;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/k8s-access.log  main;</span><br><span class="line"></span><br><span class="line">    upstream k8s-apiserver &#123;</span><br><span class="line">                server 192.168.171.134:6443;</span><br><span class="line">                server 192.168.171.135:6443;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">       listen 6443;</span><br><span class="line">       proxy_pass k8s-apiserver;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">####此处↑</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       /etc/nginx/mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    #tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    #gzip  on;</span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># systemctl start nginx</span><br><span class="line"># systemctl enable nginx</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# netstat -lntp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:6443            0.0.0.0:*               LISTEN      7142/nginx: master</span><br></pre></td></tr></table></figure><h4 id="5-3、Nginx-KeepAlived高可用"><a href="#5-3、Nginx-KeepAlived高可用" class="headerlink" title="5.3、Nginx+KeepAlived高可用"></a>5.3、Nginx+KeepAlived高可用</h4><h5 id="主节点（192-168-171-138）："><a href="#主节点（192-168-171-138）：" class="headerlink" title="主节点（192.168.171.138）："></a>主节点（192.168.171.138）：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum install keepalived</span><br><span class="line"></span><br><span class="line"># vim /etc/keepalived/keepalived.conf</span><br><span class="line">global_defs &#123; </span><br><span class="line">   notification_email &#123; </span><br><span class="line">     acassen@firewall.loc </span><br><span class="line">     failover@firewall.loc </span><br><span class="line">     sysadmin@firewall.loc </span><br><span class="line">   &#125; </span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc  </span><br><span class="line">   smtp_server 127.0.0.1 </span><br><span class="line">   smtp_connect_timeout 30 </span><br><span class="line">   router_id NGINX_MASTER</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">vrrp_script check_nginx &#123;</span><br><span class="line">    script &quot;/etc/keepalived/check_nginx.sh&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line">    state MASTER </span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 </span><br><span class="line">    priority 100    # 优先级，备服务器设置 90 </span><br><span class="line">    advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒 </span><br><span class="line">    authentication &#123; </span><br><span class="line">        auth_type PASS      </span><br><span class="line">        auth_pass 1111 </span><br><span class="line">    &#125;  </span><br><span class="line">    virtual_ipaddress &#123; </span><br><span class="line">        192.168.171.188/24</span><br><span class="line">    &#125; </span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_nginx</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># cat /etc/keepalived/check_nginx.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">count=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)</span><br><span class="line"></span><br><span class="line">if [ &quot;$count&quot; -eq 0 ];then</span><br><span class="line">    exit 1</span><br><span class="line">else</span><br><span class="line">    exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># chmod +x /etc/keepalived/check_nginx.sh</span><br><span class="line"></span><br><span class="line"># systemctl start keepalived</span><br><span class="line"># systemctl enable keepalived</span><br></pre></td></tr></table></figure><h4 id="备节点（192-168-171-139）："><a href="#备节点（192-168-171-139）：" class="headerlink" title="备节点（192.168.171.139）："></a>备节点（192.168.171.139）：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /etc/keepalived/keepalived.conf</span><br><span class="line">global_defs &#123; </span><br><span class="line">   notification_email &#123; </span><br><span class="line">     acassen@firewall.loc </span><br><span class="line">     failover@firewall.loc </span><br><span class="line">     sysadmin@firewall.loc </span><br><span class="line">   &#125; </span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc  </span><br><span class="line">   smtp_server 127.0.0.1 </span><br><span class="line">   smtp_connect_timeout 30 </span><br><span class="line">   router_id NGINX_BACKUP</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">vrrp_script check_nginx &#123;</span><br><span class="line">    script &quot;/etc/keepalived/check_nginx.sh&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123; </span><br><span class="line">    state BACKUP </span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 </span><br><span class="line">    priority 90    # 优先级，备服务器设置 90 </span><br><span class="line">    advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒 </span><br><span class="line">    authentication &#123; </span><br><span class="line">        auth_type PASS      </span><br><span class="line">        auth_pass 1111 </span><br><span class="line">    &#125;  </span><br><span class="line">    virtual_ipaddress &#123; </span><br><span class="line">        192.168.171.188/24</span><br><span class="line">    &#125; </span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_nginx</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># cat /etc/keepalived/check_nginx.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">count=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)</span><br><span class="line"></span><br><span class="line">if [ &quot;$count&quot; -eq 0 ];then</span><br><span class="line">    exit 1</span><br><span class="line">else</span><br><span class="line">    exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># chmod +x /etc/keepalived/check_nginx.sh</span><br><span class="line"></span><br><span class="line"># systemctl start keepalived</span><br><span class="line"># systemctl enable keepalived</span><br></pre></td></tr></table></figure><p>查看虚拟VIP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:9b:85:86 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.171.138/24 brd 192.168.171.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.171.188/24 scope global secondary ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::d3c5:e3e2:26f6:f6b5/64 scope link noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p><h4 id="5-4、修改Node连接VIP"><a href="#5-4、修改Node连接VIP" class="headerlink" title="5.4、修改Node连接VIP"></a>5.4、修改Node连接VIP</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cd /opt/kubernetes/cfg/</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 cfg]# grep 192 *</span><br><span class="line">bootstrap.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line">kubelet.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line">kube-proxy.kubeconfig:    server: https://192.168.171.134:6443</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 cfg]# sed -i &apos;s#192.168.171.134#192.168.171.188#&apos; *</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 cfg]# grep 192 *</span><br><span class="line">bootstrap.kubeconfig:    server: https://192.168.171.188:6443</span><br><span class="line">kubelet.kubeconfig:    server: https://192.168.171.188:6443</span><br><span class="line">kube-proxy.kubeconfig:    server: https://192.168.171.188:6443</span><br><span class="line"></span><br><span class="line">[root@k8s-node1 cfg]# systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy</span><br><span class="line"></span><br><span class="line">同理操作其它node节点</span><br></pre></td></tr></table></figure><p>测试VIP是否正常工作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-node2 cfg]# curl -k --header &quot;Authorization: Bearer c47ffb939f5ca36231d9e3121a252940&quot; https://192.168.171.188:6443/version</span><br><span class="line">&#123;</span><br><span class="line">  &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;minor&quot;: &quot;16&quot;,</span><br><span class="line">  &quot;gitVersion&quot;: &quot;v1.16.0&quot;,</span><br><span class="line">  &quot;gitCommit&quot;: &quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;,</span><br><span class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">  &quot;buildDate&quot;: &quot;2019-09-18T14:27:17Z&quot;,</span><br><span class="line">  &quot;goVersion&quot;: &quot;go1.12.9&quot;,</span><br><span class="line">  &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">  &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">分别在node1和node2上测试，你会发现nginx会以轮训的方式分别请求apiserver；</span><br></pre></td></tr></table></figure></p><p>至此生产级K8S高可用集群搭建完毕！</p><hr>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建一个生产级K8S高可用集群（1）</title>
    <link href="/2019/11/26/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%881%EF%BC%89/"/>
    <url>/2019/11/26/%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">不多说了，2019年底再开始~~</span><br></pre></td></tr></table></figure><h3 id="一、K8S特性"><a href="#一、K8S特性" class="headerlink" title="一、K8S特性"></a>一、K8S特性</h3><h4 id="①自我修复"><a href="#①自我修复" class="headerlink" title="①自我修复"></a>①自我修复</h4><p>在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端的请求，确保服务不中断。</p><h4 id="②弹性伸缩"><a href="#②弹性伸缩" class="headerlink" title="②弹性伸缩"></a>②弹性伸缩</h4><p>使用命令、UI管控或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行业务。</p><h4 id="③自动部署和回滚"><a href="#③自动部署和回滚" class="headerlink" title="③自动部署和回滚"></a>③自动部署和回滚</h4><p>K8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不影响业务。</p><h4 id="④服务发现和负载均衡"><a href="#④服务发现和负载均衡" class="headerlink" title="④服务发现和负载均衡"></a>④服务发现和负载均衡</h4><p>K8S为多个容器提供一个统一的访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。</p><h4 id="⑤机密和配置管理"><a href="#⑤机密和配置管理" class="headerlink" title="⑤机密和配置管理"></a>⑤机密和配置管理</h4><p>管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。</p><h4 id="⑥存储编排"><a href="#⑥存储编排" class="headerlink" title="⑥存储编排"></a>⑥存储编排</h4><p>挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网路存储（如NFS，ClusterFS，Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。</p><h4 id="⑦批处理"><a href="#⑦批处理" class="headerlink" title="⑦批处理"></a>⑦批处理</h4><p>提供一次性任务，定时任务；满足批量数据处理和分析的场景。</p><h3 id="二、K8S集群架构和组件"><a href="#二、K8S集群架构和组件" class="headerlink" title="二、K8S集群架构和组件"></a>二、K8S集群架构和组件</h3><p><img src="http://myimage.okay686.cn/okay686cn/20191127/ugJ3GH6QGA99.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><ul><li>Master</li></ul><p>Master主要负责资源调度，控制副本，和提供统一访问集群的入口。</p><ul><li>Node</li></ul><p>Node由Master管理，并汇报容器状态给Master，同时根据Master要求管理容器生命周期。</p><ul><li>Pod</li></ul><p>Docker最小部署单元是容器，而Kubernetes最小部署单元是Pod，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，一个Pod在同一台Node上运行。</p><ul><li>Service</li></ul><p>Service一个应用服务抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。Service代理Pod集合对外表现是为一个访问入口，分配一个集群IP地址，来自这个IP的请求将负载均衡转发到后端Pod中的容器。<br>用过负载均衡器的朋友可能很好理解，其实Service就是一个抽象的负载均衡器。<br>Service通过Lable Selector选择一组Pod提供服务。</p><ul><li>Lable</li></ul><p>标签是一个key=value的键值对，附加在某个资源上，每个对象可以有多个标签，然后根据这个lable关联、查询和筛选。<br>就像Service与Pod，当多个Service、多个Pod情况下，访问某个Service怎么就知道转发到指定Pod呢？</p><ul><li>Volume</li></ul><p>数据卷，挂载宿主机文件、目录或者外部存储到Pod中，为应用服务提供存储，也可以Pod中容器之间共享数据。</p><ul><li>Namespace</li></ul><p>命名空间将资源对象逻辑上分配到不同Namespace，可以是不同的项目、用户等区分管理，并设定控制策略，从而实现多租户。命名空间也称为虚拟集群。</p><h4 id="2-1、Master组件"><a href="#2-1、Master组件" class="headerlink" title="2.1、Master组件"></a>2.1、Master组件</h4><ul><li>kube-apiserver</li></ul><p>Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。</p><ul><li>kube-controller-manager</li></ul><p>处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。</p><ul><li>kube-scheduler</li></ul><p>根据调度算法为新创建的Pod选择一个Node节点，可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上。</p><ul><li>etcd</li></ul><p>分布式键值存储系统。用于保存集群状态数据，比如Pod、Service等对象信息。</p><h4 id="2-2、Node组件"><a href="#2-2、Node组件" class="headerlink" title="2.2、Node组件"></a>2.2、Node组件</h4><ul><li>kubelet</li></ul><p>kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。<br>cadvise:监控容器和节点资源</p><ul><li>kube-proxy</li></ul><p>service 在逻辑上代表了后端的多个 Pod，外界通过 service 访问 Pod。service 接收到的请求是如何转发到 Pod 的呢？这就是 kube-proxy 要完成的工作。<br>每个 Node 都会运行 kube-proxy 服务，它负责将访问 service 的 TCP/UPD 数据流转发到后端的容器。如果有多个副本，kube-proxy 会实现负载均衡。</p><h3 id="三、下面是更高层次抽象对象："><a href="#三、下面是更高层次抽象对象：" class="headerlink" title="三、下面是更高层次抽象对象："></a>三、下面是更高层次抽象对象：</h3><ul><li>ReplicaSet（确保预期的Pod副本数量）</li></ul><p>确保任何给定时间指定的Pod副本数量，并提供声明式更新等功能。</p><ul><li>Deployment（无状态应用部署）</li></ul><p>Deployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。<br>官方建议使用Deployment管理ReplicaSets，而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象，因此Deployment将会是使用最频繁的资源对象。</p><ul><li>StatefulSet（有状态应用部署）</li></ul><p>StatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。<br><strong>典型场景</strong>：++Zookeper集群++</p><ul><li>DaemonSet（确保所有Node运行同一个Pod）</li></ul><p>DaemonSet确保所有节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。<br><strong>典型场景</strong>：++在每个节点部署日志收集程序（如filebeat），监控程序（agent）++</p><ul><li>Job（一次性任务）</li></ul><p>一次性任务，运行完成后Pod销毁，不再重新启动新容器。还可以任务定时运行。</p><ul><li>Cron Job（定时任务）</li></ul><p>定时任务，一个CronJob对象就像一个crontab文件的一行。给定时间定期运行，以Cron格式编写。<br>典型场景：数据库备份，发送邮件</p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生产级k8s二进制v14.1高可用部署</title>
    <link href="/2019/06/18/%E7%94%9F%E4%BA%A7%E7%BA%A7k8s%E4%BA%8C%E8%BF%9B%E5%88%B6v14.1%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/"/>
    <url>/2019/06/18/%E7%94%9F%E4%BA%A7%E7%BA%A7k8s%E4%BA%8C%E8%BF%9B%E5%88%B6v14.1%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><h4 id="1-1、角色划分"><a href="#1-1、角色划分" class="headerlink" title="1.1、角色划分"></a>1.1、角色划分</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.8.13.80   vip  </span><br><span class="line">10.8.13.81   master01  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.82   master02  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.83   master03  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.84   node01    kubelet、docker、kube_proxy、flanneld</span><br><span class="line">10.8.13.85   node02    kubelet、docker、kube_proxy、flanneld</span><br></pre></td></tr></table></figure><h4 id="1-2、各主机ssh互通"><a href="#1-2、各主机ssh互通" class="headerlink" title="1.2、各主机ssh互通"></a>1.2、各主机ssh互通</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#ssh-keygen</span><br><span class="line">#ssh-copy-id 10.8.13.82(83-85)</span><br></pre></td></tr></table></figure><h4 id="1-3、环境初始化"><a href="#1-3、环境初始化" class="headerlink" title="1.3、环境初始化"></a>1.3、环境初始化</h4><h5 id="1-3-1、停止iptables"><a href="#1-3-1、停止iptables" class="headerlink" title="1.3.1、停止iptables"></a>1.3.1、停止iptables</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld.service </span><br><span class="line">systemctl disable  firewalld.service</span><br></pre></td></tr></table></figure><h5 id="1-3-2、关闭selinux"><a href="#1-3-2、关闭selinux" class="headerlink" title="1.3.2、关闭selinux"></a>1.3.2、关闭selinux</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /etc/selinux/config </span><br><span class="line">SELINUX=disabled</span><br><span class="line"># setenforce 0</span><br></pre></td></tr></table></figure><h5 id="1-3-3、设置sysctl，开启路由转发"><a href="#1-3-3、设置sysctl，开启路由转发" class="headerlink" title="1.3.3、设置sysctl，开启路由转发"></a>1.3.3、设置sysctl，开启路由转发</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /etc/sysctl.conf</span><br><span class="line"> fs.file-max=1000000</span><br><span class="line"> net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line"> net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"> net.ipv4.ip_forward = 1</span><br><span class="line"> vm.swappiness = 0</span><br><span class="line"> net.ipv4.ip_forward = 1</span><br><span class="line"> net.ipv4.tcp_max_tw_buckets = 6000</span><br><span class="line"> net.ipv4.tcp_sack = 1</span><br><span class="line"> net.ipv4.tcp_window_scaling = 1</span><br><span class="line"> net.ipv4.tcp_rmem = 4096 87380 4194304</span><br><span class="line"> net.ipv4.tcp_wmem = 4096 16384 4194304</span><br><span class="line"> net.ipv4.tcp_max_syn_backlog = 16384</span><br><span class="line"> net.core.netdev_max_backlog = 32768</span><br><span class="line"> net.core.somaxconn = 32768</span><br><span class="line"> net.core.wmem_default = 8388608</span><br><span class="line"> net.core.rmem_default = 8388608</span><br><span class="line"> net.core.rmem_max = 16777216</span><br><span class="line"> net.core.wmem_max = 16777216</span><br><span class="line"> net.ipv4.tcp_timestamps = 1</span><br><span class="line"> net.ipv4.tcp_fin_timeout = 20</span><br><span class="line"> net.ipv4.tcp_synack_retries = 2</span><br><span class="line"> net.ipv4.tcp_syn_retries = 2</span><br><span class="line"> net.ipv4.tcp_syncookies = 1</span><br><span class="line"></span><br><span class="line"> net.ipv4.tcp_tw_reuse = 1</span><br><span class="line"> net.ipv4.tcp_mem = 94500000 915000000 927000000</span><br><span class="line"> net.ipv4.tcp_max_orphans = 3276800</span><br><span class="line"> net.ipv4.ip_local_port_range = 1024 65000</span><br><span class="line"> net.nf_conntrack_max = 6553500</span><br><span class="line"> net.netfilter.nf_conntrack_max = 6553500</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_established = 3600</span><br></pre></td></tr></table></figure><h5 id="1-3-4、加载ipvs"><a href="#1-3-4、加载ipvs" class="headerlink" title="1.3.4、加载ipvs"></a>1.3.4、加载ipvs</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee /etc/sysconfig/modules/ipvs.modules</span><br><span class="line">#!/bin/bash</span><br><span class="line"> modprobe -- ip_vs</span><br><span class="line"> modprobe -- ip_vs_rr</span><br><span class="line"> modprobe -- ip_vs_wrr</span><br><span class="line"> modprobe -- ip_vs_sh</span><br><span class="line"> modprobe -- nf_conntrack_ipv4</span><br><span class="line"> EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure><h3 id="二、集群各功能模块描述"><a href="#二、集群各功能模块描述" class="headerlink" title="二、集群各功能模块描述"></a>二、集群各功能模块描述</h3><p><img src="http://myimage.okay686.cn/okay686cn/180302/B41emjc68G.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h5 id="Master节点："><a href="#Master节点：" class="headerlink" title="Master节点："></a>Master节点：</h5><pre><code>Master节点上面主要由四个模块组成，etcd，APIServer，schedule,controller-manager（haproxy、keepalived高可用后面单独说）</code></pre><h5 id="etcd："><a href="#etcd：" class="headerlink" title="etcd："></a>etcd：</h5><pre><code>etcd是一个高可用的键值存储系统，kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。</code></pre><h5 id="APIServer"><a href="#APIServer" class="headerlink" title="APIServer:"></a>APIServer:</h5><pre><code>APIServer负责对外提供Restful的kubernetes API的服务，它是系统管理指令的统一接口，任何对资源的增删该查都要交给APIServer处理后再交给etcd。kubectl(kubernetes提供的客户端工具，该工具内部是对kubernetes API的调用）是直接和APIServer交互的。</code></pre><h5 id="schedule"><a href="#schedule" class="headerlink" title="schedule:"></a>schedule:</h5><pre><code>schedule负责调度Pod到合适的Node上，如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定。 kubernetes目前提供了调度算法，同样也保留了接口。用户根据自己的需求定义自己的调度算法。</code></pre><h5 id="controller-manager"><a href="#controller-manager" class="headerlink" title="controller manager:"></a>controller manager:</h5><pre><code>如果APIServer做的是前台的工作的话，那么controller manager就是负责后台的。每一个资源都对应一个控制器。而control manager就是负责管理这些控制器的，比如我们通过APIServer创建了一个Pod，当这个Pod创建成功后，APIServer的任务就算完成了。</code></pre><h4 id="Node节点："><a href="#Node节点：" class="headerlink" title="Node节点："></a>Node节点：</h4><p>每个Node节点主要由四个模板组成：kublet， kube-proxy，docker，flanneld</p><h5 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy:"></a>kube-proxy:</h5><pre><code>该模块实现了kubernetes中的服务发现和反向代理功能。kube-proxy支持TCP和UDP连接转发，默认基Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响，另外，kube-proxy还支持session affinity。</code></pre><h5 id="kublet："><a href="#kublet：" class="headerlink" title="kublet："></a>kublet：</h5><pre><code>kublet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上的所有容器，但是如果容器不是通过kubernetes创建的，它并不会管理。本质上，它负责使Pod的运行状态与期望的状态一致。</code></pre><h5 id="flanneld："><a href="#flanneld：" class="headerlink" title="flanneld："></a>flanneld：</h5><pre><code>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。</code></pre><h3 id="三、下载链接"><a href="#三、下载链接" class="headerlink" title="三、下载链接"></a>三、下载链接</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Client Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">Server Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">Node Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">etcd</span><br><span class="line">https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">flannel</span><br><span class="line">https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><h3 id="四、Master部署"><a href="#四、Master部署" class="headerlink" title="四、Master部署"></a>四、Master部署</h3><p>以下操作都在<strong>master01</strong>上执行，生成证书之后拷贝到master02和master03</p><h4 id="4-1、下载软件"><a href="#4-1、下载软件" class="headerlink" title="4.1、下载软件"></a>4.1、下载软件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">wget https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">wget https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><h4 id="4-2、ssl安装"><a href="#4-2、ssl安装" class="headerlink" title="4.2、ssl安装"></a>4.2、ssl安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /usr/local/bin/cfssl</span><br><span class="line">mv cfssljson_linux-amd64 /usr/local/bin/cfssljson</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo</span><br></pre></td></tr></table></figure><h4 id="4-3、创建etcd证书"><a href="#4-3、创建etcd证书" class="headerlink" title="4.3、创建etcd证书"></a>4.3、创建etcd证书</h4><p>在所有节点（master01-03、node01-02）创建此路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /k8s/etcd/&#123;bin,cfg,ssl&#125; -p</span><br><span class="line">mkdir /k8s/kubernetes/&#123;bin,cfg,ssl&#125; -p</span><br></pre></td></tr></table></figure></p><p>1)、etcd ca配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /k8s/etcd/ssl/</span><br><span class="line">cat &lt;&lt; EOF | tee ca-config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;etcd&quot;: &#123;</span><br><span class="line">         &quot;expiry&quot;: &quot;87600h&quot;,</span><br><span class="line">         &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><p>2)、etcd ca证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee ca-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;etcd CA&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><p>3)、etcd server证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee server-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    &quot;10.8.13.81&quot;,</span><br><span class="line">    &quot;10.8.13.82&quot;,</span><br><span class="line">    &quot;10.8.13.83&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><p>4)、生成etcd ca证书和私钥</p><p>初始化ca<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca </span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca-csr.json  server-csr.json</span><br><span class="line">[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca </span><br><span class="line">2019/05/01 16:13:54 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] generate received request</span><br><span class="line">2019/05/01 16:13:54 [INFO] received CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 16:13:54 [INFO] encoded CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] signed certificate with serial number 144752911121073185391033754516204538929473929443</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server-csr.json</span><br></pre></td></tr></table></figure></p><p>生成server证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server</span><br><span class="line">2019/05/01 16:18:53 [INFO] generate received request</span><br><span class="line">2019/05/01 16:18:53 [INFO] received CSR</span><br><span class="line">2019/05/01 16:18:53 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 16:18:54 [INFO] encoded CSR</span><br><span class="line">2019/05/01 16:18:54 [INFO] signed certificate with serial number 388122587040599986639159163167557684970159030057</span><br><span class="line">2019/05/01 16:18:54 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. </span><br><span class="line">For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p><h4 id="4-4、etcd安装"><a href="#4-4、etcd安装" class="headerlink" title="4.4、etcd安装"></a>4.4、etcd安装</h4><p>1）解压缩<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">cd etcd-v3.3.11-linux-amd64/</span><br><span class="line">cp etcd etcdctl /k8s/etcd/bin/</span><br><span class="line">mkdir /data1/etcd</span><br></pre></td></tr></table></figure></p><p>2）配置etcd主文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/etcd/cfg/etcd.conf   </span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd01&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.81:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;</span><br><span class="line"> </span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.81:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p><p>3）配置etcd启动文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/data1/etcd/</span><br><span class="line">EnvironmentFile=-/k8s/etcd/cfg/etcd.conf</span><br><span class="line"># set GOMAXPROCS to number of processors</span><br><span class="line">ExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /k8s/etcd/bin/etcd --name=\&quot;$&#123;ETCD_NAME&#125;\&quot; --data-dir=\&quot;$&#123;ETCD_DATA_DIR&#125;\&quot; --listen-client-urls=\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\&quot; --listen-peer-urls=\&quot;$&#123;ETCD_LISTEN_PEER_URLS&#125;\&quot; --advertise-client-urls=\&quot;$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\&quot; --initial-cluster-token=\&quot;$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\&quot; --initial-cluster=\&quot;$&#123;ETCD_INITIAL_CLUSTER&#125;\&quot; --initial-cluster-state=\&quot;$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\&quot; --cert-file=\&quot;$&#123;ETCD_CERT_FILE&#125;\&quot; --key-file=\&quot;$&#123;ETCD_KEY_FILE&#125;\&quot; --trusted-ca-file=\&quot;$&#123;ETCD_TRUSTED_CA_FILE&#125;\&quot; --client-cert-auth=\&quot;$&#123;ETCD_CLIENT_CERT_AUTH&#125;\&quot; --peer-cert-file=\&quot;$&#123;ETCD_PEER_CERT_FILE&#125;\&quot; --peer-key-file=\&quot;$&#123;ETCD_PEER_KEY_FILE&#125;\&quot; --peer-trusted-ca-file=\&quot;$&#123;ETCD_PEER_TRUSTED_CA_FILE&#125;\&quot; --peer-client-cert-auth=\&quot;$&#123;ETCD_PEER_CLIENT_CERT_AUTH&#125;\&quot;&quot;</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>4)、拷贝master01etcd的证书、配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/etcd/ssl/* 10.8.13.82:/k8s/etcd/ssl/</span><br><span class="line">scp /k8s/etcd/ssl/* 10.8.13.83:/k8s/etcd/ssl/</span><br><span class="line">scp /k8s/etcd/cfg/* 10.8.13.82:/k8s/etcd/cfg/</span><br><span class="line">scp /k8s/etcd/cfg/* 10.8.13.83:/k8s/etcd/cfg/</span><br><span class="line">scp /k8s/etcd/bin/* 10.8.13.82:/k8s/etcd/bin/</span><br><span class="line">scp /k8s/etcd/bin/* 10.8.13.83:/k8s/etcd/bin/</span><br><span class="line">scp /usr/lib/systemd/system/etcd.service 10.8.13.82:/usr/lib/systemd/system/etcd.service</span><br><span class="line">scp /usr/lib/systemd/system/etcd.service 10.8.13.83:/usr/lib/systemd/system/etcd.service</span><br></pre></td></tr></table></figure></p><p>5)、修改master02、master03 etcd的conf配置文件</p><p>matser02 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.82</span><br><span class="line">vim /k8s/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd02&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.82:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.82:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p><p>matser03 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.83</span><br><span class="line">vim /k8s/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd03&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p><p>6)、启动etcd服务，并加入开机自启动(master三个节点都执行)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure></p><p>7)、etcd服务检查<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379&quot; cluster-health</span><br><span class="line">以下为输出：</span><br><span class="line">member 262d942ab474feaa is healthy: got healthy result from https://10.8.13.82:2379</span><br><span class="line">member 3e95c59733e7d54f is healthy: got healthy result from https://10.8.13.83:2379</span><br><span class="line">member fe03446cb13e0221 is healthy: got healthy result from https://10.8.13.81:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure></p><p>至此etcd安装完成。。。</p><hr><h4 id="4-5、haproxy安装配置"><a href="#4-5、haproxy安装配置" class="headerlink" title="4.5、haproxy安装配置"></a>4.5、haproxy安装配置</h4><p>1)、master01配置(需要注意的是端口自定义为16443)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install haproxy</span><br></pre></td></tr></table></figure><p>master01、master02、master03都安装haproxy</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    log         127.0.0.1 local2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">    # turn on stats unix socket</span><br><span class="line">    stats socket /var/lib/haproxy/stats</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class="line"># use if not designated in their block</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  httplog</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option forwardfor       except 127.0.0.0/8</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># kubernetes apiserver frontend which proxys to the backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">frontend kubernetes-apiserver</span><br><span class="line">    mode                 tcp</span><br><span class="line">    bind                 *:16443</span><br><span class="line">    option               tcplog</span><br><span class="line">    default_backend      kubernetes-apiserver</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># round robin balancing between the various backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">backend kubernetes-apiserver</span><br><span class="line">    mode        tcp</span><br><span class="line">    balance     roundrobin</span><br><span class="line">    server      k8s01 10.8.13.81:6443 check</span><br><span class="line">    server      k8s02 10.8.13.82:6443 check</span><br><span class="line">    server      k8s03 10.8.13.83:6443 check</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># collection haproxy statistics message</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">listen stats</span><br><span class="line">    bind                 *:1080</span><br><span class="line">    stats auth           admin:awesomePassword</span><br><span class="line">    stats refresh        5s</span><br><span class="line">    stats realm          HAProxy\ Statistics</span><br><span class="line">    stats uri            /admin?stats</span><br></pre></td></tr></table></figure><p>2）拷贝master01的haproxy到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /etc/haproxy/haproxy.cfg 10.8.13.82:/etc/haproxy/haproxy.cfg</span><br><span class="line">scp /etc/haproxy/haproxy.cfg 10.8.13.83:/etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure></p><p>3)启动haproxy服务，并加入开机自启动(master三个节点都执行)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure></p><h4 id="4-6、keepalived安装配置"><a href="#4-6、keepalived安装配置" class="headerlink" title="4.6、keepalived安装配置"></a>4.6、keepalived安装配置</h4><p>1）master01配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install keepalived</span><br></pre></td></tr></table></figure><p>master01、master02、master03都安装keepalived<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 100</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2）master02配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 99</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>3）master03配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 98</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>4）启动keepalived服务（vip在master01上）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable keepalived</span><br><span class="line">systemctl start keepalived</span><br><span class="line">[root@master01 ~]# systemctl status keepalived</span><br><span class="line">● keepalived.service - LVS and VRRP High Availability Monitor</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/keepalived.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:33 CST; 3 days ago</span><br><span class="line">  Process: 992 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1115 (keepalived)</span><br><span class="line">   CGroup: /system.slice/keepalived.service</span><br><span class="line">           ├─1115 /usr/sbin/keepalived -D</span><br><span class="line">           ├─1116 /usr/sbin/keepalived -D</span><br><span class="line">           └─1117 /usr/sbin/keepalived -D</span><br><span class="line"></span><br><span class="line">Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</span><br><span class="line">[root@hwzx-test-cmpmaster01 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 00:50:56:90:22:79 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.8.13.81/24 brd 10.8.13.255 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.8.13.80/32 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::6772:8bb6:b50c:57fe/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p><p>5)keepalived配置注意事项<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;1.killall -0 根据进程名称检测进程是否存活，如果服务器没有该命令，请使用yum install psmisc -y安装</span><br><span class="line"></span><br><span class="line">&gt;2.第一个master节点的state为MASTER，其他master节点的state为BACKUP</span><br><span class="line"></span><br><span class="line">&gt;3.priority表示各个节点的优先级，范围：0～250（非强制要求）</span><br></pre></td></tr></table></figure></p><h4 id="4-7、生成kubernets证书与私钥"><a href="#4-7、生成kubernets证书与私钥" class="headerlink" title="4.7、生成kubernets证书与私钥"></a>4.7、生成kubernets证书与私钥</h4><p>1）制作kubernetes ca证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /k8s/kubernetes/ssl</span><br><span class="line">cat &lt;&lt; EOF | tee ca-config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">         &quot;expiry&quot;: &quot;87600h&quot;,</span><br><span class="line">         &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee ca-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class="line">[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class="line">2019/05/01 09:47:08 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] generate received request</span><br><span class="line">2019/05/01 09:47:08 [INFO] received CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:47:08 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] signed certificate with serial number 156611735285008649323551446985295933852737436614</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</span><br></pre></td></tr></table></figure><p>2）制作apiserver证书</p><p>==注意hosts处，所有IP都写进去，包括vip==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee server-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;10.254.0.1&quot;,</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;10.8.13.81&quot;,</span><br><span class="line">      &quot;10.8.13.82&quot;,</span><br><span class="line">      &quot;10.8.13.83&quot;,</span><br><span class="line">      &quot;10.8.13.84&quot;,</span><br><span class="line">      &quot;10.8.13.85&quot;,</span><br><span class="line">      &quot;10.8.13.80&quot;,</span><br><span class="line">      &quot;kubernetes&quot;,</span><br><span class="line">      &quot;kubernetes.default&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</span><br><span class="line">2019/05/01 09:51:56 [INFO] generate received request</span><br><span class="line">2019/05/01 09:51:56 [INFO] received CSR</span><br><span class="line">2019/05/01 09:51:56 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:51:56 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:51:56 [INFO] signed certificate with serial number 399376216731194654868387199081648887334508501005</span><br><span class="line">2019/05/01 09:51:56 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></figure><p>3）制作kube-proxy证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee kube-proxy-csr.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">2019/05/01 09:52:40 [INFO] generate received request</span><br><span class="line">2019/05/01 09:52:40 [INFO] received CSR</span><br><span class="line">2019/05/01 09:52:40 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:52:40 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:52:40 [INFO] signed certificate with serial number 633932731787505365511506755558794469389165123417</span><br><span class="line">2019/05/01 09:52:40 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca-csr.json  ca.pem          kube-proxy-csr.json  kube-proxy.pem  server-csr.json  server.pem</span><br><span class="line">ca.csr          ca-key.pem   kube-proxy.csr  kube-proxy-key.pem   server.csr      server-key.pem</span><br></pre></td></tr></table></figure><h4 id="4-8部署kubernetes-server"><a href="#4-8部署kubernetes-server" class="headerlink" title="4.8部署kubernetes server"></a>4.8部署kubernetes server</h4><p>kubernetes master 节点运行如下组件：<br>kube-apiserver<br>kube-scheduler<br>kube-controller-manager<br>kube-scheduler 和 kube-controller-manager 以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。</p><p>1）解压缩文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf kubernetes-server-linux-amd64.tar.gz </span><br><span class="line">cd kubernetes/server/bin/</span><br><span class="line">cp kube-scheduler kube-apiserver kube-controller-manager kubectl /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure><p>2）部署kube-apiserver组件（==注意保留此KEY，下面还会需要==）</p><p>创建TLS Bootstrapping Token<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 bin]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;</span><br><span class="line">af93a4194e7bcf7f05dc0bab3a6e97cd</span><br><span class="line"> </span><br><span class="line">vim /k8s/kubernetes/cfg/token.csv</span><br><span class="line">af93a4194e7bcf7f05dc0bab3a6e97cd,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><br></pre></td></tr></table></figure></p><p>创建Apiserver配置文件</p><p><strong>注</strong>：–bind-address=当前节点ip<br>–advertise-address=当前节点ip<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.81 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.81 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p><p>创建apiserver systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-apiserver.service </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-apiserver</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>拷贝master01 kubernetes的证书、配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/ssl/* 10.8.13.82:/k8s/kubernetes/ssl/</span><br><span class="line">scp /k8s/kubernetes/ssl/* 10.8.13.83:/k8s/kubernetes/ssl/</span><br><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.82:/k8s/kubernetes/cfg/</span><br><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.83:/k8s/kubernetes/cfg/</span><br><span class="line">scp /k8s/kubernetes/bin/* 10.8.13.82:/k8s/kubernetes/bin/</span><br><span class="line">scp /k8s/kubernetes/bin/* 10.8.13.83:/k8s/kubernetes/bin/</span><br><span class="line">scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.82:/usr/lib/systemd/system</span><br><span class="line">scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.83:/usr/lib/systemd/system</span><br></pre></td></tr></table></figure></p><p>5)、修改master02、master03 etcd的conf配置文件</p><p>matser02 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.82</span><br><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.82 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.82 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p><p>matser03 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.83</span><br><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.83 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.83 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p><p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">[root@elasticsearch01 bin]# systemctl status kube-apiserver</span><br><span class="line">● kube-apiserver.service - Kubernetes API Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 705 (kube-apiserver)</span><br><span class="line">   CGroup: /system.slice/kube-apiserver.service</span><br><span class="line">           └─705 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --s...</span><br><span class="line"></span><br><span class="line">5月 13 16:00:43 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:43.495504     705 wrap.go:47] GET /api/v1/namespaces/default/endpoints/kubernetes: (3.700854ms) 200 [kube-apiserver/v1.13.1 (linux/amd64) kubernetes/eec55b9 10.8.13.81:56744]</span><br><span class="line">5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.955530     705 wrap.go:47] GET /api/v1/services?resourceVersion=37540&amp;timeout=6m29s&amp;timeoutSeconds=389&amp;watch=true: (6m29.001574609s) 200 [kube-proxy/v1.13.1 (linux/amd64) kub... 10.8.13.81:56844]</span><br><span class="line">5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.958607     705 get.go:247] Starting watch for /api/v1/services, rv=37540 labels= fields= timeout=8m28s</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.323978     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.410282ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.371766     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (3.606335ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.376888     705 wrap.go:47] GET /apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=32859&amp;timeout=5m5s&amp;timeoutSeconds=305&amp;watch=true: (5m5.001015872s) 200 [kube-apiser... 10.8.13.81:56744]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.377312     705 reflector.go:357] k8s.io/kube-aggregator/pkg/client/informers/internalversion/factory.go:117: Watch close - *apiregistration.APIService total 0 items received</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.378469     705 get.go:247] Starting watch for /apis/apiregistration.k8s.io/v1/apiservices, rv=32859 labels= fields= timeout=8m12s</span><br><span class="line">5月 13 16:00:49 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:49.206602     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (4.541086ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]</span><br><span class="line">5月 13 16:00:50 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:50.027213     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.418662ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show in full.</span><br><span class="line"></span><br><span class="line">[root@master01 bin]# ps -ef |grep kube-apiserver</span><br><span class="line">root       705     1  3 5月10 ?       02:35:10 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --service-cluster-ip-range=10.254.0.0/16 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth --token-auth-file=/k8s/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/k8s/kubernetes/ssl/server.pem --tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem --client-ca-file=/k8s/kubernetes/ssl/ca.pem --service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem --etcd-cafile=/k8s/etcd/ssl/ca.pem --etcd-certfile=/k8s/etcd/ssl/server.pem --etcd-keyfile=/k8s/etcd/ssl/server-key.pem</span><br><span class="line">root      7098 24767  0 15:57 pts/0    00:00:00 grep --color=auto kube-apiserver</span><br><span class="line">[root@master01 bin]# netstat -tulpn |grep kube-apiserve</span><br><span class="line">tcp        0      0 10.8.13.81:6443         0.0.0.0:*               LISTEN      705/kube-apiserver  </span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      705/kube-apiserver</span><br></pre></td></tr></table></figure></p><p>3）部署kube-scheduler组件<br>创建kube-scheduler配置文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim  /k8s/kubernetes/cfg/kube-scheduler </span><br><span class="line">KUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot;</span><br></pre></td></tr></table></figure></p><p>参数备注：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；</span><br><span class="line">--kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</span><br><span class="line">--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</span><br><span class="line">创建kube-scheduler systemd文件</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-scheduler.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>拷贝master01 kube-scheduler配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.82:/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.83:/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">scp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.82:/usr/lib/systemd/system/kube-scheduler.service</span><br><span class="line">scp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.83:/usr/lib/systemd/system/kube-scheduler.service</span><br></pre></td></tr></table></figure></p><p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler.service </span><br><span class="line">systemctl start kube-scheduler.service</span><br><span class="line">[root@master01 bin]# systemctl status kube-scheduler.service</span><br><span class="line">● kube-scheduler.service - Kubernetes Scheduler</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 693 (kube-scheduler)</span><br><span class="line">   CGroup: /system.slice/kube-scheduler.service</span><br><span class="line">           └─693 /k8s/kubernetes/bin/kube-scheduler --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect</span><br><span class="line"></span><br><span class="line">5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024121     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024161     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151743     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151799     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434965     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434999     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571674     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571707     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914369     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914411     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br></pre></td></tr></table></figure></p><p>4）部署kube-controller-manager组件</p><p>创建kube-controller-manager配置文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">KUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--master=127.0.0.1:8080 \</span><br><span class="line">--leader-elect=true \</span><br><span class="line">--address=127.0.0.1 \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--cluster-name=kubernetes \</span><br><span class="line">--cluster-signing-cert-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--cluster-signing-key-file=/k8s/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">--root-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-private-key-file=/k8s/kubernetes/ssl/ca-key.pem&quot;</span><br></pre></td></tr></table></figure></p><p>创建kube-controller-manager systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-controller-manager.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>拷贝master01 kube-controller-manager配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.82:/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.83:/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">scp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.82:/usr/lib/systemd/system/kube-controller-manager.service</span><br><span class="line">scp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.83:/usr/lib/systemd/system/kube-controller-manager.service</span><br></pre></td></tr></table></figure></p><p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">[root@master01 bin]# systemctl status kube-controller-manager</span><br><span class="line">● kube-controller-manager.service - Kubernetes Controller Manager</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 685 (kube-controller)</span><br><span class="line">   CGroup: /system.slice/kube-controller-manager.service</span><br><span class="line">           └─685 /k8s/kubernetes/bin/kube-controller-manager --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect=true --address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/k8s/kubernetes/ssl/ca...</span><br><span class="line"></span><br><span class="line">5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539102     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539136     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767187     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767221     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939294     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939329     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212185     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212218     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291399     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291430     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br></pre></td></tr></table></figure></p><h4 id="4-9、验证kubeserver服务"><a href="#4-9、验证kubeserver服务" class="headerlink" title="4.9、验证kubeserver服务"></a>4.9、验证kubeserver服务</h4><p>设置环境变量(==所有服务器都执行此步==)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">PATH=/k8s/kubernetes/bin:$PATH</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>查看master服务状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ~]# kubectl get cs,nodes</span><br><span class="line">NAME                                 STATUS    MESSAGE             ERROR</span><br><span class="line">componentstatus/scheduler            Healthy   ok                  </span><br><span class="line">componentstatus/controller-manager   Healthy   ok                  </span><br><span class="line">componentstatus/etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure></p><p>至此master组件安装完毕</p><hr><h3 id="五、Node部署-node01、node02安装"><a href="#五、Node部署-node01、node02安装" class="headerlink" title="五、Node部署(node01、node02安装)"></a>五、Node部署(node01、node02安装)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubernetes work 节点运行如下组件：</span><br><span class="line">docker</span><br><span class="line">kubelet</span><br><span class="line">kube-proxy</span><br><span class="line">flannel</span><br></pre></td></tr></table></figure><h4 id="5-1-Docker环境安装"><a href="#5-1-Docker环境安装" class="headerlink" title="5.1 Docker环境安装"></a>5.1 Docker环境安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">yum list docker-ce --showduplicates | sort -r</span><br><span class="line">yum install docker-ce -y</span><br><span class="line">systemctl start docker &amp;&amp; systemctl enable docker</span><br></pre></td></tr></table></figure><h4 id="5-2-部署kubelet组件"><a href="#5-2-部署kubelet组件" class="headerlink" title="5.2 部署kubelet组件"></a>5.2 部署kubelet组件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如exec、run、logs 等;</span><br><span class="line">kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况;</span><br><span class="line">为确保安全，只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如apiserver、heapster)。</span><br></pre></td></tr></table></figure><p>1)、安装二进制文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.13.1/kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">tar zxvf kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes/node/bin/</span><br><span class="line">cp kube-proxy kubelet kubectl /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure></p><p>2)、从master01复制相关证书到node01和node02节点<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# cd /k8s/kubernetes/ssl/</span><br><span class="line">[root@master01 ssl]# scp *.pem 10.8.13.84:/k8s/kubernetes/ssl/</span><br><span class="line">root@10.8.13.84&apos;s password: </span><br><span class="line">ca-key.pem                                                                                         100% 1679   914.6KB/s   00:00    </span><br><span class="line">ca.pem                                                                                             100% 1359     1.0MB/s   00:00    </span><br><span class="line">kube-proxy-key.pem                                                                                 100% 1675     1.2MB/s   00:00    </span><br><span class="line">kube-proxy.pem                                                                                     100% 1403     1.1MB/s   00:00    </span><br><span class="line">server-key.pem                                                                                     100% 1679   809.1KB/s   00:00    </span><br><span class="line">server.pem                                                                                         100% 1675     1.2MB/s   00:00</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.84:/k8s/etcd/ssl/</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.84:/k8s/etcd/bin/</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# scp *.pem 10.8.13.85:/k8s/kubernetes/ssl/</span><br><span class="line">root@10.8.13.85&apos;s password: </span><br><span class="line">ca-key.pem                                                                                         100% 1679   914.6KB/s   00:00    </span><br><span class="line">ca.pem                                                                                             100% 1359     1.0MB/s   00:00    </span><br><span class="line">kube-proxy-key.pem                                                                                 100% 1675     1.2MB/s   00:00    </span><br><span class="line">kube-proxy.pem                                                                                     100% 1403     1.1MB/s   00:00    </span><br><span class="line">server-key.pem                                                                                     100% 1679   809.1KB/s   00:00    </span><br><span class="line">server.pem                                                                                         100% 1675     1.2MB/s   00:00</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.85:/k8s/etcd/ssl/</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.85:/k8s/etcd/bin/</span><br></pre></td></tr></table></figure><p>3)、创建kubelet bootstrap kubeconfig文件</p><p>通过脚本实现<br>KUBE_APISERVER=vip:haproxy中自定义的端口<br>BOOTSTRAP_TOKEN=部署kube-apiserver中生成的token<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/environment.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">#创建kubelet bootstrapping kubeconfig </span><br><span class="line">BOOTSTRAP_TOKEN=af93a4194e7bcf7f05dc0bab3a6e97cd</span><br><span class="line">KUBE_APISERVER=&quot;https://10.8.13.80:16443&quot;</span><br><span class="line">#设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line">#设置客户端认证参数</span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line">#----------------------</span><br><span class="line"> </span><br><span class="line"># 创建kube-proxy kubeconfig文件</span><br><span class="line"> </span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate=/k8s/kubernetes/ssl/kube-proxy.pem \</span><br><span class="line">  --client-key=/k8s/kubernetes/ssl/kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure></p><p>执行脚本<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 cfg]# cd /k8s/kubernetes/cfg/</span><br><span class="line">[root@node01 cfg]# sh environment.sh </span><br><span class="line">Cluster &quot;kubernetes&quot; set.</span><br><span class="line">User &quot;kubelet-bootstrap&quot; set.</span><br><span class="line">Context &quot;default&quot; created.</span><br><span class="line">Switched to context &quot;default&quot;.</span><br><span class="line">Cluster &quot;kubernetes&quot; set.</span><br><span class="line">User &quot;kube-proxy&quot; set.</span><br><span class="line">Context &quot;default&quot; created.</span><br><span class="line">Switched to context &quot;default&quot;.</span><br><span class="line">[root@node01 cfg]# ls</span><br><span class="line">bootstrap.kubeconfig  environment.sh  kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure></p><p>4)、创建kubelet参数配置模板文件</p><p>address:node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet.config</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 10.8.13.84</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">clusterDNS: [&quot;10.254.0.10&quot;]</span><br><span class="line">clusterDomain: cluster.local.</span><br><span class="line">failSwapOn: false</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure></p><p>5)、创建kubelet配置文件</p><p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet</span><br><span class="line"> </span><br><span class="line">KUBELET_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.84 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \</span><br><span class="line">--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \</span><br><span class="line">--config=/k8s/kubernetes/cfg/kubelet.config \</span><br><span class="line">--cert-dir=/k8s/kubernetes/ssl \</span><br><span class="line">--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;</span><br></pre></td></tr></table></figure></p><p>6)、创建kubelet systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kubelet.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=/k8s/kubernetes/cfg/kubelet</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kubelet $KUBELET_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line">KillMode=process</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>7)、将kubelet-bootstrap用户绑定到系统集群角色(==在master01执行==)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kubelet-bootstrap \</span><br><span class="line">  --clusterrole=system:node-bootstrapper \</span><br><span class="line">  --user=kubelet-bootstrap</span><br></pre></td></tr></table></figure></p><p>注意这个默认连接localhost:8080端口，可以在master上操作<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl create clusterrolebinding kubelet-bootstrap \</span><br><span class="line">&gt;   --clusterrole=system:node-bootstrapper \</span><br><span class="line">&gt;   --user=kubelet-bootstrap</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created</span><br></pre></td></tr></table></figure></p><p>8)、复制node01kubelet配置和启动服务文件到node02相对应路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.85:/k8s/kubernetes/cfg/</span><br><span class="line">scp /usr/lib/systemd/system/kubelet.service 10.8.13.85:/usr/lib/systemd/system/kubelet.service</span><br></pre></td></tr></table></figure></p><p>9)、修改node02中kubelet.config和kubelet文件中的nodeIP</p><p>node02中kubelet.config配置</p><p>address:node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet.config</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 10.8.13.85</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">clusterDNS: [&quot;10.254.0.10&quot;]</span><br><span class="line">clusterDomain: cluster.local.</span><br><span class="line">failSwapOn: false</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure></p><p>node02中kubelet配置</p><p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet</span><br><span class="line"> </span><br><span class="line">KUBELET_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.85 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \</span><br><span class="line">--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \</span><br><span class="line">--config=/k8s/kubernetes/cfg/kubelet.config \</span><br><span class="line">--cert-dir=/k8s/kubernetes/ssl \</span><br><span class="line">--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;</span><br></pre></td></tr></table></figure></p><p>10)、启动服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# systemctl status kubelet</span><br><span class="line">● kubelet.service - Kubernetes Kubelet</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-05-10 20:31:30 CST; 3 days ago</span><br><span class="line"> Main PID: 8583 (kubelet)</span><br><span class="line">   Memory: 45.5M</span><br><span class="line">   CGroup: /system.slice/kubelet.service</span><br><span class="line">           └─8583 /k8s/kubernetes/bin/kubelet --logtostderr=true --v=4 --hostname-override=10.8.13.84 --kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig --config=/k8s/kubernetes/cfg/kubelet.config --cer...</span><br></pre></td></tr></table></figure><p>11)、Master接受kubelet CSR请求(master01操作，接受两个node节点)<br>可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书，如下是手动 approve CSR请求操作方法</p><p>查看CSR列表</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl get csr</span><br><span class="line">NAME                                                   AGE    REQUESTOR           CONDITION</span><br><span class="line">node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc   102s   kubelet-bootstrap   Pending</span><br></pre></td></tr></table></figure><p>接受node</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl certificate approve node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc approved</span><br></pre></td></tr></table></figure><p>再查看CSR</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl get csr</span><br><span class="line">NAME                                                   AGE     REQUESTOR           CONDITION</span><br><span class="line">node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc   5m13s   kubelet-bootstrap   Approved,Issued</span><br></pre></td></tr></table></figure><h4 id="5-3部署kube-proxy组件-node01执行"><a href="#5-3部署kube-proxy组件-node01执行" class="headerlink" title="5.3部署kube-proxy组件(node01执行)"></a>5.3部署kube-proxy组件(node01执行)</h4><p>kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡</p><p>1)、创建 kube-proxy 配置文件</p><p>–hostname-override=node节点IP</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">KUBE_PROXY_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.84 \</span><br><span class="line">--cluster-cidr=10.254.0.0/16 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot;</span><br></pre></td></tr></table></figure><p>2)、创建kube-proxy systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-proxy.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Proxy</span><br><span class="line">After=network.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>3)、复制node01kube-proxy配置和服务启动文件到node02相对应路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-proxy 10.8.13.85:/k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">scp /usr/lib/systemd/system/kube-proxy.service 10.8.13.85:/usr/lib/systemd/system/kube-proxy.service</span><br></pre></td></tr></table></figure></p><p>4)、修改node02kube-proxy配置文件如下</p><p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">KUBE_PROXY_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.85 \</span><br><span class="line">--cluster-cidr=10.254.0.0/16 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot;</span><br></pre></td></tr></table></figure></p><p>5)、启动服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-proxy</span><br><span class="line">systemctl start kube-proxy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# systemctl status kube-proxy.service </span><br><span class="line">● kube-proxy.service - Kubernetes Proxy</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-05-10 20:31:31 CST; 3 days ago</span><br><span class="line"> Main PID: 8669 (kube-proxy)</span><br><span class="line">   Memory: 9.9M</span><br><span class="line">   CGroup: /system.slice/kube-proxy.service</span><br><span class="line">           ‣ 8669 /k8s/kubernetes/bin/kube-proxy --logtostderr=true --v=4 --hostname-override=10.8.13.84 --cluster-cidr=10.254.0.0/16 --kubeconfig...</span><br><span class="line"></span><br><span class="line">May 14 09:07:50 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:50.634641    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:51 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:51.365166    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:52 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:52.647317    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:53 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:53.375833    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:54 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:54.658691    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:55 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:55.387881    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:56 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:56.670562    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:57 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:57.398763    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:58 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:58.682049    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:59 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:59.411141    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br></pre></td></tr></table></figure><p>6)、查看集群状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES    AGE     VERSION</span><br><span class="line">10.8.13.84   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line">10.8.13.85   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br></pre></td></tr></table></figure></p><p>至此node组件安装完成</p><hr><h3 id="六、Flanneld网络部署-以node01为例，node02同样操作"><a href="#六、Flanneld网络部署-以node01为例，node02同样操作" class="headerlink" title="六、Flanneld网络部署(以node01为例，node02同样操作)"></a>六、Flanneld网络部署(以node01为例，node02同样操作)</h3><p>默认没有flanneld网络，Node节点间的pod不能通信，只能Node内通信，为了部署步骤简洁明了，故flanneld放在后面安装<br>flannel服务需要先于docker启动。flannel服务启动时主要做了以下几步的工作：</p><ul><li>从etcd中获取network的配置信息</li><li>划分subnet，并在etcd中进行注册</li><li>将子网信息记录到/run/flannel/subnet.env中</li></ul><h4 id="6-1-etcd注册网段"><a href="#6-1-etcd注册网段" class="headerlink" title="6.1 etcd注册网段"></a>6.1 etcd注册网段</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# /k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379&quot;  set /k8s/network/config  &apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;</span><br><span class="line">&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><p>flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；<br>写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；</p><h4 id="6-2-flannel安装"><a href="#6-2-flannel安装" class="headerlink" title="6.2 flannel安装"></a>6.2 flannel安装</h4><p>1)、解压安装<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf flannel-v0.11.0-linux-amd64.tar.gz</span><br><span class="line">mv flanneld mk-docker-opts.sh /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure></p><p>2)、配置flanneld<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/flanneld</span><br><span class="line">FLANNEL_OPTIONS=&quot;--etcd-endpoints=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379 -etcd-cafile=/k8s/etcd/ssl/ca.pem -etcd-certfile=/k8s/etcd/ssl/server.pem -etcd-keyfile=/k8s/etcd/ssl/server-key.pem -etcd-prefix=/k8s/network&quot;</span><br></pre></td></tr></table></figure></p><p>3)、创建flanneld systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/flanneld.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network-online.target network.target</span><br><span class="line">Before=docker.service</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/k8s/kubernetes/cfg/flanneld</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS</span><br><span class="line">ExecStartPost=/k8s/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><p>==注意：==</p><ul><li>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥；</li><li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口;</li><li>flanneld 运行时需要 root 权限；</li></ul><p>3）配置Docker启动指定子网</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">添加EnvironmentFile=/run/flannel/subnet.env，修改ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/docker.service </span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">After=network-online.target firewalld.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/run/flannel/subnet.env</span><br><span class="line">ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">StartLimitBurst=3</span><br><span class="line">StartLimitInterval=60s</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>4)、启动服务</p><p>注意启动flannel前要关闭docker及相关的kubelet这样flannel才会覆盖docker0网桥<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl stop docker</span><br><span class="line">systemctl start flanneld</span><br><span class="line">systemctl enable flanneld</span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart kube-proxy</span><br></pre></td></tr></table></figure></p><p>5)、验证服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 bin]# cat /run/flannel/subnet.env </span><br><span class="line">DOCKER_OPT_BIP=&quot;--bip=10.254.88.1/24&quot;</span><br><span class="line">DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;</span><br><span class="line">DOCKER_OPT_MTU=&quot;--mtu=1450&quot;</span><br><span class="line">DOCKER_NETWORK_OPTIONS=&quot; --bip=10.254.88.1/24 --ip-masq=false --mtu=1450&quot;</span><br></pre></td></tr></table></figure></p><p>注意查看docker0和flannel是不是属于同一网段<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 00:50:56:90:67:d1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.8.13.84/24 brd 10.8.13.255 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::802:2c0f:a197:38a7/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </span><br><span class="line">    link/ether 02:42:5c:18:5b:93 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.254.88.1/24 brd 10.254.88.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:5cff:fe18:5b93/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </span><br><span class="line">    link/ether 8e:f6:f8:87:47:ee brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.254.88.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8cf6:f8ff:fe87:47ee/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p><p>至此flannel安装完成</p><hr><h3 id="查看NODE和etcd"><a href="#查看NODE和etcd" class="headerlink" title="查看NODE和etcd"></a>查看NODE和etcd</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hwzx-test-cmpmaster01 ~]# kubectl get nodes,cs</span><br><span class="line">NAME              STATUS   ROLES    AGE     VERSION</span><br><span class="line">node/10.8.13.84   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line">node/10.8.13.85   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line"></span><br><span class="line">NAME                                 STATUS    MESSAGE             ERROR</span><br><span class="line">componentstatus/controller-manager   Healthy   ok                  </span><br><span class="line">componentstatus/scheduler            Healthy   ok                  </span><br><span class="line">componentstatus/etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>容器网络 calico 基本原理和模拟</title>
    <link href="/2019/04/22/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%20calico%20%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E6%A8%A1%E6%8B%9F/"/>
    <url>/2019/04/22/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%20calico%20%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E6%A8%A1%E6%8B%9F/</url>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>在容器网络跨 Host 互联方案中，除了 Flannel 的隧道实现方案，还有一种比较主流的纯三层路由的方案 Calico ，与 Flannel 不同的是 Calico 不使用隧道或 NAT 来实现转发，而是巧妙的把所有二三层流量转换成三层流量，并通过 host 上路由配置完成跨 Host 转发，本文对 Calico 的方案进行基本原理分析和模拟验证。</p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><h4 id="Calico-官方定义如下："><a href="#Calico-官方定义如下：" class="headerlink" title="Calico 官方定义如下："></a>Calico 官方定义如下：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Calico provides secure network connectivity for containers and virtual machine workloads.</span><br><span class="line"></span><br><span class="line">Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.</span><br><span class="line"></span><br><span class="line">Calico also provides dynamic enforcement of network security rules. Using Calico’s simple policy language, you can achieve fine-grained control over communications between containers, virtual machine workloads, and bare metal host endpoints.</span><br></pre></td></tr></table></figure><p>总结如下：</p><p>Calico 为容器和 vm 等提供一个安全的网路互联方法，我们把 VM、Container、白盒等实例统称为 workloads，通过给 workload 分配一个扁平的三层路由可达 IP 地址实现转发，是一种纯三层转发的方案，workload 之间不使用隧道或 NAT 技术，这种方式提供更好的网络性能，提高易维护和可交互性。同时也支持 IPIP 隧道和与 Flannel 集成能力。Calico 提供动态实施的网络安全策略，可使用简单的安全模型语言实现细粒度的安全控制。</p><h4 id="相对-Overlay，为什么用-Calico？"><a href="#相对-Overlay，为什么用-Calico？" class="headerlink" title="相对 Overlay，为什么用 Calico？"></a>相对 Overlay，为什么用 Calico？</h4><p>Calico 是一种 workloads 之间互通的网络方案，并支持以上任意一种场景。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对 workloads 做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。</p><p>而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现 workloads 的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。</p><p>那么有更好的方案吗？我们在审视了二层方案并思考如何支持大型网络时从 Internet 网络实现中获得了灵感。我们知道在 Internet 的网络中，路由器作为网关连接着自己的子网络，之间通过 BGP 相互学习，并使用防火墙控制不同子网之间安全策略，所有这些子网络共同组成了 Internet 网络，那么，这种方式能否也应用到虚拟化基础平台中呢？</p><p>借鉴这种思路，我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案，整个方案的优势为：</p><ul><li>更优的资源利用：</li></ul><p>二层网络通讯需要依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，Calico 使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。</p><p>另外，二层网络使用 VLAN 隔离技术，天生有 4096 个规格限制，即便可以使用 vxlan 解决，但 vxlan 又带来了隧道开销的新问题。而 Calico 不使用 vlan 或 vxlan 技术，使资源利用率更高。</p><ul><li>可扩展性：</li></ul><p>Calico 使用与 Internet 类似的方案，Internet 的网络比任何数据中心都大，Calico 同样天然具有可扩展性。</p><ul><li>简单而更容易 debug：</li></ul><p>因为没有隧道，意味着 workloads 之间路径更短更简单，配置更少，在 host 上更容易进行 debug 调试。</p><ul><li>更少的依赖：</li></ul><p>Calico 仅依赖三层路由可达。</p><ul><li>可适配性：</li></ul><p>Calico 较少的依赖性使它能适配所有 VM、Container、白盒或者混合环境场景。</p><p>除了以上，还有更多其他优势，因此，如果你在为 OpenStack 或 docker 构建虚拟化网络环境的话，可以好好考虑下 Calico 的方案。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190425/qKz7i8J6CIl7.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>Calico 由 5 部分组件组成，整体构架如下：</p><ul><li>Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等</li><li>Orchestrator Plugin：编排插件，并不是独立运行的某些进程，而是设计与 k8s、OpenStack 等平台集成的插件，如 Neutron’s ML2 plugin 用于用户使用 Neutron API 来管理 Calico，本质是要解决模型和 API 间的兼容性问题。</li><li>Etcd：Calico 模型的存储引擎。</li><li>BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。</li><li>BGP Route Reflector(BIRD)：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。</li></ul><h3 id="模拟组网"><a href="#模拟组网" class="headerlink" title="模拟组网"></a>模拟组网</h3><p>组网如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190425/RRwiuSjn4cYW.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><ul><li>guest 配置 169.254.1.1 的默认路由；</li><li>host 上配置 10.20.2.0/24 和 10.20.1.3/32 路由；</li><li>开启 arp proxy 和 ip_forward 能力；</li></ul><p>网络连通性测试：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># HOST0</span><br><span class="line">[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.2.2</span><br><span class="line">PING 10.20.2.2 (10.20.2.2) 56(84) bytes of data.</span><br><span class="line"> bytes from 10.20.2.2: icmp_seq=1 ttl=62 time=0.774 ms</span><br><span class="line"> bytes from 10.20.2.2: icmp_seq=2 ttl=62 time=0.332 ms</span><br></pre></td></tr></table></figure><p>10.20.1.2 与跨 Host 跨子网 10.20.2.2 互通成功<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># HOST0</span><br><span class="line">[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.1.3</span><br><span class="line">PING 10.20.1.3 (10.20.1.3) 56(84) bytes of data.</span><br><span class="line"> bytes from 10.20.1.3: icmp_seq=1 ttl=62 time=957 ms</span><br><span class="line"> bytes from 10.20.1.3: icmp_seq=2 ttl=62 time=0.432 ms</span><br><span class="line"> bytes from 10.20.1.3: icmp_seq=3 ttl=62 time=0.563 ms</span><br></pre></td></tr></table></figure></p><p>10.20.1.2 与跨 Host 同子网 10.20.1.3 互通成功</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@i-7dlclo08 ~]# ip netns exec ns0 ping 192.168.100.3</span><br><span class="line">PING 192.168.100.3 (192.168.100.3) 56(84) bytes of data.</span><br><span class="line"> bytes from 192.168.100.3: icmp_seq=1 ttl=63 time=1.00 ms</span><br><span class="line"> bytes from 192.168.100.3: icmp_seq=2 ttl=63 time=0.695 ms</span><br></pre></td></tr></table></figure><p>在未做安全策略下，10.20.1.2 与 Host 192.168.100.3 互通成功</p><p>转发过程：</p><ul><li>guest0 本地所有数据包都转发到一个虚假的地址 169.254.1.1，发送 ARP Req。</li><li>Host0 的 veth 端收到 ARP Req 时通过开启网卡的 proxy arp 代理功能直接把自己的 MAC 地址返回给 guest0</li><li>guest0 发送目的地址为 guest1 的 IP 数据包</li><li>因为使用了 169.254.1.1 这样的地址，Host 判断为三层路由转发，查询本地路由 10.20.2.0/24 via 192.168.0.3 dev eth0 发送给对端 host1，如果配置 BGP，这里会看到 proto 协议为 BIRD</li><li>在发送之前匹配本地的 iptables 规则进行安全策略控制，这里略</li><li>当 host1 收到 10.20.2.2 的数据包时查找本地路由表匹配 10.20.2.2/32 dev veth0 scope link 转发到对应的 veth0 端从而到达 guest1</li><li>回程类似，略</li></ul><p>整体转发流程简单清晰。因此可以看到，Calico 需要给所有 guest 配置一条特别的路由并利用 veth 的 proxy arp 的能力让 guest 出来的所有转发都变成三层路由转发，再利用 host 的路由表进行转发，这种方式不仅仅实现了同 host 的二三层转发，也能实现跨 host 的转发。</p><h3 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h3><ul><li>1、租户隔离问题</li></ul><p>Calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。</p><ul><li>2、路由规模问题</li></ul><p>通过路由规则可以看出，路由规模和 guest 分布有关，如果 guest 离散分布在 host 集群中，势必会产生较多的路由项。</p><ul><li>3、iptables 规则规模问题</li></ul><p>1台 Host 上可能虚拟化十几或几十个容器实例，过多的 iptables 规则造成复杂性和不可调试性，同时也存在性能损耗。</p><ul><li>4、跨子网时的网关路由问题</li></ul><p>当对端网络不为二层可达时，需要通过三层路由机时，需要网关支持自定义路由配置，即 guest 的目的地址为本网段的网关地址，再由网关进行跨三层转发。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul><li>1、Calico 通过巧妙的引导 workload 所有的流量至一个特殊的网关 169.254.1.1，从而引流到 host 的 calixxx 网络设备上，形成了二三层流量全部转换 host 的三层流量转发。</li><li>2、在 Host 上通过开启 arp proxy 的能力实现 arp 代答，arp 广播被抑制在 host 里，arp 记录变成“无效记录”，抑制了广播风暴和不会有 arp 表膨胀的问题。</li><li>3、使用 iptables 在 host 做 policy 实现的复杂的安全模型，安全策略应用在每一台虚拟路由器上，最终形成了一个分布式的安全系统。</li></ul>]]></content>
    
    
    <categories>
      
      <category>K8S</category>
      
    </categories>
    
    
    <tags>
      
      <tag>calico</tag>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：URL与视图函数（九）</title>
    <link href="/2019/04/13/%E4%B9%9D%E3%80%81URL%E4%B8%8E%E8%A7%86%E5%9B%BE%E5%87%BD%E6%95%B0/"/>
    <url>/2019/04/13/%E4%B9%9D%E3%80%81URL%E4%B8%8E%E8%A7%86%E5%9B%BE%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>在讲URL与视图函数之前我们先给大家简单介绍一下用户访问网站的流程。我们访问一个网站的时候，一般先打开浏览器，然后在浏览器的地址栏里输入一个网址，也就是URL，然后回车，我们就可以在浏览器里看到这个网址返回的内容。这是我们能看得见的过程，还有一些我们看不见的过程，那就是：++当我们在浏览器里输入网址（URL）时，回车，然后浏览器就会向目标网址发送一个HTTP请求，服务器收到请求之后就会给这个请求做出一个响应，这个响应就是把对应的内容通过浏览器渲染出来，呈现给我们看++。这个过程就是请求与响应。</p><p>下图，就是请求响应的过程。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/ceRYC42787YO.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>上面我们提到了URL，这个URL在我们的Django中，其实是由我们自己构造的。(这个说法不太严谨，但为了方便大家理解之后的内容，先当这说辞是正确的。)</p><p>Django中，我们约定URL是在项目同名目录下的urls.py文件里urlpatterns列表构造的。</p><p>myblog/myblog/urls.py</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/06SOp1CAFwPa.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>表现形式如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">urlpatterns = [</span><br><span class="line">    path(正则表达式, views视图函数，参数，别名),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">括号里的参数说明：</span><br><span class="line">1、一个正则表达式字符串</span><br><span class="line">2、一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串</span><br><span class="line">3、可选的要传递给视图函数的默认参数（字典形式）</span><br><span class="line">4、一个可选的name参数(别名)</span><br></pre></td></tr></table></figure><p>完整的URL应该要这么写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path(正则表达式, views视图函数，参数，别名)</span><br><span class="line">：</span><br><span class="line">里面的正则表达式, views视图函数，是必须要写的，而参数，别名是可选的。我们在有特殊需要的时候才写。</span><br></pre></td></tr></table></figure></p><p>通过上面我们可以看到，每个URL都对应一个views视图函数名，视图函数名不能相同，否则会报错。视图函数，Django中约定写在APP应用里的views.py文件里。然后在urls.py文件里通过下面的方式导入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from APP应用名 import views</span><br><span class="line">from APP应用名.vews import 函数名或类名</span><br></pre></td></tr></table></figure></p><p>视图函数是一个简单的Python 函数，它接受Web请求并且返回Web响应。响应可以是一张网页的HTML内容，一个重定向，一个404错误，一个XML文档，或者一张图片. . . 是任何东西都可以。无论视图本身包含什么逻辑，都要返回响应。这个视图函数代码一般约定是放置在项目或应用程序目录中的名为views.py的文件中。</p><p>http请求中产生两个核心对象：</p><ul><li>1、http请求—-&gt;HttpRequest对象，用户请求相关的所有信息（对象）</li><li>2、http响应—-&gt;HttpResponse对象，响应字符串</li></ul><p>首先，打开打开bolg目录下的views.py文件，写一个hello视图函数，在里面输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def hello(request):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  写一个hello函数，通过request接收URL或者说是http请求信息，</span><br><span class="line">  然后给这个请求返回一个HttpResponse对象</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">    return HttpResponse(&apos;欢迎使用Django！&apos;)</span><br></pre></td></tr></table></figure></p><p>例子里，我们用到的request，就是HttpRequest对象。HttpResponse(“欢迎使用Django！”)，就是HttpRequest对象，它向http请求响应了一段字符串对象。</p><p>我们打开myblog目录下的urls.py文件中先导入视图函数，然后构造一个URL，代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from blog import views  #导入视图函数</span><br><span class="line">urlpatterns = [</span><br><span class="line">    ...</span><br><span class="line">    path(&apos;&apos;, views.hello),   #这个是我们构造的URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>代码写完之后，启动项目就可以在浏览器里看到视图函数返回的字符串”欢迎使用Django！”</p><p>每一个URL都会对应一个视图函数，当一个用户请求访问Django站点的一个页面时，然后就由Django路由系统（URL配置文件）去决定要执行哪个视图函数使用的算法。</p><p>通过URL对应关系匹配 -&gt;找到对应的函数（或者类）-&gt;返回字符串(或者读取Html之后返回渲染的字符串）这个过程也就是我们Django请求的生命周期。</p><p>视图函数，就是围绕着HttpRequest和HttpResponse这两个对象进行的。</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：使用富文本编辑器（八）</title>
    <link href="/2019/04/12/%E5%85%AB%E3%80%81%E4%BD%BF%E7%94%A8%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE/"/>
    <url>/2019/04/12/%E5%85%AB%E3%80%81%E4%BD%BF%E7%94%A8%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>在Django admin后台添加数据的时候，文章内容文本框想发布一篇图文并茂的文章需就得手写Html代码，这十分吃力，也没法上传图片和文件。这显然不是我等高大上程序猿想要的。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/5aJX93PN8acE.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>为提升效率，我们可以使用富文本编辑器添加数据。支持Django的富文本编辑器很多，这里我推荐使用DjangoUeditor，Ueditor是百度开发的一个富文本编辑器，功能强大。下面教大家安装如何使用DjangoUeditor。</p><h4 id="1、首先我们先下载DjangoUeditor包"><a href="#1、首先我们先下载DjangoUeditor包" class="headerlink" title="1、首先我们先下载DjangoUeditor包"></a>1、首先我们先下载DjangoUeditor包</h4><p>点击下面的链接进行下载！下载完成然后解压到项目根目录里。</p><p><a href="https://www.django.cn/media/upfile/DjangoUeditor_20181010013851_248.zip" target="_blank" rel="noopener">https://www.django.cn/media/upfile/DjangoUeditor_20181010013851_248.zip</a></p><h4 id="2、settings-py里注册APP"><a href="#2、settings-py里注册APP" class="headerlink" title="2、settings.py里注册APP"></a>2、settings.py里注册APP</h4><p>在INSTALLED_APPS里添加’DjangoUeditor’</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.y</span><br><span class="line">INSTALLED_APPS = [</span><br><span class="line">    &apos;django.contrib.admin&apos;,</span><br><span class="line">    ....</span><br><span class="line">    &apos;DjangoUeditor&apos;, #注册APP应用</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">##验证是否已经增加正确，按住 ctrl 点击DjangoUdeitor，只要能跳转到目录即可；</span><br></pre></td></tr></table></figure><h4 id="3、myblog-urls-py里添加url。"><a href="#3、myblog-urls-py里添加url。" class="headerlink" title="3、myblog/urls.py里添加url。"></a>3、myblog/urls.py里添加url。</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/urls.py</span><br><span class="line">...</span><br><span class="line">from django.urls import path, include</span><br><span class="line">#留意上面这行比原来多了一个include</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&apos;admin/&apos;, admin.site.urls),</span><br><span class="line">    path(&apos;&apos;, views.hello),</span><br><span class="line">    path(&apos;ueditor/&apos;, include(&apos;DjangoUeditor.urls&apos;)), #添加DjangoUeditor的URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>4、修改blog/models.py里需要使用富文本编辑器渲染的字段。这里面我们要修改的是Article表里的body字段。</p><p>把原来的：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blog/models.py</span><br><span class="line"></span><br><span class="line">body = models.TextField()</span><br></pre></td></tr></table></figure><p>修改成：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blog/models.py</span><br><span class="line">from DjangoUeditor.models import UEditorField #头部增加这行代码导入UEditorField</span><br><span class="line"></span><br><span class="line">body = UEditorField(&apos;内容&apos;, width=800, height=500, </span><br><span class="line">                    toolbars=&quot;full&quot;, imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot;,</span><br><span class="line">                    upload_settings=&#123;&quot;imageMaxSize&quot;: 1204000&#125;,</span><br><span class="line">                    settings=&#123;&#125;, command=None, blank=True</span><br><span class="line">                    )</span><br></pre></td></tr></table></figure></p><p>留意里面的<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot;</span><br></pre></td></tr></table></figure></p><p> 这两个是图片和文件上传的路径，我们上传文件，++会自动上传到项目根目录media文件夹下对应的upimg和upfile目录里++，这个目录名可以自行定义。有的人问，为什么会上传到media目录里去呢？那是因为之前我们在基础配置文章里，设置了上传文件目录media。</p><p>上面步骤完成后，我们启动项目，进入文章发布页面。提示出错：</p><h4 id="错误一："><a href="#错误一：" class="headerlink" title="错误一："></a>错误一：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File &quot;C:\Python37\lib\site-packages\django\views\debug.py&quot;, line 332, in get_traceback_html</span><br><span class="line">    t = DEBUG_ENGINE.from_string(fh.read())</span><br><span class="line">UnicodeDecodeError: &apos;gbk&apos; codec can&apos;t decode byte 0xa6 in position 9737: illegal multibyte sequence</span><br></pre></td></tr></table></figure><p>查看错误栈最后一行发现是 编码问题<br>找到django 源码 “C:\Python37\lib\site-packages\django\views\debug.py” 332行位置 ,增加utf-8编码  open( encoding=’utf-8’)，问题解决：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_traceback_html(self):</span><br><span class="line">  &quot;&quot;&quot;Return HTML version of debug 500 HTTP error page.&quot;&quot;&quot;</span><br><span class="line">  with Path(CURRENT_DIR, &apos;templates&apos;, &apos;technical_500.html&apos;).open( encoding=&apos;utf-8&apos;) as fh:</span><br><span class="line">      t = DEBUG_ENGINE.from_string(fh.read())</span><br><span class="line">  c = Context(self.get_traceback_data(), use_l10n=False)</span><br><span class="line">  return t.render(c)</span><br></pre></td></tr></table></figure></p><h4 id="错误二："><a href="#错误二：" class="headerlink" title="错误二："></a>错误二：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">render() got an unexpected keyword argument &apos;renderer&apos;</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/20190420/Ki7f1qbrNjSm.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>我这里使用的是最新版本的Django2.1.1所以报错，解决办法很简单。打开这个文件的93行，注释这行即可。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/HjUJX4IaD9Tq.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>修改成之后，重新刷新页面，就可以看到我们的富文本编辑器正常显示。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/jdjuTJ862k5u.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>留意，如果我们在富文本编辑器里，上传图片，在编辑器内容里不显示上传的图片。那我们还需要进行如下设置，打开myblog/urls.py文件，在里面输入如下代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/urls.py</span><br><span class="line">....</span><br><span class="line">from django.urls import path, include, re_path</span><br><span class="line">#上面这行多加了一个re_path</span><br><span class="line">from django.views.static import serve</span><br><span class="line">#导入静态文件模块</span><br><span class="line">from django.conf import settings</span><br><span class="line">#导入配置文件里的文件上传配置</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&apos;admin/&apos;, admin.site.urls),</span><br><span class="line">    ....</span><br><span class="line">    re_path(&apos;^media/(?P&lt;path&gt;.*)$&apos;, serve, &#123;&apos;document_root&apos;: settings.MEDIA_ROOT&#125;),#增加此行</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>设置好了之后，图片就会正常显示。这样我们就可以用DjangoUeditor富文本编辑器发布图文并茂的文章了。</p><p>随便测试了一篇文章：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/Jw8Tpj1Ht24P.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：用Admin管理后台数据（七）</title>
    <link href="/2019/04/11/%E4%B8%83%E3%80%81%E7%94%A8Admin%E7%AE%A1%E7%90%86%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE/"/>
    <url>/2019/04/11/%E4%B8%83%E3%80%81%E7%94%A8Admin%E7%AE%A1%E7%90%86%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>django的admin后台管理它可以让我们快速便捷管理数据，我们可以在各个app目录下的admin.py文件中对其进行控制。想要对APP应用进行管理，最基本的前提是要先在settings里对其进行注册，就是在INSTALLED_APPS里把APP名添加进去。</p><p>注册APP应用之后，我们想要在admin后台里对数据库表进行操作，我们还得在应用APP下的admin.py文件里对数据库表先进行注册。我们的APP应用是blog，所以我们需要在blog/admin.py文件里进行注册：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blog/admin.py</span><br><span class="line"></span><br><span class="line">from django.contrib import admin</span><br><span class="line">from .models import Banner, Category, Tag, Tui, Article, Link </span><br><span class="line">#导入需要管理的数据库表</span><br><span class="line"></span><br><span class="line">@admin.register(Article)</span><br><span class="line">class ArticleAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;category&apos;, &apos;title&apos;, &apos;tui&apos;, &apos;user&apos;, &apos;views&apos;, &apos;created_time&apos;)</span><br><span class="line">    # 文章列表里显示想要显示的字段</span><br><span class="line">    list_per_page = 50</span><br><span class="line">    # 满50条数据就自动分页</span><br><span class="line">    ordering = (&apos;-created_time&apos;,)</span><br><span class="line">    #后台数据列表排序方式</span><br><span class="line">    list_display_links = (&apos;id&apos;, &apos;title&apos;)</span><br><span class="line">    # 设置哪些字段可以点击进入编辑界面</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@admin.register(Banner)</span><br><span class="line">class BannerAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;text_info&apos;, &apos;img&apos;, &apos;link_url&apos;, &apos;is_active&apos;)</span><br><span class="line"></span><br><span class="line">@admin.register(Category)</span><br><span class="line">class CategoryAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;name&apos;, &apos;index&apos;)</span><br><span class="line"></span><br><span class="line">@admin.register(Tag)</span><br><span class="line">class TagAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;name&apos;)</span><br><span class="line"></span><br><span class="line">@admin.register(Tui)</span><br><span class="line">class TuiAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;name&apos;)</span><br><span class="line"></span><br><span class="line">@admin.register(Link)</span><br><span class="line">class LinkAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = (&apos;id&apos;, &apos;name&apos;,&apos;linkurl&apos;)</span><br></pre></td></tr></table></figure><p>登录管理后台 <a href="http://127.0.0.1:8000/admin/" target="_blank" rel="noopener">http://127.0.0.1:8000/admin/</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/GWW02Dwk8eGI.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>多出了之前我们在models里创建的表。我们可以在后台里面对这些表进行增、删、改方面的操作。</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：创建数据库模型（六）</title>
    <link href="/2019/04/10/%E5%85%AD%E3%80%81%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B/"/>
    <url>/2019/04/10/%E5%85%AD%E3%80%81%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>Django是通过Model操作数据库，不管你数据库的类型是MySql或者Sqlite，Django它自动帮你生成相应数据库类型的SQL语句，所以不需要关注SQL语句和类型，对数据的操作Django帮我们自动完成。只要回写Model就可以了！</p><p>django根据代码中定义的类来自动生成数据库表。我们写的类表示数据库的表，如果根据这个类创建的对象是数据库表里的一行数据，对象.id 对象.value是每一行里的数据。</p><p>基本的原则如下：</p><ul><li>每个模型在Django中的存在形式为一个Python类</li><li>每个模型都是django.db.models.Model的子类</li><li>模型里的每个类代表数据库中的一个表</li><li>模型的每个字段（属性）代表数据表的某一列</li><li>Django将自动为你生成数据库访问API</li></ul><p>完成博客，我们需要存储六种数据：文章分类、文章、文章标签、幻灯图、推荐位、友情链接。每种数据一个表。</p><p><strong>分类表结构设计：</strong></p><p>表名：Category、分类名：name</p><p><strong>标签表设计：</strong></p><p>表名：Tag、标签名：name</p><p><strong>文章表结构设计：</strong></p><p>表名：Article、标题：title、摘要：excerpt、分类：category、标签：tags、推荐位、内容：body、创建时间：created_time、作者：user、文章封面图片img</p><p><strong>幻灯图表结构设计：</strong></p><p>表名：Banner、图片文本text_info、图片img、图片链接link_url、图片状态is_active。</p><p><strong>推荐位表结构设计：</strong></p><p>表名：Tui、推荐位名name。</p><p><strong>友情链接表结构设计：</strong></p><p>表名：Link、链接名name、链接网址linkurl。</p><p>其中：</p><p>++文章和分类是一对多的关系，文章和标签是多对多的关系，文章和作者是一对多的关系，文章和推荐位是一对多关系(看自己的需求，也可以设计成多对多)。++</p><p>打开blog/models.py,输入代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.db import models</span><br><span class="line">from django.contrib.auth.models import User </span><br><span class="line">#导入Django自带用户模块</span><br><span class="line"></span><br><span class="line"># 文章分类</span><br><span class="line">class Category(models.Model):</span><br><span class="line">    name = models.CharField(&apos;博客分类&apos;, max_length=100)</span><br><span class="line">    index = models.IntegerField(default=999, verbose_name=&apos;分类排序&apos;)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;博客分类&apos;</span><br><span class="line">        verbose_name_plural = verbose_name</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line">#文章标签</span><br><span class="line">class Tag(models.Model):</span><br><span class="line">    name = models.CharField(&apos;文章标签&apos;,max_length=100)</span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;文章标签&apos;</span><br><span class="line">        verbose_name_plural = verbose_name</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line">#推荐位</span><br><span class="line">class Tui(models.Model):</span><br><span class="line">    name = models.CharField(&apos;推荐位&apos;,max_length=100)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;推荐位&apos;</span><br><span class="line">        verbose_name_plural = verbose_name</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">#文章</span><br><span class="line">class Article(models.Model):</span><br><span class="line">    title = models.CharField(&apos;标题&apos;, max_length=70)</span><br><span class="line">    excerpt = models.TextField(&apos;摘要&apos;, max_length=200, blank=True)</span><br><span class="line">    category = models.ForeignKey(Category, on_delete=models.DO_NOTHING, verbose_name=&apos;分类&apos;, blank=True, null=True)</span><br><span class="line">     #使用外键关联分类表与分类是一对多关系</span><br><span class="line">    tags = models.ManyToManyField(Tag,verbose_name=&apos;标签&apos;, blank=True)</span><br><span class="line">    #使用外键关联标签表与标签是多对多关系</span><br><span class="line">    img = models.ImageField(upload_to=&apos;article_img/%Y/%m/%d/&apos;, verbose_name=&apos;文章图片&apos;, blank=True, null=True)</span><br><span class="line">     body = models.TextField()</span><br><span class="line">    user = models.ForeignKey(User, on_delete=models.CASCADE, verbose_name=&apos;作者&apos;)</span><br><span class="line">     &quot;&quot;&quot;</span><br><span class="line">     文章作者，这里User是从django.contrib.auth.models导入的。</span><br><span class="line">     这里我们通过 ForeignKey 把文章和 User 关联了起来。</span><br><span class="line">     &quot;&quot;&quot;</span><br><span class="line">    views = models.PositiveIntegerField(&apos;阅读量&apos;, default=0)</span><br><span class="line">    tui = models.ForeignKey(Tui, on_delete=models.DO_NOTHING, verbose_name=&apos;推荐位&apos;, blank=True, null=True)</span><br><span class="line">    </span><br><span class="line">    created_time = models.DateTimeField(&apos;发布时间&apos;, auto_now_add=True)</span><br><span class="line">    modified_time = models.DateTimeField(&apos;修改时间&apos;, auto_now=True)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;文章&apos;</span><br><span class="line">        verbose_name_plural = &apos;文章&apos;</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.title</span><br><span class="line"></span><br><span class="line">#Banner</span><br><span class="line">class Banner(models.Model):</span><br><span class="line">    text_info = models.CharField(&apos;标题&apos;, max_length=50, default=&apos;&apos;)</span><br><span class="line">    img = models.ImageField(&apos;轮播图&apos;, upload_to=&apos;banner/&apos;)</span><br><span class="line">    link_url = models.URLField(&apos;图片链接&apos;, max_length=100)</span><br><span class="line">    is_active = models.BooleanField(&apos;是否是active&apos;, default=False)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.text_info</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;轮播图&apos;</span><br><span class="line">        verbose_name_plural = &apos;轮播图&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#友情链接</span><br><span class="line">class Link(models.Model):</span><br><span class="line">    name = models.CharField(&apos;链接名称&apos;, max_length=20)</span><br><span class="line">    linkurl = models.URLField(&apos;网址&apos;,max_length=100)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;友情链接&apos;</span><br><span class="line">        verbose_name_plural = &apos;友情链接&apos;</span><br></pre></td></tr></table></figure></p><p>这里面我们多增加了一个img图片封面字段，用于上传文章封面图片的，article_img/为上传目录，%Y/%m/%d/为自动在上传的图片上加上文件上传的时间。</p><p>我们已经编写了博客数据库模型的代码，但那还只是 Python 代码而已，Django 还没有把它翻译成数据库语言，因此实际上这些数据库表还没有真正的在数据库中创建。我们需要进行数据库迁移。</p><p>在迁移之前，我们先需要设置数据库，如果我们使用默认的sqlite数据库的话，就不需要设置，Django默认使用；</p><p>sqlite3数据库，如果我们想使用Mysql数据库的话，则需要我们单独配置。我们打开settings.py文件，找到DATABASES，然后把它修改成如下代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">############修改成mysql如下：</span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    &apos;default&apos;: &#123;</span><br><span class="line">        &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;,</span><br><span class="line">        &apos;NAME&apos;: &apos;test&apos;,    #你的数据库名称</span><br><span class="line">        &apos;USER&apos;: &apos;root&apos;,   #你的数据库用户名</span><br><span class="line">        &apos;PASSWORD&apos;: &apos;445813&apos;, #你的数据库密码</span><br><span class="line">        &apos;HOST&apos;: &apos;&apos;, #你的数据库主机，留空默认为localhost</span><br><span class="line">        &apos;PORT&apos;: &apos;3306&apos;, #你的数据库端口</span><br><span class="line">    &#125;&#125;</span><br><span class="line"></span><br><span class="line">#由于mysql默认引擎为MySQLdb，在__init__.py文件中添加下面代码</span><br><span class="line">#在python3中须替换为pymysql,可在主配置文件（和项目同名的文件下，不是app配置文件）中增加如下代码</span><br><span class="line">#import pymysql</span><br><span class="line">#pymysql.install_as_MySQLdb()</span><br><span class="line">#如果找不到pymysql板块，则通过pip install pymysql进行安装。</span><br></pre></td></tr></table></figure><p>数据库设置好之后，我们就依次输入下面的命令进行数据库迁移：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure><p>迁移的时候，会有如下提示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/YLckQShHEkFH.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>出现这个原因是因为我们的幻灯图使用到图片字段，我们需要引入图片处理包。提示里也给了我们处理方案，输入如下命令，安装Pillow模块即可：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure></p><p>然后再次迁移即可；</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：欢迎页面（五）</title>
    <link href="/2019/04/09/%E4%BA%94%E3%80%81%E6%AC%A2%E8%BF%8E%E9%A1%B5%E9%9D%A2/"/>
    <url>/2019/04/09/%E4%BA%94%E3%80%81%E6%AC%A2%E8%BF%8E%E9%A1%B5%E9%9D%A2/</url>
    
    <content type="html"><![CDATA[<p>基础配置做好了之后，我们就可以先迁移数据到数据库，然后启动我们的项目，感受Django的魅力。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure><p>迁移数据之后，网站目录里自动会创建一个数据库文件db.sqlite3，里面存放着我们的数据。</p><p>之后输入下面命令创建管理帐号和密码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py createsuperuser</span><br></pre></td></tr></table></figure><p>最后，我们设置下启动命令，启动我们的Django项目：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190420/Ha3OxK80ofwV.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>启动后登陆 <a href="http://0.0.0.0:8000/" target="_blank" rel="noopener">http://0.0.0.0:8000/</a> 就可以看到我们的欢迎页。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190417/EfVJ0Ar9T6Tt.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>另外一点，我们当然也可以设置自定义的主页：</p><p>首先，打开打开bolg目录下的views.py文件，在里面输入：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/blog/views.py</span><br><span class="line"></span><br><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def hello(request):</span><br><span class="line">    return HttpResponse(&apos;Welcome to Django Zone！&apos;)</span><br></pre></td></tr></table></figure><p>再打开myblog目录下的urls.py文件，在文件里添加两行代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/myblog/urls.py</span><br><span class="line"></span><br><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import path</span><br><span class="line">from blog import views</span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&apos;admin/&apos;, admin.site.urls),</span><br><span class="line">    path(&apos;&apos;, views.hello),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>再次刷新启动后，即可看到新的欢迎页面。</p><p>当然，我们在浏览器里面访问：<a href="http://127.0.0.1:8000/admin" target="_blank" rel="noopener">http://127.0.0.1:8000/admin</a>  就可以进入Django自带的后台管理。</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：基础配置（四）</title>
    <link href="/2019/04/08/%E5%9B%9B%E3%80%81%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/"/>
    <url>/2019/04/08/%E5%9B%9B%E3%80%81%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p>打开myblog目录下的settings.py文件：</p><h4 id="一、设置域名访问权限"><a href="#一、设置域名访问权限" class="headerlink" title="一、设置域名访问权限"></a>一、设置域名访问权限</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.py</span><br><span class="line"></span><br><span class="line">ALLOWED_HOSTS = []      #修改前</span><br><span class="line">ALLOWED_HOSTS = [&apos;*&apos;]</span><br><span class="line"></span><br><span class="line">#修改后，表示任何域名都能访问。如果指定域名的话，在&apos;&apos;里放入指定的域名即可</span><br></pre></td></tr></table></figure><h4 id="二、设置TEMPLATES里的’DIRS’"><a href="#二、设置TEMPLATES里的’DIRS’" class="headerlink" title="二、设置TEMPLATES里的’DIRS’"></a>二、设置TEMPLATES里的’DIRS’</h4><p>添加模板目录templates的路径，后面我们做网站模板的时候用得着。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.py</span><br><span class="line"></span><br><span class="line">#修改前</span><br><span class="line">&apos;DIRS&apos;: []</span><br><span class="line"></span><br><span class="line">#修改后</span><br><span class="line">&apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)]</span><br><span class="line"></span><br><span class="line">注：使用pycharm创建的话会自动添加</span><br></pre></td></tr></table></figure><h4 id="三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。"><a href="#三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。" class="headerlink" title="三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。"></a>三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。</h4><h4 id="四、在INSTALLED-APPS添加APP应用名称。"><a href="#四、在INSTALLED-APPS添加APP应用名称。" class="headerlink" title="四、在INSTALLED_APPS添加APP应用名称。"></a>四、在INSTALLED_APPS添加APP应用名称。</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.py</span><br><span class="line"></span><br><span class="line">INSTALLED_APPS = [</span><br><span class="line">    &apos;django.contrib.admin&apos;,</span><br><span class="line">    ....</span><br><span class="line">    &apos;blog.apps.BlogConfig&apos;,#注册APP应用</span><br><span class="line">]</span><br><span class="line">#使用pycharm创建的话，这里自动添加了，如果是终端命令创建的话，需要手动添加应用名称如&apos;blog&apos;,</span><br></pre></td></tr></table></figure><h4 id="五、修改项目语言和时区"><a href="#五、修改项目语言和时区" class="headerlink" title="五、修改项目语言和时区"></a>五、修改项目语言和时区</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.py</span><br><span class="line">#修改前为英文</span><br><span class="line">LANGUAGE_CODE = &apos;en-us&apos;</span><br><span class="line">#修改后</span><br><span class="line">LANGUAGE_CODE = &apos;zh-hans&apos; #语言修改为中文</span><br><span class="line">#时区，修改前</span><br><span class="line">TIME_ZONE = &apos;UTC&apos;</span><br><span class="line">#修改后</span><br><span class="line">TIME_ZONE = &apos;Asia/Shanghai&apos;</span><br></pre></td></tr></table></figure><h4 id="六、在项目根目录里创建static和media"><a href="#六、在项目根目录里创建static和media" class="headerlink" title="六、在项目根目录里创建static和media"></a>六、在项目根目录里创建static和media</h4><p>static用来存放模板CSS、JS、图片等静态资源，media用来存放上传的文件，后面我们在讲解数据库创建的时候有说明。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myblog/settings.py</span><br><span class="line"></span><br><span class="line">#设置静态文件目录和名称</span><br><span class="line">STATIC_URL = &apos;/static/&apos;</span><br><span class="line"></span><br><span class="line">#加入下面代码</span><br><span class="line"></span><br><span class="line">#这个是设置静态文件夹目录的路径</span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, &apos;static&apos;),</span><br><span class="line">)</span><br><span class="line">#设置文件上传路径，图片上传、文件上传都会存放在此目录里</span><br><span class="line">MEDIA_URL = &apos;/media/&apos;</span><br><span class="line">MEDIA_ROOT = os.path.join(BASE_DIR, &apos;media&apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：创建项目（三）</title>
    <link href="/2019/04/07/%E4%B8%89%E3%80%81%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE/"/>
    <url>/2019/04/07/%E4%B8%89%E3%80%81%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="基础环境："><a href="#基础环境：" class="headerlink" title="基础环境："></a>基础环境：</h3><ul><li>Python3.6</li><li>django2.1.1</li><li>开发工具为Pycharm</li></ul><p><img src="http://myimage.okay686.cn/okay686cn/20190418/I7vTvTDCrPp4.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><p>1、为项目保存路径，myblog为项目名。</p><p>2、为选择使用的虚拟环境软件，这里选virtualenv。</p><p>3、为虚拟环境保存目录，我把它保存在项目里，虚拟环境默认名为env，我系统里有多个项目为了区分出来命名为myblogenv</p><p>4、为使用的模板语言，我们默认用django模板语言。</p><p>5、为创建项目的时候建立一个模板文件目录，用来存放模板文件。用CMD命令创建项目的话，模板目录需要自己手动创建。</p><p>6、为创建一个名为blog的APP应用。同样的用CMD命令创建的话，需要手动通过python manage.py startapp blog命令来进行创建。</p><p>点击创建之后，Pycharm自动帮我们完成Django软件下载安装和Django的项目创建。</p><p><strong>注意</strong>：如果对需要指定Django版本的话，不能直接使用这个方法，这个方法会直接下载最新版本的Django。指定版本的话，请使用CMD通过命令如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install django==2.0.1</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/20190418/xUHXyvjepzFd.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>第一个黑色的 <strong>myblog</strong> 为项目文件夹目录。</p><p>blog为APP应用目录，也是我们上面设置第6项才创建的。myblog为项目配置目录，myblogvenv为Pycharm创建的虚拟环境目录，与项目无关，不用理会。</p><p>目录里的文件含义如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blog                #APP应用名和目录</span><br><span class="line">│  admin.py        #对应应用后台管理配置文件。</span><br><span class="line">│  apps.py         #对应应用的配置文件。</span><br><span class="line">│  models.py       #数据模块，数据库设计就在此文件中设计。后面重点讲解</span><br><span class="line">│  tests.py        #自动化测试模块，可在里面编写测试脚本自动化测试</span><br><span class="line">│  views.py        #视图文件，用来执行响应代码的。你在浏览器所见所得都是它处理的。</span><br><span class="line">│  __init__.py</span><br><span class="line">│</span><br><span class="line">├─migrations        #数据迁移、移植文目录，记录数据库操作记录，内容自动生成。</span><br><span class="line">│  │  __init__.py</span><br><span class="line">myblog               #项目配置目录</span><br><span class="line">│  __init__.py       #初始化文件，一般情况下不用做任何修改。</span><br><span class="line">│  settings.py        #项目配置文件，具体如何配置后面有介绍。</span><br><span class="line">│  url.py             #项目URL设置文件，可理解为路由，可以控制你访问去处。</span><br><span class="line">│  wsgi.py          #为Python服务器网关接口，是Python与WEB服务器之间的接口。</span><br><span class="line">myblogvenv            #Pycharm创建的虚拟环境目录，和项目无关，不需要管它。</span><br><span class="line">templates           #项目模板文件目录，用来存放模板文件</span><br><span class="line">manage.py     #命令行工具，通过可以与项目与行交互。</span><br><span class="line">在终端输入python manege.py help，可以查看功能。</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：数据库设计分析（二）</title>
    <link href="/2019/04/06/%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90/"/>
    <url>/2019/04/06/%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>++从网站需求分析及网站功能、页面设计可以知道，我们的Blog主要以文章内容为主。所以我们在设计数据库的时候，我们主要以文章信息为核心数据，然后逐步向外扩展相关联的数据信息。++</p><p>从如下图片中可以看到，文章有标题、分类、作者、浏览次数、发布时间、文章标签等信息。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190411/lNu2QfqNYDQF.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这其中，文章与分类的关系是一对多的关系，什么是一对多？就是一篇文章只能有一个分类，而一个分类里可以有多篇文章。文章与标签的关系是多对多的关系，多对多简单理解就是，一篇文章可以有多个标签，一个标签里同样可以有多篇文章。</p><p>我们将文章表命名为<strong>Article</strong>，通过前面的分析得出文章信息表Article的数据库结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>title</td><td>CharField类型，长度为100</td><td>文章标题</td></tr><tr><td>category</td><td>ForeignKey</td><td>外键，关联文章分类表</td></tr><tr><td>tags</td><td>ManyToManyField</td><td>多对多，关联标签列表</td></tr><tr><td>body</td><td>TextField</td><td>文章内容</td></tr><tr><td>user</td><td>ForeignKey</td><td>外键，文章作者关联用户模型，系统自带的</td></tr><tr><td>views</td><td>PositiveIntegerField</td><td>文章浏览数，正的整数，不能为负</td></tr><tr><td>tui</td><td>ForeignKey</td><td>外键，关联推荐位表</td></tr><tr><td>created_time</td><td>DateTimeField</td><td>文章发布时间</td></tr></tbody></table><p>从文章表里，我们关联了一个<strong>分类表</strong>，我们把这个分类表命名为category，category表的数据库结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>name</td><td>CharField类型，长度为30</td><td>分类名</td></tr></tbody></table><p>文章关联的<strong>标签表</strong>，我们命名为tag，结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>name</td><td>CharField类型，长度为30</td><td>标签名</td></tr></tbody></table><p>文章关联的<strong>推荐位表</strong>，命名为tui，结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>name</td><td>CharField类型，长度为30</td><td>标签名</td></tr></tbody></table><p>除此之外，我们还有两个独立的表，和文章没有关联的，一个是幻灯图片的表，一个是友情链接的表。</p><p><strong>幻灯图表</strong>，命名为banner，数据库结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>text_info</td><td>CharField类型，长度为100</td><td>标题，图片文本信息</td></tr><tr><td>img</td><td>ImageField类型</td><td>图片类型，保存传图片的路径</td></tr><tr><td>link_url</td><td>URLField类型</td><td>图片链接的URL</td></tr><tr><td>is_active</td><td>BooleanField布尔类型</td><td>有True 和False两个值，意思为是否激活</td></tr></tbody></table><p><strong>友情链接表</strong>命名为link，结构如下：</p><table><thead><tr><th>表字段</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int类型，长度为11</td><td>主键，由系统自动生成</td></tr><tr><td>name</td><td>CharField类型，长度为70</td><td>友情链接的名称</td></tr><tr><td>linkurl</td><td>URLField类型</td><td>友情链接的URL</td></tr></tbody></table><p>至此，我们的数据库构造大致完成，后期如果还有其它的需求，我们可以在这基础上进行增加或者删除。下面我们就开始进行项目的创建与开发。</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django博客开发：项目需求分析（一）</title>
    <link href="/2019/04/05/%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/"/>
    <url>/2019/04/05/%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>当我们要开发一个项目的时候，首先需要了解我们项目的具体需求，根据需求类型划分网站功能，并详细了解这些需求的业务流程。然后更具需求和业务流程进行数据库设计。</p><p>blog的功能相对比较简单，主要以文章为主。</p><p>==从功能需求来看==，这个Blog的功能分为：网站首页、文章分类、文章内容、幻灯图片、文章推荐、文章排行、热门推荐、文章搜索、友情链接。</p><p><strong>1、网站首页</strong>：网站首页是整个网站的主界面，也是网站入口界面，里面主要展示Blog的动态信息及Blog功能导。网站动态信息以文章为主，如最新文章、幻灯图片、推荐阅读、文章排行、热门推荐、友情链接等。导航栏主要是将文章的分类的链接展示在首页，方便用户浏览。</p><p><strong>2、文章分类</strong>：主要展示文章分类信息及链接，方便用户按需查看。文章分类可以在后台添加删除。</p><p><strong>3、文章内容</strong>：主要展示文章所属分类、文章所属标签、文章内容、作者信息，发布时间信息。可以通过后台增、删、改。</p><p><strong>4、幻灯图片</strong>：在网站首页，通过图片和文字展示一些重要信息，可以通过后台添加图片、图片描述、图片链接。</p><p><strong>5、文章推荐</strong>：推荐一些重要的文章，可以在后台进行推荐。</p><p><strong>6、文章排行</strong>：可根据文章浏览数，按时间段进行查询，然后展示出来。具体可根据自己的需求修改。</p><p><strong>7、热门推荐</strong>：同样的推荐一些需要推荐的文章，可以在后台按需求或推荐位进行设置。</p><p><strong>8、文章搜索</strong>：通过关键词搜索文章。</p><p><strong>9、友情链接</strong>：展示相互链接的网站的名称与链接，可以通过后台添加与删除。</p><p><strong>10、单页面</strong>：展示网站介绍，作者联系方式等信息，此类信息不经常变动，可以通过后台实现修改，也可以通过修改模板实现。</p><p>了解需求之后，就由UI设计师根据网站需求来设计网站页面，然后由前端工程师根据设计好的页面进行切图，实现HTML静态页面，最后由后端根据HTML页面和需求实现数据库构建和网站后台开发。</p><p>从设计方面来看，Blog主要分为六个页面，分别是：网站首页、文章分类列表页、文章内容页、搜索列表页、标签列表页、单页面。</p><p><strong>1、网站首页</strong>：信息聚合的地方，展示多种信息；</p><p><strong>2、文章分类列表页</strong>：点击分类，进入一个同一分类文章展示的列表页面；</p><p><strong>3、文章内容页</strong>：文章内容展示页面，对应演示站这个地址；</p><p><strong>4、搜索列表页</strong>：通过首页搜索按钮，展示出与搜索 词相关的文章列表；</p><p><strong>5、标签列表页</strong>：展示同一个标签下的所有文章；</p><p><strong>6、单页面</strong>：展示网站介绍、作者介绍或者联系方式等信息；</p>]]></content>
    
    
    <categories>
      
      <category>Django2</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>django2.0</tag>
      
      <tag>django_blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BeautifulSoup中find和find_all的用法</title>
    <link href="/2019/04/04/BeautifulSoup%E4%B8%ADfind%E5%92%8Cfind_all%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <url>/2019/04/04/BeautifulSoup%E4%B8%ADfind%E5%92%8Cfind_all%E7%9A%84%E7%94%A8%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>在爬取网页中有用的信息时，通常是对存在于网页中的文本或各种不同标签的属性值进行查找，Beautiful Soup中内置了一些查找方式，最常用的是 find() 和 find_all() 函数。</p><p><strong>同时通过soup.find_all()得到的所有符合条件的结果和soup.select()一样都是列表list，而soup.find()只返回第一个符合条件的结果，所以soup.find()后面可以直接接.text或者get_text()来获得标签中的文本。</strong></p><h3 id="一、find-用法"><a href="#一、find-用法" class="headerlink" title="一、find()用法"></a>一、find()用法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find(name,attrs,recursive,text,**wargs)</span><br></pre></td></tr></table></figure><h4 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul id=&quot;producers&quot;&gt;  </span><br><span class="line">        &lt;li class=&quot;producerlist&quot;&gt;  </span><br><span class="line">            &lt;div class=&quot;name&quot;&gt;plants&lt;/div&gt;  </span><br><span class="line">            &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt;  </span><br><span class="line">        &lt;/li&gt;  </span><br><span class="line">        &lt;li class=&quot;producerlist&quot;&gt;  </span><br><span class="line">            &lt;div class=&quot;name&quot;&gt;algae&lt;/div&gt;  </span><br><span class="line">            &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt;  </span><br><span class="line">        &lt;/li&gt;  </span><br><span class="line">&lt;/ul&gt;</span><br></pre></td></tr></table></figure><p>(1)ul,li,div这些就是标签；</p><pre><code>用法p=soup.find(&apos;ul&apos;) ，那么返回结果是第一个ul标签以及&lt;xx&gt;...&lt;/xx&gt;的所有内容，即上面的代码；注意若用p=soup.find(&apos;ul&apos;).get_text()那么结果不是...的所有内    容，而应该是plants 10000 algae 10000，即...中的标签不算text文本。</code></pre><p>(2)<xx>…</xx>之间的内容就是文本；<br>基于文本内容的查找也可以用soup.find()，但必须用到参数text，</p><pre><code>用法p=soup.find(text=&apos;algae&apos;)，print(p)得到的结果就是algae</code></pre><p>(3)正则表达式后面自己另外去学习；</p><p>(4)ul id=”producers”&gt;中的id即标签属性，那么我们可以查找具有特定标签的属性；</p><pre><code>用法p=soup.find(&apos;ul&apos;, id=&quot;producers&quot;)，那么可以得到&lt;xx&gt;...&lt;/xx&gt;的所有结果，其特点是把标签更一步精确化以便于查找。对于大多数的情况可以用上面的方法解决，但是有两种情况则要用到参数attrs:一是标签字符中带有-，比如data-custom;二是class不能看作标签属性。解决的办法是在attrs属性用字典进行传递参数：</code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">soup.find(attrs=&#123;&apos;data-custom&apos;:&apos;xxx&apos;&#125;)</span><br><span class="line">以及：</span><br><span class="line">soup.find(attrs=&#123;&apos;class&apos;:&apos;xxx&apos;&#125;)</span><br></pre></td></tr></table></figure><h3 id="二、find-all-用法"><a href="#二、find-all-用法" class="headerlink" title="二、find_all()用法"></a>二、find_all()用法</h3><pre><code>应用到find()中的不同过滤参数同理可以用到find_all()中，相比find()，find_all()有个额外的参数limit；</code></pre><p>如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p=soup.find_all(text=&apos;algae&apos;,limit=2)</span><br><span class="line"></span><br><span class="line">实际上find()也就是当limit=1时的find_all()。</span><br></pre></td></tr></table></figure><p><a href="http://blog.csdn.net/abclixu123/article/details/38502993" target="_blank" rel="noopener">[参考文档引自]</a></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python3</tag>
      
      <tag>BeautifulSoup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python3爬取墨迹天气并发送给微信好友</title>
    <link href="/2019/04/03/python3%E7%88%AC%E5%8F%96%E5%A2%A8%E8%BF%B9%E5%A4%A9%E6%B0%94%E5%B9%B6%E5%8F%91%E9%80%81%E7%BB%99%E5%BE%AE%E4%BF%A1%E5%A5%BD%E5%8F%8B/"/>
    <url>/2019/04/03/python3%E7%88%AC%E5%8F%96%E5%A2%A8%E8%BF%B9%E5%A4%A9%E6%B0%94%E5%B9%B6%E5%8F%91%E9%80%81%E7%BB%99%E5%BE%AE%E4%BF%A1%E5%A5%BD%E5%8F%8B/</url>
    
    <content type="html"><![CDATA[<p><strong>需求：</strong></p><ol><li>爬取墨迹天气的信息，包括温湿度、风速，生活tips等信息；</li><li>可选输入需要查询的城市，自动爬取相应信息；</li><li>链接微信，发送给指定好友或群；</li></ol><p>思路比较清晰，主要分两块，一是爬虫，二是用python链接微信（非企业版微信）</p><p>先随便观察一个城市的墨迹天气，例如苏州市的url</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https://tianqi.moji.com/weather/china/jiangsu/suzhou</span><br></pre></td></tr></table></figure><p>多观察几个城市的url可发现共同点就是，前面的都一样，后面的是以省拼音/市拼音结尾的。当然直辖市两者拼音一样。当然还有一些额外情况，比如山西和陕西，后者的拼音是Shaanxi，这个用户输入的时候注意一下；</p><h4 id="第一部分："><a href="#第一部分：" class="headerlink" title="第一部分："></a>第一部分：</h4><ul><li>将汉字转换为拼音；</li><li>安装了第三方库xpinyin；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># prov = input(&quot;请输入省份：&quot;)</span><br><span class="line"># city = input(&quot;请输入城市：&quot;)</span><br><span class="line">prov = &quot;江苏&quot;</span><br><span class="line">city = &quot;苏州&quot;</span><br><span class="line">pin = Pinyin()</span><br><span class="line"></span><br><span class="line">prov_pin = pin.get_pinyin(prov,&apos;&apos;)      #将汉字转为拼音</span><br><span class="line">city_pin = pin.get_pinyin(city,&apos;&apos;)</span><br><span class="line">url = &quot;https://tianqi.moji.com/weather/china/&quot;</span><br><span class="line"></span><br><span class="line">weather_url = url+prov_pin+&quot;/&quot;+city_pin</span><br><span class="line"></span><br><span class="line"># print(weather_url)</span><br></pre></td></tr></table></figure></li></ul><h4 id="第二部分："><a href="#第二部分：" class="headerlink" title="第二部分："></a>第二部分：</h4><ul><li>定位需要获取的信息；</li><li>获取今日天气信息；</li><li>使用select筛选的的是class名或者id名，注意同级和下一级的书写形式；find和find_all是查找的标签；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;)      #打开页面源代码</span><br><span class="line">bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)</span><br><span class="line"># print(bs4.prettify())</span><br><span class="line"></span><br><span class="line">weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;)       #定位需要拿到的信息</span><br><span class="line"># print(weather)</span><br><span class="line"></span><br><span class="line">temp1 = weather.find(&apos;em&apos;).get_text()</span><br><span class="line">temp2 = weather.find(&apos;b&apos;).get_text()</span><br><span class="line"></span><br><span class="line">AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()</span><br><span class="line"># print(AQI)</span><br><span class="line"></span><br><span class="line">SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text()      #湿度</span><br><span class="line">FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text()       #风速</span><br><span class="line">TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text()          #今日天气提示</span><br><span class="line">DATE = str(datetime.date.today())               ##今天的日期</span><br><span class="line">WEEK = time.strftime(&quot;%w&quot;, time.localtime())</span><br><span class="line"></span><br><span class="line">INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos;  &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPS</span><br><span class="line">print(INFO)</span><br></pre></td></tr></table></figure></li></ul><h4 id="第三部分："><a href="#第三部分：" class="headerlink" title="第三部分："></a>第三部分：</h4><ul><li>获取明日天气信息；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##获取明日天气</span><br><span class="line">tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)</span><br><span class="line"># print(tomorrow)</span><br><span class="line">t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]</span><br><span class="line">t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text()        ##明日风速</span><br><span class="line">t_AQI = tomorrow[-1].get_text().strip()         ##明日空气质量</span><br><span class="line">t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;</span><br><span class="line">print(t_info)</span><br></pre></td></tr></table></figure></li></ul><h4 id="第四部分："><a href="#第四部分：" class="headerlink" title="第四部分："></a>第四部分：</h4><ul><li>链接微信需要安装第三方库itchat，链接只需要这一句话，很简单。初次链接会弹出二维码，手机扫二维码登陆；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;</span><br><span class="line"></span><br><span class="line">itchat.auto_login(hotReload=True)       #在一段时间内运行不需要扫二维码登陆</span><br><span class="line"></span><br><span class="line">def sendToPersion(nickName):</span><br><span class="line">    user = itchat.search_friends(name=nickName)</span><br><span class="line">    print(user)</span><br><span class="line">    userName = user[0][&apos;UserName&apos;]</span><br><span class="line">    itchat.send(info_all, toUserName=userName)</span><br><span class="line">    print(&apos;send it to HJ succeed&apos;)</span><br><span class="line"></span><br><span class="line">def sendToRoom(nickName):</span><br><span class="line">    group = itchat.search_chatrooms(name=nickName)</span><br><span class="line">    print(group)</span><br><span class="line">    userName = group[0][&apos;UserName&apos;]</span><br><span class="line">    itchat.send(info_all, toUserName=userName)</span><br><span class="line">    print(&quot;send it to Group succeed&quot;)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sendToPersion(&quot;微信好友备注名&quot;)</span><br><span class="line">    sendToRoom(&quot;微信群组备注名&quot;)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 给自己的文件助手filehelper发送信息,此时无需访问通讯录</span><br><span class="line">- #itchat.send(&apos;❤来自XXX的天气问候❤&apos;,toUserName=&apos;filehelper&apos;)</span><br><span class="line"></span><br><span class="line">- #I = itchat.search_friends()# 获取自己的信息，返回自己的属性字典</span><br><span class="line">- #friends = itchat.get_friends(update=True)#返回值类型&lt;class &apos;itchat.storage.templates.ContactList&apos;&gt;。可以看做是列表，列表里的每个元素是一个字典，对应一个好友信息</span><br></pre></td></tr></table></figure><h4 id="全部代码："><a href="#全部代码：" class="headerlink" title="全部代码："></a>全部代码：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib import request</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from xpinyin import Pinyin</span><br><span class="line">import time</span><br><span class="line">import itchat</span><br><span class="line"></span><br><span class="line"># prov = input(&quot;请输入省份：&quot;)</span><br><span class="line"># city = input(&quot;请输入城市：&quot;)</span><br><span class="line">prov = &quot;江苏&quot;</span><br><span class="line">city = &quot;苏州&quot;</span><br><span class="line">pin = Pinyin()</span><br><span class="line"></span><br><span class="line">prov_pin = pin.get_pinyin(prov,&apos;&apos;)      #将汉字转为拼音</span><br><span class="line">city_pin = pin.get_pinyin(city,&apos;&apos;)</span><br><span class="line">url = &quot;https://tianqi.moji.com/weather/china/&quot;</span><br><span class="line"></span><br><span class="line">weather_url = url+prov_pin+&quot;/&quot;+city_pin</span><br><span class="line"></span><br><span class="line"># print(weather_url)</span><br><span class="line"></span><br><span class="line">htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;)      #打开页面源代码</span><br><span class="line">bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)</span><br><span class="line"># print(bs4.prettify())</span><br><span class="line"></span><br><span class="line">weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;)       #定位需要拿到的信息</span><br><span class="line"># print(weather)</span><br><span class="line"></span><br><span class="line">temp1 = weather.find(&apos;em&apos;).get_text()</span><br><span class="line">temp2 = weather.find(&apos;b&apos;).get_text()</span><br><span class="line"></span><br><span class="line">AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()</span><br><span class="line"># print(AQI)</span><br><span class="line"></span><br><span class="line">SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text()      #湿度</span><br><span class="line">FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text()       #风速</span><br><span class="line">TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text()          #今日天气提示</span><br><span class="line">DATE = str(datetime.date.today())               ##今天的日期</span><br><span class="line">WEEK = time.strftime(&quot;%w&quot;, time.localtime())</span><br><span class="line"></span><br><span class="line">INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos;  &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPS</span><br><span class="line">print(INFO)</span><br><span class="line"></span><br><span class="line">##获取明日天气</span><br><span class="line">tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)</span><br><span class="line"># print(tomorrow)</span><br><span class="line">t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]</span><br><span class="line">t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text()        ##明日风速</span><br><span class="line">t_AQI = tomorrow[-1].get_text().strip()         ##明日空气质量</span><br><span class="line">t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;</span><br><span class="line">print(t_info)</span><br><span class="line"></span><br><span class="line">info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;</span><br><span class="line"></span><br><span class="line">itchat.auto_login(hotReload=True)       #在一段时间内运行不需要扫二维码登陆</span><br><span class="line"></span><br><span class="line">def sendToPersion(nickName):</span><br><span class="line">    user = itchat.search_friends(name=nickName)</span><br><span class="line">    print(user)</span><br><span class="line">    userName = user[0][&apos;UserName&apos;]</span><br><span class="line">    itchat.send(info_all, toUserName=userName)</span><br><span class="line">    print(&apos;send it to HJ succeed&apos;)</span><br><span class="line"></span><br><span class="line">def sendToRoom(nickName):</span><br><span class="line">    group = itchat.search_chatrooms(name=nickName)</span><br><span class="line">    print(group)</span><br><span class="line">    userName = group[0][&apos;UserName&apos;]</span><br><span class="line">    itchat.send(info_all, toUserName=userName)</span><br><span class="line">    print(&quot;send it to Group succeed&quot;)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sendToPersion(&quot;微信好友备注名&quot;)</span><br><span class="line">    sendToRoom(&quot;微信群组备注名&quot;)</span><br></pre></td></tr></table></figure><p>微信好友：<br><img src="http://myimage.okay686.cn/okay686cn/20190403/O7JQKmfLvbne.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>微信群组：<br><img src="http://myimage.okay686.cn/okay686cn/20190403/cBbti52VbsmQ.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>抓取墨迹天气的源码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang=&quot;en&quot;&gt;</span><br><span class="line"> &lt;head&gt;</span><br><span class="line">  &lt;meta charset=&quot;utf-8&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;width=device-width, initial-scale=1&quot; name=&quot;viewport&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;苏州市今天实况：15度 阴，湿度：53%，东南风：2级。白天：16度,阴。 夜间：阴，9度，天气偏凉了，墨迹天气建议您穿上厚些的外套或是保暖的羊毛衫，年老体弱者可以选择保暖的摇粒绒外套。&quot; name=&quot;description&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;苏州市天气预报，苏州市天气查询&quot; name=&quot;keywords&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;Moji Weather Web Dev Team&quot; name=&quot;author&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/favicon.ico&quot; rel=&quot;shortcut icon&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/custom_icon.png&quot; rel=&quot;apple-touch-icon&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-60.png&quot; rel=&quot;apple-touch-icon&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-76.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;76x76&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-retina-120.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;120x120&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-retina-152.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;152x152&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;format=html5;url=https://m.moji.com/weather/china/jiangsu/suzhou&quot; name=&quot;mobile-agent&quot;/&gt;</span><br><span class="line">  &lt;link href=&quot;https://m.moji.com/weather/china/jiangsu/suzhou&quot; media=&quot;only screen and(max-width: 640px)&quot; rel=&quot;alternate&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;IE=EmulateIE8; charset=UTF-8&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt;</span><br><span class="line">  &lt;meta content=&quot;IE=edge,chrome=1&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt;</span><br><span class="line">  &lt;title&gt;</span><br><span class="line">   【苏州市天气】_苏州市天气预报_天气查询 - 墨迹天气</span><br><span class="line">  &lt;/title&gt;</span><br><span class="line">  &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/reset.css&quot; rel=&quot;stylesheet&quot;/&gt;</span><br><span class="line">  &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/index.css&quot; rel=&quot;stylesheet&quot;/&gt;</span><br><span class="line">  &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/chanle.css&quot; rel=&quot;stylesheet&quot;/&gt;</span><br><span class="line"> &lt;/head&gt;</span><br><span class="line"> &lt;body&gt;</span><br><span class="line">  &lt;div class=&quot;head_box&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;head clearfix&quot;&gt;</span><br><span class="line">    &lt;a class=&quot;logo&quot; href=&quot;http://www.moji.com/&quot;&gt;</span><br><span class="line">     &lt;img alt=&quot;墨迹天气&quot; height=&quot;31&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/index/i_logo.png&quot;/&gt;</span><br><span class="line">    &lt;/a&gt;</span><br><span class="line">    &lt;div class=&quot;phone&quot;&gt;</span><br><span class="line">     &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt;</span><br><span class="line">      &lt;i class=&quot;shake shake-rotate&quot; id=&quot;head_shake&quot;&gt;</span><br><span class="line">       &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/icon/phone.png&quot;/&gt;</span><br><span class="line">      &lt;/i&gt;</span><br><span class="line">      &lt;span&gt;</span><br><span class="line">       随时随地 想查就查</span><br><span class="line">      &lt;/span&gt;</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;nav&quot;&gt;</span><br><span class="line">     &lt;a href=&quot;http://www.moji.com/&quot;&gt;</span><br><span class="line">      首页</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">     &lt;a href=&quot;https://tianqi.moji.com&quot;&gt;</span><br><span class="line">      天气</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">     &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt;</span><br><span class="line">      下载</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">     &lt;!--            &lt;a href=&quot;http://www.moji.com/tob/&quot;&gt;天气服务&lt;sup style=&quot;margin: -32px 0 0 56px;&quot;&gt;</span><br><span class="line">                &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/head_hot.png&quot;&gt;&lt;/sup&gt;--&gt;</span><br><span class="line">     &lt;a href=&quot;https://tianqi.moji.com/news/index&quot;&gt;</span><br><span class="line">      资讯</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">     &lt;a href=&quot;http://www.moji.com/about/&quot;&gt;</span><br><span class="line">      关于墨迹</span><br><span class="line">     &lt;/a&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;div data-data=&quot;13&quot; data-url=&quot;https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg&quot; id=&quot;skin&quot; style=&quot;background: url(https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg) no-repeat center top;background-size: 100% 100%;&quot;&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;div class=&quot;wrap clearfix&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;comm_box&quot;&gt;</span><br><span class="line">    &lt;!--面包屑--&gt;</span><br><span class="line">    &lt;div class=&quot;crumb clearfix&quot;&gt;</span><br><span class="line">     &lt;ul&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com&quot;&gt;</span><br><span class="line">        天气</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">       &lt;i&gt;</span><br><span class="line">       &lt;/i&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/weather/china&quot;&gt;</span><br><span class="line">        中国</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">       &lt;i&gt;</span><br><span class="line">       &lt;/i&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu&quot;&gt;</span><br><span class="line">        江苏省</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">       &lt;i&gt;</span><br><span class="line">       &lt;/i&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       苏州市</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">   &lt;div id=&quot;search&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;search&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;search_default&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       苏州市， 江苏省， 中国</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;strong id=&quot;locate&quot;&gt;</span><br><span class="line">      &lt;/strong&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;!--icon--&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;input placeholder=&quot;输入你要查找的城市&quot; type=&quot;text&quot;/&gt;</span><br><span class="line">      &lt;i&gt;</span><br><span class="line">      &lt;/i&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;search_more&quot;&gt;</span><br><span class="line">      &lt;a href=&quot;https://tianqi.moji.com/findmycity&quot;&gt;</span><br><span class="line">       更多城市</span><br><span class="line">      &lt;/a&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;search_city&quot; style=&quot;display: none;&quot;&gt;</span><br><span class="line">     &lt;ul&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;div class=&quot;wrap clearfix wea_info&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;left&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;wea_alert clearfix&quot;&gt;</span><br><span class="line">     &lt;ul&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">        &lt;span class=&quot;level level_2&quot;&gt;</span><br><span class="line">         &lt;img alt=&quot;63 良&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/aqi/2.png&quot;/&gt;</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;em&gt;</span><br><span class="line">         63 良</span><br><span class="line">        &lt;/em&gt;</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;wea_weather clearfix&quot;&gt;</span><br><span class="line">     &lt;em&gt;</span><br><span class="line">      15</span><br><span class="line">     &lt;/em&gt;</span><br><span class="line">     &lt;span&gt;</span><br><span class="line">      &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">     &lt;/span&gt;</span><br><span class="line">     &lt;b&gt;</span><br><span class="line">      阴</span><br><span class="line">     &lt;/b&gt;</span><br><span class="line">     &lt;strong class=&quot;info_uptime&quot;&gt;</span><br><span class="line">      今天8:45更新</span><br><span class="line">     &lt;/strong&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;wea_about clearfix&quot;&gt;</span><br><span class="line">     &lt;span&gt;</span><br><span class="line">      湿度 53%</span><br><span class="line">     &lt;/span&gt;</span><br><span class="line">     &lt;em&gt;</span><br><span class="line">      东南风2级</span><br><span class="line">     &lt;/em&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;wea_tips clearfix&quot;&gt;</span><br><span class="line">     &lt;span&gt;</span><br><span class="line">      今日天气提示</span><br><span class="line">     &lt;/span&gt;</span><br><span class="line">     &lt;em&gt;</span><br><span class="line">      略微偏凉，还是蛮舒适的。</span><br><span class="line">     &lt;/em&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">   &lt;div class=&quot;right&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;wea_info_avator&quot;&gt;</span><br><span class="line">     &lt;img alt=&quot;墨迹天气 小墨哥&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/avator/icon/7.png&quot;/&gt;</span><br><span class="line">     &lt;div id=&quot;windows_download&quot;&gt;</span><br><span class="line">      &lt;img alt=&quot;Windows 下载&quot; height=&quot;35&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/avator_windows.png&quot;/&gt;</span><br><span class="line">      &lt;a href=&quot;http://download.moji001.com/mojiapp/windoz/MoWeatherInstall_1.8.1.1.exe&quot; target=&quot;_blank&quot;&gt;</span><br><span class="line">       Windows 下载</span><br><span class="line">      &lt;/a&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;div class=&quot;wrap clearfix&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;left&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;forecast clearfix&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;g_title&quot;&gt;</span><br><span class="line">      &lt;span&gt;</span><br><span class="line">       预报</span><br><span class="line">      &lt;/span&gt;</span><br><span class="line">      &lt;ul class=&quot;nav&quot;&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         7天预报</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         10天预报</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         15天预报</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;ul class=&quot;days clearfix&quot;&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">        今天</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;span&gt;</span><br><span class="line">        &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">       &lt;/span&gt;</span><br><span class="line">       阴</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       9° / 16°</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;em&gt;</span><br><span class="line">        东南风</span><br><span class="line">       &lt;/em&gt;</span><br><span class="line">       &lt;b&gt;</span><br><span class="line">        3级</span><br><span class="line">       &lt;/b&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;strong class=&quot;level_2&quot;&gt;</span><br><span class="line">        63 良</span><br><span class="line">       &lt;/strong&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">     &lt;ul class=&quot;days clearfix&quot;&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">        明天</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;span&gt;</span><br><span class="line">        &lt;img alt=&quot;多云&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">       &lt;/span&gt;</span><br><span class="line">       多云</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       10° / 19°</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;em&gt;</span><br><span class="line">        东南风</span><br><span class="line">       &lt;/em&gt;</span><br><span class="line">       &lt;b&gt;</span><br><span class="line">        3级</span><br><span class="line">       &lt;/b&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;strong class=&quot;level_2&quot;&gt;</span><br><span class="line">        62 良</span><br><span class="line">       &lt;/strong&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">     &lt;ul class=&quot;days clearfix&quot;&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">        后天</span><br><span class="line">       &lt;/a&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;span&gt;</span><br><span class="line">        &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">       &lt;/span&gt;</span><br><span class="line">       阴</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       11° / 15°</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;em&gt;</span><br><span class="line">        东南风</span><br><span class="line">       &lt;/em&gt;</span><br><span class="line">       &lt;b&gt;</span><br><span class="line">        3级</span><br><span class="line">       &lt;/b&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;strong class=&quot;level_2&quot;&gt;</span><br><span class="line">        68 良</span><br><span class="line">       &lt;/strong&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;hours&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;g_title&quot;&gt;</span><br><span class="line">      &lt;span&gt;</span><br><span class="line">       24小时预报</span><br><span class="line">      &lt;/span&gt;</span><br><span class="line">      &lt;ul class=&quot;nav&quot;&gt;</span><br><span class="line">       &lt;li class=&quot;active&quot;&gt;</span><br><span class="line">        温度</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        风力</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;charts clearfix&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;chart chart_temp clearfix&quot; id=&quot;chart_temp&quot;&gt;</span><br><span class="line">       &lt;div class=&quot;prev&quot;&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;next&quot;&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;num&quot;&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         30°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         20°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         10°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         0°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;canvas&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;canvas_box&quot;&gt;</span><br><span class="line">         &lt;canvas height=&quot;300&quot; id=&quot;temp&quot; width=&quot;4000&quot;&gt;</span><br><span class="line">         &lt;/canvas&gt;</span><br><span class="line">         &lt;div class=&quot;canvas_point&quot;&gt;</span><br><span class="line">          &lt;span&gt;</span><br><span class="line">          &lt;/span&gt;</span><br><span class="line">          &lt;div&gt;</span><br><span class="line">           &lt;em&gt;</span><br><span class="line">            29°</span><br><span class="line">           &lt;/em&gt;</span><br><span class="line">           &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">         &lt;/div&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">      &lt;div class=&quot;chart chart_wind clearfix&quot; id=&quot;chart_wind&quot; style=&quot;display: none;&quot;&gt;</span><br><span class="line">       &lt;div class=&quot;prev&quot;&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;next&quot;&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;num&quot;&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         30°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         20°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         10°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">        &lt;span&gt;</span><br><span class="line">         0°</span><br><span class="line">        &lt;/span&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">       &lt;div class=&quot;canvas&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;canvas_box&quot;&gt;</span><br><span class="line">         &lt;canvas height=&quot;300&quot; id=&quot;wind&quot; width=&quot;4000&quot;&gt;</span><br><span class="line">         &lt;/canvas&gt;</span><br><span class="line">         &lt;div class=&quot;canvas_point&quot;&gt;</span><br><span class="line">          &lt;span&gt;</span><br><span class="line">          &lt;/span&gt;</span><br><span class="line">          &lt;div&gt;</span><br><span class="line">           &lt;em&gt;</span><br><span class="line">            29°</span><br><span class="line">           &lt;/em&gt;</span><br><span class="line">           &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">         &lt;/div&gt;</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">       &lt;/div&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;!--生活指数--&gt;</span><br><span class="line">    &lt;div id=&quot;live_index&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;live_index_title&quot;&gt;</span><br><span class="line">      &lt;h2&gt;</span><br><span class="line">       生活指数</span><br><span class="line">      &lt;/h2&gt;</span><br><span class="line">      &lt;span&gt;</span><br><span class="line">      &lt;/span&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;live_index_grid&quot;&gt;</span><br><span class="line">      &lt;ul class=&quot;clearfix&quot;&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/2.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           适宜</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           旅游</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/cold/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/12.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           易发</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           感冒</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/fish/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/28.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           较适宜</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           钓鱼</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/makeup/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/7.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           控油</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           化妆</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/sport/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/26.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           不适宜</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           运动</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/5.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           很差</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           交通</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/car/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/17.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           较适宜</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           洗车</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/pollution/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/0.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           较差</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           空气污染扩散</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/dress/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/20.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           温凉</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           穿衣</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/uray/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/21.png&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;dl&gt;</span><br><span class="line">          &lt;dt&gt;</span><br><span class="line">           最弱</span><br><span class="line">          &lt;/dt&gt;</span><br><span class="line">          &lt;dd&gt;</span><br><span class="line">           紫外线</span><br><span class="line">          &lt;/dd&gt;</span><br><span class="line">         &lt;/dl&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">   &lt;div class=&quot;right&quot;&gt;</span><br><span class="line">    &lt;!--热门时景--&gt;</span><br><span class="line">    &lt;div class=&quot;liveview liveview_index&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;title&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       热门时景</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">       更多</span><br><span class="line">      &lt;/a&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81736828&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img alt=&quot;四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区&quot; data-height=&quot;1301&quot; data-width=&quot;1080&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/06/15072215250.83494900.1182_android.jpg&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;h2&gt;</span><br><span class="line">          四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区</span><br><span class="line">         &lt;/h2&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81749154&quot;&gt;</span><br><span class="line">         &lt;span&gt;</span><br><span class="line">          &lt;img alt=&quot;安徽省黄山市休宁县溪口镇詹家山&quot; data-height=&quot;720&quot; data-width=&quot;960&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/08/15074223300.94412000.1764_android.jpg&quot;/&gt;</span><br><span class="line">         &lt;/span&gt;</span><br><span class="line">         &lt;h2&gt;</span><br><span class="line">          安徽省黄山市休宁县溪口镇詹家山</span><br><span class="line">         &lt;/h2&gt;</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;near&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;title&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       附近地区</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;a href=&quot;https://tianqi.moji.com/nearcity/weather/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">       更多</span><br><span class="line">      &lt;/a&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;item clearfix&quot;&gt;</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuzhong-district&quot;&gt;</span><br><span class="line">         吴中区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/changshu&quot;&gt;</span><br><span class="line">         常熟市</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/zhangjiagang&quot;&gt;</span><br><span class="line">         张家港市</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/kunshan&quot;&gt;</span><br><span class="line">         昆山市</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wujiang-district&quot;&gt;</span><br><span class="line">         吴江区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/taicang&quot;&gt;</span><br><span class="line">         太仓市</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dongshan-town&quot;&gt;</span><br><span class="line">         东山镇</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuxian&quot;&gt;</span><br><span class="line">         吴县市（现吴中区、相城区）</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;near&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;title&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       附近景点</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;a href=&quot;https://tianqi.moji.com/nearscenic/weather/china/jiangsu/suzhou&quot;&gt;</span><br><span class="line">       更多</span><br><span class="line">      &lt;/a&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;item clearfix&quot;&gt;</span><br><span class="line">      &lt;ul&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dong-mountain-scenic-spot&quot;&gt;</span><br><span class="line">         东山景区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/huqiu-mountain-scenic-spot&quot;&gt;</span><br><span class="line">         虎丘山风景名胜区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/jinji-lake-scenic-spot&quot;&gt;</span><br><span class="line">         金鸡湖景区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-lingering-garden&quot;&gt;</span><br><span class="line">         留园</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/qionku-mountain-scenic-spot&quot;&gt;</span><br><span class="line">         穹窿山景区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/tianping-mountain-scenic-spot&quot;&gt;</span><br><span class="line">         天平山风景名胜区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wang-mountain-scenic-spot&quot;&gt;</span><br><span class="line">         旺山景区</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-humble-administrator&apos;s-garden&quot;&gt;</span><br><span class="line">         拙政园</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;div class=&quot;wrap clearfix calendar&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;g_title clearfix&quot;&gt;</span><br><span class="line">    &lt;span&gt;</span><br><span class="line">     天气日历</span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">    &lt;em&gt;</span><br><span class="line">     &lt;!--今天8:05更新--&gt;</span><br><span class="line">    &lt;/em&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">   &lt;div class=&quot;grid_title clearfix&quot;&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期日</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期一</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期二</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期三</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期四</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期五</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li&gt;</span><br><span class="line">      星期六</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">   &lt;div class=&quot;grid clearfix&quot; id=&quot;calendar_grid&quot;&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       01</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;晴                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w0.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       7/18°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       西南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item active&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       02</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;阴                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       9/16°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       03</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       10/19°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       04</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;阴                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       11/15°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       05</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       11/22°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       06</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       13/22°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       07</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       11/25°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       北风  3-4级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       08</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       9/15°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       09</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;雨                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       9/15°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东风  4-5级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       10</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;雨                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       9/18°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东北风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       11</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;雨                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       12/18°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  4-5级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       12</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;阴                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       13/23°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       13</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       13/24°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       14</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;雨                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       15/21°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东南风  4-5级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       15</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;雨                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       8/19°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       西北风  5-6级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       16</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;b&gt;</span><br><span class="line">       &lt;img alt=&quot;多云                                &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt;</span><br><span class="line">      &lt;/b&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       8/19°</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       东北风  3级</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       17</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       18</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       19</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       20</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       21</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       22</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       23</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       24</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       25</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       26</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       27</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       28</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       29</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">      &lt;em&gt;</span><br><span class="line">       30</span><br><span class="line">      &lt;/em&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">     &lt;li class=&quot;item&quot;&gt;</span><br><span class="line">     &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;!--新闻列表--&gt;</span><br><span class="line">  &lt;input id=&quot;staticdomain&quot; type=&quot;hidden&quot; value=&quot;https://h5tq.moji.com/tianqi&quot;/&gt;</span><br><span class="line">  &lt;input id=&quot;staticmd5&quot; type=&quot;hidden&quot; value=&quot;&quot;/&gt;</span><br><span class="line">  &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets/scripts/libs/jquery.min.js&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.charts.js&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.js&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;iframe frameborder=&quot;0&quot; height=&quot;0&quot; src=&quot;http://miniweb.cntv.cn/hezuo/mo.html&quot; style=&quot;display: none; overflow: hidden;&quot; width=&quot;0&quot;&gt;</span><br><span class="line">  &lt;/iframe&gt;</span><br><span class="line">  &lt;div class=&quot;foot_box clearfix&quot;&gt;</span><br><span class="line">   &lt;div class=&quot;foot clearfix&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;related_link&quot;&gt;</span><br><span class="line">     &lt;ul&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         今天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/today/china&quot;&gt;</span><br><span class="line">          今天省份表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu&quot;&gt;</span><br><span class="line">          今天城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         明天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china&quot;&gt;</span><br><span class="line">          明天省份表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu&quot;&gt;</span><br><span class="line">          明天城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         后天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/tdat/china&quot;&gt;</span><br><span class="line">          后天省份表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu&quot;&gt;</span><br><span class="line">          后天城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         7天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast7/china&quot;&gt;</span><br><span class="line">          7天预报省份表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu&quot;&gt;</span><br><span class="line">          7天预报城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         10天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast10/china&quot;&gt;</span><br><span class="line">          10天预报省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu&quot;&gt;</span><br><span class="line">          10天预报城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         15天预报</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast15/china&quot;&gt;</span><br><span class="line">          15天预报省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu&quot;&gt;</span><br><span class="line">          15天预报城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         空气指数</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/aqi/china&quot;&gt;</span><br><span class="line">          空气指数省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu&quot;&gt;</span><br><span class="line">          空气指数城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         pm2.5</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/pm/china&quot;&gt;</span><br><span class="line">          pm2.5省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/pm/china/jiangsu&quot;&gt;</span><br><span class="line">          pm2.5城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         污染指数</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/pollution/china&quot;&gt;</span><br><span class="line">          污染指数省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/pollution/china/jiangsu&quot;&gt;</span><br><span class="line">          污染指数城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">      &lt;li&gt;</span><br><span class="line">       &lt;ol&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         时景</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/liveview/china&quot;&gt;</span><br><span class="line">          时景省份列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li&gt;</span><br><span class="line">         &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu&quot;&gt;</span><br><span class="line">          时景城市列表</span><br><span class="line">         &lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">       &lt;/ol&gt;</span><br><span class="line">      &lt;/li&gt;</span><br><span class="line">     &lt;/ul&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;foot clearfix&quot;&gt;</span><br><span class="line">     &lt;div class=&quot;address&quot;&gt;</span><br><span class="line">      &lt;p&gt;</span><br><span class="line">       公司地址：北京市朝阳区酒仙桥路14号兆维华灯大厦A1区3门A216 联系电话：400-880-0599</span><br><span class="line">      &lt;/p&gt;</span><br><span class="line">      &lt;ul class=&quot;f_nav&quot;&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;http://www.moji.com/updata/android/&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">         升级日志</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;http://www.moji.com/faq/android/&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">         常见问题</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;http://designer.moji.com/signin&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">         设计师平台</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">       &lt;li&gt;</span><br><span class="line">        &lt;a href=&quot;http://www.moji.com/about/agreement/&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">         服务协议</span><br><span class="line">        &lt;/a&gt;</span><br><span class="line">       &lt;/li&gt;</span><br><span class="line">      &lt;/ul&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">     &lt;div class=&quot;copyright&quot;&gt;</span><br><span class="line">     &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">   &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">   var _hmt = _hmt || [];</span><br><span class="line">(function() &#123;</span><br><span class="line">      var hm = document.createElement(&quot;script&quot;);</span><br><span class="line">        hm.src = &quot;//hm.baidu.com/hm.js?49e9e3e54ae5bf8f8c637e11b3994c74&quot;;</span><br><span class="line">        var s = document.getElementsByTagName(&quot;script&quot;)[0]; </span><br><span class="line">          s.parentNode.insertBefore(hm, s);</span><br><span class="line">&#125;)();</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">   (function(i,s,o,g,r,a,m)&#123;i[&apos;GoogleAnalyticsObject&apos;]=r;i[r]=i[r]||function()&#123;</span><br><span class="line">        (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),</span><br><span class="line">              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)</span><br><span class="line">                &#125;)(window,document,&apos;script&apos;,&apos;//www.google-analytics.com/analytics.js&apos;,&apos;ga&apos;);</span><br><span class="line"></span><br><span class="line">  ga(&apos;create&apos;, &apos;UA-49812585-12&apos;, &apos;auto&apos;);</span><br><span class="line">  ga(&apos;send&apos;, &apos;pageview&apos;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">   //自动推送</span><br><span class="line">(function()&#123;</span><br><span class="line">    var bp = document.createElement(&apos;script&apos;);</span><br><span class="line">    var curProtocol = window.location.protocol.split(&apos;:&apos;)[0];</span><br><span class="line">    if (curProtocol === &apos;https&apos;) &#123;</span><br><span class="line">        bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;;        </span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">    var s = document.getElementsByTagName(&quot;script&quot;)[0];</span><br><span class="line">    s.parentNode.insertBefore(bp, s);</span><br><span class="line">&#125;)();</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">   (function()&#123;</span><br><span class="line">    //手机抖动动画</span><br><span class="line">    function phoneAnimate() &#123;</span><br><span class="line">        $(&quot;#head_shake&quot;).addClass(&quot;shake&quot;);</span><br><span class="line">        var phone = setTimeout(&apos;$(&quot;#head_shake&quot;).removeClass(&quot;shake&quot;)&apos;,3000);</span><br><span class="line">    &#125;</span><br><span class="line">    var d = new Date();</span><br><span class="line">    var nowYear = d.getFullYear();</span><br><span class="line"></span><br><span class="line">    var html = &quot;Copyright© 2009-&quot;+nowYear+&quot; 北京墨迹风云科技股份有限公司 All Rights Reserved&lt;br /&gt;京ICP备10021324号  京公网安备11010502023583&lt;br /&gt; 客服服务热线：400-880-0599 违法和不良信息举报电话：400-880-0599 举报邮箱：AS@moji.com&quot;;</span><br><span class="line">    $(&quot;.copyright&quot;).html(html);</span><br><span class="line">    phoneAnimate();</span><br><span class="line">&#125;)();</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"> &lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python3</tag>
      
      <tag>BeautifulSoup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Python3爬虫】常见反爬虫措施及解决办法</title>
    <link href="/2019/03/31/%E3%80%90Python3%E7%88%AC%E8%99%AB%E3%80%91%E5%B8%B8%E8%A7%81%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8E%AA%E6%96%BD%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <url>/2019/03/31/%E3%80%90Python3%E7%88%AC%E8%99%AB%E3%80%91%E5%B8%B8%E8%A7%81%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8E%AA%E6%96%BD%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h3 id="一、UserAgent"><a href="#一、UserAgent" class="headerlink" title="一、UserAgent"></a>一、UserAgent</h3><p>UserAgent中文名为用户代理，它使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本等信息。对于一些网站来说，它会检查我们发送的请求中所携带的UserAgent字段，如果非浏览器，就会被识别为爬虫，一旦被识别出来， 我们的爬虫也就无法正常爬取数据了。这里先看一下在不设置UserAgent字段时该字段的值会是什么：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;http://www.baidu.com&quot;</span><br><span class="line">res = requests.get(url)</span><br></pre></td></tr></table></figure></p><h4 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h4><p>1、收集整理常见的UserAgent以供使用<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ua_list = </span><br><span class="line">    [&quot;Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;,</span><br><span class="line">    &quot;Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;,</span><br><span class="line">    &quot;MQQBrowser/25 (Linux; U; 2.3.3; zh-cn; HTC Desire S Build/GRI40;480*800)&quot;,</span><br><span class="line">    &quot;Mozilla/5.0 (Linux; U; Android 2.3.3; zh-cn; HTC_DesireS_S510e Build/GRI40) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1&quot;,</span><br><span class="line">    &quot;Mozilla/5.0 (SymbianOS/9.3; U; Series60/3.2 NokiaE75-1 /110.48.125 Profile/MIDP-2.1 Configuration/CLDC-1.1 ) AppleWebKit/413 (KHTML, like Gecko) Safari/413&quot; ...]</span><br></pre></td></tr></table></figure></p><p>2、使用第三方库–fake_useragent<br>使用方法如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from fake_useragent import UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()</span><br><span class="line">for i in range(3):</span><br><span class="line">    print(ua.random)</span><br></pre></td></tr></table></figure></p><p>剖析：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(ua.ie)   #随机打印ie浏览器任意版本</span><br><span class="line">print(ua.firefox) #随机打印firefox浏览器任意版本</span><br><span class="line">print(ua.chrome)  #随机打印chrome浏览器任意版本</span><br><span class="line">print(ua.random)  #随机打印任意厂家的浏览器</span><br></pre></td></tr></table></figure></p><h3 id="二、IP"><a href="#二、IP" class="headerlink" title="二、IP"></a>二、IP</h3><p>对于一些网站来说，如果某个IP在单位时间里的访问次数超过了某个阈值，那么服务器就会ban掉这个IP了，它就会返回给你一些错误的数据。一般来说，当我们的IP被ban了，我们的爬虫也就无法正常获取数据了，但是用浏览器还是可以正常访问，但是如果用浏览器都无法访问，那就真的GG了。很多网站都会对IP进行检测，比如知乎，如果单个IP访问频率过高就会被封掉。</p><h4 id="解决办法：-1"><a href="#解决办法：-1" class="headerlink" title="解决办法："></a>解决办法：</h4><p>使用代理IP。网上有很多免费代理和付费代理可供选择，免费代理比如：西刺代理、快代理等等，付费代理比如：代理云、阿布云等等。</p><h3 id="三、Referer防盗链"><a href="#三、Referer防盗链" class="headerlink" title="三、Referer防盗链"></a>三、Referer防盗链</h3><p>防盗链主要是针对客户端请求过程中所携带的一些关键信息来验证请求的合法性，而防盗链又有很多种，比如Referer防盗链，还有Cookie防盗链和时间戳防盗链。</p><p>Cookie防盗链常见于论坛、社区。当访客请求一个资源的时候，他会检查这个访客的Cookie，如果不是他自己的用户的Cookie，就不会给这个访客正确的资源，也就达到了防盗的目的。时间戳防盗链指的是在他的url后面加上一个时间戳参数，所以如果你直接请求网站的url是无法得到真实的页面的，只有带上时间戳才可以。</p><p>这次的例子是<a href="http://pp.tianya.cn/" target="_blank" rel="noopener">天涯社区的图片分社区</a>：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/tRIXfNxTqme0.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这里我们先打开开发者工具，然后任意选择一张图片，得到这个图片的链接，然后用requests来下载一下这张图片，注意带上Referer字段，看结果如何：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;http://img3.laibafile.cn/p/l/305989961.jpg&quot;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;,</span><br><span class="line">    &quot;UserAgent&quot;:&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36&quot;</span><br><span class="line">&#125;</span><br><span class="line">res = requests.get(url)</span><br><span class="line">with open(&apos;test.jpg&apos;, &apos;wb&apos;) as f:</span><br><span class="line">    f.write(res.content)</span><br></pre></td></tr></table></figure></p><p>我们的爬虫正常运行了，也看到生成了一个test.jpg文件，先别急着高兴，打开图片看一下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/ign0DACblB5o.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="解决办法：-2"><a href="#解决办法：-2" class="headerlink" title="解决办法："></a>解决办法：</h4><p>既然他说仅供天涯社区用户分享，那我们也成为他的用户不就行了吗？二话不说就去注册了个账号，然后登录，再拿到登录后的Cookie：</p><p>注意：Cookie是有时效性的，具体多久就会失效我没测试。紧接着把Cookie添加到代码中，然后运行，可以看到成功把图片下载下来了；</p><p>搞了这么久才下了一张图片，我们怎么可能就这么满足呢？分析页面可知一个页面上有十五张图片，然后往下拉的时候会看到”正在加载，请稍后”：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/MVyHb54OJ3I7.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>我们立马反应过来这是通过AJAX来加载的，于是打开开发者工具查看，可以找到如下内容：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/lxdyXKrzwVl7.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>可以看到每个链接“？”前面的部分都是基本一样的，“list_”后面跟的数字表示页数，而“_=”后面这一串数字是什么呢？有经验的人很快就能意识到这是一个时间戳，所以我们来测试一下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import time</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">t = time.time()*1000</span><br><span class="line">url = &quot;http://pp.tianya.cn/qt/list_4.shtml?_=&#123;&#125;&quot;.format(t)</span><br><span class="line">res = requests.get(url)</span><br><span class="line">print(res.text)</span><br></pre></td></tr></table></figure></p><p>最后编写程序并运行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">import time</span><br><span class="line">import requests</span><br><span class="line">from fake_useragent import UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;,</span><br><span class="line">    &quot;Cookie&quot;: &quot;user=w=ASD9577&amp;id=139400111&amp;f=1; right=web4=n&amp;portal=n; __u_a=v2.2.4; sso=r=2037194054&amp;sid=&amp;wsid=54DECCFD58BB393C19060A02D963FFFC; temp=k=32072170&amp;s=&amp;t=1553817544&amp;b=bdacde9b28f59113991fd271bdba3f65&amp;ct=1553817544&amp;et=1556409544; temp4=rm=0e74c53c8e0cfe3ec18596583e42c38f; ty_msg=1553817664458_139400111_2_0_0_0_0_0_2_0_0_0_0_0; bbs_msg=1553817664463_139400111_0_0_0_0; time=ct=1553817677.647&quot;,</span><br><span class="line">    &quot;UserAgent&quot;: ua.random</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def crawl(url):</span><br><span class="line">    res = requests.get(url, headers=headers)</span><br><span class="line">    res.encoding = &quot;utf-8&quot;</span><br><span class="line">    print(&quot;status_code:&quot;, res.status_code)</span><br><span class="line">    # print(&quot;res.text:&quot;, res.text)</span><br><span class="line">    if res.status_code == 200:</span><br><span class="line">        # result = re.findall(r&apos;src=&quot;.*?&quot; alt=&quot;.*?&quot;&apos;, res.text)</span><br><span class="line">        result = re.findall(&apos;&lt;img.+src=&quot;(.*?)&quot; alt=&quot;(.*?)&quot;.+&gt;&apos;, res.text)</span><br><span class="line">        print(result)</span><br><span class="line">        for i in result:</span><br><span class="line">            # print(&quot;iiiiiiiiiii&quot;, i[0], i[1])</span><br><span class="line">            download(i[0], i[1])</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Error Request!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download(href, name):</span><br><span class="line">    try:</span><br><span class="line">        res = requests.get(href, headers=headers)</span><br><span class="line">        with open(&apos;&#123;&#125;.jpg&apos;.format(name), &apos;wb&apos;) as f:</span><br><span class="line">            f.write(res.content)</span><br><span class="line">            print(&apos;[INFO]&#123;&#125;.jpg已下载！&apos;.format(name))</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;Error Download!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    for num in range(1, 3):  # 最大页数2页</span><br><span class="line">        time.sleep(2)</span><br><span class="line">        t = int(time.time() * 1000)  # 获取13位时间戳</span><br><span class="line">        print(t)</span><br><span class="line">        page_url = &quot;http://pp.tianya.cn/qt/list_&#123;&#125;.shtml?_=&#123;&#125;&quot;.format(num, t)  # 构造链接</span><br><span class="line">        crawl(page_url)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>urllib.parse.urlencode转换get请求参数</title>
    <link href="/2019/03/22/urllib.parse.urlencode%E8%BD%AC%E6%8D%A2get%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0/"/>
    <url>/2019/03/22/urllib.parse.urlencode%E8%BD%AC%E6%8D%A2get%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>浏览器地址栏搜索 刘若英<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https://www.baidu.com/s?word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8</span><br></pre></td></tr></table></figure></p><p>但是复制到文件中是这样的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https://www.baidu.com/s?word=%E5%88%98%E8%8B%A5%E8%8B%B1&amp;tn=71069079_1_hao_pg&amp;ie=utf-8</span><br></pre></td></tr></table></figure></p><p>这是因为浏览器对中文请求参数进行了转码<br>用代码访问网站所发的请求中如果有中文也必须是转码之后的。这里需要用到<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlencode</span><br></pre></td></tr></table></figure></p><p> 方法。<br>这个方法的作用就是将字典里面所有的键值转化为query-string格式（key=value&amp;key=value），并且将中文转码。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">url = &apos;http://www.baidu.com/s?&apos;</span><br><span class="line"></span><br><span class="line">wd = input(&apos;请输入要搜索关键字： &apos;)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;word&apos;: wd,</span><br><span class="line">    &apos;tn&apos;: &apos;71069079_1_hao_pg&apos;,</span><br><span class="line">    &apos;ie&apos;: &apos;utf-8&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">query_string = urllib.parse.urlencode(data)</span><br><span class="line"># 拼接获取完整url</span><br><span class="line">url += query_string</span><br><span class="line"># 发起请求，获取响应</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line"></span><br><span class="line">filename = wd + &apos;.html&apos;</span><br><span class="line"></span><br><span class="line">dirname = &apos;./html&apos;</span><br><span class="line"></span><br><span class="line">if not os.path.exists(dirname):</span><br><span class="line">    os.mkdir(dirname)</span><br><span class="line"></span><br><span class="line">filepath = dirname + &apos;/&apos; + filename</span><br><span class="line"></span><br><span class="line"># 以二进制写入文件</span><br><span class="line"># with open(filepath, &apos;wb&apos;) as fp:</span><br><span class="line">#   fp.write(response.read())</span><br><span class="line"></span><br><span class="line"># 或者以utf8编码写入文件</span><br><span class="line">with open (filepath, &apos;w&apos;, encoding=&apos;utf8&apos;) as fp:</span><br><span class="line">    fp.write(response.read().decode(&apos;utf8&apos;))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-dashboard仪表盘（十二）</title>
    <link href="/2019/03/15/12%E3%80%81dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98/"/>
    <url>/2019/03/15/12%E3%80%81dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<p>对于运维管理平台，一个总览的dashboard仪表盘界面是必须有的，不但提升整体格调，也有利于向老板‘邀功请赏’。</p><p>dashboard页面必须酷炫吊炸天，所以界面元素应当美观、丰富、富有冲击力。</p><p>完整的dashboard.html文件代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dashboard.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html</span><br></pre></td></tr></table></figure></p><h3 id="一、资产状态占比图"><a href="#一、资产状态占比图" class="headerlink" title="一、资产状态占比图"></a>一、资产状态占比图</h3><p>首先，制作一个资产状态百分比表盘，用于显示上线、下线、未知、故障和备用五种资产在总资产中的占比。注意是占比，不是数量！</p><p>按照AdminLTE中提供的示例，在HTML中添加相应的标签，在script中添加相应的JS代码（jQueryKnob）。JS代码基本照抄，不需要改动。对于显示的圆圈，可以修改其颜色、大小、形态、是否只读等属性，可以参照AdminLTE中的范例。</p><p>最重要的是，需要从数据库中获取相应的数据，修改assets/views.py中的dashboard视图，最终如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def dashboard(request):</span><br><span class="line">    total = models.Asset.objects.count()</span><br><span class="line">    upline = models.Asset.objects.filter(status=0).count()</span><br><span class="line">    offline = models.Asset.objects.filter(status=1).count()</span><br><span class="line">    unknown = models.Asset.objects.filter(status=2).count()</span><br><span class="line">    breakdown = models.Asset.objects.filter(status=3).count()</span><br><span class="line">    backup = models.Asset.objects.filter(status=4).count()</span><br><span class="line">    up_rate = round(upline/total*100)</span><br><span class="line">    o_rate = round(offline/total*100)</span><br><span class="line">    un_rate = round(unknown/total*100)</span><br><span class="line">    bd_rate = round(breakdown/total*100)</span><br><span class="line">    bu_rate = round(backup/total*100)</span><br><span class="line">    server_number = models.Server.objects.count()</span><br><span class="line">    networkdevice_number = models.NetworkDevice.objects.count()</span><br><span class="line">    storagedevice_number = models.StorageDevice.objects.count()</span><br><span class="line">    securitydevice_number = models.SecurityDevice.objects.count()</span><br><span class="line">    software_number = models.Software.objects.count()</span><br><span class="line"></span><br><span class="line">    return render(request, &apos;assets/dashboard.html&apos;, locals())</span><br></pre></td></tr></table></figure></p><p>代码很简单，分别获取资产总数量，上线、下线、未知、故障和备用资产的数量，然后计算出各自的占比，例如上线率up_rate。同时获取服务器、网络设备、安全设备和软件设备的数量，后面需要使用。</p><p>在dashboard.html中修改各input框的value属性为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">value=&quot;&#123;&#123; up_rate &#125;&#125;&quot;</span><br></pre></td></tr></table></figure></p><p>（以上线率为例），这是最关键的步骤，前端会根据这个值的大小，决定圆圈的幅度。</p><p>完成后的页面如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190401/04T3X8UVz2tC.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="二、不同状态资产数量统计柱状图"><a href="#二、不同状态资产数量统计柱状图" class="headerlink" title="二、不同状态资产数量统计柱状图"></a>二、不同状态资产数量统计柱状图</h3><p>要绘制柱状图，不可能我们自己一步步从无到有写起，建议使用第三方插件。AdminLTE中内置的是Chartjs插件，但更建议大家使用百度开源的Echarts插件，功能更强大，更容易学习。</p><p><a href="http://echarts.baidu.com/" target="_blank" rel="noopener">百度Echarts的网址</a>，提供插件下载和说明文档、在线帮助等功能。</p><p>教程提供了一个echarts.js源文件，当然你也可以自行下载并安装。</p><p>使用Echarts的柱状图很简单，首先生成一个用于放置图形的容器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;col-md-6&quot;&gt;</span><br><span class="line">    &lt;!-- BAR CHART --&gt;</span><br><span class="line">  &lt;div class=&quot;box box-success&quot;&gt;</span><br><span class="line"></span><br><span class="line">    &lt;div class=&quot;box-header with-border&quot;&gt;</span><br><span class="line">      &lt;h3 class=&quot;box-title&quot;&gt;各状态资产数量统计：&lt;/h3&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div class=&quot;box-tools pull-right&quot;&gt;</span><br><span class="line">        &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;collapse&quot;&gt;&lt;i class=&quot;fa fa-minus&quot;&gt;&lt;/i&gt;</span><br><span class="line">        &lt;/button&gt;</span><br><span class="line">        &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;remove&quot;&gt;&lt;i class=&quot;fa fa-times&quot;&gt;&lt;/i&gt;&lt;/button&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;div class=&quot;box-body&quot;&gt;</span><br><span class="line">        &lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;!-- /.box-body --&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure></p><p>上面的核心是<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure></p><p>这句，它指明了图表的id和容器大小。其它的都是AdminLTE框架需要的元素，用于生成表头和折叠、关闭动作按钮。我们的容器是可以折叠和删除的，也是移动端自适应的。</p><p>构造了容器后，需要在页面底部首先引入 <strong>echarts.js</strong> 文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src=&quot;&#123;% static &apos;plugins/echarts.js&apos; %&#125;&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>然后在<script></script>中，添加初始化的js代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$(function () &#123;</span><br><span class="line">        // 基于准备好的dom，初始化echarts实例</span><br><span class="line">        var myChart = echarts.init(document.getElementById(&apos;barChart&apos;));</span><br><span class="line"></span><br><span class="line">        // 指定图表的配置项和数据</span><br><span class="line">        var option = &#123;</span><br><span class="line">            color: [&apos;#3398DB&apos;],</span><br><span class="line">            title: &#123;</span><br><span class="line">                text: &apos;数量&apos;</span><br><span class="line">            &#125;,</span><br><span class="line">            tooltip: &#123;&#125;,</span><br><span class="line">            legend: &#123; data:[&apos;&apos;]</span><br><span class="line">            &#125;,</span><br><span class="line">            xAxis: &#123;</span><br><span class="line">                data: [&quot;在线&quot;, &quot;下线&quot;,&quot;故障&quot;,&quot;备用&quot;,&quot;未知&quot;] &#125;,</span><br><span class="line">            yAxis: &#123;&#125;,</span><br><span class="line">            series:</span><br><span class="line">                [&#123;</span><br><span class="line">                name: &apos;数量&apos;,</span><br><span class="line">                type: &apos;bar&apos;,</span><br><span class="line">                barWidth: &apos;50%&apos;,</span><br><span class="line">                data: [&#123;&#123; upline &#125;&#125;, &#123;&#123; offline &#125;&#125;, &#123;&#123; breakdown &#125;&#125;, &#123;&#123; backup &#125;&#125;, &#123;&#123; unknown &#125;&#125;]</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;;</span><br><span class="line">            // 使用刚指定的配置项和数据显示图表。</span><br><span class="line">            myChart.setOption(option);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure></p><p>上面的js代码中，中文文字部分很容易理解，就是x轴的说明文字。还可以设置柱状图的颜色、宽度等特性。关键是series列表，其中的type指定该charts是什么类型，bar表示柱状图，而data就是至关重要的具体数据了，利用模板语言，将从数据库中获取的具体数值传入进来，Echarts插件会根据数值进行动态调整。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190401/L8z1yQC8Dvb2.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="三、各类型资产数量统计饼图"><a href="#三、各类型资产数量统计饼图" class="headerlink" title="三、各类型资产数量统计饼图"></a>三、各类型资产数量统计饼图</h3><p>类似上面的柱状图，在HTML中需要先添加一个容器。不同之处在于初始化的JS代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//资产类型数量统计 饼图</span><br><span class="line">    $(function () &#123;</span><br><span class="line">        // 基于准备好的dom，初始化echarts实例</span><br><span class="line">        var myChart = echarts.init(document.getElementById(&apos;donutChart&apos;));</span><br><span class="line"></span><br><span class="line">        // 指定图表的配置项和数据</span><br><span class="line">        option = &#123;</span><br><span class="line">            title : &#123;</span><br><span class="line">                x:&apos;center&apos;</span><br><span class="line">            &#125;,</span><br><span class="line">            tooltip : &#123;</span><br><span class="line">                trigger: &apos;item&apos;,</span><br><span class="line">                formatter: &quot;&#123;a&#125; &lt;br/&gt;&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            legend: &#123;</span><br><span class="line">                orient: &apos;vertical&apos;,</span><br><span class="line">                left: &apos;left&apos;,</span><br><span class="line">                data: [&apos;服务器&apos;,&apos;网络设备&apos;,&apos;存储设备&apos;,&apos;安全设备&apos;,&apos;软件资产&apos;]</span><br><span class="line">            &#125;,</span><br><span class="line">            series : [</span><br><span class="line">                &#123;</span><br><span class="line">                    name: &apos;资产类型&apos;,</span><br><span class="line">                    type: &apos;pie&apos;,</span><br><span class="line">                    radius : &apos;55%&apos;,</span><br><span class="line">                    center: [&apos;50%&apos;, &apos;60%&apos;],</span><br><span class="line">                    data:[</span><br><span class="line">                        &#123;value:&#123;&#123; server_number &#125;&#125;, name:&apos;服务器&apos;&#125;,</span><br><span class="line">                        &#123;value:&#123;&#123; networkdevice_number &#125;&#125;, name:&apos;网络设备&apos;&#125;,</span><br><span class="line">                        &#123;value:&#123;&#123; storagedevice_number &#125;&#125;, name:&apos;存储设备&apos;&#125;,</span><br><span class="line">                        &#123;value:&#123;&#123; securitydevice_number &#125;&#125;, name:&apos;安全设备&apos;&#125;,</span><br><span class="line">                        &#123;value:&#123;&#123; software_number &#125;&#125;, name:&apos;软件资产&apos;&#125;</span><br><span class="line">                    ],</span><br><span class="line">                    itemStyle: &#123;</span><br><span class="line">                        emphasis: &#123;</span><br><span class="line">                            shadowBlur: 10,</span><br><span class="line">                            shadowOffsetX: 0,</span><br><span class="line">                            shadowColor: &apos;rgba(0, 0, 0, 0.5)&apos;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;;</span><br><span class="line">            // 使用刚指定的配置项和数据显示图表。</span><br><span class="line">            myChart.setOption(option);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure></p><p>series中的type指定为pie类型表示饼图，data列表动态传入各种资产类型的数量。其它的设置可参考官方文档。</p><p>为了展示的方便，我们在admin中新建一些网络设备、安全设备、软件资产等其它类型的资产，然后查看资产总表和饼图。这里我分别添加了一台网络、安全和存储设备和两个软件资产。</p><p>查看资产总表如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190401/DqvoWq85pJAV.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>查看dashboard如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190401/HCpEAE7eqItv.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="四、项目总结"><a href="#四、项目总结" class="headerlink" title="四、项目总结"></a>四、项目总结</h3><p>至此，CMDB项目就基本讲解完毕。</p><p>还是要强调的是，这是一个demo版都不能算的教学版，很多内容和细节没有实现，必然存在bug和不足。但不管怎么样，它至少包含CMDB资产管理的主体内容，如果你能从中有点收获，那么教程的目的就达到了。</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-资产详细页面（十一）</title>
    <link href="/2019/03/14/11%E3%80%81%E8%B5%84%E4%BA%A7%E8%AF%A6%E7%BB%86%E9%A1%B5%E9%9D%A2/"/>
    <url>/2019/03/14/11%E3%80%81%E8%B5%84%E4%BA%A7%E8%AF%A6%E7%BB%86%E9%A1%B5%E9%9D%A2/</url>
    
    <content type="html"><![CDATA[<p>在资产的详细页面，我们将尽可能地将所有的信息都显示出来，并保持美观、整齐。</p><p>教程中实现了主要的服务器资产页面，对于其它类型的资产详细页面，可参照完成，并不复杂。</p><p>完整的detail.html页面代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">detail.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html</span><br></pre></td></tr></table></figure></p><p>主要代码全部集中在：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;section class=&quot;content&quot;&gt;</span><br></pre></td></tr></table></figure></p><p>分别用几个表格将概览、服务器、CPU、内存、硬盘和网卡的信息展示出来了。并且，AdminLTE为我们提供了一个折叠的功能，也是非常酷的。</p><p>这个HTML文件没有太多需要额外解释的内容，都是一些很基础的模板语言，构造<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;table&gt;</span><br></pre></td></tr></table></figure></p><p>然后插入数据。如果没有数据，就以‘N/A’代替。最后在底部添加一个返回资产总表的链接。</p><p>下面是一个展示图：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190401/uUQqPfLf2qQ8.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-资产总表（十）</title>
    <link href="/2019/03/13/10%E3%80%81%E8%B5%84%E4%BA%A7%E6%80%BB%E8%A1%A8/"/>
    <url>/2019/03/13/10%E3%80%81%E8%B5%84%E4%BA%A7%E6%80%BB%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<p>当前，我们的资产总表如下图所示，还没有任何数据：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/qovlrK6JVV9c.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这需要我们从数据库中查询数据，然后渲染到前端页面中。</p><p>数据的获取很简单，一句：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assets = models.Asset.objects.all()</span><br></pre></td></tr></table></figure></p><p>就搞定。当然，你也可以设置过滤条件，添加分页等等。</p><p>而在前端，我们往往需要以表格的形式，规整、美观、可排序的展示出来。这里推荐一个前端插件 <strong>datatables</strong>，是一个非常好的表格插件，功能强大、配置简单。</p><p>其官网为：<a href="https://datatables.net/" target="_blank" rel="noopener">https://datatables.net/</a> 中文网站：<a href="http://datatables.club/" target="_blank" rel="noopener">http://datatables.club/</a></p><p>在AdminLTE中，集成了datatables插件，无需额外下载和安装，直接引入使用就可以。</p><p>下面给出一个完整的index.html模板代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">index.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html</span><br></pre></td></tr></table></figure></p><p>主要是新增了表格相关的html代码和初始化表格的js代码。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;table id=&quot;assets_table&quot; class=&quot;table table-bordered table-striped&quot;&gt;</span><br></pre></td></tr></table></figure><p>id属性非常重要，用于关联相应的初始化js代码。</p><p>表格中，循环每一个资产：</p><ul><li>首先生成一个排序的列；</li><li>再根据资产类型的不同，用不同的颜色生成不同的资产类型名和子类型名；</li><li>通过asset.get_asset_type_display的模板语法，拿到资产类型的直观名称，比如‘服务器’，而不是显示呆板的‘server’；</li><li>通过asset.server.get_sub_asset_type_display，获取资产对应类型的子类型。这是Django特有的模板语法，非常类似其ORM的语法；</li><li>在资产名的栏目，增加了超级链接，用于显示资产的详细内容。这里只实现了服务器类型资产的详细页面，其它类型请自行完善；</li><li>根据资产状态的不同，用不同的颜色显示；</li><li>利用asset.m_time|date:”Y/m/d [H:m:s]”调整时间的显示格式；</li><li>由于资产和tas标签属于多对多的关系，所以需要一个循环，遍历每个tas并打印其名称；</li><li>通过asset.tags.all可以获取一个资产对应的多对多字段的全部对象，很类似ORM的做法。</li></ul><p>表格的初始化JS代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">  $(function () &#123;</span><br><span class="line">    $(&apos;#assets_table&apos;).DataTable(&#123;</span><br><span class="line">      &quot;paging&quot;: true,       &lt;!-- 允许分页 --&gt;</span><br><span class="line">      &quot;lengthChange&quot;: true, &lt;!-- 允许改变每页显示的行数 --&gt;</span><br><span class="line">      &quot;searching&quot;: true,    &lt;!-- 允许内容搜索 --&gt;</span><br><span class="line">      &quot;ordering&quot;: true,     &lt;!-- 允许排序 --&gt;</span><br><span class="line">      &quot;info&quot;: true,         &lt;!-- 显示信息 --&gt;</span><br><span class="line">      &quot;autoWidth&quot;: false    &lt;!-- 固定宽度 --&gt;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><p>其中可定义是否允许分页、改变显示的行数、搜索、排序、显示信息、固定宽度等等，通过表格的id进行关联。</p><p>下面，我们通过后台admin界面，多增加几个服务器实例，并修改其类型、业务线、状态、厂商、机房、标签，再刷新资产总表，可以看到效果如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190331/m7x7woCSatFa.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>试着使用一下排序和搜索功能吧！datatables还是相当强大的！</p><p>现在点击资产名称，可以链接到资产详细页面，但没有任何数据显示，在下一节中，我们来实现它。</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-前端框架AdminLTE（九）</title>
    <link href="/2019/03/12/9%E3%80%81%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6AdminLTE/"/>
    <url>/2019/03/12/9%E3%80%81%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6AdminLTE/</url>
    
    <content type="html"><![CDATA[<p>作为CMDB资产管理项目，必须有一个丰富、直观、酷炫的前端页面。</p><p>适合运维平台的前端框架有很多，开源的也不少，这里选用的是AdminLTE。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190323/JELdwGVS3gkz.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>AdminLTE托管在GitHub上，可以通过下面的地址下载：</p><p><a href="https://github.com/almasaeed2010/AdminLTE/releases" target="_blank" rel="noopener">https://github.com/almasaeed2010/AdminLTE/releases</a></p><p>AdminLTE自带JQuery和Bootstrap3框架，无需另外下载。</p><p>AdminLTE自带多种配色皮肤，可根据需要实时调整。</p><p>AdminLTE是移动端自适应的，无需单独考虑。</p><p>AdminLTE自带大量插件，比如表格、Charts等等，可根据需要载入。</p><p>但是AdminLTE的源文件包内，缺少font-awesome-4.6.3和ionicons-2.0.1这两个图标插件，它是通过CDN的形式加载的，如果网络不太好，加载可能比较困难或者缓慢，最好用本地静态文件的形式。教程在Github的包内附带上了这两个插件，可以直接使用，当然你自己下载安装也行。</p><h3 id="一、创建base-html"><a href="#一、创建base-html" class="headerlink" title="一、创建base.html"></a>一、创建base.html</h3><p>AdminLTE源文件包里有个index.html页面文件，可以利用它修改出我们CMDB项目需要的基本框架。</p><p>在项目的根目录cmdb下新建static目录，在settings文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">STATIC_URL = &apos;/static/&apos;</span><br><span class="line"></span><br><span class="line">STATICFILES_DIRS = [</span><br><span class="line">    os.path.join(BASE_DIR, &quot;mycmdb/static&quot;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>为了以后扩展的方便，将AdminLTE源文件包里的 <strong>bootstrap</strong>、<strong>dist</strong> 和 <strong>plugins</strong> 三个文件夹，全部拷贝到 <strong>static</strong> 目录中，这样做的话文件会比较大，比较多，但可以防止出现引用文件找不到、插件缺失等情况的发生，等以后对AdminLTE非常熟悉了，可以对static中无用的文件进行删减。</p><p>在cmdb根目录下的templates目录下，新建<strong>base.html</strong>文件，将AdminLTE源文件包中的index.html中的内容拷贝过去。然后，根据我们项目的具体情况修改文件引用、页面框架、title、CSS、主体和script块。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">base.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/base.html</span><br></pre></td></tr></table></figure><p>这是一个适合当前CMDB的精简版本。</p><h3 id="二、创建路由、视图"><a href="#二、创建路由、视图" class="headerlink" title="二、创建路由、视图"></a>二、创建路由、视图</h3><p>这里设计了三个视图和页面，分别是：</p><ul><li><strong>dashboard</strong>：仪表盘，图形化的数据展示</li><li><strong>index</strong>：资产总表，表格的形式展示资产信息</li><li><strong>detail</strong>：单个资产的详细信息页面</li></ul><p>将assets/urls.py修改成下面的样子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url</span><br><span class="line">from assets import views</span><br><span class="line"></span><br><span class="line">app_name = &apos;assets&apos;</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;),</span><br><span class="line">    url(r&apos;^dashboard/&apos;, views.dashboard, name=&apos;dashboard&apos;),</span><br><span class="line">    url(r&apos;^index/&apos;, views.index, name=&apos;index&apos;),</span><br><span class="line">    url(r&apos;^detail/(?P&lt;asset_id&gt;[0-9]+)/$&apos;, views.detail, name=&quot;detail&quot;),</span><br><span class="line">    url(r&apos;^$&apos;, views.dashboard),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>在 <strong>assets/views.py</strong> 中，增加下面三个视图：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import get_object_or_404</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line"></span><br><span class="line">    assets = models.Asset.objects.all()</span><br><span class="line">    return render(request, &apos;assets/index.html&apos;, locals())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dashboard(request):</span><br><span class="line">    pass</span><br><span class="line">    return render(request, &apos;assets/dashboard.html&apos;, locals())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def detail(request, asset_id):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    以显示服务器类型资产详细为例，安全设备、存储设备、网络设备等参照此例。</span><br><span class="line">    :param request:</span><br><span class="line">    :param asset_id:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    asset = get_object_or_404(models.Asset, id=asset_id)</span><br><span class="line">    return render(request, &apos;assets/detail.html&apos;, locals())</span><br></pre></td></tr></table></figure></p><p>注意需要提前<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import get_object_or_404</span><br></pre></td></tr></table></figure></p><p>导入get_object_or_404()方法，这是一个非常常用的内置方法。</p><h3 id="三、创建模版"><a href="#三、创建模版" class="headerlink" title="三、创建模版"></a>三、创建模版</h3><h4 id="1-dashboard-html"><a href="#1-dashboard-html" class="headerlink" title="1.dashboard.html"></a>1.dashboard.html</h4><p>在assets目录下创建 <strong>templates/assets/dashboard.html</strong> 文件，写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dashboard.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html</span><br></pre></td></tr></table></figure></p><h4 id="2-index-html"><a href="#2-index-html" class="headerlink" title="2.index.html"></a>2.index.html</h4><p>在assets目录下创建 <strong>templates/assets/index.html</strong> 文件，写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">index.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html</span><br></pre></td></tr></table></figure></p><h4 id="3-detail-html"><a href="#3-detail-html" class="headerlink" title="3.detail.html"></a>3.detail.html</h4><p>在assets目录下创建 <strong>templates/assets/detail.html</strong> 文件，写入下面的代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">detail.html</span><br><span class="line">https://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html</span><br></pre></td></tr></table></figure><p>以上三个模板都很简单，就是下面的流程：</p><ul><li>extends继承‘base.html’；</li><li>load staticfiles：载入静态文件；</li><li>block title：资产详细endblock，定制title;</li><li>block css：载入当前页面的专用CSS文件；</li><li>block script：载入当前页面的专用js文件；</li><li>最后在block content：中，编写一个当前页面的面包屑导航；</li><li>页面的主体内容在后面的章节进行充实。</li></ul><h3 id="四、访问页面"><a href="#四、访问页面" class="headerlink" title="四、访问页面"></a>四、访问页面</h3><p>重启CMDB服务器，访问<a href="http://192.168.1.3:8000/assets/dashboard/，可以看到下面的页面。" target="_blank" rel="noopener">http://192.168.1.3:8000/assets/dashboard/，可以看到下面的页面。</a></p><p><img src="http://myimage.okay686.cn/okay686cn/20190328/CuDQtw3hdClN.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-已上线资产信息更新（八）</title>
    <link href="/2019/03/11/8%E3%80%81%E5%B7%B2%E4%B8%8A%E7%BA%BF%E8%B5%84%E4%BA%A7%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0/"/>
    <url>/2019/03/11/8%E3%80%81%E5%B7%B2%E4%B8%8A%E7%BA%BF%E8%B5%84%E4%BA%A7%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0/</url>
    
    <content type="html"><![CDATA[<p>前面，我们已经实现了资产进入待审批区、更新待审批区的资产信息以及审批资产上线三个主要功能，还剩下一个最主要的实时更新已上线资产信息的功能。</p><p>在assets/views.py中的report视图，目前是把已上线资产的数据更新流程‘pass’了，现在将其替换成下面的语句：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data)</span><br></pre></td></tr></table></figure><p><strong>report视图变成了下面的样子：views.py</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import render, HttpResponse</span><br><span class="line">from django.views.decorators.csrf import csrf_exempt</span><br><span class="line">import json</span><br><span class="line">from . import models</span><br><span class="line">from . import asset_handler</span><br><span class="line"></span><br><span class="line"># Create your views here.</span><br><span class="line"></span><br><span class="line">@csrf_exempt</span><br><span class="line">def report(request):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。</span><br><span class="line">    可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。</span><br><span class="line">    :param request:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if request.method == &quot;POST&quot;:</span><br><span class="line">        asset_data = request.POST.get(&apos;asset_data&apos;)</span><br><span class="line">        data = json.loads(asset_data)</span><br><span class="line">        # 各种数据检查，请自行添加和完善！</span><br><span class="line">        if not data:</span><br><span class="line">            return HttpResponse(&quot;没有数据！&quot;)</span><br><span class="line">        if not issubclass(dict, type(data)):</span><br><span class="line">            return HttpResponse(&quot;数据必须为字典格式！&quot;)</span><br><span class="line">        # 是否携带了关键的sn号</span><br><span class="line">        sn = data.get(&apos;sn&apos;, None)</span><br><span class="line">        if sn:</span><br><span class="line">            # 进入审批流程</span><br><span class="line">            # 首先判断是否在上线资产中存在该sn</span><br><span class="line">            asset_obj = models.Asset.objects.filter(sn=sn)</span><br><span class="line">            if asset_obj:</span><br><span class="line">                # 进入已上线资产的数据更新流程</span><br><span class="line">                update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data)</span><br><span class="line">                return HttpResponse(&quot;资产数据已经更新！&quot;)</span><br><span class="line">            else:   # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。</span><br><span class="line">                obj = asset_handler.NewAsset(request, data)</span><br><span class="line">                response = obj.add_to_new_assets_zone()</span><br><span class="line">                return HttpResponse(response)</span><br><span class="line">        else:</span><br><span class="line">            return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;)</span><br></pre></td></tr></table></figure><p>然后，进入assets/asset_handler.py模块，修改log()方法，增加UpdateAsset类，最终的asset_handler.py如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-8 07:49</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : asset_handler.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">from . import models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NewAsset(object):</span><br><span class="line">    def __init__(self, request, data):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.data = data</span><br><span class="line">        print(&quot;asset_handler--&gt;&quot;, self.data)</span><br><span class="line"></span><br><span class="line">    def add_to_new_assets_zone(self):</span><br><span class="line">        defaults = &#123;</span><br><span class="line">            &apos;data&apos;: json.dumps(self.data),</span><br><span class="line">            &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;),</span><br><span class="line">            &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;),</span><br><span class="line">            &apos;model&apos;: self.data.get(&apos;model&apos;),</span><br><span class="line">            &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;),</span><br><span class="line">            &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;),</span><br><span class="line">            &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;),</span><br><span class="line">            &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;),</span><br><span class="line">            &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;),</span><br><span class="line">            &apos;os_release&apos;: self.data.get(&apos;os_release&apos;),</span><br><span class="line">            &apos;os_type&apos;: self.data.get(&apos;os_type&apos;),</span><br><span class="line">        &#125;</span><br><span class="line">        models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults)</span><br><span class="line">        return &apos;资产已经加入或更新待审批区！&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def log(log_type, msg=None, asset=None, new_asset=None, request=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    记录日志，被程序调用</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    event = models.EventLog()</span><br><span class="line">    if log_type == &quot;upline&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  上线&quot; % (asset.name, asset.sn)</span><br><span class="line">        event.asset = asset</span><br><span class="line">        event.detail = &quot;资产成功上线！&quot;</span><br><span class="line">        event.user = request.user</span><br><span class="line">    elif log_type == &quot;approve_failed&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  审批失败&quot; % (new_asset.asset_type, new_asset.sn)</span><br><span class="line">        event.new_asset = new_asset</span><br><span class="line">        event.detail = &quot;审批失败！\n%s&quot; % msg</span><br><span class="line">        event.user = request.user</span><br><span class="line">    elif log_type == &quot;update&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  数据更新！&quot; % (asset.asset_type, asset.sn)</span><br><span class="line">        event.asset = asset</span><br><span class="line">        event.detail = &quot;更新成功！&quot;</span><br><span class="line">    elif log_type == &quot;update_failed&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  更新失败&quot; % (asset.asset_type, asset.sn)</span><br><span class="line">        event.asset = asset</span><br><span class="line">        event.detail = &quot;更新失败！\n%s&quot; % msg</span><br><span class="line">        # 更多日志类型.....</span><br><span class="line">    event.save()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ApproveAsset:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    审批资产并上线。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, request, asset_id):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id)</span><br><span class="line">        self.data = json.loads(self.new_asset.data)</span><br><span class="line"></span><br><span class="line">    def asset_upline(self):</span><br><span class="line">        # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写)</span><br><span class="line">        func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type)</span><br><span class="line">        ret = func()</span><br><span class="line">        return ret and True</span><br><span class="line"></span><br><span class="line">    def _server_upline(self):</span><br><span class="line">        asset = self._create_asset()</span><br><span class="line">        try:</span><br><span class="line">            self._create_manufacturer(asset)  # 创建厂商</span><br><span class="line">            self._create_server(asset)  # 创建服务器</span><br><span class="line">            self._create_CPU(asset)  # 创建CPU</span><br><span class="line">            self._create_RAM(asset)  # 创建内存</span><br><span class="line">            self._create_disk(asset)  # 创建硬盘</span><br><span class="line">            self._create_nic(asset)  # 创建网卡</span><br><span class="line">            self._delete_original_asset()  # 从待审批资产区删除已审批上线的资产</span><br><span class="line">        except Exception as e:</span><br><span class="line">            asset.delete()</span><br><span class="line">            log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request)</span><br><span class="line">            print(e)</span><br><span class="line">            return False</span><br><span class="line">        else:</span><br><span class="line">            log(&apos;upline&apos;, asset=asset, request=self.request)</span><br><span class="line">            print(&quot;新服务器上线&quot;)</span><br><span class="line">            return True</span><br><span class="line"></span><br><span class="line">    def _create_asset(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建资产并上线</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。</span><br><span class="line">        asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type,</span><br><span class="line">                                            name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn),</span><br><span class="line">                                            sn=self.new_asset.sn,</span><br><span class="line">                                            approved_by=self.request.user,</span><br><span class="line">                                            )</span><br><span class="line">        return asset</span><br><span class="line"></span><br><span class="line">    def _create_manufacturer(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建厂商</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。</span><br><span class="line">        m = self.new_asset.manufacturer</span><br><span class="line">        if m:</span><br><span class="line">            manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m)</span><br><span class="line">            print(&quot;asset_handler--&gt;create_manufacturer--&gt;&quot;, manufacturer_obj, _)</span><br><span class="line">            asset.manufacturer = manufacturer_obj</span><br><span class="line">            asset.save()</span><br><span class="line"></span><br><span class="line">    def _create_server(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建服务器</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        models.Server.objects.create(asset=asset,</span><br><span class="line">                                     model=self.new_asset.model,</span><br><span class="line">                                     os_type=self.new_asset.os_type,</span><br><span class="line">                                     os_distribution=self.new_asset.os_distribution,</span><br><span class="line">                                     os_release=self.new_asset.os_release,</span><br><span class="line">                                     )</span><br><span class="line"></span><br><span class="line">    def _create_CPU(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建CPU.</span><br><span class="line">        教程这里对发送过来的数据采取了最大限度的容忍，</span><br><span class="line">        实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，</span><br><span class="line">        根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。</span><br><span class="line">        这里的业务逻辑非常复杂，不可能面面俱到。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cpu = models.CPU.objects.create(asset=asset)</span><br><span class="line">        cpu.cpu_model = self.new_asset.cpu_model</span><br><span class="line">        cpu.cpu_count = self.new_asset.cpu_count</span><br><span class="line">        cpu.cpu_core_count = self.new_asset.cpu_core_count</span><br><span class="line">        cpu.save()</span><br><span class="line"></span><br><span class="line">    def _create_RAM(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建内存。通常有多条内存</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        ram_list = self.data.get(&apos;ram&apos;)</span><br><span class="line">        if not ram_list:    # 万一一条内存数据都没有</span><br><span class="line">            return</span><br><span class="line">        for ram_dict in ram_list:</span><br><span class="line">            if not ram_dict.get(&apos;slot&apos;):</span><br><span class="line">                raise ValueError(&quot;未知的内存插槽！&quot;)  # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。</span><br><span class="line">            ram = models.RAM()</span><br><span class="line">            ram.asset = asset</span><br><span class="line">            ram.slot = ram_dict.get(&apos;slot&apos;)</span><br><span class="line">            ram.sn = ram_dict.get(&apos;sn&apos;)</span><br><span class="line">            ram.model = ram_dict.get(&apos;model&apos;)</span><br><span class="line">            ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;)</span><br><span class="line">            ram.capacity = ram_dict.get(&apos;capacity&apos;, 0)</span><br><span class="line">            ram.save()</span><br><span class="line"></span><br><span class="line">    def _create_disk(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        存储设备种类多，还有Raid情况，需要根据实际情况具体解决。</span><br><span class="line">        这里只以简单的SATA硬盘为例子。可能有多块硬盘。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        disk_list = self.data.get(&apos;physical_disk_driver&apos;)</span><br><span class="line">        if not disk_list:  # 一条硬盘数据都没有</span><br><span class="line">            return</span><br><span class="line">        for disk_dict in disk_list:</span><br><span class="line">            if not disk_dict.get(&apos;sn&apos;):</span><br><span class="line">                raise ValueError(&quot;未知sn的硬盘！&quot;)  # 根据sn确定具体某块硬盘。</span><br><span class="line">            disk = models.Disk()</span><br><span class="line">            disk.asset = asset</span><br><span class="line">            disk.sn = disk_dict.get(&apos;sn&apos;)</span><br><span class="line">            disk.model = disk_dict.get(&apos;model&apos;)</span><br><span class="line">            disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;),</span><br><span class="line">            disk.slot = disk_dict.get(&apos;slot&apos;)</span><br><span class="line">            disk.capacity = disk_dict.get(&apos;capacity&apos;, 0)</span><br><span class="line">            iface = disk_dict.get(&apos;iface_type&apos;)</span><br><span class="line">            if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]:</span><br><span class="line">                disk.interface_type = iface</span><br><span class="line">            disk.save()</span><br><span class="line"></span><br><span class="line">    def _create_nic(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建网卡。可能有多个网卡，甚至虚拟网卡。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        nic_list = self.data.get(&quot;nic&quot;)</span><br><span class="line">        if not nic_list:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        for nic_dict in nic_list:</span><br><span class="line">            if not nic_dict.get(&apos;mac&apos;):</span><br><span class="line">                raise ValueError(&quot;网卡缺少mac地址！&quot;)</span><br><span class="line">            if not nic_dict.get(&apos;model&apos;):</span><br><span class="line">                raise ValueError(&quot;网卡型号未知！&quot;)</span><br><span class="line"></span><br><span class="line">            nic = models.NIC()</span><br><span class="line">            nic.asset = asset</span><br><span class="line">            nic.name = nic_dict.get(&apos;name&apos;)</span><br><span class="line">            nic.model = nic_dict.get(&apos;model&apos;)</span><br><span class="line">            nic.mac = nic_dict.get(&apos;mac&apos;)</span><br><span class="line">            nic.ip_address = nic_dict.get(&apos;ip_address&apos;)</span><br><span class="line">            if nic_dict.get(&apos;net_mask&apos;):</span><br><span class="line">                if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0:</span><br><span class="line">                    nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0]</span><br><span class="line">            nic.save()</span><br><span class="line"></span><br><span class="line">    def _delete_original_asset(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        这里的逻辑是已经审批上线的资产，就从待审批区删除。</span><br><span class="line">        也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。</span><br><span class="line">        不过这样可能导致待审批区越来越大。</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.new_asset.delete()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class UpdateAsset:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    自动更新已上线的资产。</span><br><span class="line">    如果想让记录的日志更详细，可以逐条对比数据项，将更新过的项目记录到log信息中。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, request, asset, report_data):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.asset = asset</span><br><span class="line">        self.report_data = report_data            # 此处的数据是由客户端发送过来的整个数据字符串</span><br><span class="line">        self.asset_update()</span><br><span class="line"></span><br><span class="line">    def asset_update(self):</span><br><span class="line">        # 为以后的其它类型资产扩展留下接口</span><br><span class="line">        func = getattr(self, &quot;_%s_update&quot; % self.report_data[&apos;asset_type&apos;])</span><br><span class="line">        func()</span><br><span class="line"></span><br><span class="line">    def _server_update(self):</span><br><span class="line">        try:</span><br><span class="line">            self._update_manufacturer()   # 更新厂商</span><br><span class="line">            self._update_server()         # 更新服务器</span><br><span class="line">            self._update_CPU()            # 更新CPU</span><br><span class="line">            self._update_RAM()            # 更新内存</span><br><span class="line">            self._update_disk()           # 更新硬盘</span><br><span class="line">            self._update_nic()            # 更新网卡</span><br><span class="line">            self.asset.save()</span><br><span class="line">        except Exception as e:</span><br><span class="line">            log(&apos;update_failed&apos;, msg=e, asset=self.asset, request=self.request)</span><br><span class="line">            print(e)</span><br><span class="line">        else:</span><br><span class="line">            # 添加日志</span><br><span class="line">            log(&quot;update&quot;, asset=self.asset)</span><br><span class="line">            print(&quot;资产数据被更新!&quot;)</span><br><span class="line"></span><br><span class="line">    def _update_manufacturer(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新厂商</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        m = self.report_data.get(&apos;manufacturer&apos;)</span><br><span class="line">        if m:</span><br><span class="line">            manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m)</span><br><span class="line">            self.asset.manufacturer = manufacturer_obj</span><br><span class="line">        else:</span><br><span class="line">            self.asset.manufacturer = None</span><br><span class="line">        self.asset.manufacturer.save()</span><br><span class="line"></span><br><span class="line">    def _update_server(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新服务器</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.asset.server.model = self.report_data.get(&apos;model&apos;)</span><br><span class="line">        self.asset.server.os_type = self.report_data.get(&apos;os_type&apos;)</span><br><span class="line">        self.asset.server.os_distribution = self.report_data.get(&apos;os_distribution&apos;)</span><br><span class="line">        self.asset.server.os_release = self.report_data.get(&apos;os_release&apos;)</span><br><span class="line">        self.asset.server.save()</span><br><span class="line"></span><br><span class="line">    def _update_CPU(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新CPU信息</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.asset.cpu.cpu_model = self.report_data.get(&apos;cpu_model&apos;)</span><br><span class="line">        self.asset.cpu.cpu_count = self.report_data.get(&apos;cpu_count&apos;)</span><br><span class="line">        self.asset.cpu.cpu_core_count = self.report_data.get(&apos;cpu_core_count&apos;)</span><br><span class="line">        self.asset.cpu.save()</span><br><span class="line"></span><br><span class="line">    def _update_RAM(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新内存信息。</span><br><span class="line">        使用集合数据类型中差的概念，处理不同的情况。</span><br><span class="line">        如果新数据有，但原数据没有，则新增；</span><br><span class="line">        如果新数据没有，但原数据有，则删除原来多余的部分；</span><br><span class="line">        如果新的和原数据都有，则更新。</span><br><span class="line">        在原则上，下面的代码应该写成一个复用的函数，</span><br><span class="line">        但是由于内存、硬盘、网卡在某些方面的差别，导致很难提取出重用的代码。</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 获取已有内存信息，并转成字典格式</span><br><span class="line">        old_rams = models.RAM.objects.filter(asset=self.asset)</span><br><span class="line">        old_rams_dict = dict()</span><br><span class="line">        if old_rams:</span><br><span class="line">            for ram in old_rams:</span><br><span class="line">                old_rams_dict[ram.slot] = ram</span><br><span class="line">        # 获取新数据中的内存信息，并转成字典格式</span><br><span class="line">        new_rams_list = self.report_data[&apos;ram&apos;]</span><br><span class="line">        new_rams_dict = dict()</span><br><span class="line">        if new_rams_list:</span><br><span class="line">            for item in new_rams_list:</span><br><span class="line">                new_rams_dict[item[&apos;slot&apos;]] = item</span><br><span class="line"></span><br><span class="line">        # 利用set类型的差集功能，获得需要删除的内存数据对象</span><br><span class="line">        need_deleted_keys = set(old_rams_dict.keys()) - set(new_rams_dict.keys())</span><br><span class="line">        if need_deleted_keys:</span><br><span class="line">            for key in need_deleted_keys:</span><br><span class="line">                old_rams_dict[key].delete()</span><br><span class="line"></span><br><span class="line">        # 需要新增或更新的</span><br><span class="line">        if new_rams_dict:</span><br><span class="line">            for key in new_rams_dict:</span><br><span class="line">                defaults = &#123;</span><br><span class="line">                            &apos;sn&apos;: new_rams_dict[key].get(&apos;sn&apos;),</span><br><span class="line">                            &apos;model&apos;: new_rams_dict[key].get(&apos;model&apos;),</span><br><span class="line">                            &apos;manufacturer&apos;: new_rams_dict[key].get(&apos;manufacturer&apos;),</span><br><span class="line">                            &apos;capacity&apos;: new_rams_dict[key].get(&apos;capacity&apos;, 0),</span><br><span class="line">                            &#125;</span><br><span class="line">                models.RAM.objects.update_or_create(asset=self.asset, slot=key, defaults=defaults)</span><br><span class="line"></span><br><span class="line">    def _update_disk(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新硬盘信息。类似更新内存。</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        old_disks = models.Disk.objects.filter(asset=self.asset)</span><br><span class="line">        old_disks_dict = dict()</span><br><span class="line">        if old_disks:</span><br><span class="line">            for disk in old_disks:</span><br><span class="line">                old_disks_dict[disk.sn] = disk</span><br><span class="line"></span><br><span class="line">        new_disks_list = self.report_data[&apos;physical_disk_driver&apos;]</span><br><span class="line">        new_disks_dict = dict()</span><br><span class="line">        if new_disks_list:</span><br><span class="line">            for item in new_disks_list:</span><br><span class="line">                new_disks_dict[item[&apos;sn&apos;]] = item</span><br><span class="line"></span><br><span class="line">        # 需要删除的</span><br><span class="line">        need_deleted_keys = set(old_disks_dict.keys()) - set(new_disks_dict.keys())</span><br><span class="line">        if need_deleted_keys:</span><br><span class="line">            for key in need_deleted_keys:</span><br><span class="line">                old_disks_dict[key].delete()</span><br><span class="line"></span><br><span class="line">        # 需要新增或更新的</span><br><span class="line">        if new_disks_dict:</span><br><span class="line">            for key in new_disks_dict:</span><br><span class="line">                interface_type = new_disks_dict[key].get(&apos;iface_type&apos;, &apos;unknown&apos;)</span><br><span class="line">                if interface_type not in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]:</span><br><span class="line">                    interface_type = &apos;unknown&apos;</span><br><span class="line">                defaults = &#123;</span><br><span class="line">                    &apos;slot&apos;: new_disks_dict[key].get(&apos;slot&apos;),</span><br><span class="line">                    &apos;model&apos;: new_disks_dict[key].get(&apos;model&apos;),</span><br><span class="line">                    &apos;manufacturer&apos;: new_disks_dict[key].get(&apos;manufacturer&apos;),</span><br><span class="line">                    &apos;capacity&apos;: new_disks_dict[key].get(&apos;capacity&apos;, 0),</span><br><span class="line">                    &apos;interface_type&apos;: interface_type,</span><br><span class="line">                &#125;</span><br><span class="line">                models.Disk.objects.update_or_create(asset=self.asset, sn=key, defaults=defaults)</span><br><span class="line"></span><br><span class="line">    def _update_nic(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新网卡信息。类似更新内存。</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        old_nics = models.NIC.objects.filter(asset=self.asset)</span><br><span class="line">        old_nics_dict = dict()</span><br><span class="line">        if old_nics:</span><br><span class="line">            for nic in old_nics:</span><br><span class="line">                old_nics_dict[nic.model+nic.mac] = nic</span><br><span class="line"></span><br><span class="line">        new_nics_list = self.report_data[&apos;nic&apos;]</span><br><span class="line">        new_nics_dict = dict()</span><br><span class="line">        if new_nics_list:</span><br><span class="line">            for item in new_nics_list:</span><br><span class="line">                new_nics_dict[item[&apos;model&apos;]+item[&apos;mac&apos;]] = item</span><br><span class="line"></span><br><span class="line">        # 需要删除的</span><br><span class="line">        need_deleted_keys = set(old_nics_dict.keys()) - set(new_nics_dict.keys())</span><br><span class="line">        if need_deleted_keys:</span><br><span class="line">            for key in need_deleted_keys:</span><br><span class="line">                old_nics_dict[key].delete()</span><br><span class="line"></span><br><span class="line">        # 需要新增或更新的</span><br><span class="line">        if new_nics_dict:</span><br><span class="line">            for key in new_nics_dict:</span><br><span class="line">                if new_nics_dict[key].get(&apos;net_mask&apos;) and len(new_nics_dict[key].get(&apos;net_mask&apos;)) &gt; 0:</span><br><span class="line">                    net_mask = new_nics_dict[key].get(&apos;net_mask&apos;)[0]</span><br><span class="line">                else:</span><br><span class="line">                    net_mask = &quot;&quot;</span><br><span class="line">                defaults = &#123;</span><br><span class="line">                    &apos;name&apos;: new_nics_dict[key].get(&apos;name&apos;),</span><br><span class="line">                    &apos;ip_address&apos;: new_nics_dict[key].get(&apos;ip_address&apos;),</span><br><span class="line">                    &apos;net_mask&apos;: net_mask,</span><br><span class="line">                &#125;</span><br><span class="line">                models.NIC.objects.update_or_create(asset=self.asset, model=new_nics_dict[key][&apos;model&apos;],</span><br><span class="line">                                                    mac=new_nics_dict[key][&apos;mac&apos;], defaults=defaults)</span><br><span class="line"></span><br><span class="line">        print(&apos;更新成功！&apos;)</span><br></pre></td></tr></table></figure></p><p>对于log()函数，只是增加了两种数据更新的日志类型，分别记录不同的日志情况，没什么特别的。</p><p>对于UpdateAsset类，类似前面的ApproveAsset类：</p><ul><li>首先初始化动作，自动执行asset_update()方法；</li><li>依然是通过反射，决定要调用的更新方法；</li><li>教程实现了主要的服务器类型资产的更新，对于网络设备、安全设备等请自行完善，基本类似；</li><li>_server_update(self)方法中，分别更新厂商、服务器本身、CPU、内存、网卡、硬盘等信息。然后保存数据，这些事务应该是原子性的，所以要抓取异常；</li><li>不管成功还是失败，都要记录日志。</li></ul><p>最主要的，对于_update_CPU(self)等方法，以内存为例，由于内存可能有多条，新的数据中可能出现三种情况，拔除、新增、信息变更，因此要分别对待和处理。</p><ul><li>首先，获取已有内存信息，并转成字典格式；</li><li>其次，获取新数据中的内存信息，并转成字典格式；</li><li>利用set类型的差集功能，获得需要删除的内存数据对象</li><li>对要删除的对象，执行delete()方法；</li><li>对于需要新增或更新的内存对象，首先生成defaults数据字典；</li><li>然后，使用update_or_create(asset=self.asset, slot=key, defaults=defaults)方法，一次性完成新增或者更新数据的操作，不用写两个方法的代码；</li><li>硬盘和网卡的操作类同内存的操作。</li></ul><p>数据更新完毕后，需要保存asset对象，也就是self.asset.save()，否则前面的工作无法关联保存下来。</p><p>现在，可以测试一下资产数据的更新了。重启CMDB，然后转到Client/report_assetss.py脚本，直接运行：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190322/rzEAKlYGx2Vk.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>修改其中的一些数据，删除或增加一些内存、硬盘、网卡的条目。<strong>注意数据格式必须正确，sn必须不能变。</strong></p><p>再次运行脚本，报告数据。进入admin中查看相关内容，可以看到数据已经得到更新了。</p><p>至此，CMDB自动资产管理系统的后台部分已经完成了。</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-审批新资产（七）</title>
    <link href="/2019/03/10/7%E3%80%81%E5%AE%A1%E6%89%B9%E6%96%B0%E8%B5%84%E4%BA%A7/"/>
    <url>/2019/03/10/7%E3%80%81%E5%AE%A1%E6%89%B9%E6%96%B0%E8%B5%84%E4%BA%A7/</url>
    
    <content type="html"><![CDATA[<h3 id="一、自定义admin的actions"><a href="#一、自定义admin的actions" class="headerlink" title="一、自定义admin的actions"></a>一、自定义admin的actions</h3><p>需要有专门的审批员来审批新资产，对资产的合法性、健全性、可用性等更多方面进行审核，如果没有问题，那么就批准上线。</p><p>批准上线这一操作是通过admin的自定义actions来实现的。</p><p>Django的admin默认有一个delete操作的action，所有在admin中的模型都有这个action，更多的就需要我们自己编写了。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190309/jWxSrz7t4V7C.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>修改/assets/admin.py的代码，新的代码如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line"></span><br><span class="line"># Register your models here.</span><br><span class="line">from assets import models</span><br><span class="line">from . import asset_handler</span><br><span class="line"></span><br><span class="line">class NewAssetAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;]</span><br><span class="line">    list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;]</span><br><span class="line">    search_fields = (&apos;sn&apos;,)</span><br><span class="line"></span><br><span class="line">    actions = [&apos;approve_selected_new_assets&apos;]</span><br><span class="line"></span><br><span class="line">    def approve_selected_new_assets(self, request, queryset):</span><br><span class="line">        # 获得被打钩的checkbox对应的资产</span><br><span class="line">        selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME)</span><br><span class="line">        success_upline_number = 0</span><br><span class="line">        for asset_id in selected:</span><br><span class="line">            obj = asset_handler.ApproveAsset(request, asset_id)</span><br><span class="line">            ret = obj.asset_upline()</span><br><span class="line">            if ret:</span><br><span class="line">                success_upline_number += 1</span><br><span class="line">        # 顶部绿色提示信息</span><br><span class="line">        self.message_user(request, &quot;成功批准  %s  条新资产上线！&quot; % success_upline_number)</span><br><span class="line">    approve_selected_new_assets.short_description = &quot;批准选择的新资产&quot;</span><br><span class="line"></span><br><span class="line">class AssetAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &quot;m_time&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">admin.site.register(models.Asset, AssetAdmin)</span><br><span class="line">admin.site.register(models.Server)</span><br><span class="line">admin.site.register(models.StorageDevice)</span><br><span class="line">admin.site.register(models.SecurityDevice)</span><br><span class="line">admin.site.register(models.BusinessUnit)</span><br><span class="line">admin.site.register(models.Contract)</span><br><span class="line">admin.site.register(models.CPU)</span><br><span class="line">admin.site.register(models.Disk)</span><br><span class="line">admin.site.register(models.EventLog)</span><br><span class="line">admin.site.register(models.IDC)</span><br><span class="line">admin.site.register(models.Manufacturer)</span><br><span class="line">admin.site.register(models.NetworkDevice)</span><br><span class="line">admin.site.register(models.NIC)</span><br><span class="line">admin.site.register(models.RAM)</span><br><span class="line">admin.site.register(models.Software)</span><br><span class="line">admin.site.register(models.Tag)</span><br><span class="line">admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin)</span><br></pre></td></tr></table></figure><h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><ul><li>通过actions = [‘approve_selected_new_assets’]定义当前模型的新acitons列表；</li><li>approve_selected_new_assets()方法包含具体的动作逻辑；</li><li>自定义的action接收至少三个参数，第一个是self，第二个是request即请求，第三个是被选中的数据对象集合queryset。</li><li>首先通过request.POST.getlist()方法获取被打钩的checkbox对应的资产；</li><li>可能同时有多个资产被选择，所以这是个批量操作，需要进行循环；</li><li>selected是一个包含了被选中资产的id值的列表；</li><li>对于每一个资产，创建一个asset_handler.ApproveAsset()的实例，然后调用实例的asset_upline()方法，并获取返回值。如果返回值为True，说明该资产被成功批准，那么success_upline_number变量+1，保存成功批准的资产数；</li><li>最后，在admin中给与提示信息。</li><li>approve_selected_new_assets.short_description = “批准选择的新资产”用于在admin界面中为action提供中文显示。你可以尝试去掉这条，看看效果。</li></ul><p>重新启动CMDB，进入admin的待审批资产区，查看上方的acitons动作条，如下所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190314/iViha1pfEX92.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="二、创建测试用例"><a href="#二、创建测试用例" class="headerlink" title="二、创建测试用例"></a>二、创建测试用例</h3><p>由于没有真实的服务器供测试，这里需要<strong>手动创建一些虚假的服务器用例</strong>，方便后面的使用和展示。</p><p>首先，将先前的所有资产条目全部从admin中删除，确保数据库内没有任何数据。</p><p>然后，在Client/bin/目录下新建一个report_assets脚本，其内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-8 07:49</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : asset_handler.py</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.getcwd())</span><br><span class="line"># 设置工作目录，使得包和模块能够正常导入</span><br><span class="line">sys.path.append(BASE_DIR)</span><br><span class="line">from conf import settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def update_test(data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建测试用例</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 将数据打包到一个字典内，并转换为json格式</span><br><span class="line">    data = &#123;&quot;asset_data&quot;: json.dumps(data)&#125;</span><br><span class="line">    # 根据settings中的配置，构造url</span><br><span class="line">    url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;])</span><br><span class="line">    print(&apos;正在将数据发送至： [%s]  ......&apos; % url)</span><br><span class="line">    try:</span><br><span class="line">        # 使用Python内置的urllib.request库，发送post请求。</span><br><span class="line">        # 需要先将数据进行封装，并转换成bytes类型</span><br><span class="line">        data_encode = urllib.parse.urlencode(data).encode()     ##转码</span><br><span class="line">        response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;])</span><br><span class="line">        print(&quot;\033[31;1m发送完毕！\033[0m &quot;)</span><br><span class="line">        message = response.read().decode()</span><br><span class="line">        print(&quot;返回结果：%s&quot; % message)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        message = &quot;发送失败&quot;</span><br><span class="line">        print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    windows_data = &#123;</span><br><span class="line">        &quot;os_type&quot;: &quot;Windows&quot;,</span><br><span class="line">        &quot;os_release&quot;: &quot;7 64bit  6.1.7601 &quot;,</span><br><span class="line">        &quot;os_distribution&quot;: &quot;Microsoft&quot;,</span><br><span class="line">        &quot;asset_type&quot;: &quot;server&quot;,</span><br><span class="line">        &quot;cpu_count&quot;: 2,</span><br><span class="line">        &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;,</span><br><span class="line">        &quot;cpu_core_count&quot;: 8,</span><br><span class="line">        &quot;ram&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;slot&quot;: &quot;A1&quot;,</span><br><span class="line">                &quot;capacity&quot;: 8,</span><br><span class="line">                &quot;model&quot;: &quot;Physical Memory&quot;,</span><br><span class="line">                &quot;manufacturer&quot;: &quot;kingstone &quot;,</span><br><span class="line">                &quot;sn&quot;: &quot;456&quot;</span><br><span class="line">            &#125;,</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        &quot;manufacturer&quot;: &quot;Intel&quot;,</span><br><span class="line">        &quot;model&quot;: &quot;P67X-UD3R-B3&quot;,</span><br><span class="line">        &quot;wake_up_type&quot;: 6,</span><br><span class="line">        &quot;sn&quot;: &quot;00426-OEM-8992662-111111&quot;,</span><br><span class="line">        &quot;physical_disk_driver&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;iface_type&quot;: &quot;unknown&quot;,</span><br><span class="line">                &quot;slot&quot;: 0,</span><br><span class="line">                &quot;sn&quot;: &quot;3830414130423230343234362020202020202020&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;,</span><br><span class="line">                &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;,</span><br><span class="line">                &quot;capacity&quot;: 128</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;iface_type&quot;: &quot;SATA&quot;,</span><br><span class="line">                &quot;slot&quot;: 1,</span><br><span class="line">                &quot;sn&quot;: &quot;383041413042323023234362020102020202020&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;,</span><br><span class="line">                &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;,</span><br><span class="line">                &quot;capacity&quot;: 2048</span><br><span class="line">            &#125;,</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        &quot;nic&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;[00000011] Realtek RTL8192CU Wireless LAN 802.11n USB 2.0 Network Adapter&quot;,</span><br><span class="line">                &quot;name&quot;: 11,</span><br><span class="line">                &quot;ip_address&quot;: &quot;192.168.1.110&quot;,</span><br><span class="line">                &quot;net_mask&quot;: [</span><br><span class="line">                    &quot;255.255.255.0&quot;,</span><br><span class="line">                    &quot;64&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;mac&quot;: &quot;0A:01:27:00:00:00&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;[00000013] VirtualBox Host-Only Ethernet Adapter&quot;,</span><br><span class="line">                &quot;name&quot;: 13,</span><br><span class="line">                &quot;ip_address&quot;: &quot;192.168.56.1&quot;,</span><br><span class="line">                &quot;net_mask&quot;: [</span><br><span class="line">                    &quot;255.255.255.0&quot;,</span><br><span class="line">                    &quot;64&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;[00000017] Microsoft Virtual WiFi Miniport Adapter&quot;,</span><br><span class="line">                &quot;name&quot;: 17,</span><br><span class="line">                &quot;ip_address&quot;: &quot;&quot;,</span><br><span class="line">                &quot;net_mask&quot;: &quot;&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;,</span><br><span class="line">                &quot;model&quot;: &quot;Intel Adapter&quot;,</span><br><span class="line">                &quot;name&quot;: 17,</span><br><span class="line">                &quot;ip_address&quot;: &quot;192.1.1.1&quot;,</span><br><span class="line">                &quot;net_mask&quot;: &quot;&quot;</span><br><span class="line">            &#125;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    linux_data = &#123;</span><br><span class="line">        &quot;asset_type&quot;: &quot;server&quot;,</span><br><span class="line">        &quot;manufacturer&quot;: &quot;innotek GmbH&quot;,</span><br><span class="line">        &quot;sn&quot;: &quot;00001&quot;,</span><br><span class="line">        &quot;model&quot;: &quot;VirtualBox&quot;,</span><br><span class="line">        &quot;uuid&quot;: &quot;E8DE611C-4279-495C-9B58-502B6FCED076&quot;,</span><br><span class="line">        &quot;wake_up_type&quot;: &quot;Power Switch&quot;,</span><br><span class="line">        &quot;os_distribution&quot;: &quot;Ubuntu&quot;,</span><br><span class="line">        &quot;os_release&quot;: &quot;Ubuntu 16.04.3 LTS&quot;,</span><br><span class="line">        &quot;os_type&quot;: &quot;Linux&quot;,</span><br><span class="line">        &quot;cpu_count&quot;: &quot;2&quot;,</span><br><span class="line">        &quot;cpu_core_count&quot;: &quot;4&quot;,</span><br><span class="line">        &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;,</span><br><span class="line">        &quot;ram&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;slot&quot;: &quot;A1&quot;,</span><br><span class="line">                &quot;capacity&quot;: 8,</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;ram_size&quot;: 3.858997344970703,</span><br><span class="line">        &quot;nic&quot;: [],</span><br><span class="line">        &quot;physical_disk_driver&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;model&quot;: &quot;VBOX HARDDISK&quot;,</span><br><span class="line">                &quot;size&quot;: &quot;50&quot;,</span><br><span class="line">                &quot;sn&quot;: &quot;VBeee1ba73-09085302&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    update_test(linux_data)</span><br><span class="line">    update_test(windows_data)</span><br></pre></td></tr></table></figure></p><p>该脚本的作用很简单，人为虚构了两台服务器（一台windows，一台Linux）的信息，并发送给CMDB。单独执行该脚本，在admin的新资产待审批区可以看到添加了两条新资产信息。</p><p>要添加更多的资产，只需修改脚本中windows_data和linux_data的数据即可。但是要注意的是，如果不修改sn，那么会变成资产数据更新，而不是增加新资产，这一点一定要注意。</p><h3 id="三、批准资产上线"><a href="#三、批准资产上线" class="headerlink" title="三、批准资产上线"></a>三、批准资产上线</h3><p>在/assets/asset_handler.py中添加下面的代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-8 07:49</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : asset_handler.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">from . import models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NewAsset(object):</span><br><span class="line">    def __init__(self, request, data):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.data = data</span><br><span class="line">        print(&quot;asset_handler--&gt;&quot;, self.data)</span><br><span class="line">    def add_to_new_assets_zone(self):</span><br><span class="line">        defaults = &#123;</span><br><span class="line">            &apos;data&apos;: json.dumps(self.data),</span><br><span class="line">            &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;),</span><br><span class="line">            &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;),</span><br><span class="line">            &apos;model&apos;: self.data.get(&apos;model&apos;),</span><br><span class="line">            &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;),</span><br><span class="line">            &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;),</span><br><span class="line">            &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;),</span><br><span class="line">            &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;),</span><br><span class="line">            &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;),</span><br><span class="line">            &apos;os_release&apos;: self.data.get(&apos;os_release&apos;),</span><br><span class="line">            &apos;os_type&apos;: self.data.get(&apos;os_type&apos;),</span><br><span class="line">        &#125;</span><br><span class="line">        models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults)</span><br><span class="line">        return &apos;资产已经加入或更新待审批区！&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def log(log_type, msg=None, asset=None, new_asset=None, request=None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    记录日志，被程序调用</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    event = models.EventLog()</span><br><span class="line">    if log_type == &quot;upline&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  上线&quot; % (asset.name, asset.sn)</span><br><span class="line">        event.asset = asset</span><br><span class="line">        event.detail = &quot;资产成功上线！&quot;</span><br><span class="line">        event.user = request.user</span><br><span class="line">    elif log_type == &quot;approve_failed&quot;:</span><br><span class="line">        event.name = &quot;%s &lt;%s&gt; ：  审批失败&quot; % (new_asset.asset_type, new_asset.sn)</span><br><span class="line">        event.new_asset = new_asset</span><br><span class="line">        event.detail = &quot;审批失败！\n%s&quot; % msg</span><br><span class="line">        event.user = request.user</span><br><span class="line">    # 更多日志类型.....</span><br><span class="line">    event.save()</span><br><span class="line"></span><br><span class="line">class ApproveAsset:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    审批资产并上线。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, request, asset_id):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id)</span><br><span class="line">        self.data = json.loads(self.new_asset.data)</span><br><span class="line"></span><br><span class="line">    def asset_upline(self):</span><br><span class="line">        # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写)</span><br><span class="line">        func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type)</span><br><span class="line">        ret = func()</span><br><span class="line">        return ret and True</span><br><span class="line"></span><br><span class="line">    def _server_upline(self):</span><br><span class="line">        asset = self._create_asset()</span><br><span class="line">        try:</span><br><span class="line">            self._create_manufacturer(asset)  # 创建厂商</span><br><span class="line">            self._create_server(asset)  # 创建服务器</span><br><span class="line">            self._create_CPU(asset)  # 创建CPU</span><br><span class="line">            self._create_RAM(asset)  # 创建内存</span><br><span class="line">            self._create_disk(asset)  # 创建硬盘</span><br><span class="line">            self._create_nic(asset)  # 创建网卡</span><br><span class="line">            self._delete_original_asset()  # 从待审批资产区删除已审批上线的资产</span><br><span class="line">        except Exception as e:</span><br><span class="line">            asset.delete()</span><br><span class="line">            log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request)</span><br><span class="line">            print(e)</span><br><span class="line">            return False</span><br><span class="line">        else:</span><br><span class="line">            log(&apos;upline&apos;, asset=asset, request=self.request)</span><br><span class="line">            print(&quot;新服务器上线&quot;)</span><br><span class="line">            return True</span><br><span class="line"></span><br><span class="line">    def _create_asset(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建资产并上线</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。</span><br><span class="line">        asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type,</span><br><span class="line">                                            name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn),</span><br><span class="line">                                            sn=self.new_asset.sn,</span><br><span class="line">                                            approved_by=self.request.user,</span><br><span class="line">                                            )</span><br><span class="line">        return asset</span><br><span class="line"></span><br><span class="line">    def _create_manufacturer(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建厂商</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。</span><br><span class="line">        m = self.new_asset.manufacturer</span><br><span class="line">        if m:</span><br><span class="line">            manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m)</span><br><span class="line">            print(&quot;asset_handler--&gt;&quot;, manufacturer_obj, _)</span><br><span class="line">            asset.manufacturer = manufacturer_obj</span><br><span class="line">            asset.save()</span><br><span class="line"></span><br><span class="line">    def _create_server(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建服务器</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        models.Server.objects.create(asset=asset,</span><br><span class="line">                                     model=self.new_asset.model,</span><br><span class="line">                                     os_type=self.new_asset.os_type,</span><br><span class="line">                                     os_distribution=self.new_asset.os_distribution,</span><br><span class="line">                                     os_release=self.new_asset.os_release,</span><br><span class="line">                                     )</span><br><span class="line"></span><br><span class="line">    def _create_CPU(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建CPU.</span><br><span class="line">        教程这里对发送过来的数据采取了最大限度的容忍，</span><br><span class="line">        实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，</span><br><span class="line">        根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。</span><br><span class="line">        这里的业务逻辑非常复杂，不可能面面俱到。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cpu = models.CPU.objects.create(asset=asset)</span><br><span class="line">        cpu.cpu_model = self.new_asset.cpu_model</span><br><span class="line">        cpu.cpu_count = self.new_asset.cpu_count</span><br><span class="line">        cpu.cpu_core_count = self.new_asset.cpu_core_count</span><br><span class="line">        cpu.save()</span><br><span class="line"></span><br><span class="line">    def _create_RAM(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建内存。通常有多条内存</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        ram_list = self.data.get(&apos;ram&apos;)</span><br><span class="line">        if not ram_list:    # 万一一条内存数据都没有</span><br><span class="line">            return</span><br><span class="line">        for ram_dict in ram_list:</span><br><span class="line">            if not ram_dict.get(&apos;slot&apos;):</span><br><span class="line">                raise ValueError(&quot;未知的内存插槽！&quot;)  # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。</span><br><span class="line">            ram = models.RAM()</span><br><span class="line">            ram.asset = asset</span><br><span class="line">            ram.slot = ram_dict.get(&apos;slot&apos;)</span><br><span class="line">            ram.sn = ram_dict.get(&apos;sn&apos;)</span><br><span class="line">            ram.model = ram_dict.get(&apos;model&apos;)</span><br><span class="line">            ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;)</span><br><span class="line">            ram.capacity = ram_dict.get(&apos;capacity&apos;, 0)</span><br><span class="line">            ram.save()</span><br><span class="line"></span><br><span class="line">    def _create_disk(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        存储设备种类多，还有Raid情况，需要根据实际情况具体解决。</span><br><span class="line">        这里只以简单的SATA硬盘为例子。可能有多块硬盘。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        disk_list = self.data.get(&apos;physical_disk_driver&apos;)</span><br><span class="line">        if not disk_list:  # 一条硬盘数据都没有</span><br><span class="line">            return</span><br><span class="line">        for disk_dict in disk_list:</span><br><span class="line">            if not disk_dict.get(&apos;sn&apos;):</span><br><span class="line">                raise ValueError(&quot;未知sn的硬盘！&quot;)  # 根据sn确定具体某块硬盘。</span><br><span class="line">            disk = models.Disk()</span><br><span class="line">            disk.asset = asset</span><br><span class="line">            disk.sn = disk_dict.get(&apos;sn&apos;)</span><br><span class="line">            disk.model = disk_dict.get(&apos;model&apos;)</span><br><span class="line">            disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;),</span><br><span class="line">            disk.slot = disk_dict.get(&apos;slot&apos;)</span><br><span class="line">            disk.capacity = disk_dict.get(&apos;capacity&apos;, 0)</span><br><span class="line">            iface = disk_dict.get(&apos;iface_type&apos;)</span><br><span class="line">            if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]:</span><br><span class="line">                disk.interface_type = iface</span><br><span class="line">            disk.save()</span><br><span class="line"></span><br><span class="line">    def _create_nic(self, asset):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        创建网卡。可能有多个网卡，甚至虚拟网卡。</span><br><span class="line">        :param asset:</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        nic_list = self.data.get(&quot;nic&quot;)</span><br><span class="line">        if not nic_list:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        for nic_dict in nic_list:</span><br><span class="line">            if not nic_dict.get(&apos;mac&apos;):</span><br><span class="line">                raise ValueError(&quot;网卡缺少mac地址！&quot;)</span><br><span class="line">            if not nic_dict.get(&apos;model&apos;):</span><br><span class="line">                raise ValueError(&quot;网卡型号未知！&quot;)</span><br><span class="line"></span><br><span class="line">            nic = models.NIC()</span><br><span class="line">            nic.asset = asset</span><br><span class="line">            nic.name = nic_dict.get(&apos;name&apos;)</span><br><span class="line">            nic.model = nic_dict.get(&apos;model&apos;)</span><br><span class="line">            nic.mac = nic_dict.get(&apos;mac&apos;)</span><br><span class="line">            nic.ip_address = nic_dict.get(&apos;ip_address&apos;)</span><br><span class="line">            if nic_dict.get(&apos;net_mask&apos;):</span><br><span class="line">                if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0:</span><br><span class="line">                    nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0]</span><br><span class="line">            nic.save()</span><br><span class="line"></span><br><span class="line">    def _delete_original_asset(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        这里的逻辑是已经审批上线的资产，就从待审批区删除。</span><br><span class="line">        也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。</span><br><span class="line">        不过这样可能导致待审批区越来越大。</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.new_asset.delete()</span><br></pre></td></tr></table></figure><p>核心就是增加了一个记录日志的log()函数以及审批资产的ApproveAsset类。</p><p>log()函数很简单，根据日志类型的不同，保存日志需要的各种信息，比如日志名称、关联的资产对象、日志详细内容和审批人员等等。所有的日志都被保存在数据库中，可以在admin中查看。</p><p>对于关键的ApproveAsset类，说明如下：</p><ul><li>初始化方法接收reqeust和待审批资产的id；</li><li>分别提前获取资产对象和所有数据data；</li><li>asset_upline()是入口方法，通过反射，获取一个类似_server_upline的方法。之所以这么做，是为后面的网络设别、安全设备、存储设备等更多类型资产的审批留下扩展接口。本教程里只实现了服务器类型资产的审批方法，更多的请自行完善，过程基本类似。</li></ul><p>_server_upline()是服务器类型资产上线的核心方法：</p><ul><li>它首先新建了一个Asset资产对象（注意要和待审批区的资产区分开）；</li><li>然后利用该对象，分别创建了对应的厂商、服务器、CPU、内存、硬盘和网卡，并删除待审批区的对应资产；</li><li>在实际的生产环境中，上面的操作应该是原子性的整体事务，任何一步出现异常，所有操作都要回滚；</li><li>如果任何一步出现错误，上面的操作全部撤销，也就是asset.delete()。记录错误日志，返回False；</li><li>如果没问题，那么记录正确日志，返回True。</li></ul><p>对于_create_asset(self)方法，利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。</p><p>对于_create_manufacturer(self, asset)方法，先判断厂商数据是否存在，再决定是获取还是创建。</p><p>对于_create_CPU(self, asset)等方法，教程这里对数据采取了最大限度的容忍，实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。这里的业务逻辑非常复杂，不可能面面俱到。后面的内存、硬盘和网卡也是一样的。</p><p>对于_delete_original_asset(self)方法，这里的逻辑是已经审批上线的资产，就从待审批区删除。也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示，不过这样可能导致待审批区越来越大。</p><h3 id="四、测试资产上线功能"><a href="#四、测试资产上线功能" class="headerlink" title="四、测试资产上线功能"></a>四、测试资产上线功能</h3><p>运行 report_assets.py 脚本：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190314/3e6pBv80JgwC.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在admin的新资产待审批区选择刚才的3条资产，然后选择上线action并点击‘执行’按钮，稍等片刻，显示成功批准 3 条新资产上线！的绿色提示信息，同时新资产也从待审批区被删除了，如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190314/uA2Gi5FKPKsG.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>往后，如果我们再次发送这3个服务器资产的信息，那就不是在待审批区了，而是已上线资产了。</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-新资产待审批区（六）</title>
    <link href="/2019/03/09/6%E3%80%81%E6%96%B0%E8%B5%84%E4%BA%A7%E5%BE%85%E5%AE%A1%E6%89%B9%E5%8C%BA/"/>
    <url>/2019/03/09/6%E3%80%81%E6%96%B0%E8%B5%84%E4%BA%A7%E5%BE%85%E5%AE%A1%E6%89%B9%E5%8C%BA/</url>
    
    <content type="html"><![CDATA[<h3 id="一、启用admin"><a href="#一、启用admin" class="headerlink" title="一、启用admin"></a>一、启用admin</h3><p>前面，我们已经完成了数据收集客户端的编写和测试，下面我们就可以在admin中展示和管理资产数据了。</p><p>首先，通过<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py createsuperuser</span><br></pre></td></tr></table></figure></p><p>创建一个管理员账户。</p><p>然后，进入/assets/admin.py文件，写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line"></span><br><span class="line"># Register your models here.</span><br><span class="line">from assets import models</span><br><span class="line"></span><br><span class="line">class NewAssetAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;]</span><br><span class="line">    list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;]</span><br><span class="line">    search_fields = (&apos;sn&apos;,)</span><br><span class="line"></span><br><span class="line">class AssetAdmin(admin.ModelAdmin):</span><br><span class="line">    list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &apos;m_time&apos;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">admin.site.register(models.Asset, AssetAdmin)</span><br><span class="line">admin.site.register(models.Server)</span><br><span class="line">admin.site.register(models.StorageDevice)</span><br><span class="line">admin.site.register(models.SecurityDevice)</span><br><span class="line">admin.site.register(models.BusinessUnit)</span><br><span class="line">admin.site.register(models.Contract)</span><br><span class="line">admin.site.register(models.CPU)</span><br><span class="line">admin.site.register(models.Disk)</span><br><span class="line">admin.site.register(models.EventLog)</span><br><span class="line">admin.site.register(models.IDC)</span><br><span class="line">admin.site.register(models.Manufacturer)</span><br><span class="line">admin.site.register(models.NetworkDevice)</span><br><span class="line">admin.site.register(models.NIC)</span><br><span class="line">admin.site.register(models.RAM)</span><br><span class="line">admin.site.register(models.Software)</span><br><span class="line">admin.site.register(models.Tag)</span><br><span class="line">admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin)</span><br></pre></td></tr></table></figure></p><p>利用刚才创建的管理员用户，登录admin站点：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190307/fRFGOKfTSlc2.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这里略微对admin界面做了些简单地配置，但目前还没有数据。</p><h3 id="二、创建新资产"><a href="#二、创建新资产" class="headerlink" title="二、创建新资产"></a>二、创建新资产</h3><p>前面我们只是在Pycharm中获取并打印数据，并没有将数据保存到数据库里。下面我们来实现这一功能。</p><p>修改/assets/views.py文件，代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import render, HttpResponse</span><br><span class="line">from django.views.decorators.csrf import csrf_exempt</span><br><span class="line">import json</span><br><span class="line">from . import models</span><br><span class="line">from . import asset_handler</span><br><span class="line"></span><br><span class="line"># Create your views here.</span><br><span class="line"></span><br><span class="line">@csrf_exempt</span><br><span class="line">def report(request):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。</span><br><span class="line">    可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。</span><br><span class="line">    :param request:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if request.method == &quot;POST&quot;:</span><br><span class="line">        asset_data = request.POST.get(&apos;asset_data&apos;)</span><br><span class="line">        data = json.loads(asset_data)</span><br><span class="line">        # 各种数据检查，请自行添加和完善！</span><br><span class="line">        if not data:</span><br><span class="line">            return HttpResponse(&quot;没有数据！&quot;)</span><br><span class="line">        if not issubclass(dict, type(data)):</span><br><span class="line">            return HttpResponse(&quot;数据必须为字典格式！&quot;)</span><br><span class="line">        # 是否携带了关键的sn号</span><br><span class="line">        sn = data.get(&apos;sn&apos;, None)</span><br><span class="line">        if sn:</span><br><span class="line">            # 进入审批流程</span><br><span class="line">            # 首先判断是否在上线资产中存在该sn</span><br><span class="line">            asset_obj = models.Asset.objects.filter(sn=sn)</span><br><span class="line">            if asset_obj:</span><br><span class="line">                # 进入已上线资产的数据更新流程</span><br><span class="line">                pass</span><br><span class="line">                return HttpResponse(&quot;资产数据已经更新！&quot;)</span><br><span class="line">            else:   # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。</span><br><span class="line">                obj = asset_handler.NewAsset(request, data)</span><br><span class="line">                response = obj.add_to_new_assets_zone()</span><br><span class="line">                return HttpResponse(response)</span><br><span class="line">        else:</span><br><span class="line">            return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;)</span><br></pre></td></tr></table></figure></p><p>report视图的逻辑是这样的：</p><ul><li>sn是标识一个资产的唯一字段，必须携带，不能重复！</li><li>从POST中获取发送过来的数据；</li><li>使用json转换数据类型；</li><li>进行各种数据检查（比如身份验证等等，请自行完善）；</li><li>判断数据是否为空，空则返回错误信息，结束视图；</li><li>判断data的类型是否字典类型，否则返回错误信息；</li><li>之所以要对data的类型进行判断是因为后面要大量的使用字典的get方法和中括号操作；</li><li>如果没有携带sn号，返回错误信息；</li></ul><p>当前面都没问题时，进入下面的流程：</p><ul><li>首先，利用sn值尝试在已上线的资产进行查找，如果有，则进入已上线资产的更新流程，具体实现，这里暂且跳过;</li><li>如果没有，说明这是个新资产，需要添加到新资产区；</li><li>这里又分两种情况，一种是彻底的新资产，那没得说，需要新增；另一种是新资产区已经有了，但是审批员还没来得及审批，资产数据的后续报告就已经到达了，那么需要更新数据。</li><li>创建一个asset_handler.NewAsset()对象，然后调用它的obj.add_to_new_assets_zone()方法，进行数据保存，并接收返回结果；</li><li>asset_handler是下面我们要新建的资产处理模块，NewAsset是其中的一个类。</li></ul><p>为了不让views.py文件过于庞大，通常会建立新的py文件，专门处理一些核心业务。</p><p>在assets下新建asset_handler.py文件，并写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-8 07:49</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : asset_handler.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">from . import models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NewAsset(object):</span><br><span class="line">    def __init__(self, request, data):</span><br><span class="line">        self.request = request</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    def add_to_new_assets_zone(self):</span><br><span class="line">        defaults = &#123;</span><br><span class="line">            &apos;data&apos;: json.dumps(self.data),</span><br><span class="line">            &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;),</span><br><span class="line">            &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;),</span><br><span class="line">            &apos;model&apos;: self.data.get(&apos;model&apos;),</span><br><span class="line">            &apos;ram_size&apos;: self.data.get(&apos;ram_size&apos;),</span><br><span class="line">            &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;),</span><br><span class="line">            &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;),</span><br><span class="line">            &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;),</span><br><span class="line">            &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;),</span><br><span class="line">            &apos;os_release&apos;: self.data.get(&apos;os_release&apos;),</span><br><span class="line">            &apos;os_type&apos;: self.data.get(&apos;os_type&apos;),</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults)</span><br><span class="line"></span><br><span class="line">        return &apos;资产已经加入或更新待审批区！&apos;</span><br></pre></td></tr></table></figure></p><p>NewAsset类接收两个参数，request和data，分别封装了请求和资产数据，它的唯一方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obj.add_to_new_assets_zone()</span><br></pre></td></tr></table></figure></p><p>首先构造了一个defaults字典，分别将资产数据包的各种数据打包进去，然后利用Django中特别好用的update_or_create()方法，进行数据保存！</p><p><strong>update_or_create()方法的机制</strong>：如果数据库内没有该数据，那么新增，如果有，则更新，这就大大减少了我们的代码量，不用写两个方法。该方法的参数必须为一些用于查询的指定字段（这里是sn），以及需要新增或者更新的defaults字典。而其返回值，则是一个查询对象和是否新建对象布尔值的二元元组。</p><h3 id="三、测试数据"><a href="#三、测试数据" class="headerlink" title="三、测试数据"></a>三、测试数据</h3><p><strong>重启CMDB</strong>，在linux中给Client下的main.py客户端，添加一个report_data的运行参数，然后运行main.py，发送一个资产数据给CMDB服务器，结果如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190309/KTyfri1qvGmp.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>再进入admin后台，查看新资产待审批区，可以看到资产已经成功进入待审批区：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190309/nCW1GaxHvgVY.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p><img src="http://myimage.okay686.cn/okay686cn/20190309/RRnAdQb3EE4m.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>这里我们显示了资产的汇报和更新日期，过几分钟后，重新汇报该资产数据，然后刷新admin中的页面，可以看到，待审批区的资产数据也一并被更新了。</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-Linux下收集数据（五）</title>
    <link href="/2019/03/04/5%E3%80%81Linux%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE/"/>
    <url>/2019/03/04/5%E3%80%81Linux%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<p>Linux下收集数据就有很多命令和工具了，比Windows方便多了。</p><p>但是要在Python的进程中运行操作系统级别的命令，我们通常需要使用subprocess模块。这个模块的具体用法，请查看Python教程中相关部分的内容。</p><p>下面，我们在<strong>Client/plugins下创建一个linux包，再到包里创建一个sys_info.py文件</strong>，写入下面的代码：</p><p>前提需要现在被收集的虚机上面安装必须的组件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y lsb</span><br></pre></td></tr></table></figure></p><p>关于disk的获取与原著有差别，更容易理解：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##获取厂商：</span><br><span class="line">[root@python_master pythontest]# dmidecode -s system-manufacturer</span><br><span class="line">VMware, Inc.</span><br><span class="line"></span><br><span class="line">##获取型号：</span><br><span class="line">[root@python_master pythontest]# dmidecode -s system-product-name</span><br><span class="line">VMware Virtual Platform</span><br><span class="line"></span><br><span class="line">##获取sn：</span><br><span class="line">[root@python_master pythontest]# dmidecode -s system-serial-number</span><br><span class="line">VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-3 13:16</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : sys_info.py</span><br><span class="line"></span><br><span class="line">import subprocess</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def collect():</span><br><span class="line">    filter_keys = [&apos;Manufacturer&apos;, &apos;Serial Number&apos;, &apos;Product Name&apos;, &apos;UUID&apos;, &apos;Wake-up Type&apos;]</span><br><span class="line">    raw_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    for key in filter_keys:</span><br><span class="line">        try:</span><br><span class="line">            res = subprocess.Popen(&quot;sudo dmidecode -t system|grep &apos;%s&apos;&quot; % key,</span><br><span class="line">                                   stdout=subprocess.PIPE, shell=True)</span><br><span class="line">            result = res.stdout.read().decode()</span><br><span class="line">            data_list = result.split(&apos;:&apos;)</span><br><span class="line"></span><br><span class="line">            if len(data_list) &gt; 1:</span><br><span class="line">                raw_data[key] = data_list[1].strip()</span><br><span class="line">            else:</span><br><span class="line">                raw_data[key] = -1</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(e)</span><br><span class="line">            raw_data[key] = -2</span><br><span class="line"></span><br><span class="line">    data = dict()</span><br><span class="line">    data[&apos;asset_type&apos;] = &apos;server&apos;</span><br><span class="line">    data[&apos;manufacturer&apos;] = raw_data[&apos;Manufacturer&apos;]</span><br><span class="line">    data[&apos;sn&apos;] = raw_data[&apos;Serial Number&apos;]</span><br><span class="line">    data[&apos;model&apos;] = raw_data[&apos;Product Name&apos;]</span><br><span class="line">    data[&apos;uuid&apos;] = raw_data[&apos;UUID&apos;]</span><br><span class="line">    data[&apos;wake_up_type&apos;] = raw_data[&apos;Wake-up Type&apos;]</span><br><span class="line"></span><br><span class="line">    data.update(get_os_info())</span><br><span class="line">    data.update(get_cpu_info())</span><br><span class="line">    data.update(get_ram_info())</span><br><span class="line">    data.update(get_nic_info())</span><br><span class="line">    data.update(get_disk_info())</span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_os_info():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取操作系统信息</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    distributor = subprocess.Popen(&quot;lsb_release -a|grep &apos;Distributor ID&apos;&quot;,</span><br><span class="line">                                   stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    distributor = distributor.stdout.read().decode().split(&quot;:&quot;)</span><br><span class="line"></span><br><span class="line">    release = subprocess.Popen(&quot;lsb_release -a|grep &apos;Description&apos;&quot;,</span><br><span class="line">                               stdout=subprocess.PIPE, shell=True)</span><br><span class="line"></span><br><span class="line">    release = release.stdout.read().decode().split(&quot;:&quot;)</span><br><span class="line">    data_dic = &#123;</span><br><span class="line">        &quot;os_distribution&quot;: distributor[1].strip() if len(distributor) &gt; 1 else &quot;&quot;,</span><br><span class="line">        &quot;os_release&quot;: release[1].strip() if len(release) &gt; 1 else &quot;&quot;,</span><br><span class="line">        &quot;os_type&quot;: &quot;Linux&quot;,</span><br><span class="line">    &#125;</span><br><span class="line">    return data_dic</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_cpu_info():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取cpu信息</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    base_cmd = &apos;cat /proc/cpuinfo&apos;</span><br><span class="line"></span><br><span class="line">    raw_data = &#123;</span><br><span class="line">        &apos;cpu_model&apos;: &quot;%s |grep &apos;model name&apos; |head -1 &quot; % base_cmd,</span><br><span class="line">        &apos;cpu_count&apos;:  &quot;%s |grep  &apos;processor&apos;|wc -l &quot; % base_cmd,</span><br><span class="line">        &apos;cpu_core_count&apos;: &quot;%s |grep &apos;cpu cores&apos; |awk -F: &apos;&#123;SUM +=$2&#125; END &#123;print SUM&#125;&apos;&quot; % base_cmd,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for key, cmd in raw_data.items():</span><br><span class="line">        try:</span><br><span class="line">            cmd_res = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">            raw_data[key] = cmd_res.stdout.read().decode().strip()</span><br><span class="line">        except ValueError as e:</span><br><span class="line">            print(e)</span><br><span class="line">            raw_data[key] = &quot;&quot;</span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">        &quot;cpu_count&quot;: raw_data[&quot;cpu_count&quot;],</span><br><span class="line">        &quot;cpu_core_count&quot;: raw_data[&quot;cpu_core_count&quot;]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    cpu_model = raw_data[&quot;cpu_model&quot;].split(&quot;:&quot;)</span><br><span class="line"></span><br><span class="line">    if len(cpu_model) &gt; 1:</span><br><span class="line">        data[&quot;cpu_model&quot;] = cpu_model[1].strip()</span><br><span class="line">    else:</span><br><span class="line">        data[&quot;cpu_model&quot;] = -1</span><br><span class="line"></span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_ram_info():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取内存信息</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    raw_data = subprocess.Popen(&quot;sudo dmidecode -t memory&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    raw_list = raw_data.stdout.read().decode().split(&quot;\n&quot;)</span><br><span class="line">    raw_ram_list = []</span><br><span class="line">    item_list = []</span><br><span class="line">    for line in raw_list:</span><br><span class="line">        if line.startswith(&quot;Memory Device&quot;):</span><br><span class="line">            raw_ram_list.append(item_list)</span><br><span class="line">            item_list = []</span><br><span class="line">        else:</span><br><span class="line">            item_list.append(line.strip())</span><br><span class="line"></span><br><span class="line">    ram_list = []</span><br><span class="line">    for item in raw_ram_list:</span><br><span class="line">        item_ram_size = 0</span><br><span class="line">        ram_item_to_dic = &#123;&#125;</span><br><span class="line">        for i in item:</span><br><span class="line">            data = i.split(&quot;:&quot;)</span><br><span class="line">            if len(data) == 2:</span><br><span class="line">                key, v = data</span><br><span class="line">                if key == &apos;Size&apos;:</span><br><span class="line">                    if v.strip() != &quot;No Module Installed&quot;:</span><br><span class="line">                        ram_item_to_dic[&apos;capacity&apos;] = v.split()[0].strip()</span><br><span class="line">                        item_ram_size = round(float(v.split()[0]))</span><br><span class="line">                    else:</span><br><span class="line">                        ram_item_to_dic[&apos;capacity&apos;] = 0</span><br><span class="line"></span><br><span class="line">                if key == &apos;Type&apos;:</span><br><span class="line">                    ram_item_to_dic[&apos;model&apos;] = v.strip()</span><br><span class="line">                if key == &apos;Manufacturer&apos;:</span><br><span class="line">                    ram_item_to_dic[&apos;manufacturer&apos;] = v.strip()</span><br><span class="line">                if key == &apos;Serial Number&apos;:</span><br><span class="line">                    ram_item_to_dic[&apos;sn&apos;] = v.strip()</span><br><span class="line">                if key == &apos;Asset Tag&apos;:</span><br><span class="line">                    ram_item_to_dic[&apos;asset_tag&apos;] = v.strip()</span><br><span class="line">                if key == &apos;Locator&apos;:</span><br><span class="line">                    ram_item_to_dic[&apos;slot&apos;] = v.strip()</span><br><span class="line"></span><br><span class="line">        if item_ram_size == 0:</span><br><span class="line">            pass</span><br><span class="line">        else:</span><br><span class="line">            ram_list.append(ram_item_to_dic)</span><br><span class="line"></span><br><span class="line">    raw_total_size = subprocess.Popen(&quot;cat /proc/meminfo|grep MemTotal &quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    raw_total_size = raw_total_size.stdout.read().decode().split(&quot;:&quot;)</span><br><span class="line">    ram_data = &#123;&apos;ram&apos;: ram_list&#125;</span><br><span class="line">    if len(raw_total_size) == 2:</span><br><span class="line">        total_gb_size = int(raw_total_size[1].split()[0]) / 1024**2</span><br><span class="line">        ram_data[&apos;ram_size&apos;] = total_gb_size</span><br><span class="line"></span><br><span class="line">    return ram_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_nic_info():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取网卡信息</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    raw_data = subprocess.Popen(&quot;ifconfig -a&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line"></span><br><span class="line">    raw_data = raw_data.stdout.read().decode().split(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    nic_dic = dict()</span><br><span class="line">    next_ip_line = False</span><br><span class="line">    last_mac_addr = None</span><br><span class="line"></span><br><span class="line">    for line in raw_data:</span><br><span class="line">        if next_ip_line:</span><br><span class="line">            next_ip_line = False</span><br><span class="line">            nic_name = last_mac_addr.split()[0]</span><br><span class="line">            mac_addr = last_mac_addr.split(&quot;HWaddr&quot;)[1].strip()</span><br><span class="line">            raw_ip_addr = line.split(&quot;inet addr:&quot;)</span><br><span class="line">            raw_bcast = line.split(&quot;Bcast:&quot;)</span><br><span class="line">            raw_netmask = line.split(&quot;Mask:&quot;)</span><br><span class="line">            if len(raw_ip_addr) &gt; 1:</span><br><span class="line">                ip_addr = raw_ip_addr[1].split()[0]</span><br><span class="line">                network = raw_bcast[1].split()[0]</span><br><span class="line">                netmask = raw_netmask[1].split()[0]</span><br><span class="line">            else:</span><br><span class="line">                ip_addr = None</span><br><span class="line">                network = None</span><br><span class="line">                netmask = None</span><br><span class="line">            if mac_addr not in nic_dic:</span><br><span class="line">                nic_dic[mac_addr] = &#123;&apos;name&apos;: nic_name,</span><br><span class="line">                                     &apos;mac&apos;: mac_addr,</span><br><span class="line">                                     &apos;net_mask&apos;: netmask,</span><br><span class="line">                                     &apos;network&apos;: network,</span><br><span class="line">                                     &apos;bonding&apos;: 0,</span><br><span class="line">                                     &apos;model&apos;: &apos;unknown&apos;,</span><br><span class="line">                                     &apos;ip_address&apos;: ip_addr,</span><br><span class="line">                                     &#125;</span><br><span class="line">            else:</span><br><span class="line">                if &apos;%s_bonding_addr&apos; % (mac_addr,) not in nic_dic:</span><br><span class="line">                    random_mac_addr = &apos;%s_bonding_addr&apos; % (mac_addr,)</span><br><span class="line">                else:</span><br><span class="line">                    random_mac_addr = &apos;%s_bonding_addr2&apos; % (mac_addr,)</span><br><span class="line"></span><br><span class="line">                nic_dic[random_mac_addr] = &#123;&apos;name&apos;: nic_name,</span><br><span class="line">                                            &apos;mac&apos;: random_mac_addr,</span><br><span class="line">                                            &apos;net_mask&apos;: netmask,</span><br><span class="line">                                            &apos;network&apos;: network,</span><br><span class="line">                                            &apos;bonding&apos;: 1,</span><br><span class="line">                                            &apos;model&apos;: &apos;unknown&apos;,</span><br><span class="line">                                            &apos;ip_address&apos;: ip_addr,</span><br><span class="line">                                            &#125;</span><br><span class="line"></span><br><span class="line">        if &quot;HWaddr&quot; in line:</span><br><span class="line">            next_ip_line = True</span><br><span class="line">            last_mac_addr = line</span><br><span class="line">    nic_list = []</span><br><span class="line">    for k, v in nic_dic.items():</span><br><span class="line">        nic_list.append(v)</span><br><span class="line"></span><br><span class="line">    return &#123;&apos;nic&apos;: nic_list&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_disk_info():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    获取存储信息。</span><br><span class="line">    本脚本只针对centos7.6中使用sda2，且只有一块虚拟硬盘的情况。</span><br><span class="line">    具体查看硬盘信息的命令，请根据实际情况，实际调整。</span><br><span class="line">    如果需要查看Raid信息，可以尝试MegaCli工具。</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sn_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-serial-number&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    sn = sn_raw_data.stdout.read().decode()</span><br><span class="line">    model_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-product-name&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    model = model_raw_data.stdout.read().decode()</span><br><span class="line"></span><br><span class="line">    #size_data = subprocess.Popen(&quot;sudo fdisk -l /dev/sda2 | grep Disk|head -1&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    #size_data = size_data.stdout.read().decode()</span><br><span class="line">    #size = size_data.split(&quot;:&quot;)[1].strip().split(&quot; &quot;)[0]</span><br><span class="line"></span><br><span class="line">    size_raw_data = subprocess.Popen(&quot;sudo smartctl -a /dev/sda2 |grep Capacity&quot;, stdout=subprocess.PIPE, shell=True)</span><br><span class="line">    raw_data = size_raw_data.stdout.read().decode()</span><br><span class="line">    data_list = raw_data.split()[4]</span><br><span class="line">    size = data_list.split(&apos;[&apos;)[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    result = &#123;&apos;physical_disk_driver&apos;: []&#125;</span><br><span class="line">    disk_dict = dict()</span><br><span class="line">    disk_dict[&quot;model&quot;] = model</span><br><span class="line">    disk_dict[&quot;size&quot;] = size</span><br><span class="line">    disk_dict[&quot;sn&quot;] = sn</span><br><span class="line">    result[&apos;physical_disk_driver&apos;].append(disk_dict)</span><br><span class="line"></span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    # 收集信息功能测试</span><br><span class="line">    d = collect()</span><br><span class="line">    print(d)</span><br></pre></td></tr></table></figure><p>先来个输出在 centos7.6虚机上面的测试（可以读出所有数据）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;asset_type&apos;: &apos;server&apos;, &apos;manufacturer&apos;: &apos;VMware, Inc.&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60&apos;, &apos;model&apos;: &apos;VMware Virtual Platform&apos;, &apos;uuid&apos;: &apos;68254d56-ee5c-fbdc-a15e-776a5fe76660&apos;, &apos;wake_up_type&apos;: &apos;Power Switch&apos;, &apos;os_distribution&apos;: &apos;CentOS&apos;, &apos;os_release&apos;: &apos;CentOS Linux release 7.6.1810 (Core)&apos;, &apos;os_type&apos;: &apos;Linux&apos;, &apos;cpu_count&apos;: &apos;1&apos;, &apos;cpu_core_count&apos;: &apos;1&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;ram&apos;: [&#123;&apos;capacity&apos;: &apos;1024&apos;, &apos;slot&apos;: &apos;RAM slot #0&apos;, &apos;model&apos;: &apos;DRAM&apos;, &apos;manufacturer&apos;: &apos;Not Specified&apos;, &apos;sn&apos;: &apos;Not Specified&apos;, &apos;asset_tag&apos;: &apos;Not Specified&apos;&#125;], &apos;ram_size&apos;: 0.9497604370117188, &apos;nic&apos;: [], &apos;physical_disk_driver&apos;: [&#123;&apos;model&apos;: &apos;VMware Virtual Platform\n&apos;, &apos;size&apos;: &apos;32.2&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60\n&apos;&#125;]&#125;</span><br></pre></td></tr></table></figure></p><p>代码整体没有什么难点，无非就是使用subprocess.Popen()方法执行Linux的命令，然后获取返回值，并以规定的格式打包到data字典里。</p><p>需要说明的问题有：</p><ul><li>当Linux中存在好几个Python解释器版本时，要注意调用方式，前面已经强调过了；</li><li>不同的Linux发行版，<strong>有些命令可能没有，需要额外安装</strong>；</li><li>所使用的查看硬件信息的命令并不一定必须和这里的一样，只要能获得数据就行；</li><li>有一些命令在ubuntu中涉及sudo的问题，需要特别对待；</li><li>最终数据字典的格式一定要正确。</li><li>可以在Linux下配置cronb或其它定时服务，设置定期的数据收集、报告任务。<br>下面，我们在Linux虚拟机上，测试一下客户端。</li></ul><p>将Pycharm中的Client客户端文件夹，拷贝到Linux虚拟机中，我这里是centos7.6</p><p>进入bin目录，运行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 main.py report_data</span><br></pre></td></tr></table></figure></p><p>一切顺利的话应该能得到如下的反馈：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在将数据发送至： [http://10.101.120.34:8000/assets/report/]  ......</span><br><span class="line">handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22asset_type%22%3A+%22server%22%2C+%22manufacturer%22%3A+%22VMware%2C+Inc.%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%22%2C+%22model%22%3A+%22VMware+Virtual+Platform%22%2C+%22uuid%22%3A+%2268254d56-ee5c-fbdc-a15e-776a5fe76660%22%2C+%22wake_up_type%22%3A+%22Power+Switch%22%2C+%22os_distribution%22%3A+%22CentOS%22%2C+%22os_release%22%3A+%22CentOS+Linux+release+7.6.1810+%28Core%29%22%2C+%22os_type%22%3A+%22Linux%22%2C+%22cpu_count%22%3A+%221%22%2C+%22cpu_core_count%22%3A+%221%22%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22ram%22%3A+%5B%7B%22capacity%22%3A+%221024%22%2C+%22slot%22%3A+%22RAM+slot+%230%22%2C+%22model%22%3A+%22DRAM%22%2C+%22manufacturer%22%3A+%22Not+Specified%22%2C+%22sn%22%3A+%22Not+Specified%22%2C+%22asset_tag%22%3A+%22Not+Specified%22%7D%5D%2C+%22ram_size%22%3A+0.9497604370117188%2C+%22nic%22%3A+%5B%5D%2C+%22physical_disk_driver%22%3A+%5B%7B%22model%22%3A+%22VMware+Virtual+Platform%5Cn%22%2C+%22size%22%3A+%2232.2%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%5Cn%22%7D%5D%7D&apos;</span><br><span class="line">发送完毕！</span><br><span class="line">返回结果：成功收到数据！</span><br><span class="line">日志记录成功！</span><br></pre></td></tr></table></figure></p><p>然后我们可以在pycharm 页面收到：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190304/fWKMrrjIgh2p.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>规整如下：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190304/WUx9PrE2BOMP.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB-Windows下收集数据（四）</title>
    <link href="/2019/03/03/4%E3%80%81Windows%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE/"/>
    <url>/2019/03/03/4%E3%80%81Windows%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="一、windows中收集硬件信息"><a href="#一、windows中收集硬件信息" class="headerlink" title="一、windows中收集硬件信息"></a>一、windows中收集硬件信息</h3><p>为了收集运行Windows操作系统的服务器的硬件信息，我们需要编写一个专门的脚本。</p><p>在Pycharm的<strong>Client目录下的plugins包中，新建一个windows包，然后创建一个sys_info.py文件</strong>，写入下面的代码：</p><p>==&lt;如下打印data的语句，是为了调试查看输出，可以去掉&gt;==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-2 16:41</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : sys_info.py</span><br><span class="line"></span><br><span class="line">import platform</span><br><span class="line">import win32com</span><br><span class="line">import wmi</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">本模块基于windows操作系统，依赖wmi和win32com库，需要提前使用pip进行安装，</span><br><span class="line">或者下载安装包手动安装。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def collect():</span><br><span class="line">    data = &#123;</span><br><span class="line">        &apos;os_type&apos;: platform.system(),</span><br><span class="line">        &apos;os_release&apos;: &quot;%s %s  %s &quot; % (platform.release(), platform.architecture()[0], platform.version()),</span><br><span class="line">        &apos;os_distribution&apos;: &apos;Microsoft&apos;,</span><br><span class="line">        &apos;asset_type&apos;: &apos;server&apos;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # 分别获取各种硬件信息</span><br><span class="line">    win32obj = Win32Info()</span><br><span class="line">    data.update(win32obj.get_cpu_info())</span><br><span class="line">    data.update(win32obj.get_ram_info())</span><br><span class="line">    data.update(win32obj.get_motherboard_info())</span><br><span class="line">    data.update(win32obj.get_disk_info())</span><br><span class="line">    data.update(win32obj.get_nic_info())</span><br><span class="line">    # 最后返回一个数据字典</span><br><span class="line">    print(&quot;data11&quot;, data)</span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Win32Info(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 固定用法，更多内容请参考模块说明</span><br><span class="line">        self.wmi_obj = wmi.WMI()</span><br><span class="line">        self.wmi_service_obj = win32com.client.Dispatch(&quot;WbemScripting.SWbemLocator&quot;)</span><br><span class="line">        self.wmi_service_connector = self.wmi_service_obj.ConnectServer(&quot;.&quot;, &quot;root\cimv2&quot;)</span><br><span class="line"></span><br><span class="line">    def get_cpu_info(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        获取CPU的相关数据，这里只采集了三个数据，实际有更多，请自行选择需要的数据</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        data = &#123;&#125;</span><br><span class="line">        cpu_lists = self.wmi_obj.Win32_Processor()</span><br><span class="line">        cpu_core_count = 0</span><br><span class="line">        for cpu in cpu_lists:</span><br><span class="line">            cpu_core_count += cpu.NumberOfCores</span><br><span class="line"></span><br><span class="line">        cpu_model = cpu_lists[0].Name   # CPU型号（所有的CPU型号都是一样的）</span><br><span class="line">        data[&quot;cpu_count&quot;] = len(cpu_lists)      # CPU个数</span><br><span class="line">        data[&quot;cpu_model&quot;] = cpu_model</span><br><span class="line">        data[&quot;cpu_core_count&quot;] = cpu_core_count  # CPU总的核数</span><br><span class="line">        print(&quot;data22&quot;, data)</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">    def get_ram_info(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        收集内存信息</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        data = []</span><br><span class="line">        # 这个模块用SQL语言获取数据</span><br><span class="line">        ram_collections = self.wmi_service_connector.ExecQuery(&quot;Select * from Win32_PhysicalMemory&quot;)</span><br><span class="line">        for item in ram_collections:    # 主机中存在很多根内存，要循环所有的内存数据</span><br><span class="line">            ram_size = int(int(item.Capacity) / (1024**3))  # 转换内存单位为GB</span><br><span class="line">            item_data = &#123;</span><br><span class="line">                &quot;slot&quot;: item.DeviceLocator.strip(),</span><br><span class="line">                &quot;capacity&quot;: ram_size,</span><br><span class="line">                &quot;model&quot;: item.Caption,</span><br><span class="line">                &quot;manufacturer&quot;: item.Manufacturer,</span><br><span class="line">                &quot;sn&quot;: item. SerialNumber,</span><br><span class="line">            &#125;</span><br><span class="line">            data.append(item_data)  # 将每条内存的信息，添加到一个列表里</span><br><span class="line">        print(&quot;data33&quot;, data)</span><br><span class="line">        return &#123;&quot;ram&quot;: data&#125;    # 再对data列表封装一层，返回一个字典，方便上级方法的调用</span><br><span class="line"></span><br><span class="line">    def get_motherboard_info(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        获取主板信息</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        computer_info = self.wmi_obj.Win32_ComputerSystem()[0]</span><br><span class="line">        system_info = self.wmi_obj.Win32_OperatingSystem()[0]</span><br><span class="line">        data = dict()</span><br><span class="line">        data[&apos;manufacturer&apos;] = computer_info.Manufacturer</span><br><span class="line">        data[&apos;model&apos;] = computer_info.Model</span><br><span class="line">        data[&apos;wake_up_type&apos;] = computer_info.WakeUpType</span><br><span class="line">        data[&apos;sn&apos;] = system_info.SerialNumber</span><br><span class="line">        print(&quot;data44&quot;, data)</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">    def get_disk_info(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        硬盘信息</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        data = []</span><br><span class="line">        for disk in self.wmi_obj.Win32_DiskDrive():     # 每块硬盘都要获取相应信息</span><br><span class="line">            item_data = dict()</span><br><span class="line">            iface_choices = [&quot;SAS&quot;, &quot;SCSI&quot;, &quot;SATA&quot;, &quot;SSD&quot;]</span><br><span class="line">            for iface in iface_choices:</span><br><span class="line">                if iface in disk.Model:</span><br><span class="line">                    item_data[&apos;iface_type&apos;] = iface</span><br><span class="line">                    break</span><br><span class="line">            else:</span><br><span class="line">                item_data[&apos;iface_type&apos;] = &apos;unknown&apos;</span><br><span class="line">            item_data[&apos;slot&apos;] = disk.Index</span><br><span class="line">            item_data[&apos;sn&apos;] = disk.SerialNumber</span><br><span class="line">            item_data[&apos;model&apos;] = disk.Model</span><br><span class="line">            item_data[&apos;manufacturer&apos;] = disk.Manufacturer</span><br><span class="line">            item_data[&apos;capacity&apos;] = int(int(disk.Size) / (1024**3))</span><br><span class="line">            data.append(item_data)</span><br><span class="line">        print(&quot;data55&quot;, data)</span><br><span class="line">        return &#123;&apos;physical_disk_driver&apos;: data&#125;</span><br><span class="line"></span><br><span class="line">    def get_nic_info(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        网卡信息</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        data = []</span><br><span class="line">        for nic in self.wmi_obj.Win32_NetworkAdapterConfiguration():</span><br><span class="line">            if nic.MACAddress is not None:</span><br><span class="line">                item_data = dict()</span><br><span class="line">                item_data[&apos;mac&apos;] = nic.MACAddress</span><br><span class="line">                item_data[&apos;model&apos;] = nic.Caption</span><br><span class="line">                item_data[&apos;name&apos;] = nic.Index</span><br><span class="line">                if nic.IPAddress is not None:</span><br><span class="line">                    item_data[&apos;ip_address&apos;] = nic.IPAddress[0]</span><br><span class="line">                    item_data[&apos;net_mask&apos;] = nic.IPSubnet</span><br><span class="line">                else:</span><br><span class="line">                    item_data[&apos;ip_address&apos;] = &apos;&apos;</span><br><span class="line">                    item_data[&apos;net_mask&apos;] = &apos;&apos;</span><br><span class="line">                data.append(item_data)</span><br><span class="line">        print(&quot;data66&quot;, data)</span><br><span class="line">        return &#123;&apos;nic&apos;: data&#125;</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    # 测试代码(仅限于在当前页面运行获取本机的信息）</span><br><span class="line">    dic = collect()</span><br><span class="line">    print(dic)</span><br></pre></td></tr></table></figure></p><p>windows中没有方便的命令可以获取硬件信息，但是有额外的模块可以帮助我们实现目的，这个模块叫做wmi。可以使用<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install wmi</span><br></pre></td></tr></table></figure></p><p>的方式安装，当前版本是1.4.9。但是wmi安装后，import wmi依然会出错，因为它依赖一个叫做win32com的模块。</p><p>我们依然可以通过<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install pypiwin32</span><br></pre></td></tr></table></figure></p><p>来安装win32com模块，但是不幸的是，据反映，有些机器无法通过pip成功安装。所以，这里我在github中提供了一个手动安装包==pywin32-220.win-amd64-py3.5(配合wmi模块，获取主机信息的模块).exe==，方便大家。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https://pan.baidu.com/s/14ZUKPJnmlwuUHsYLUUApKg </span><br><span class="line">提取码：nq5i</span><br></pre></td></tr></table></figure></p><p>依赖包的问题解决后，我们来看一下==sys_info.py==脚本的代码。</p><ul><li>核心在于collect()方法！</li><li>该方法首先通过platform模块获取平台的信息，然后保存到一个data字典中。</li><li>然后创建一个Win32Info对象，并调用win32的各种功能方法，分别获取CPU、RAM、主板、硬盘和网卡的信息。</li><li>类Win32Info是我们编写的封装了具体数据收集逻辑的类；</li><li>该类中有很多方法，每个方法针对一项数据；</li><li>其中对Win32模块的调用方式是固定的，有兴趣的可以自行学习这个模块的官方文档</li><li>每一类的数据收集完成后都会作为一个新的子字典，update到开始的data字典中，最终形成完整的信息字典。</li><li>最后在脚本末尾有一个测试入口。</li></ul><p>整个脚本的代码其实很简单，我们只要将Win32的方法调用当作透明的空气，剩下的不过就是将获得的数据，按照我们指定的格式打包成一个数据字典。</p><p>==<strong>强调：数据字典的格式和键值是非常重要的，是预设的，不可以随意改变！</strong>==</p><h3 id="二、信息收集测试"><a href="#二、信息收集测试" class="headerlink" title="二、信息收集测试"></a>二、信息收集测试</h3><p>单独运行一下该脚本（注意不是运行CMDB项目），查看一下生成的数据：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data22 &#123;&apos;cpu_core_count&apos;: 4, &apos;cpu_count&apos;: 1, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;&#125;</span><br><span class="line">data33 [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;]</span><br><span class="line">data44 &#123;&apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;wake_up_type&apos;: 6&#125;</span><br><span class="line">data55 [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos;      S3U0NY0K244546&apos;&#125;]</span><br><span class="line">data66 [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;]</span><br><span class="line">data11 &#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos;      S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit  10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125;</span><br><span class="line">&#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos;      S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit  10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125;</span><br></pre></td></tr></table></figure></p><p>上面的信息包含操作系统、主板、CPU、内存、硬盘、网卡等各种信息。可以看到我有1条内存，1块SSD硬盘，以及4块网卡。四块网卡有出现mac地址相同的情况，因为那是虚拟机的。</p><p>你自己的数据和我的肯定不一样，但是数据格式和键值必须一样，我们后面自动分析数据、填充数据，都依靠这个固定格式的数据字典。</p><p>通过测试我们发现数据可以收集到了，那么再测试一下数据能否正常发送到服务器。</p><h3 id="三、数据发送测试"><a href="#三、数据发送测试" class="headerlink" title="三、数据发送测试"></a>三、数据发送测试</h3><p>由于我这里采用了Linux虚拟机作为测试用例，我们的Django服务器就不能再运行在127.0.0.1:8000上面了。</p><p>查看一下当前机器的IP，发现是192.168.1.100，修改项目的settings.py文件，将ALLOWED_HOSTS修改如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALLOWED_HOSTS = [&quot;*&quot;]</span><br></pre></td></tr></table></figure><p>这表示接收所有同一局域网内的网络访问。</p><p>然后以<strong>0.0.0.0:8000的参数启动CMDB</strong>，表示对局域网内所有ip开放服务。</p><p><img src="http://myimage.okay686.cn/okay686cn/20190303/wNHqtOQNReIs.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>回到客户端，进入Client/bin目录，运行<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python main.py report_data</span><br></pre></td></tr></table></figure></p><p>可以看到如下结果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在将数据发送至： [http://10.10.7.26:8000/assets/report/]  ......</span><br><span class="line">handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22cpu_core_count%22%3A+4%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22wake_up</span><br><span class="line">...&lt;中间信息省略&gt;...</span><br><span class="line">71b+M.2+2280+256GB%22%2C+%22capacity%22%3A+238%7D%5D%2C+%22cpu_count%22%3A+1%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22asset_type%22%3A+%22server%22%7D</span><br><span class="line">&apos;</span><br><span class="line">?[31;1m发送失败，HTTP Error 404: Not Found?[0m</span><br><span class="line">日志记录成功！</span><br></pre></td></tr></table></figure></p><p>这是一个404错误，表示服务器地址没找到，这是因为我们还没有为Django编写接收数据的视图和路由。</p><p>这时，打开log目录下的日志文件，内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">发送时间：2019-03-03 10:49:26  服务器地址：http://10.10.7.26:8000/assets/report/  返回结果：发送失败</span><br></pre></td></tr></table></figure></p><h4 id="四、接收数据"><a href="#四、接收数据" class="headerlink" title="四、接收数据"></a>四、接收数据</h4><p>进入==cmdb/urls.py==文件中，编写一个二级路由，将所有++assets相关的数据都转发到assets.urls++中，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.contrib import admin</span><br><span class="line">from django.conf.urls import url, include</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(&apos;admin/&apos;, admin.site.urls),</span><br><span class="line">    url(r&apos;^assets/&apos;, include(&apos;assets.urls&apos;)),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>然后，我们在assets中新建一个urls.py文件，写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url</span><br><span class="line">from assets import views</span><br><span class="line"></span><br><span class="line">app_name = &apos;assets&apos;</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>这样，我们的路由就写好了。</p><p>转过头，我们进入++assets/views.py++文件，写一个简单的视图。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import render</span><br><span class="line"></span><br><span class="line"># Create your views here.</span><br><span class="line"></span><br><span class="line">from django.shortcuts import render, HttpResponse</span><br><span class="line"></span><br><span class="line">def report(request):</span><br><span class="line">    if request.method == &quot;POST&quot;:</span><br><span class="line">        asset_data = request.POST.get(&apos;asset_data&apos;)</span><br><span class="line">        print(asset_data)</span><br><span class="line">        return HttpResponse(&quot;成功收到数据！&quot;)</span><br></pre></td></tr></table></figure></p><p>代码很简单，接收POST过来的数据，打印出来，然后返回成功的消息。</p><p>重新运<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">行python main.py report_data</span><br></pre></td></tr></table></figure></p><p>可以看到：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在将数据发送至： [http://10.10.7.26:8000/assets/report/]  ......</span><br><span class="line">handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22ram%22%3A+%5B%7B%22manufacturer%22%3A+%22802C0</span><br><span class="line">...&lt;中间部分省略&gt;...</span><br><span class="line">%22%3A+%22Latitude+5480%22%2C+%22sn%22%3A+%2200330-80000-00000-AA069%22%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22manufacturer%22%3A+%22Dell+Inc.%22%7D</span><br><span class="line">&apos;</span><br><span class="line">?[31;1m发送失败，&lt;urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。&gt;?[0m</span><br><span class="line">日志记录成功！</span><br></pre></td></tr></table></figure><p>遇到拒绝服务的错误了。</p><p>原因在于我们模拟浏览器发送了一个POST请求给Django，但是请求中没有携带Django需要的csrf安全令牌，所以拒绝了请求。</p><p>为了解决这个问题，我们需要在这个report视图上忽略csrf验证，可以通过Django的@csrf_exempt装饰器。修改代码如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import render, HttpResponse</span><br><span class="line">from django.views.decorators.csrf import csrf_exempt</span><br><span class="line"></span><br><span class="line"># Create your views here.</span><br><span class="line"></span><br><span class="line">@csrf_exempt</span><br><span class="line">def report(request):</span><br><span class="line">    if request.method == &quot;POST&quot;:</span><br><span class="line">        asset_data = request.POST.get(&apos;asset_data&apos;)</span><br><span class="line">        print(asset_data)</span><br><span class="line">        return HttpResponse(&quot;成功收到数据！&quot;)</span><br></pre></td></tr></table></figure></p><p><strong>重启CMDB服务器</strong>，再次从客户端报告数据，可以看到返回结果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">正在将数据发送至： [http://10.10.7.26:8000/assets/report/]  ......</span><br><span class="line">handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22nic%22%3A+%5B%7B%22name%22%3A+1%2C+%22model%22%3A+%22%5B00000001%5D+VMware+Virt</span><br><span class="line">...&lt;中间部分省略&gt;...</span><br><span class="line">facturer%22%3A+%22802C0000802C%22%2C+%22slot%22%3A+%22DIMM+A%22%2C+%22sn%22%3A+%221B458788%22%2C+%22model%22%3A+%22%5Cu7269%5Cu7406%5Cu5185%5Cu5b58%22%7D%5D%7D</span><br><span class="line">&apos;</span><br><span class="line">?[31;1m发送完毕！?[0m</span><br><span class="line">返回结果：成功收到数据！</span><br><span class="line">日志记录成功！</span><br></pre></td></tr></table></figure></p><p>看下日志记录：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">发送时间：2019-03-03 10:49:26  服务器地址：http://10.10.7.26:8000/assets/report/  返回结果：发送失败 </span><br><span class="line">发送时间：2019-03-03 11:00:53  服务器地址：http://10.10.7.26:8000/assets/report/  返回结果：发送失败 </span><br><span class="line">发送时间：2019-03-03 11:05:36  服务器地址：http://10.10.7.26:8000/assets/report/  返回结果：发送失败 </span><br><span class="line">发送时间：2019-03-03 11:06:51  服务器地址：http://10.10.7.26:8000/assets/report/  返回结果：成功收到数据！</span><br></pre></td></tr></table></figure></p><p>这表明数据发送成功了。</p><p>再看Pycharm中，也打印出了接收到的数据，一切OK！</p><p>CSRF验证的问题解决了，但是又带来新的安全问题。我们可以通过增加用户名、密码，或者md5验证或者自定义安全令牌的方式解决，这部分内容就不展开了。</p><p>Windows下的客户端已经验证完毕了，然后我们就可以通过各种方式让脚本定时运行、收集和报告数据，一切都自动化。</p><h4 id="最后补充："><a href="#最后补充：" class="headerlink" title="最后补充："></a>最后补充：</h4><p>++CMDB系统是部署在A服务器上，那么客户端CLIENT是需要部署在B服务器上的，那就意味着每台需要被采集数据的服务器都要安装PYTHON及所用到的包。++</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB数据收集客户端（三）</title>
    <link href="/2019/03/02/3%E3%80%81%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
    <url>/2019/03/02/3%E3%80%81%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
    
    <content type="html"><![CDATA[<blockquote><p>CMDB最主要的管理对象：服务器，其数据信息自然不可能通过手工收集，必须以客户端的方式，定时自动收集并报告给远程的服务器。</p></blockquote><p>下面，让我们暂时忘掉Django，进入Python运维的世界……</p><h3 id="一、客户端程序组织"><a href="#一、客户端程序组织" class="headerlink" title="一、客户端程序组织"></a>一、客户端程序组织</h3><p>编写客户端，不能一个py脚本包打天下，要有组织有目的，通常我们会采取下面的结构：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190302/rVgefQByDvNn.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>在Pycharm下，创建一个Client文件夹，作为客户端的根目录。</p><p>在Client下，创建上面的包。注意是包，不是文件夹：</p><ul><li>bin是客户端启动脚本的所在目录</li><li>conf是配置文件目录</li><li>core是核心代码目录</li><li>log是日志文件保存目录</li><li>plugins是插件或工具目录</li></ul><h3 id="二、开发数据收集客户端"><a href="#二、开发数据收集客户端" class="headerlink" title="二、开发数据收集客户端"></a>二、开发数据收集客户端</h3><h4 id="1-程序入口脚本"><a href="#1-程序入口脚本" class="headerlink" title="1.程序入口脚本"></a>1.程序入口脚本</h4><p>++在bin目录中新建main.py文件++，写入下面的代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.getcwd())</span><br><span class="line"># 设置工作目录，使得包和模块能够正常导入</span><br><span class="line">sys.path.append(BASE_DIR)</span><br><span class="line"></span><br><span class="line">from core import handler</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">    handler.ArgvHandler(sys.argv)   ##获取参数，传入到ArgvHandler()</span><br></pre></td></tr></table></figure><ul><li>通过os和sys模块的配合，将当前客户端所在目录设置为工作目录，如果不这么做，会无法导入其它模块；</li><li>handler模块是核心代码模块，在core目录中，我们一会来实现它。</li><li>以后调用客户端就只需要执行python main.py 参数就可以了</li></ul><p><strong>++这里有个问题一定要强调一下，那就是Python解释器的调用，执行命令的方式和代码第一行#!/usr/bin/env python的指定方式一定不能冲突，要根据你的实际情况实际操作和修改代码，很多新手连Python本身都没搞明白就上来执行脚本，碰到各种解释器不合法的错误，请回去补足基础！++</strong></p><h4 id="2-主功能模块"><a href="#2-主功能模块" class="headerlink" title="2.主功能模块"></a>2.主功能模块</h4><p>++在core下，创建handler.py文件++，写入下面的代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-2 13:51</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : handler.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">import urllib.parse</span><br><span class="line">import urllib.request</span><br><span class="line">from core import info_collection</span><br><span class="line">from conf import settings</span><br><span class="line"></span><br><span class="line">class ArgvHandler(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, args):</span><br><span class="line">        self.args = args</span><br><span class="line">        self.parse_args()</span><br><span class="line"></span><br><span class="line">    def parse_args(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        分析参数，如果有参数指定的功能，则执行该功能，如果没有，打印帮助说明。</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if len(self.args) &gt; 1 and hasattr(self, self.args[1]):</span><br><span class="line">            func = getattr(self, self.args[1])</span><br><span class="line">            func()</span><br><span class="line">        else:</span><br><span class="line">            self.help_msg()     ##如果执行程序没有带参数就会提示如下信息：help_msg()</span><br><span class="line"></span><br><span class="line">    @staticmethod       #静态方法 类或实例均可调用,静态方法函数里不传入self，这样如上self.help_msg()就可以调用了</span><br><span class="line">    def help_msg():</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        帮助说明</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        msg = &apos;&apos;&apos;</span><br><span class="line">        collect_data        收集硬件信息</span><br><span class="line">        report_data         收集硬件信息并汇报</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        print(msg)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def collect_data():</span><br><span class="line">        &quot;&quot;&quot;收集硬件信息,用于测试！&quot;&quot;&quot;</span><br><span class="line">        info = info_collection.InfoCollection()</span><br><span class="line">        asset_data = info.collect()</span><br><span class="line">        print(asset_data)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def report_data():</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        收集硬件信息，然后发送到服务器。</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 收集信息</span><br><span class="line">        info = info_collection.InfoCollection()</span><br><span class="line">        asset_data = info.collect()</span><br><span class="line">        # 将数据打包到一个字典内，并转换为json格式</span><br><span class="line">        data = &#123;&quot;asset_data&quot;: json.dumps(asset_data)&#125;</span><br><span class="line">        print(&quot;handler_data--&gt;&gt;&quot;, data)</span><br><span class="line">        # 根据settings中的配置，构造url</span><br><span class="line">        url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;])</span><br><span class="line">        print(&quot;handler_url--&gt;&gt;&quot;, url)</span><br><span class="line">        print(&apos;正在将数据发送至： [%s]  ......&apos; % url)</span><br><span class="line">        try:</span><br><span class="line">            # 使用Python内置的urllib.request库，发送post请求。</span><br><span class="line">            # 需要先将数据进行封装，并转换成bytes类型</span><br><span class="line">            data_encode = urllib.parse.urlencode(data).encode()</span><br><span class="line">            print(&quot;handler_data_encode--&gt;&gt;&quot;, data_encode)</span><br><span class="line">            response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;])</span><br><span class="line">            print(&quot;\033[31;1m发送完毕！\033[0m &quot;)</span><br><span class="line">            message = response.read().decode()</span><br><span class="line">            print(&quot;返回结果：%s&quot; % message)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            message = &quot;发送失败&quot;</span><br><span class="line">            print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e)</span><br><span class="line">        # 记录发送日志</span><br><span class="line">        with open(settings.PATH, &apos;ab&apos;) as f:        ##a追加,b二进制文件</span><br><span class="line">            string = &apos;发送时间：%s \t 服务器地址：%s \t 返回结果：%s \n&apos; % (time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;), url, message)</span><br><span class="line">            f.write(string.encode())</span><br><span class="line">            print(&quot;日志记录成功！&quot;)</span><br></pre></td></tr></table></figure><p><strong>说明：</strong></p><ul><li>handler模块中只有一个ArgvHandler类；</li><li>在main模块中也是实例化了一个ArgvHandler类的对象，并将调用参数传递进去；</li><li>首先，初始化方法会保存调用参数，然后执行parse_args()方法分析参数；</li><li>如果ArgvHandler类有参数指定的功能，则执行该功能，如果没有，打印帮助说明。</li><li>目前ArgvHandler类只有两个核心方法：collect_data和report_dataa；</li><li>这两个方法一个是收集数据并打印到屏幕，用于测试；report_data方法才会将实际的数据发往服务器。</li><li>数据的收集由info_collection.InfoCollection类负责，一会再看；</li><li>report_data方法会将收集到的数据打包到一个字典内，并转换为json格式；</li><li>然后通过settings中的配置，构造发送目的地url；</li><li>通过Python内置的urllib.parse对数据进行封装；</li><li>通过urllib.request将数据发送到目的url；</li><li>接收服务器返回的信息；</li><li>将成功或者失败的信息写入日志文件中。</li></ul><p>以后，我们要测试数据收集，执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python main.py collect_data</span><br></pre></td></tr></table></figure></p><p>要实际往服务器发送收集到的数据，则执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python main.py report_data</span><br></pre></td></tr></table></figure></p><h4 id="3-配置文件"><a href="#3-配置文件" class="headerlink" title="3.配置文件"></a>3.配置文件</h4><p>要将所有可能修改的数据、常量、配置等都尽量以配置文件的形式组织起来，尽量不要在代码中写死任何数据。</p><p>++在conf中，新建settings.py文件++，写入下面的代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-2 14:21</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : settings.py</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 远端服务器配置</span><br><span class="line">Params = &#123;</span><br><span class="line">    &quot;server&quot;: &quot;10.10.7.26&quot;,</span><br><span class="line">    &quot;port&quot;: 8000,</span><br><span class="line">    &apos;url&apos;: &apos;/assets/report/&apos;,</span><br><span class="line">    &apos;request_timeout&apos;: 30,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 日志文件配置</span><br><span class="line"></span><br><span class="line">PATH = os.path.join(os.path.dirname(os.getcwd()), &apos;log&apos;, &apos;cmdb.log&apos;)</span><br><span class="line">print(&quot;conf_settings--&gt;&gt;&quot;, PATH)</span><br><span class="line"></span><br><span class="line"># 更多配置，请都集中在此文件中</span><br></pre></td></tr></table></figure></p><p>这里，配置了服务器地址、端口、发送的url、请求的超时时间，以及日志文件路径。请根据你的实际情况进行修改。</p><p>==如上server端我是直接启动的django服务 也就是如上10.10.7.26是我笔记本的IP地址，这样默认我笔记本的10.10.7.26:8000就对外开放了！==</p><h4 id="4-信息收集模块"><a href="#4-信息收集模块" class="headerlink" title="4.信息收集模块"></a>4.信息收集模块</h4><p>++在core中新建info_collection.py文件++，写入下面的代码：</p><p>==&lt;关于如下：from plugins.linux import sys_info 以及 from plugins.windows import sys_info as win_sys_info 稍后章节我们会建立一系列的目录！！&gt;==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2019-3-2 14:48</span><br><span class="line"># @Author  : zhdya@zhdya.cn</span><br><span class="line"># @File    : info_collection.py</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">import platform</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def linux_sys_info():</span><br><span class="line">    from plugins.linux import sys_info</span><br><span class="line">    return sys_info.collect()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def windows_sys_info():</span><br><span class="line">    from plugins.windows import sys_info as win_sys_info</span><br><span class="line">    return win_sys_info.collect()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class InfoCollection(object):</span><br><span class="line"></span><br><span class="line">    def collect(self):</span><br><span class="line">        # 收集平台信息</span><br><span class="line">        # 首先判断当前平台，根据平台的不同，执行不同的方法</span><br><span class="line">        try:</span><br><span class="line">            func = getattr(self, platform.system())</span><br><span class="line">            info_data = func()</span><br><span class="line">            formatted_data = self.build_report_data(info_data)</span><br><span class="line">            return formatted_data</span><br><span class="line">        except AttributeError:</span><br><span class="line">            sys.exit(&quot;不支持当前操作系统： [%s]! &quot; % platform.system())</span><br><span class="line"></span><br><span class="line">    def Linux(self):</span><br><span class="line"></span><br><span class="line">        return linux_sys_info()</span><br><span class="line"></span><br><span class="line">    def Windows(self):</span><br><span class="line">        return windows_sys_info()</span><br><span class="line"></span><br><span class="line">    def build_report_data(self, data):</span><br><span class="line">        # 留下一个接口，方便以后增加功能或者过滤数据</span><br><span class="line">        return data</span><br></pre></td></tr></table></figure></p><p>该模块的作用很简单：</p><ul><li>首先通过Python内置的platform模块获取执行main脚本的操作系统类别，通常是windows和Linux；</li><li>根据操作系统的不同，反射获取相应的信息收集方法，并执行</li><li>如果是客户端不支持的操作系统，比如苹果系统，则提示并退出客户端。</li></ul><p>因为windows和Linux两大操作系统的巨大平台差异，我们必须写两个收集信息的脚本。</p><p>到目前未知，我们的客户端结构如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190303/Cn2Ul2PnR36h.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB模型设计（二）</title>
    <link href="/2019/03/01/2%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/"/>
    <url>/2019/03/01/2%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h3 id="一、创建项目"><a href="#一、创建项目" class="headerlink" title="一、创建项目"></a>一、创建项目</h3><p>创建Django项目cmdb，配置好settings中的语言和时区，最后新建一个app，名字就叫做assets。这些基本过程以后就不再赘述了，不熟悉的请参考教程的前面部分。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">django版本：1.11.11</span><br></pre></td></tr></table></figure><p>创建成功后，初始状态如下图所示：</p><p><img src="http://myimage.okay686.cn/okay686cn/20190302/FMQqqBKM9bRQ.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>二、模型设计<br>说明：本项目依然采用SQLite数据库，等下一个项目再使用Mysql。</p><p>模型设计是整个项目的重中之重，其它所有的内容其实都是围绕它展开的。</p><p>而我们设计数据模型的原则和参考依据是前一节分析的项目需求和数据分类表。</p><h4 id="1-资产共有数据模型"><a href="#1-资产共有数据模型" class="headerlink" title="1.资产共有数据模型"></a>1.资产共有数据模型</h4><p>打开assets/models.py文件，首先我们要设计一张资产表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.db import models</span><br><span class="line">from django.contrib.auth.models import User</span><br><span class="line"></span><br><span class="line"># Create your models here.</span><br><span class="line"></span><br><span class="line">class Asset(models.Model):</span><br><span class="line">    &quot;&quot;&quot;    所有资产的共有数据表    &quot;&quot;&quot;</span><br><span class="line">    asset_type_choice = (</span><br><span class="line">        (&apos;server&apos;, &apos;服务器&apos;),</span><br><span class="line">        (&apos;networkdevice&apos;, &apos;网络设备&apos;),</span><br><span class="line">        (&apos;storagedevice&apos;, &apos;存储设备&apos;),</span><br><span class="line">        (&apos;securitydevice&apos;, &apos;安全设备&apos;),</span><br><span class="line">        (&apos;software&apos;, &apos;软件资产&apos;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset_status = (</span><br><span class="line">        (0, &apos;在线&apos;),</span><br><span class="line">        (1, &apos;下线&apos;),</span><br><span class="line">        (2, &apos;未知&apos;),</span><br><span class="line">        (3, &apos;故障&apos;),</span><br><span class="line">        (4, &apos;备用&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset_type = models.CharField(choices=asset_type_choice, max_length=64, default=&apos;server&apos;, verbose_name=&quot;资产类型&quot;)</span><br><span class="line">    name = models.CharField(max_length=64, unique=True, verbose_name=&quot;资产名称&quot;)        ##唯一，不可重复</span><br><span class="line">    sn = models.CharField(max_length=128, unique=True, verbose_name=&quot;资产序列号&quot;)        ##唯一，不可重复</span><br><span class="line">    business_unit = models.ForeignKey(&apos;BusinessUnit&apos;, null=True, blank=True, verbose_name=&quot;所属业务线&quot;)</span><br><span class="line">    status = models.SmallIntegerField(choices=asset_status, default=0, verbose_name=&quot;设备状态&quot;)</span><br><span class="line"></span><br><span class="line">    manufacturer = models.ForeignKey(&apos;Manufacturer&apos;, null=True, blank=True, verbose_name=&quot;制造商&quot;)</span><br><span class="line">    manage_ip = models.GenericIPAddressField(null=True, blank=True, verbose_name=&quot;管理IP&quot;)</span><br><span class="line">    tags = models.ManyToManyField(&apos;Tag&apos;, blank=True, verbose_name=&quot;标签&quot;)</span><br><span class="line">    admin = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;资产管理员&quot;, related_name=&apos;admin&apos;)</span><br><span class="line">    idc = models.ForeignKey(&apos;IDC&apos;, null=True, blank=True, verbose_name=&quot;所在机房&quot;)</span><br><span class="line">    contract = models.ForeignKey(&apos;Contract&apos;, null=True, blank=True, verbose_name=&quot;合同&quot;)</span><br><span class="line"></span><br><span class="line">    purchase_day =  models.DateField(null=True, blank=True, verbose_name=&quot;购买日期&quot;)</span><br><span class="line">    expire_day = models.DateField(null=True, blank=True, verbose_name=&quot;过保日期&quot;)</span><br><span class="line">    price = models.FloatField(null=True, blank=True, verbose_name=&quot;价格&quot;)</span><br><span class="line"></span><br><span class="line">    approved_by = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;批准人&quot;, related_name=&quot;approved_by&quot;)</span><br><span class="line"></span><br><span class="line">    memo = models.TextField(null=True, blank=True, verbose_name=&quot;备注&quot;)</span><br><span class="line">    c_time = models.DateTimeField(auto_now_add=True, verbose_name=&quot;批准日期&quot;)   ##auto_add_now默认=False：储存当对象被创建时的时间，可以用来存储比如说博客什么时候创建的，后来你再更改博客，它的值也不会变。</span><br><span class="line">    m_time = models.DateTimeField(auto_now=True, verbose_name=&quot;更新日期&quot;)       ##auto_now默认=False:当对象被存储时自动将对象的时间更新为当前时间</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;&lt;%s&gt;  %s&apos; %(self.get_asset_type_display(), self.name)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &quot;资产总表&quot;</span><br><span class="line">        verbose_name_plural = &quot;资产总表&quot;</span><br><span class="line">        ordering = [&apos;-c_time&apos;]</span><br></pre></td></tr></table></figure><p><strong>说明：</strong></p><ul><li>sn这个数据字段是所有资产都必须有，并且唯一不可重复的！通常来自自动收集的数据中；</li><li>name和sn一样，也是唯一的；</li><li>asset_type_choice和asset_status分别设计为两个选择类型</li><li>admin和approved_by是分别是当前资产的管理员和将该资产上线的审批员；</li><li>导入Django内置的User表，作为我们CMDB项目的用户表，用于保存管理员和审判员等人员信息；</li><li>asset表中的很多字段内容都无法自动获取，需要我们手动输入，比如合同、备注。</li></ul><h4 id="2-服务器模型"><a href="#2-服务器模型" class="headerlink" title="2.服务器模型"></a>2.服务器模型</h4><p>服务器作为资产的一种，而且是最主要的管理对象，包含了一些主要的信息，其模型结构如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Server(models.Model):</span><br><span class="line">    &quot;&quot;&quot;服务器设备&quot;&quot;&quot;</span><br><span class="line">    sub_asset_type_choice = (</span><br><span class="line">        (0, &apos;PC服务器&apos;),</span><br><span class="line">        (1, &apos;刀片型&apos;),</span><br><span class="line">        (2, &apos;小型机&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    created_by_choice = (</span><br><span class="line">        (&apos;auto&apos;, &apos;自动添加&apos;),</span><br><span class="line">        (&apos;manual&apos;, &apos;手动录入&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset = models.OneToOneField(&apos;Asset&apos;)       # 非常关键的一对一关联！</span><br><span class="line">    sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=1, verbose_name=&quot;服务器类型&quot;)</span><br><span class="line">    created_by = models.CharField(choices=created_by_choice, max_length=32, default=&apos;auto&apos;, verbose_name=&quot;添加方式&quot;)</span><br><span class="line">    hosted_on = models.ForeignKey(&apos;self&apos;, related_name=&apos;hosted_on_server&apos;, blank=True, null=True, verbose_name=&quot;宿主机&quot;)       ##虚拟机专用字段</span><br><span class="line">    model = models.CharField(max_length=512, blank=True, null=True, verbose_name=&quot;Raid类型&quot;)</span><br><span class="line"></span><br><span class="line">    os_type = models.CharField(&apos;操作系统类型&apos;, max_length=64, blank=True, null=True)</span><br><span class="line">    os_distribution = models.CharField(&apos;发行版本&apos;, max_length=64, blank=True, null=True)</span><br><span class="line">    os_release = models.CharField(&apos;操作系统版本&apos;, max_length=64, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; %(self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;服务器&apos;</span><br><span class="line">        verbose_name_plural = &quot;服务器&quot;</span><br></pre></td></tr></table></figure></p><p><strong>说明：</strong></p><ul><li>每台服务器都唯一关联着一个资产对象，因此使用OneToOneField构建了一个一对一字段，这非常重要!</li><li>服务器又可分为几种子类型，这里定义了三种；</li><li>服务器添加的方式可以分为手动和自动；</li><li>有些服务器是虚拟机或者docker生成的，没有物理实体，存在于宿主机中，因此需要增加一个hosted_on字段；</li><li>服务器有型号信息，如果硬件信息中不包含，那么指的就是主板型号；</li><li>Raid类型在采用了Raid的时候才有，否则为空;</li><li>操作系统相关信息包含类型、发行版本和具体版本。</li></ul><h3 id="3-安全、网络、存储设备和软件资产的模型"><a href="#3-安全、网络、存储设备和软件资产的模型" class="headerlink" title="3.安全、网络、存储设备和软件资产的模型"></a>3.安全、网络、存储设备和软件资产的模型</h3><p>这部分内容不是项目的主要内容，而且数据大多数不能自动收集和报告，很多都需要手工录入。我这里给出了范例，更多的数据字段，可以自行添加。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class SecurityDevice(models.Model):</span><br><span class="line">    &quot;&quot;&quot;安全设备&quot;&quot;&quot;</span><br><span class="line">    sub_asset_type_choice = (</span><br><span class="line">        (0, &apos;防火墙&apos;),</span><br><span class="line">        (1, &apos;入侵检测设备&apos;),</span><br><span class="line">        (2, &apos;互联网网关&apos;),</span><br><span class="line">        (4, &apos;运维审计系统&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset = models.OneToOneField(&apos;Asset&apos;)</span><br><span class="line">    sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;安全设备类型&quot;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;安全设备&apos;</span><br><span class="line">        verbose_name_plural = &quot;安全设备&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class StorageDevice(models.Model):</span><br><span class="line">    &quot;&quot;&quot;存储设备&quot;&quot;&quot;</span><br><span class="line">    sub_asset_type_choice = (</span><br><span class="line">        (0, &apos;磁盘阵列&apos;),</span><br><span class="line">        (1, &apos;网络存储器&apos;),</span><br><span class="line">        (2, &apos;磁带库&apos;),</span><br><span class="line">        (4, &apos;磁带机&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset = models.OneToOneField(&apos;Asset&apos;)</span><br><span class="line">    sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;存储设备类型&quot;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;存储设备&apos;</span><br><span class="line">        verbose_name_plural = &quot;存储设备&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NetworkDevice(models.Model):</span><br><span class="line">    &quot;&quot;&quot;网络设备&quot;&quot;&quot;</span><br><span class="line">    sub_asset_type_choice = (</span><br><span class="line">        (0, &apos;路由器&apos;),</span><br><span class="line">        (1, &apos;交换机&apos;),</span><br><span class="line">        (2, &apos;负载均衡&apos;),</span><br><span class="line">        (4, &apos;VPN设备&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset = models.OneToOneField(&apos;Asset&apos;)</span><br><span class="line">    sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;网络设备类型&quot;)</span><br><span class="line"></span><br><span class="line">    vlan_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;VLanIP&quot;)</span><br><span class="line">    intranet_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;内网IP&quot;)</span><br><span class="line"></span><br><span class="line">    model = models.CharField(max_length=128, null=True, blank=True, verbose_name=&quot;网络设备型号&quot;)</span><br><span class="line">    firmware = models.CharField(max_length=128, blank=True, null=True, verbose_name=&quot;设备固件版本&quot;)</span><br><span class="line">    port_num = models.SmallIntegerField(null=True, blank=True, verbose_name=&quot;端口个数&quot;)</span><br><span class="line">    device_detail = models.TextField(null=True, blank=True, verbose_name=&quot;详细配置&quot;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; % (self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;网络设备&apos;</span><br><span class="line">        verbose_name_plural = &quot;网络设备&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Software(models.Model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    只保存付费购买的软件</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sub_asset_type_choice = (</span><br><span class="line">        (0, &apos;操作系统&apos;),</span><br><span class="line">        (1, &apos;办公\开发软件&apos;),</span><br><span class="line">        (2, &apos;业务软件&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;软件类型&quot;)</span><br><span class="line">    license_num = models.IntegerField(default=1, verbose_name=&quot;授权数量&quot;)</span><br><span class="line">    version = models.CharField(max_length=64, unique=True, help_text=&apos;例如: CentOS release 6.7 (Final)&apos;,</span><br><span class="line">                               verbose_name=&apos;软件/系统版本&apos;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s--%s&apos; % (self.get_sub_asset_type_display(), self.version)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;软件/系统&apos;</span><br><span class="line">        verbose_name_plural = &quot;软件/系统&quot;</span><br></pre></td></tr></table></figure></p><p><strong>说明：</strong></p><ul><li>每台安全、网络、存储设备都通过一对一的方式唯一关联这一个资产对象。</li><li>通过sub_asset_type又细分设备的子类型</li><li>对于软件，它没有物理形体，因此无须关联一个资产对象；</li><li>软件只管理那些大型的收费软件，关注点是授权数量和软件版本。对于那些开源的或者免费的软件，显然不算公司的资产。</li></ul><h3 id="4-机房、制造商、业务线、合同、资产标签等数据模型"><a href="#4-机房、制造商、业务线、合同、资产标签等数据模型" class="headerlink" title="4.机房、制造商、业务线、合同、资产标签等数据模型"></a>4.机房、制造商、业务线、合同、资产标签等数据模型</h3><p>这一部分是CMDB中相关的内容，数据表建立后，可以通过手动添加。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class IDC(models.Model):</span><br><span class="line">    &quot;&quot;&quot;机房&quot;&quot;&quot;</span><br><span class="line">    name = models.CharField(max_length=64, unique=True, verbose_name=&quot;机房名称&quot;)</span><br><span class="line">    memo = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;备注&apos;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;机房&apos;</span><br><span class="line">        verbose_name_plural = &quot;机房&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Manufacturer(models.Model):</span><br><span class="line">    &quot;&quot;&quot;厂商&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    name = models.CharField(&apos;厂商名称&apos;, max_length=64, unique=True)</span><br><span class="line">    telephone = models.CharField(&apos;支持电话&apos;, max_length=30, blank=True, null=True)</span><br><span class="line">    memo = models.CharField(&apos;备注&apos;, max_length=128, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;厂商&apos;</span><br><span class="line">        verbose_name_plural = &quot;厂商&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BusinessUnit(models.Model):</span><br><span class="line">    &quot;&quot;&quot;业务线&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    parent_unit = models.ForeignKey(&apos;self&apos;, blank=True, null=True, related_name=&apos;parent_level&apos;)</span><br><span class="line">    name = models.CharField(&apos;业务线&apos;, max_length=64, unique=True)</span><br><span class="line">    memo = models.CharField(&apos;备注&apos;, max_length=64, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;业务线&apos;</span><br><span class="line">        verbose_name_plural = &quot;业务线&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Contract(models.Model):</span><br><span class="line">    &quot;&quot;&quot;合同&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    sn = models.CharField(&apos;合同号&apos;, max_length=128, unique=True)</span><br><span class="line">    name = models.CharField(&apos;合同名称&apos;, max_length=64)</span><br><span class="line">    memo = models.TextField(&apos;备注&apos;, blank=True, null=True)</span><br><span class="line">    price = models.IntegerField(&apos;合同金额&apos;)</span><br><span class="line">    detail = models.TextField(&apos;合同详细&apos;, blank=True, null=True)</span><br><span class="line">    start_day = models.DateField(&apos;开始日期&apos;, blank=True, null=True)</span><br><span class="line">    end_day = models.DateField(&apos;失效日期&apos;, blank=True, null=True)</span><br><span class="line">    license_num = models.IntegerField(&apos;license数量&apos;, blank=True, null=True)</span><br><span class="line">    c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True)</span><br><span class="line">    m_day = models.DateField(&apos;修改日期&apos;, auto_now=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;合同&apos;</span><br><span class="line">        verbose_name_plural = &quot;合同&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Tag(models.Model):</span><br><span class="line">    &quot;&quot;&quot;标签&quot;&quot;&quot;</span><br><span class="line">    name = models.CharField(&apos;标签名&apos;, max_length=32, unique=True)</span><br><span class="line">    c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;标签&apos;</span><br><span class="line">        verbose_name_plural = &quot;标签&quot;</span><br></pre></td></tr></table></figure></p><p><strong>说明：</strong></p><ul><li>机房可以有很多其它字段，比如城市、楼号、楼层和未知等等，如有需要可自行添加；</li><li>业务线可以有子业务线，因此使用一个外键关联自身模型；</li><li>合同模型主要存储财务部门关心的数据；</li><li>资产标签模型与资产是多对多的关系。</li></ul><h3 id="5-CPU模型"><a href="#5-CPU模型" class="headerlink" title="5.CPU模型"></a>5.CPU模型</h3><p>通常一台服务器中只能有一种CPU型号，所以这里使用OneToOneField唯一关联一个资产对象，而不是外键关系。服务器上可以有多个物理CPU，它们的型号都是一样的。每个物理CPU又可能包含多核。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class CPU(models.Model):</span><br><span class="line">    &quot;&quot;&quot;CPU组件&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    asset = models.OneToOneField(&apos;Asset&apos;)  # 设备上的cpu肯定都是一样的，所以不需要建立多个cpu数据，一条就可以，因此使用一对一。</span><br><span class="line">    cpu_model = models.CharField(&apos;CPU型号&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    cpu_count = models.PositiveSmallIntegerField(&apos;物理CPU个数&apos;, default=1)</span><br><span class="line">    cpu_core_count = models.PositiveSmallIntegerField(&apos;CPU核数&apos;, default=1)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.asset.name + &quot;:   &quot; + self.cpu_model</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;CPU&apos;</span><br><span class="line">        verbose_name_plural = &quot;CPU&quot;</span><br></pre></td></tr></table></figure></p><h3 id="6-RAM模型"><a href="#6-RAM模型" class="headerlink" title="6.RAM模型"></a>6.RAM模型</h3><p>某个资产中可能有多条内存，所以这里必须是外键关系。其次，内存的sn号可能无法获得，就必须通过内存所在的插槽未知来唯一确定每条内存。因此，unique_together = (‘asset’, ‘slot’)这条设置非常关键，相当于内存的主键了，每条内存数据必须包含slot字段，否则就不合法。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class RAM(models.Model):</span><br><span class="line">    &quot;&quot;&quot;内存组件&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    asset = models.ForeignKey(&apos;Asset&apos;)  # 只能通过外键关联Asset。否则不能同时关联服务器、网络设备等等。</span><br><span class="line">    sn = models.CharField(&apos;SN号&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    model = models.CharField(&apos;内存型号&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    manufacturer = models.CharField(&apos;内存制造商&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    slot = models.CharField(&apos;插槽&apos;, max_length=64)</span><br><span class="line">    capacity = models.IntegerField(&apos;内存大小(GB)&apos;, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s: %s: %s: %s&apos; % (self.asset.name, self.model, self.slot, self.capacity)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;内存&apos;</span><br><span class="line">        verbose_name_plural = &quot;内存&quot;</span><br><span class="line">        unique_together = (&apos;asset&apos;, &apos;slot&apos;)  #unique_together，也就是联合唯一，同一资产下的内存，根据插槽slot的不同，必须唯一</span><br></pre></td></tr></table></figure></p><h3 id="7-硬盘模型"><a href="#7-硬盘模型" class="headerlink" title="7. 硬盘模型"></a>7. 硬盘模型</h3><p>与内存相同的是，硬盘也可能有很多块，所以也是外键关系。不同的是，硬盘通常都能获取到sn号，使用sn作为唯一值比较合适，也就是unique_together = (‘asset’, ‘sn’)。硬盘有不同的接口，这里设置了4种以及unknown，可自行添加其它类别。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Disk(models.Model):</span><br><span class="line">    &quot;&quot;&quot;存储设备&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    disk_interface_type_choice = (</span><br><span class="line">        (&apos;SATA&apos;, &apos;SATA&apos;),</span><br><span class="line">        (&apos;SAS&apos;, &apos;SAS&apos;),</span><br><span class="line">        (&apos;SCSI&apos;, &apos;SCSI&apos;),</span><br><span class="line">        (&apos;SSD&apos;, &apos;SSD&apos;),</span><br><span class="line">        (&apos;unknown&apos;, &apos;unknown&apos;),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    asset = models.ForeignKey(&apos;Asset&apos;)</span><br><span class="line">    sn = models.CharField(&apos;硬盘SN号&apos;, max_length=128)</span><br><span class="line">    slot = models.CharField(&apos;所在插槽位&apos;, max_length=64, blank=True, null=True)</span><br><span class="line">    model = models.CharField(&apos;磁盘型号&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    manufacturer = models.CharField(&apos;磁盘制造商&apos;, max_length=128, blank=True, null=True)</span><br><span class="line">    capacity = models.FloatField(&apos;磁盘容量(GB)&apos;, blank=True, null=True)</span><br><span class="line">    interface_type = models.CharField(&apos;接口类型&apos;, max_length=16, choices=disk_interface_type_choice, default=&apos;unknown&apos;)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s:  %s:  %s:  %sGB&apos; % (self.asset.name, self.model, self.slot, self.capacity)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;硬盘&apos;</span><br><span class="line">        verbose_name_plural = &quot;硬盘&quot;</span><br><span class="line">        unique_together = (&apos;asset&apos;, &apos;sn&apos;)</span><br></pre></td></tr></table></figure></p><h3 id="8-网卡模型"><a href="#8-网卡模型" class="headerlink" title="8.网卡模型"></a>8.网卡模型</h3><p>一台设备中可能有很多块网卡，所以网卡与资产也是外键的关系。另外，由于虚拟机的存在，网卡的mac地址可能会发生重复，无法唯一确定某块网卡，因此通过网卡型号加mac地址的方式来唯一确定网卡。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class NIC(models.Model):</span><br><span class="line">    &quot;&quot;&quot;网卡组件&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    asset = models.ForeignKey(&apos;Asset&apos;)  # 注意要用外键</span><br><span class="line">    name = models.CharField(&apos;网卡名称&apos;, max_length=64, blank=True, null=True)</span><br><span class="line">    model = models.CharField(&apos;网卡型号&apos;, max_length=128)</span><br><span class="line">    mac = models.CharField(&apos;MAC地址&apos;, max_length=64)  # 虚拟机有可能会出现同样的mac地址</span><br><span class="line">    ip_address = models.GenericIPAddressField(&apos;IP地址&apos;, blank=True, null=True)</span><br><span class="line">    net_mask = models.CharField(&apos;掩码&apos;, max_length=64, blank=True, null=True)</span><br><span class="line">    bonding = models.CharField(&apos;绑定地址&apos;, max_length=64, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &apos;%s:  %s:  %s&apos; % (self.asset.name, self.model, self.mac)</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;网卡&apos;</span><br><span class="line">        verbose_name_plural = &quot;网卡&quot;</span><br><span class="line">        unique_together = (&apos;asset&apos;, &apos;model&apos;, &apos;mac&apos;)  # 资产、型号和mac必须联合唯一。防止虚拟机中的特殊情况发生错误。</span><br></pre></td></tr></table></figure></p><h3 id="9-日志模型"><a href="#9-日志模型" class="headerlink" title="9.日志模型"></a>9.日志模型</h3><p>CMDB必须记录各种日志，这是毫无疑问的！我们通常要记录事件名称、类型、关联的资产、子事件、事件详情、谁导致的、发生时间。这些都很重要！</p><p>尤其要注意的是，事件日志不能随着关联资产的删除被一并删除，也就是我们设置on_delete=models.SET_NULL的意义！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class EventLog(models.Model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    日志.</span><br><span class="line">    在关联对象被删除的时候，不能一并删除，需保留日志。</span><br><span class="line">    因此，on_delete=models.SET_NULL</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    name = models.CharField(&apos;事件名称&apos;, max_length=128)</span><br><span class="line">    event_type_choice = (</span><br><span class="line">        (0, &apos;其它&apos;),</span><br><span class="line">        (1, &apos;硬件变更&apos;),</span><br><span class="line">        (2, &apos;新增配件&apos;),</span><br><span class="line">        (3, &apos;设备下线&apos;),</span><br><span class="line">        (4, &apos;设备上线&apos;),</span><br><span class="line">        (5, &apos;定期维护&apos;),</span><br><span class="line">        (6, &apos;业务上线\更新\变更&apos;),</span><br><span class="line">    )</span><br><span class="line">    asset = models.ForeignKey(&apos;Asset&apos;, blank=True, null=True, on_delete=models.SET_NULL)  # 当资产审批成功时有这项数据</span><br><span class="line">    new_asset = models.ForeignKey(&apos;NewAssetApprovalZone&apos;, blank=True, null=True, on_delete=models.SET_NULL)  # 当资产审批失败时有这项数据</span><br><span class="line">    event_type = models.SmallIntegerField(&apos;事件类型&apos;, choices=event_type_choice, default=4)</span><br><span class="line">    component = models.CharField(&apos;事件子项&apos;, max_length=256, blank=True, null=True)</span><br><span class="line">    detail = models.TextField(&apos;事件详情&apos;)</span><br><span class="line">    date = models.DateTimeField(&apos;事件时间&apos;, auto_now_add=True)</span><br><span class="line">    user = models.ForeignKey(User, blank=True, null=True, verbose_name=&apos;事件执行人&apos;, on_delete=models.SET_NULL)  # 自动更新资产数据时没有执行人</span><br><span class="line">    memo = models.TextField(&apos;备注&apos;, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.name</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;事件纪录&apos;</span><br><span class="line">        verbose_name_plural = &quot;事件纪录&quot;</span><br></pre></td></tr></table></figure></p><h3 id="10-新资产待审批区模型"><a href="#10-新资产待审批区模型" class="headerlink" title="10.新资产待审批区模型"></a>10.新资产待审批区模型</h3><p>新资产的到来，并不能直接加入CMDB数据库中，而是要通过管理员审批后，才可以上线的。这就需要一个新资产的待审批区。在该区中，以资产的sn号作为唯一值，确定不同的资产。除了关键的包含资产所有信息的data字段，为了方便审批员查看信息，我们还设计了一些厂商、型号、内存大小、CPU类型等字段。同时，有可能出现资产还未审批，更新数据就已经发过来的情况，所以需要一个数据更新日期字段。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class NewAssetApprovalZone(models.Model):</span><br><span class="line">    &quot;&quot;&quot;新资产待审批区&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    sn = models.CharField(&apos;资产SN号&apos;, max_length=128, unique=True)  # 此字段必填</span><br><span class="line">    asset_type_choice = (</span><br><span class="line">        (&apos;server&apos;, &apos;服务器&apos;),</span><br><span class="line">        (&apos;networkdevice&apos;, &apos;网络设备&apos;),</span><br><span class="line">        (&apos;storagedevice&apos;, &apos;存储设备&apos;),</span><br><span class="line">        (&apos;securitydevice&apos;, &apos;安全设备&apos;),</span><br><span class="line">        (&apos;IDC&apos;, &apos;机房&apos;),</span><br><span class="line">        (&apos;software&apos;, &apos;软件资产&apos;),</span><br><span class="line">    )</span><br><span class="line">    asset_type = models.CharField(choices=asset_type_choice, default=&apos;server&apos;, max_length=64, blank=True, null=True,</span><br><span class="line">                                  verbose_name=&apos;资产类型&apos;)</span><br><span class="line"></span><br><span class="line">    manufacturer = models.CharField(max_length=64, blank=True, null=True, verbose_name=&apos;生产厂商&apos;)</span><br><span class="line">    model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;型号&apos;)</span><br><span class="line">    ram_size = models.PositiveIntegerField(blank=True, null=True, verbose_name=&apos;内存大小&apos;)</span><br><span class="line">    cpu_model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;CPU型号&apos;)</span><br><span class="line">    cpu_count = models.PositiveSmallIntegerField(blank=True, null=True)</span><br><span class="line">    cpu_core_count = models.PositiveSmallIntegerField(blank=True, null=True)</span><br><span class="line">    os_distribution = models.CharField(max_length=64, blank=True, null=True)</span><br><span class="line">    os_type = models.CharField(max_length=64, blank=True, null=True)</span><br><span class="line">    os_release = models.CharField(max_length=64, blank=True, null=True)</span><br><span class="line"></span><br><span class="line">    data = models.TextField(&apos;资产数据&apos;)  # 此字段必填</span><br><span class="line"></span><br><span class="line">    c_time = models.DateTimeField(&apos;汇报日期&apos;, auto_now_add=True)</span><br><span class="line">    m_time = models.DateTimeField(&apos;数据更新日期&apos;, auto_now=True)</span><br><span class="line">    approved = models.BooleanField(&apos;是否批准&apos;, default=False)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return self.sn</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        verbose_name = &apos;新上线待批准资产&apos;</span><br><span class="line">        verbose_name_plural = &quot;新上线待批准资产&quot;</span><br><span class="line">        ordering = [&apos;-c_time&apos;]</span><br></pre></td></tr></table></figure></p><h3 id="11-总结"><a href="#11-总结" class="headerlink" title="11.总结"></a>11.总结</h3><p>通过前面的内容，我们可以看出CMDB数据模型的设计非常复杂，我们这里还是省略了很多不太重要的部分，就这样总共都有400多行代码。其中每个模型需要保存什么字段、采用什么类型、什么关联关系、定义哪些参数、数据是否可以为空，这些都是踩过各种坑后总结出来的，不是随便就能定义的。所以，请务必详细阅读和揣摩这些模型的内容。</p><p>一切没有问题之后，注册app，然后makemigrations以及migrate!</p><p><strong>注册app：</strong></p><p>cmdb/settings.py<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSTALLED_APPS = [</span><br><span class="line">    &apos;django.contrib.admin&apos;,</span><br><span class="line">    &apos;django.contrib.auth&apos;,</span><br><span class="line">    &apos;django.contrib.contenttypes&apos;,</span><br><span class="line">    &apos;django.contrib.sessions&apos;,</span><br><span class="line">    &apos;django.contrib.messages&apos;,</span><br><span class="line">    &apos;django.contrib.staticfiles&apos;,</span><br><span class="line">    &apos;assets&apos;,   ##此处</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p><strong>创建数据库表单：</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMDB项目需求分析（一）</title>
    <link href="/2019/02/28/1%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/"/>
    <url>/2019/02/28/1%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h3 id="一、CMDB简介"><a href="#一、CMDB简介" class="headerlink" title="一、CMDB简介"></a>一、CMDB简介</h3><p>CMDB (Configuration Management Database)配置管理数据库:</p><p>CMDB用于存储与管理企业IT架构中设备的各种配置信息，它与所有服务支持和服务交付流程都紧密相联，支持这些流程的运转、发挥配置信息的价值，同时依赖于相关流程保证数据的准确性。</p><p>CMDB是ITIL(Information Technology Infrastructure Library，信息技术基础架构库)的基础，常常被认为是构建其它ITIL流程的先决条件而优先考虑，ITIL项目的成败与是否成功建立CMDB有非常大的关系。</p><p>CMDB的核心是对整个公司的IT硬件/软件资源进行自动/手动收集、变更操作，说白了也就是对IT资产进行自动化管理，这也是本项目的重点。</p><h3 id="二、项目需求分析"><a href="#二、项目需求分析" class="headerlink" title="二、项目需求分析"></a>二、项目需求分析</h3><p>本项目不是一个完整的的CMDB系统，重点针对服务器资产的自动数据收集、报告、接收、审批、更新和展示，搭建一个基础的面向运维的主机管理平台。</p><p>下面是项目需求的总结：</p><ul><li>尽可能存储所有的IT资产数据，但不包括外设、优盘、显示器这种属于行政部门管理的设备；</li><li>硬件信息可自动收集、报告、分析、存储和展示；</li><li>具有后台管理人员的工作界面；</li><li>具有前端可视化展示的界面；</li><li>具有日志记录功能；</li><li>数据可手动添加、修改和删除。</li></ul><p>当然，实际的CMDB项目需求绝对不止这些，还有诸如用户管理、权限管理、API安全认证、REST设计等等。</p><h3 id="三、资产分类"><a href="#三、资产分类" class="headerlink" title="三、资产分类"></a>三、资产分类</h3><p>资产种类众多，不是所有的都需要CMDB管理，也不是什么都是CMDB能管理的。</p><p>下面是一个大致的分类，不一定准确、全面：</p><h4 id="资产类型包括："><a href="#资产类型包括：" class="headerlink" title="资产类型包括："></a>资产类型包括：</h4><ul><li>服务器</li><li>存储设备</li><li>安全设备</li><li>网络设备</li><li>软件资产</li></ul><h4 id="服务器又可分为："><a href="#服务器又可分为：" class="headerlink" title="服务器又可分为："></a>服务器又可分为：</h4><ul><li>刀片服务器</li><li>PC服务器</li><li>小型机</li><li>大型机</li><li>其它</li></ul><h4 id="存储设备包括："><a href="#存储设备包括：" class="headerlink" title="存储设备包括："></a>存储设备包括：</h4><ul><li>磁盘阵列</li><li>网络存储器</li><li>磁带库</li><li>磁带机</li><li>其它</li></ul><h4 id="安全设备包括："><a href="#安全设备包括：" class="headerlink" title="安全设备包括："></a>安全设备包括：</h4><ul><li>防火墙</li><li>入侵检测设备</li><li>互联网网关</li><li>漏洞扫描设备</li><li>数字签名设备</li><li>上网行为管理设备</li><li>运维审计设备</li><li>加密机</li><li>其它</li></ul><h4 id="网络设备包括："><a href="#网络设备包括：" class="headerlink" title="网络设备包括："></a>网络设备包括：</h4><ul><li>路由器</li><li>交换器</li><li>负载均衡</li><li>VPN</li><li>流量分析</li><li>其它</li></ul><h4 id="软件资产包括："><a href="#软件资产包括：" class="headerlink" title="软件资产包括："></a>软件资产包括：</h4><ul><li>操作系统授权</li><li>大型软件授权</li><li>数据库授权</li><li>其它</li></ul><p>其中，服务器是运维部门最关心的，也是CMDB中最主要、最方便进行自动化管理的资产。</p><p><strong>服务器</strong>又可以包含下面的部件：</p><ul><li>CPU</li><li>硬盘</li><li>内存</li><li>网卡</li></ul><p>除此之外，我们还要考虑下面的一些内容：</p><ul><li>机房</li><li>业务线</li><li>合同</li><li>管理员</li><li>审批员</li><li>资产标签</li><li>其它未尽事宜</li></ul><p>大概对资产进行了分类之后，就要详细考虑各细分数据条目了。</p><p><strong>共有数据条目：</strong></p><p>有一些数据条目是所有资产都应该有的，比如：</p><ul><li>资产名称</li><li>资产sn</li><li>所属业务线</li><li>设备状态</li><li>制造商</li><li>管理IP</li><li>所在机房</li><li>资产管理员</li><li>资产标签</li><li>合同</li><li>价格</li><li>购买日期</li><li>过保日期</li><li>批准人</li><li>批准日期</li><li>数据更新日期</li><li>备注</li></ul><p>另外，不同类型的资产还有各自不同的数据条目，例如服务器：</p><h4 id="服务器："><a href="#服务器：" class="headerlink" title="服务器："></a>服务器：</h4><ul><li>服务器类型</li><li>添加方式</li><li>宿主机</li><li>服务器型号</li><li>Raid类型</li><li>操作系统类型</li><li>发行版本</li><li>操作系统版本</li></ul><p>其实，在开始正式编写CMDB项目代码之前，对项目的需求分析准确与否，数据条目的安排是否合理，是决定整个CMDB项目成败的关键。这一部分工作看似简单其实复杂，看似无用其实关键，做好了，项目基础就牢固，没做好，推到重来好几遍很正常！</p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
      <tag>CMDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django基础学习Ⅰ</title>
    <link href="/2019/02/11/Django%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E2%85%A0/"/>
    <url>/2019/02/11/Django%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E2%85%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="一、Django简介"><a href="#一、Django简介" class="headerlink" title="一、Django简介"></a>一、Django简介</h3><blockquote><p>Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的，即是CMS（内容管理系统）软件。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。</p></blockquote><ul><li>Django是一个处理网络请求的webweb应用框架</li><li>Django是开源的</li></ul><p>Django有四个核心组件：</p><ul><li>1.数据模型和数据库之间的媒介ORM</li><li>2.基于正则表达式的URL分发器</li><li>3.视图处理系统</li><li>4.模板系统</li></ul><p>MVC：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m modules 模型， 和数据库字段对应</span><br><span class="line"></span><br><span class="line">v views 视图 用来展示给用户的，就是我们所学到的前端</span><br><span class="line"></span><br><span class="line">c controll url控制， 一个url，对应一个方法或者类</span><br></pre></td></tr></table></figure><h3 id="二、Django特点"><a href="#二、Django特点" class="headerlink" title="二、Django特点"></a>二、Django特点</h3><p>1) 强大的数据库功能：用python的类继承，几行代码就可以拥有一个动态的数据库操作API，如果需要也能执行SQL语句。<br>2) 自带的强大的后台功能：几行代码就让网站拥有一个强大的后台，轻松管理内容。<br>3) 优雅的网址：用正则匹配网址，传递到对应函数。<br>4) 模板系统：强大，易扩展的模板系统，设计简易，代码和样式分开设计，更易管理。<br>5) 缓存系统：与memcached或其它缓存系统联用，表现更出色，加载速度更快。<br>6) 国际化：完全支持多语言应用，允许你定义翻译的字符，轻松翻译成不同国家的语言</p><h3 id="三、安装Django"><a href="#三、安装Django" class="headerlink" title="三、安装Django"></a>三、安装Django</h3><p>使用pip工具来安装Django，直接通过下面命令来安装就可以。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># pip install Django</span><br></pre></td></tr></table></figure></p><p>用一下测试django是否安装成功：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\ZHDYA&gt;python</span><br><span class="line">Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import django</span><br><span class="line">&gt;&gt;&gt; print (django.VERSION)</span><br><span class="line">(2, 0, 6, &apos;final&apos;, 0)</span><br></pre></td></tr></table></figure></p><h3 id="四、创建项目"><a href="#四、创建项目" class="headerlink" title="四、创建项目"></a>四、创建项目</h3><p>首先，我们先通过django来创建一个项目，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># django-admin startproject firstproject</span><br></pre></td></tr></table></figure></p><p>然后在当前目录下就自动生成了一个firstproject的项目<br>然后就可以启动这个项目了：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># python manage.py runserver 127.0.0.1:8080</span><br></pre></td></tr></table></figure></p><p>默认不写ip绑定的是本机的所以ip地址，端口默认为8000</p><p><strong>也可以通过在pycharm中直接创建一个Django项目</strong>，就自动创建好了文件，然后配置manage.py脚本的参数。</p><p><img src="http://myimage.okay686.cn/okay686cn/180725/D3jDggcidI.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>直接再次运行manage.py文件就好。</p><p>然后访问url：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http://127.0.0.1:8080</span><br></pre></td></tr></table></figure></p><p>有一个欢迎的首页</p><p><img src="http://myimage.okay686.cn/okay686cn/180725/mi76HkFJEd.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="4-1、项目目录结构"><a href="#4-1、项目目录结构" class="headerlink" title="4.1、项目目录结构"></a>4.1、项目目录结构</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一层：DjangoTest    项目名称</span><br><span class="line"></span><br><span class="line">第二层： DjangoTest目录和__init__.py文件，声明是一个包，表示项目实际的python包，不要随意更改该目录，与配置有关联</span><br><span class="line"></span><br><span class="line">settings.py    项目的全局（所有项目）配置中心</span><br><span class="line"></span><br><span class="line">urls.py      项目的url配置中心</span><br><span class="line"></span><br><span class="line">wsgi.py      项目的wsgi配置中心</span><br><span class="line"></span><br><span class="line">templates模板目录</span><br><span class="line"></span><br><span class="line">manage.pydjango命令管理脚本</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/180725/LIB0BALa1a.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="setting-py"><a href="#setting-py" class="headerlink" title="setting.py"></a>setting.py</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))      ## 类似于环境变量</span><br><span class="line"></span><br><span class="line">SECRET_KEY =        ##密钥</span><br><span class="line"></span><br><span class="line">DEBUG = True        ##当报错时显示的错误信息</span><br><span class="line"></span><br><span class="line">ALLOWED_HOSTS = []       ##允许哪些机器可以访问</span><br></pre></td></tr></table></figure><h3 id="五、创建app"><a href="#五、创建app" class="headerlink" title="五、创建app"></a>五、创建app</h3><p>打个比方，jd网站：<a href="http://www.jd.com/，上面有各种各样的不同模块，我们划分为不同的app，那我们就需要在django里面创建不通的app啦。接下来，我们就来看看如何创建app" target="_blank" rel="noopener">http://www.jd.com/，上面有各种各样的不同模块，我们划分为不同的app，那我们就需要在django里面创建不通的app啦。接下来，我们就来看看如何创建app</a><br>如果是命令行模式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#python manage.py startapp linux</span><br><span class="line">#python manage.py startapp python</span><br></pre></td></tr></table></figure><p>如果是<strong>pycharm</strong>，你就需要点击：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tools-&gt;&gt;Run manage.py Task</span><br></pre></td></tr></table></figure></p><p>然后出现的交互命令行上输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">startapp newapp</span><br></pre></td></tr></table></figure></p><p>这样就创建了newapp的app。</p><p><img src="http://myimage.okay686.cn/okay686cn/180725/1Adim9ijfG.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h3 id="六、Django的解析顺序"><a href="#六、Django的解析顺序" class="headerlink" title="六、Django的解析顺序"></a>六、Django的解析顺序</h3><p>既然我们知道Django是使用的MVC的架构，那我们先来聊聊MVC是什么样的原理，首先，通过MVC中的C就是control，用来接收url请求，对应我们django中的url.py模块，M就代表Model，调用数据库的Model层，就是Django的model.py模块，然后经过业务逻辑层的处理，最后渲染到前端界面。前端就是MVC中的view层，对应django的view模块。</p><p>其实所有的参数定义都是以setting.py为准，++首先django先去setting.py中找到ROOT_URLCONF = ‘firstproject.urls’找到总url++。然后在firstproject下的urls.py文件中的urlpatterns列表变量，然后根据里面的URL的正则表达式进行解析，如果解析到，就调用第二个参数，第二个参数对应一个类或者一个函数，或者直接是一个前端页面，在经过类或者函数处理完以后，在展现在前端界面。而前端是单独的html文件，前端界面和后端处理分开，架构更加清晰。</p><p>在上面的目录结构中，每一个app都会有一个view.py， model.py，我们自己还要在创建一个url.py，通过include函数，在firstproject项目中的总url.py分出去，把属于各自的app的url分配到不通的APP的urls.py文件中，这样可以降低耦合度，增加代码的健壮性。<br>。</p><h4 id="6-1、创建urls-py文件"><a href="#6-1、创建urls-py文件" class="headerlink" title="6.1、创建urls.py文件"></a>6.1、创建urls.py文件</h4><p>urls作为程序的url入口，支持正则匹配，讲访问的url映射到view中的函数中。为了能调用每个app管理自己的url，我们首先需要在DjangoTest的urls.py文件中做如下修改：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url, include</span><br><span class="line">from django.contrib import admin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&apos;^admin/&apos;, admin.site.urls),</span><br><span class="line">    url(r&apos;^newapp/&apos;, include(&apos;newapp.urls&apos;)),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>注意事项：<br>为了避免和别的app取同样的名字，一般我们会在名字前加一个app名称作为前缀<br>url匹配，主url不需要/反斜杠：==因为django已经给域名加了一个反斜杠，如：<a href="http://127.0.0.1/" target="_blank" rel="noopener">http://127.0.0.1/</a><br>主url后面要加/， app的url前面就不需要加/了，<br>主url后面一般不要加$符号， app的url后面要加$符号==</p><p>然后在创建newapp/urls.py文件，编辑如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url, include</span><br><span class="line"></span><br><span class="line">from newapp import views</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&apos;^$&apos;, views.index)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="配置-view-py文件"><a href="#配置-view-py文件" class="headerlink" title="配置 view.py文件"></a>配置 view.py文件</h4><p>而以上：<a href="http://127.0.0.1:8080/newapp的url对应的就是view模块中的index函数，在linux的view.py中定义index函数" target="_blank" rel="noopener">http://127.0.0.1:8080/newapp的url对应的就是view模块中的index函数，在linux的view.py中定义index函数</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;This is a test Django index!!!&quot;)</span><br></pre></td></tr></table></figure><p>然后在浏览器上访问：<a href="http://127.0.0.1:8080/newapp/，如下图所示：" target="_blank" rel="noopener">http://127.0.0.1:8080/newapp/，如下图所示：</a></p><p><img src="http://myimage.okay686.cn/okay686cn/180725/iF56db0A93.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="6-2、如果再次增加内容："><a href="#6-2、如果再次增加内容：" class="headerlink" title="6.2、如果再次增加内容："></a>6.2、如果再次增加内容：</h4><h5 id="urls-py"><a href="#urls-py" class="headerlink" title="urls.py"></a>urls.py</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url, include</span><br><span class="line"></span><br><span class="line">from newapp import views</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&quot;^$&quot;, views.index),</span><br><span class="line">    url(r&apos;newapp/&apos;, views.index),</span><br><span class="line">    url(r&apos;hello/&apos;, views.hello)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h5 id="views-py"><a href="#views-py" class="headerlink" title="views.py"></a>views.py</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;This is a test Django index!!!&quot;)</span><br><span class="line"></span><br><span class="line">def hello(request):</span><br><span class="line">    return HttpResponse(&quot;&lt;h1 style=&apos;text-align:center&apos;&gt; hheello world!!!&lt;/h1&gt;&quot;)</span><br></pre></td></tr></table></figure><p>当访问：<a href="http://127.0.0.1:8000/newapp/hello/" target="_blank" rel="noopener">http://127.0.0.1:8000/newapp/hello/</a></p><p>这会出现一个一级标题且居中的字体。</p><h4 id="6-3、urls捕获参数（匹配正则表达式）"><a href="#6-3、urls捕获参数（匹配正则表达式）" class="headerlink" title="6.3、urls捕获参数（匹配正则表达式）"></a>6.3、urls捕获参数（匹配正则表达式）</h4><p>在urls.py中增加如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=hello),</span><br></pre></td></tr></table></figure></p><p>在views.py中增加如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def hello(request, p1, p2):</span><br><span class="line">    return HttpResponse(&quot;hello &#123;0&#125;, hello &#123;1&#125;&quot;.format(p1, p2))</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/180725/72fac163f6.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><h4 id="url参数的捕获有两种方式："><a href="#url参数的捕获有两种方式：" class="headerlink" title="url参数的捕获有两种方式："></a>url参数的捕获有两种方式：</h4><p>b 捕获关键字参数：在url函数中，正则表达是用（?P<keyword>）进行捕获，然后在views.py中定义即可</keyword></p><p>在urls.py中增加如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url(r&apos;keyword/(?P&lt;ip&gt;\S+)/$&apos;, views.keyword, name=keyword),</span><br></pre></td></tr></table></figure></p><p>在views.py中增加如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def keyword(request, ip):</span><br><span class="line">return HttpResponse(&quot;the ip is &#123;0&#125;&quot;.format(ip))</span><br></pre></td></tr></table></figure></p><p>在浏览器上访问url：<a href="http://127.0.0.1:8080/newapp/keyword/1.1.1.1/" target="_blank" rel="noopener">http://127.0.0.1:8080/newapp/keyword/1.1.1.1/</a></p><h3 id="七、urls重定向"><a href="#七、urls重定向" class="headerlink" title="七、urls重定向"></a>七、urls重定向</h3><p>在学习url重定向之前，我们先来看看定义url的函数是怎样一个表达形式。</p><p>注意如下，有个参数  name=  这个是必须要写的，类似于起个别名。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def url(regex, view, kwargs=None, name=None):</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">regex：url匹配的正则字符串</span><br><span class="line"></span><br><span class="line">view：一个可以调用的类型函数，或者使用include函数</span><br><span class="line"></span><br><span class="line">kwargs：关键字参数，必须是一个字典，可以通过这个传递更多参数给views.py，views通过kwargs.get(“key”)得到对应的value</span><br><span class="line"></span><br><span class="line">name：给URL取得名字，以后可以通过reverse函数进行重定向</span><br></pre></td></tr></table></figure><p>对于kwargs如何传递参数，我们来看一个例子：</p><p>在urls.py中增加如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;),</span><br></pre></td></tr></table></figure></p><p>在views.py中增加如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def test(request, **kwargs):</span><br><span class="line">    return HttpResponse(&quot;the name is : &#123;0&#125;&quot;.format(kwargs.get(&quot;name&quot;)))</span><br></pre></td></tr></table></figure><p>在浏览器上访问url：<a href="http://127.0.0.1:8080/newapp/test/" target="_blank" rel="noopener">http://127.0.0.1:8080/newapp/test/</a></p><p><img src="http://myimage.okay686.cn/okay686cn/180726/3cfmb1HhgB.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>既然已经知道name属性的用法，现在我们就来说重定向，重定向常用name属性来进行重定向</p><h5 id="修改urls-py"><a href="#修改urls-py" class="headerlink" title="修改urls.py"></a>修改urls.py</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url(r&quot;^$&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">url(r&quot;redirect/$&quot;, view=views.redirect,  name=&quot;redirect&quot;),</span><br></pre></td></tr></table></figure><h5 id="修改views-py"><a href="#修改views-py" class="headerlink" title="修改views.py"></a>修改views.py</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.http import HttpResponse, HttpResponseRedirect</span><br><span class="line">from django.urls import reverse</span><br><span class="line">def redirect(request):</span><br><span class="line">    return HttpResponseRedirect(reverse(&quot;index&quot;))</span><br></pre></td></tr></table></figure><p>在浏览器上访问url：<a href="http://127.0.0.1:8080/newapp/redirect，直接跳转到http://127.0.0.1:8080/newapp/，" target="_blank" rel="noopener">http://127.0.0.1:8080/newapp/redirect，直接跳转到http://127.0.0.1:8080/newapp/，</a></p><p><img src="http://myimage.okay686.cn/okay686cn/180726/BHLAEfGfHD.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>当然也可以指定返回数据的具体类型，例如：Json格式返回</p><p>urls.py<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from django.conf.urls import url, include</span><br><span class="line"></span><br><span class="line">from newapp import views</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    url(r&quot;^$&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">    # url(r&apos;hello/&apos;, views.hello)</span><br><span class="line">    url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=&quot;hello&quot;),</span><br><span class="line">    url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;),</span><br><span class="line">    url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;)</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>在views.py中修改主页为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def index(request):</span><br><span class="line">    test = dict()</span><br><span class="line">    test[&apos;name&apos;] = &quot;zhdya&quot;</span><br><span class="line">    test[&apos;sex&apos;] = &quot;man&quot;</span><br><span class="line">    test[&apos;age&apos;] = 28</span><br><span class="line">    # return HttpResponse(&quot;This is a test Django index!!!&quot;)</span><br><span class="line">    return HttpResponse(json.dumps(test))</span><br></pre></td></tr></table></figure></p><p><img src="http://myimage.okay686.cn/okay686cn/180726/EFdI506ah8.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>Python3</category>
      
    </categories>
    
    
    <tags>
      
      <tag>django</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes 1.11.2整理Ⅲ</title>
    <link href="/2019/02/10/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A2/"/>
    <url>/2019/02/10/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个 nginx deplyment</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1 </span><br><span class="line">kind: Deployment </span><br><span class="line">metadata: </span><br><span class="line">  name: nginx-dm</span><br><span class="line">spec: </span><br><span class="line">  replicas: 2</span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels: </span><br><span class="line">        name: nginx </span><br><span class="line">    spec: </span><br><span class="line">      containers: </span><br><span class="line">        - name: nginx </span><br><span class="line">          image: nginx:alpine </span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          ports: </span><br><span class="line">            - containerPort: 80</span><br><span class="line">            </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1 </span><br><span class="line">kind: Service</span><br><span class="line">metadata: </span><br><span class="line">  name: nginx-svc </span><br><span class="line">spec: </span><br><span class="line">  ports: </span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      protocol: TCP </span><br><span class="line">  selector: </span><br><span class="line">    name: nginx</span><br></pre></td></tr></table></figure><p>创建testnginx deployment<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ~]# kubectl create -f testnginx.yaml</span><br><span class="line">deployment.extensions/nginx-dm created</span><br><span class="line">service/nginx-svc created</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ~]# kubectl get po -o wide</span><br><span class="line">NAME                       READY     STATUS              RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">nginx-dm-fff68d674-j7dlk   1/1       Running             0          9m        10.254.108.115   node2     &lt;none&gt;</span><br><span class="line">nginx-dm-fff68d674-r5hb6   1/1       Running             0          9m        10.254.102.133   node1     &lt;none&gt;</span><br></pre></td></tr></table></figure><h4 id="在-安装了-calico-网络的node节点-里-curl"><a href="#在-安装了-calico-网络的node节点-里-curl" class="headerlink" title="在 安装了 calico 网络的node节点 里 curl"></a>在 安装了 calico 网络的node节点 里 curl</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]# curl 10.254.102.133</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><h4 id="查看-ipvs-规则"><a href="#查看-ipvs-规则" class="headerlink" title="查看 ipvs 规则"></a>查看 ipvs 规则</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2 ssl]# ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.161.161:6443         Masq    1      1          0</span><br><span class="line">  -&gt; 192.168.161.162:6443         Masq    1      0          0</span><br><span class="line">TCP  10.254.18.37:80 rr</span><br><span class="line">  -&gt; 10.254.75.1:80               Masq    1      0          0</span><br><span class="line">  -&gt; 10.254.102.133:80            Masq    1      0          0</span><br></pre></td></tr></table></figure><h2 id="配置-CoreDNS"><a href="#配置-CoreDNS" class="headerlink" title="配置 CoreDNS"></a>配置 CoreDNS</h2><p>官方 地址 <a href="https://coredns.io" target="_blank" rel="noopener">https://coredns.io</a></p><h4 id="下载-yaml-文件"><a href="#下载-yaml-文件" class="headerlink" title="下载 yaml 文件"></a>下载 yaml 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed</span><br><span class="line"></span><br><span class="line">mv coredns.yaml.sed coredns.yaml</span><br></pre></td></tr></table></figure><p>修改配置文件中的部分配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi coredns.yaml</span><br><span class="line"></span><br><span class="line">第一处：</span><br><span class="line">...</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        health</span><br><span class="line">        kubernetes cluster.local 10.254.0.0/18 &#123;</span><br><span class="line">          pods insecure</span><br><span class="line">          upstream</span><br><span class="line">          fallthrough in-addr.arpa ip6.arpa</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9153</span><br><span class="line">        proxy . /etc/resolv.conf</span><br><span class="line">        cache 30</span><br><span class="line">    &#125;</span><br><span class="line">...       </span><br><span class="line"></span><br><span class="line">第二处：搜索 /clusterIP 即可</span><br><span class="line">  clusterIP: 10.254.0.2</span><br></pre></td></tr></table></figure></p><h3 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1）errors官方没有明确解释，后面研究</span><br><span class="line"></span><br><span class="line">2）health:健康检查，提供了指定端口（默认为8080）上的HTTP端点，如果实例是健康的，则返回“OK”。</span><br><span class="line"></span><br><span class="line">3）cluster.local：CoreDNS为kubernetes提供的域，10.254.0.0/18这告诉Kubernetes中间件它负责为反向区域提供PTR请求0.0.254.10.in-addr.arpa ..换句话说，这是允许反向DNS解析服务（我们经常使用到得DNS服务器里面有两个区域，即“正向查找区域”和“反向查找区域”，正向查找区域就是我们通常所说的域名解析，反向查找区域即是这里所说的IP反向解析，它的作用就是通过查询IP地址的PTR记录来得到该IP地址指向的域名，当然，要成功得到域名就必需要有该IP地址的PTR记录。PTR记录是邮件交换记录的一种，邮件交换记录中有A记录和PTR记录，A记录解析名字到地址，而PTR记录解析地址到名字。地址是指一个客户端的IP地址，名字是指一个客户的完全合格域名。通过对PTR记录的查询，达到反查的目的。）</span><br><span class="line"></span><br><span class="line">4）proxy:这可以配置多个upstream 域名服务器，也可以用于延迟查找 /etc/resolv.conf 中定义的域名服务器</span><br><span class="line"></span><br><span class="line">5）cache:这允许缓存两个响应结果，一个是肯定结果（即，查询返回一个结果）和否定结果（查询返回“没有这样的域”），具有单独的高速缓存大小和TTLs。</span><br><span class="line"></span><br><span class="line"># 这里 kubernetes cluster.local 为 创建 svc 的 IP 段</span><br><span class="line"></span><br><span class="line">kubernetes cluster.local 10.254.0.0/18 </span><br><span class="line"></span><br><span class="line"># clusterIP  为 指定 DNS 的 IP</span><br><span class="line"></span><br><span class="line">clusterIP: 10.254.0.2</span><br></pre></td></tr></table></figure><h3 id="创建coreDNS"><a href="#创建coreDNS" class="headerlink" title="创建coreDNS"></a>创建coreDNS</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 src]# kubectl apply -f coredns.yaml</span><br><span class="line">serviceaccount/coredns created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">configmap/coredns created</span><br><span class="line">deployment.extensions/coredns created</span><br><span class="line">service/kube-dns created</span><br></pre></td></tr></table></figure><h4 id="查看创建："><a href="#查看创建：" class="headerlink" title="查看创建："></a>查看创建：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 src]# kubectl get pod,svc -n kube-system -o wide</span><br><span class="line">NAME                                          READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">pod/calico-kube-controllers-79cfd7887-scnnp   1/1       Running   1          2d        192.168.161.78   node2     &lt;none&gt;</span><br><span class="line">pod/calico-node-pwlq4                         2/2       Running   2          2d        192.168.161.77   node1     &lt;none&gt;</span><br><span class="line">pod/calico-node-vmrrq                         2/2       Running   2          2d        192.168.161.78   node2     &lt;none&gt;</span><br><span class="line">pod/coredns-55f86bf584-fqjf2                  1/1       Running   0          23s       10.254.102.139   node1     &lt;none&gt;</span><br><span class="line">pod/coredns-55f86bf584-hsrbp                  1/1       Running   0          23s       10.254.75.21     node2     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE       SELECTOR</span><br><span class="line">service/kube-dns   ClusterIP   10.254.0.2   &lt;none&gt;        53/UDP,53/TCP   23s       k8s-app=kube-dns</span><br></pre></td></tr></table></figure><h4 id="检查日志"><a href="#检查日志" class="headerlink" title="检查日志"></a>检查日志</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 src]# kubectl logs coredns-55f86bf584-hsrbp -n kube-system</span><br><span class="line">.:53</span><br><span class="line">2018/09/22 02:03:06 [INFO] CoreDNS-1.2.2</span><br><span class="line">2018/09/22 02:03:06 [INFO] linux/amd64, go1.11, eb51e8b</span><br><span class="line">CoreDNS-1.2.2</span><br><span class="line">linux/amd64, go1.11, eb51e8b</span><br></pre></td></tr></table></figure><h3 id="验证-dns-服务"><a href="#验证-dns-服务" class="headerlink" title="验证 dns 服务"></a>验证 dns 服务</h3><p>在验证 dns 之前，在 dns 未部署++之前创建的 pod 与 deployment 等，都必须删除++，重新部署，否则无法解析。</p><h4 id="创建一个-pods-来测试一下-dns"><a href="#创建一个-pods-来测试一下-dns" class="headerlink" title="创建一个 pods 来测试一下 dns"></a>创建一个 pods 来测试一下 dns</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: alpine</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: alpine</span><br><span class="line">    image: alpine</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br></pre></td></tr></table></figure><h3 id="查看-创建的服务"><a href="#查看-创建的服务" class="headerlink" title="查看 创建的服务"></a>查看 创建的服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ~]# kubectl get po,svc -o wide</span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">pod/alpine                     1/1       Running   0          52s       10.254.102.141   node1     &lt;none&gt;</span><br><span class="line">pod/nginx-dm-fff68d674-fzhqk   1/1       Running   0          3m        10.254.102.140   node1     &lt;none&gt;</span><br><span class="line">pod/nginx-dm-fff68d674-h8n79   1/1       Running   0          3m        10.254.75.22     node2     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">service/kubernetes   ClusterIP   10.254.0.1      &lt;none&gt;        443/TCP   20d       &lt;none&gt;</span><br><span class="line">service/nginx-svc    ClusterIP   10.254.10.144   &lt;none&gt;        80/TCP    3m        name=nginx</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ~]#  kubectl exec -it alpine nslookup nginx-svc</span><br><span class="line">nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolve</span><br><span class="line"></span><br><span class="line">Name:      nginx-svc</span><br><span class="line">Address 1: 10.254.10.144 nginx-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h2 id="部署-DNS-自动伸缩"><a href="#部署-DNS-自动伸缩" class="headerlink" title="部署 DNS 自动伸缩"></a>部署 DNS 自动伸缩</h2><p><strong>按照 node 数量 自动伸缩 dns 数量</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim dns-auto-scaling.yaml</span><br><span class="line"></span><br><span class="line">kind: ServiceAccount</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-dns-autoscaler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;nodes&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;replicationcontrollers/scale&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;]</span><br><span class="line">    resources: [&quot;deployments/scale&quot;, &quot;replicasets/scale&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;configmaps&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;create&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: kube-dns-autoscaler</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-dns-autoscaler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns-autoscaler</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns-autoscaler</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-dns-autoscaler</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos;</span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">      - name: autoscaler</span><br><span class="line">        image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2</span><br><span class="line">        resources:</span><br><span class="line">            requests:</span><br><span class="line">                cpu: &quot;20m&quot;</span><br><span class="line">                memory: &quot;10Mi&quot;</span><br><span class="line">        command:</span><br><span class="line">          - /cluster-proportional-autoscaler</span><br><span class="line">          - --namespace=kube-system</span><br><span class="line">          - --configmap=kube-dns-autoscaler</span><br><span class="line">          - --target=Deployment/coredns</span><br><span class="line">          - --default-params=&#123;&quot;linear&quot;:&#123;&quot;coresPerReplica&quot;:256,&quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true&#125;&#125;</span><br><span class="line">          - --logtostderr=true</span><br><span class="line">          - --v=2</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: &quot;CriticalAddonsOnly&quot;</span><br><span class="line">        operator: &quot;Exists&quot;</span><br><span class="line">      serviceAccountName: kube-dns-autoscaler</span><br></pre></td></tr></table></figure></p><h4 id="导入文件"><a href="#导入文件" class="headerlink" title="导入文件"></a>导入文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ~]# kubectl apply -f dns-auto-scaling.yaml</span><br><span class="line">serviceaccount/kube-dns-autoscaler created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created</span><br><span class="line">deployment.apps/kube-dns-autoscaler created</span><br></pre></td></tr></table></figure><p>++如下是上面所用到的镜像，如果不可以下载使用如下的即可++：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:coredns-1.2.2</span><br><span class="line"></span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cluster-proportional-autoscaler-amd64_1.1.2-r2</span><br></pre></td></tr></table></figure></p><h2 id="部署-Ingress-与-Dashboard"><a href="#部署-Ingress-与-Dashboard" class="headerlink" title="部署 Ingress 与 Dashboard"></a>部署 Ingress 与 Dashboard</h2><h3 id="部署-heapster"><a href="#部署-heapster" class="headerlink" title="部署 heapster"></a>部署 heapster</h3><p>官方 dashboard 的github <a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard</a></p><p>官方 heapster 的github <a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">https://github.com/kubernetes/heapster</a></p><h4 id="下载-heapster-相关-yaml-文件"><a href="#下载-heapster-相关-yaml-文件" class="headerlink" title="下载 heapster 相关 yaml 文件"></a>下载 heapster 相关 yaml 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml</span><br><span class="line"></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml</span><br><span class="line"></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml</span><br><span class="line"></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml</span><br></pre></td></tr></table></figure><p>==如上官方镜像一直在更新，修改的时候需要把如下的版本号也修改下↓==</p><h4 id="下载-heapster-镜像下载"><a href="#下载-heapster-镜像下载" class="headerlink" title="下载 heapster 镜像下载"></a>下载 heapster 镜像下载</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 官方镜像</span><br><span class="line">k8s.gcr.io/heapster-grafana-amd64:v4.4.3</span><br><span class="line">k8s.gcr.io/heapster-amd64:v1.5.3</span><br><span class="line">k8s.gcr.io/heapster-influxdb-amd64:v1.3.3</span><br><span class="line"></span><br><span class="line"># 个人的镜像</span><br><span class="line">jicki/heapster-grafana-amd64:v4.4.3</span><br><span class="line">jicki/heapster-amd64:v1.5.3</span><br><span class="line">jicki/heapster-influxdb-amd64:v1.3.3</span><br><span class="line"></span><br><span class="line"># 备用阿里镜像</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-grafana-amd64-v4.4.3</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-amd64-v1.5.3</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-influxdb-amd64-v1.3.3</span><br><span class="line"></span><br><span class="line"># 替换所有yaml 镜像地址</span><br><span class="line"></span><br><span class="line">sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; *.yaml</span><br></pre></td></tr></table></figure><h3 id="修改-yaml-文件"><a href="#修改-yaml-文件" class="headerlink" title="修改 yaml 文件"></a>修改 yaml 文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># heapster.yaml 文件</span><br><span class="line"></span><br><span class="line">#### 修改如下部分 #####</span><br><span class="line"></span><br><span class="line">因为 kubelet 启用了 https 所以如下配置需要增加 https 端口</span><br><span class="line"></span><br><span class="line">        - --source=kubernetes:https://kubernetes.default</span><br><span class="line">修改为</span><br><span class="line">        - --source=kubernetes:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># heapster-rbac.yaml  文件</span><br><span class="line"></span><br><span class="line">#### 修改为部分 #####</span><br><span class="line"></span><br><span class="line">将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:heapster</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster-kubelet-api</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:kubelet-api-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><h4 id="创建："><a href="#创建：" class="headerlink" title="创建："></a>创建：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 dashboard180922]# kubectl apply -f .</span><br><span class="line">deployment.extensions/monitoring-grafana created</span><br><span class="line">service/monitoring-grafana created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/heapster created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/heapster-kubelet-api created</span><br><span class="line">serviceaccount/heapster created</span><br><span class="line">deployment.extensions/heapster created</span><br><span class="line">service/heapster created</span><br><span class="line">deployment.extensions/monitoring-influxdb created</span><br><span class="line">service/monitoring-influxdb created</span><br></pre></td></tr></table></figure><p>这儿可能需要等待一下，这个取决于自己server的网络情况：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# journalctl -u kubelet -f</span><br><span class="line">-- Logs begin at 六 2018-09-22 09:07:48 CST. --</span><br><span class="line">9月 22 10:34:55 node1 kubelet[2301]: I0922 10:34:55.701016    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=======&gt;                ]  7.617MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:05 node1 kubelet[2301]: I0922 10:35:05.700868    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [========&gt;                ]  8.633MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:15 node1 kubelet[2301]: I0922 10:35:15.701193    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==========&gt;                ]  10.66MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:25 node1 kubelet[2301]: I0922 10:35:25.700980    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [============&gt;                ]  12.69MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:35 node1 kubelet[2301]: I0922 10:35:35.700779    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [===============&gt;                ]  15.74MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:45 node1 kubelet[2301]: I0922 10:35:45.701359    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================&gt;                ]  18.28MB/50.21MB&quot;</span><br><span class="line">9月 22 10:35:55 node1 kubelet[2301]: I0922 10:35:55.701618    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [====================&gt;                ]  20.82MB/50.21MB&quot;</span><br><span class="line">9月 22 10:36:05 node1 kubelet[2301]: I0922 10:36:05.701611    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=========================&gt;                ]  25.39MB/50.21MB&quot;</span><br><span class="line">9月 22 10:36:15 node1 kubelet[2301]: I0922 10:36:15.700926    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==============================&gt;                ]  30.99MB/50.21MB&quot;</span><br><span class="line">9月 22 10:36:25 node1 kubelet[2301]: I0922 10:36:25.700931    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt;                ]  34.55MB/50.21MB&quot;</span><br><span class="line">9月 22 10:36:35 node1 kubelet[2301]: I0922 10:36:35.701950    2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt;                ]  34.55MB/50.21MB&quot;</span><br></pre></td></tr></table></figure></p><h4 id="查看部署情况"><a href="#查看部署情况" class="headerlink" title="查看部署情况"></a>查看部署情况</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wide</span><br><span class="line">NAME                                          READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">pod/calico-kube-controllers-79cfd7887-scnnp   1/1       Running   1          2d        192.168.161.78   node2     &lt;none&gt;</span><br><span class="line">pod/calico-node-pwlq4                         2/2       Running   2          2d        192.168.161.77   node1     &lt;none&gt;</span><br><span class="line">pod/calico-node-vmrrq                         2/2       Running   2          2d        192.168.161.78   node2     &lt;none&gt;</span><br><span class="line">pod/coredns-55f86bf584-fqjf2                  1/1       Running   0          44m       10.254.102.139   node1     &lt;none&gt;</span><br><span class="line">pod/coredns-55f86bf584-hsrbp                  1/1       Running   0          44m       10.254.75.21     node2     &lt;none&gt;</span><br><span class="line">pod/heapster-745d7bc8b7-zk65c                 1/1       Running   0          13m       10.254.75.51     node2     &lt;none&gt;</span><br><span class="line">pod/kube-dns-autoscaler-66d448df8f-4zvw6      1/1       Running   0          32m       10.254.102.142   node1     &lt;none&gt;</span><br><span class="line">pod/monitoring-grafana-558c44f948-m2tzz       1/1       Running   0          1m        10.254.75.6      node2     &lt;none&gt;</span><br><span class="line">pod/monitoring-influxdb-f6bcc9795-496jd       1/1       Running   0          13m       10.254.102.147   node1     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE       SELECTOR</span><br><span class="line">service/heapster              ClusterIP   10.254.4.11    &lt;none&gt;        80/TCP          13m       k8s-app=heapster</span><br><span class="line">service/kube-dns              ClusterIP   10.254.0.2     &lt;none&gt;        53/UDP,53/TCP   44m       k8s-app=kube-dns</span><br><span class="line">service/monitoring-grafana    ClusterIP   10.254.25.50   &lt;none&gt;        80/TCP          1m        k8s-app=grafana</span><br><span class="line">service/monitoring-influxdb   ClusterIP   10.254.37.83   &lt;none&gt;        8086/TCP        13m       k8s-app=influxdb</span><br></pre></td></tr></table></figure><h2 id="部署-dashboard"><a href="#部署-dashboard" class="headerlink" title="部署 dashboard"></a>部署 dashboard</h2><h3 id="下载-dashboard-镜像"><a href="#下载-dashboard-镜像" class="headerlink" title="下载 dashboard 镜像"></a>下载 dashboard 镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 官方镜像</span><br><span class="line">k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3</span><br><span class="line"></span><br><span class="line"># 个人的镜像</span><br><span class="line">jicki/kubernetes-dashboard-amd64:v1.8.3</span><br><span class="line"></span><br><span class="line"># 阿里的镜像</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kubernetes-dashboard-amd64-v1.8.3</span><br></pre></td></tr></table></figure><p>下载 yaml 文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span><br></pre></td></tr></table></figure></p><p>导入 yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 替换所有的 images，注意修改镜像版本号为1.8.3</span><br><span class="line"></span><br><span class="line">sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; kubernetes-dashboard.yaml</span><br></pre></td></tr></table></figure><p>创建dashboard<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 dashboard180922]# kubectl apply -f kubernetes-dashboard.yaml</span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br></pre></td></tr></table></figure></p><p>查看创建的dashboard<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wide | grep dashboard</span><br><span class="line">pod/kubernetes-dashboard-65666d4586-bb66s     1/1       Running   0          7m        10.254.102.151   node1     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">service/kubernetes-dashboard   ClusterIP   10.254.3.42    &lt;none&gt;        443/TCP         7m        k8s-app=kubernetes-dashboard</span><br></pre></td></tr></table></figure></p><h2 id="部署-Nginx-Ingress"><a href="#部署-Nginx-Ingress" class="headerlink" title="部署 Nginx Ingress"></a>部署 Nginx Ingress</h2><p>++Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。++</p><p>官方 Nginx Ingress github: <a href="https://github.com/kubernetes/ingress-nginx/" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/</a></p><h4 id="配置-调度-node"><a href="#配置-调度-node" class="headerlink" title="配置 调度 node"></a>配置 调度 node</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ingress 有多种方式 </span><br><span class="line"></span><br><span class="line">1.  deployment 自由调度 replicas</span><br><span class="line">2.  daemonset 全局调度 分配到所有node里</span><br><span class="line"></span><br><span class="line">#  deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签</span><br><span class="line"></span><br><span class="line"># 默认如下:</span><br><span class="line">[root@master1 ~]# kubectl get node</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">node1     Ready     &lt;none&gt;    20d       v1.11.2</span><br><span class="line">node2     Ready     &lt;none&gt;    8d        v1.11.2</span><br><span class="line"></span><br><span class="line"># 对 node1 与 node2 打上 label</span><br><span class="line"></span><br><span class="line">[root@master1 ~]# kubectl label nodes node1 ingress=proxy</span><br><span class="line">node/node1 labeled</span><br><span class="line">[root@master1 ~]# kubectl label nodes node2 ingress=proxy</span><br><span class="line">node/node2 labeled</span><br><span class="line"></span><br><span class="line"># 打完标签以后</span><br><span class="line"></span><br><span class="line">[root@master1 ~]# kubectl get nodes --show-labels</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line">node1     Ready     &lt;none&gt;    20d       v1.11.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node1</span><br><span class="line">node2     Ready     &lt;none&gt;    9d        v1.11.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node2</span><br></pre></td></tr></table></figure><h3 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 官方镜像</span><br><span class="line">gcr.io/google_containers/defaultbackend:1.4</span><br><span class="line">quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2</span><br><span class="line"></span><br><span class="line"># 国内镜像</span><br><span class="line">jicki/defaultbackend:1.4</span><br><span class="line">jicki/nginx-ingress-controller:0.16.2</span><br><span class="line"></span><br><span class="line"># 阿里镜像</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:defaultbackend-1.4</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:nginx-ingress-controller-0.16.2</span><br></pre></td></tr></table></figure><h3 id="下载-yaml-文件-1"><a href="#下载-yaml-文件-1" class="headerlink" title="下载 yaml 文件"></a>下载 yaml 文件</h3><h4 id="部署-Nginx-backend-Nginx-backend-用于统一转发-没有的域名-到指定页面。"><a href="#部署-Nginx-backend-Nginx-backend-用于统一转发-没有的域名-到指定页面。" class="headerlink" title="部署 Nginx  backend , Nginx backend 用于统一转发 没有的域名 到指定页面。"></a>部署 Nginx  backend , Nginx backend 用于统一转发 没有的域名 到指定页面。</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml</span><br><span class="line"></span><br><span class="line"># 部署 Ingress RBAC 认证</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 部署 Ingress Controller 组件</span><br><span class="line"></span><br><span class="line">curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml</span><br><span class="line"></span><br><span class="line"># tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 为了更加方便理解，如下两个例子：</span><br><span class="line"></span><br><span class="line"># tcp 例子</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">data:</span><br><span class="line">  9000: &quot;default/tomcat:8080&quot;</span><br><span class="line">  </span><br><span class="line">#  以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中</span><br><span class="line"></span><br><span class="line"># udp 例子</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">data:</span><br><span class="line">  53: &quot;kube-system/kube-dns:53&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 替换所有的 images</span><br><span class="line"></span><br><span class="line">sed -i &apos;s/gcr\.io\/google_containers/jicki/g&apos; *</span><br><span class="line">sed -i &apos;s/quay\.io\/kubernetes-ingress-controller/jicki/g&apos; *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 上面 对 两个 node 打了 label 所以配置 replicas: 2</span><br><span class="line"># 修改 yaml 文件 增加 rbac 认证 , hostNetwork  还有 nodeSelector, 第二个 spec 下 增加。</span><br><span class="line"></span><br><span class="line">vim with-rbac.yaml</span><br><span class="line"></span><br><span class="line">第一处：↓</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  </span><br><span class="line">第二处：↓（搜索 /nginx-ingress-serviceaccount 即可，在其下添加）</span><br><span class="line">  ....</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      nodeSelector:</span><br><span class="line">        ingress: proxy</span><br><span class="line">    ....</span><br><span class="line">    第三处：↓</span><br><span class="line">          # 这里添加一个 other 端口做为后续tcp转发</span><br><span class="line">          ports:</span><br><span class="line">          - name: http</span><br><span class="line">            containerPort: 80</span><br><span class="line">          - name: https</span><br><span class="line">            containerPort: 443</span><br><span class="line">          - name: other</span><br><span class="line">            containerPort: 8888</span><br></pre></td></tr></table></figure><h4 id="导入-yaml-文件"><a href="#导入-yaml-文件" class="headerlink" title="导入 yaml 文件"></a>导入 yaml 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 ingress-service]# kubectl apply -f namespace.yaml</span><br><span class="line">namespace/ingress-nginx created</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]# kubectl get ns</span><br><span class="line">NAME            STATUS    AGE</span><br><span class="line">default         Active    20d</span><br><span class="line">ingress-nginx   Active    6s</span><br><span class="line">kube-public     Active    20d</span><br><span class="line">kube-system     Active    20d</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]# kubectl apply -f .</span><br><span class="line">configmap/nginx-configuration created</span><br><span class="line">deployment.extensions/default-http-backend created</span><br><span class="line">service/default-http-backend created</span><br><span class="line">namespace/ingress-nginx configured</span><br><span class="line">serviceaccount/nginx-ingress-serviceaccount created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created</span><br><span class="line">role.rbac.authorization.k8s.io/nginx-ingress-role created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created</span><br><span class="line">configmap/tcp-services created</span><br><span class="line">configmap/udp-services created</span><br><span class="line">deployment.extensions/nginx-ingress-controller created</span><br><span class="line"></span><br><span class="line"># 查看服务，可以看到这两个 pods 被分别调度到 77 与 78 中</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]# kubectl get pods -n ingress-nginx -o wide</span><br><span class="line">NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">default-http-backend-6b89c8bdcb-vvl9f      1/1       Running   0          9m        10.254.102.163   node1     &lt;none&gt;</span><br><span class="line">nginx-ingress-controller-cf8d4564d-5vz7h   1/1       Running   0          9m        10.254.75.16     node2     &lt;none&gt;</span><br><span class="line">nginx-ingress-controller-cf8d4564d-z7q4b   1/1       Running   0          9m        10.254.102.158   node1     &lt;none&gt;</span><br><span class="line"></span><br><span class="line"># 查看我们原有的 svc</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]#  kubectl get pods -o wide</span><br><span class="line">NAME                       READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">alpine                     1/1       Running   3          6h        10.254.102.141   node1     &lt;none&gt;</span><br><span class="line">nginx-dm-fff68d674-fzhqk   1/1       Running   0          6h        10.254.102.140   node1     &lt;none&gt;</span><br><span class="line">nginx-dm-fff68d674-h8n79   1/1       Running   0          6h        10.254.75.22     node2     &lt;none&gt;</span><br></pre></td></tr></table></figure><h4 id="创建一个-基于-nginx-dm-的-ingress"><a href="#创建一个-基于-nginx-dm-的-ingress" class="headerlink" title="创建一个 基于 nginx-dm 的 ingress"></a>创建一个 基于 nginx-dm 的 ingress</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi nginx-ingress.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx.zhdya.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: nginx-svc</span><br><span class="line">          servicePort: 80</span><br><span class="line">          </span><br><span class="line">理解如下:</span><br><span class="line"></span><br><span class="line">- host指虚拟出来的域名，具体地址(我理解应该是Ingress-controller那台Pod所在的主机的地址)应该加入/etc/hosts中,这样所有去nginx.zhdya.cn的请求都会发到nginx</span><br><span class="line"></span><br><span class="line">- servicePort主要是定义服务的时候的端口，不是NodePort.</span><br><span class="line"></span><br><span class="line"># 查看服务</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]# kubectl create -f nginx-ingress.yaml</span><br><span class="line">ingress.extensions/nginx-ingress created</span><br><span class="line"></span><br><span class="line">[root@master1 ingress-service]#  kubectl get ingress</span><br><span class="line">NAME            HOSTS            ADDRESS   PORTS     AGE</span><br><span class="line">nginx-ingress   nginx.zhdya.cn             80        10s</span><br><span class="line"></span><br><span class="line"># 测试访问</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# curl nginx.zhdya.cn</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>当然如果本地浏览器访问的话 我们也需要绑定hosts</p><p><img src="http://myimage.okay686.cn/okay686cn/180925/i4fDbJme64.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个基于 dashboard 的 https 的 ingress</span><br><span class="line"># 新版本的 dashboard 默认就是 ssl ,所以这里使用 tcp 代理到 443 端口</span><br><span class="line"></span><br><span class="line"># 查看 dashboard svc</span><br><span class="line"></span><br><span class="line">[root@master1 ~]# kubectl get svc -n kube-system</span><br><span class="line">NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">heapster               ClusterIP   10.254.4.11    &lt;none&gt;        80/TCP          2d</span><br><span class="line">kube-dns               ClusterIP   10.254.0.2     &lt;none&gt;        53/UDP,53/TCP   3d</span><br><span class="line">kubernetes-dashboard   ClusterIP   10.254.3.42    &lt;none&gt;        443/TCP         2d</span><br><span class="line">monitoring-grafana     ClusterIP   10.254.25.50   &lt;none&gt;        80/TCP          2d</span><br><span class="line">monitoring-influxdb    ClusterIP   10.254.37.83   &lt;none&gt;        8086/TCP        2d</span><br><span class="line"></span><br><span class="line"># 修改 tcp-services-configmap.yaml 文件</span><br><span class="line"></span><br><span class="line">[root@master1 src]# vim tcp-services-configmap.yaml</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">data:</span><br><span class="line">  8888: &quot;kube-system/kubernetes-dashboard:443&quot;</span><br><span class="line"></span><br><span class="line"># 载入配置文件</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl apply -f tcp-services-configmap.yaml</span><br><span class="line">configmap/tcp-services configured</span><br><span class="line"></span><br><span class="line"># 查看服务</span><br><span class="line"></span><br><span class="line">[root@master1 src]#  kubectl get configmap/tcp-services -n ingress-nginx</span><br><span class="line">NAME           DATA      AGE</span><br><span class="line">tcp-services   1         2d</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl describe configmap/tcp-services -n ingress-nginx</span><br><span class="line">Name:         tcp-services</span><br><span class="line">Namespace:    ingress-nginx</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;8888&quot;:&quot;kube-system/kubernetes-dashboard:443&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;tcp-services&quot;,&quot;namesp...</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">8888:</span><br><span class="line">----</span><br><span class="line">kube-system/kubernetes-dashboard:443</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason  Age   From                      Message</span><br><span class="line">  ----    ------  ----  ----                      -------</span><br><span class="line">  Normal  CREATE  2d    nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  2d    nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  2d    nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  2d    nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  20m   nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  19m   nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  CREATE  19m   nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line">  Normal  UPDATE  1m    nginx-ingress-controller  ConfigMap ingress-nginx/tcp-services</span><br><span class="line"></span><br><span class="line"># 测试访问</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888</span><br><span class="line">curl: (6) Could not resolve host: dashboard.zhdya.cn; 未知的名称或服务</span><br><span class="line">当然如上报错很正常，咱们需要绑定下hosts</span><br><span class="line"></span><br><span class="line">在master 上查询下：</span><br><span class="line">[root@master1 src]# kubectl get svc -n kube-system -o wide | grep dashboard</span><br><span class="line">kubernetes-dashboard   ClusterIP   10.254.3.42    &lt;none&gt;        443/TCP         2d        k8s-app=kubernetes-dashboard</span><br><span class="line"></span><br><span class="line">然后再node端绑定hosts </span><br><span class="line">[root@node1 ~]# vim /etc/hosts</span><br><span class="line"></span><br><span class="line">10.254.3.42 dashboard.zhdya.cn</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Accept-Ranges: bytes</span><br><span class="line">Cache-Control: no-store</span><br><span class="line">Content-Length: 990</span><br><span class="line">Content-Type: text/html; charset=utf-8</span><br><span class="line">Last-Modified: Tue, 13 Feb 2018 11:17:03 GMT</span><br><span class="line">Date: Tue, 25 Sep 2018 02:51:18 GMT</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置一个基于域名的 https , ingress</span><br><span class="line"></span><br><span class="line"># 创建一个 基于 自身域名的 证书</span><br><span class="line"></span><br><span class="line">[root@master1 dashboard-keys]# openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.zhdya.cn-key.key -out dashboard.zhdya.cn.pem -subj &quot;/CN=dashboard.zhdya.cn&quot;</span><br><span class="line">Generating a 2048 bit RSA private key</span><br><span class="line">.......+++</span><br><span class="line">..............+++</span><br><span class="line">writing new private key to &apos;dashboard.zhdya.cn-key.key&apos;</span><br><span class="line">-----</span><br><span class="line"></span><br><span class="line">[root@master1 dashboard-keys]# kubectl create secret tls dashboard-secret --namespace=kube-system --cert dashboard.zhdya.cn.pem --key dashboard.zhdya.cn-key.key</span><br><span class="line">secret/dashboard-secret created</span><br><span class="line"></span><br><span class="line"># 查看 secret</span><br><span class="line"></span><br><span class="line">[root@master1 dashboard-keys]# kubectl get secret -n kube-system | grep dashboard</span><br><span class="line">dashboard-secret                      kubernetes.io/tls                     2         55s</span><br><span class="line">kubernetes-dashboard-certs            Opaque                                0         2d</span><br><span class="line">kubernetes-dashboard-key-holder       Opaque                                2         2d</span><br><span class="line">kubernetes-dashboard-token-r98wk      kubernetes.io/service-account-token   3         2d</span><br><span class="line"></span><br><span class="line"># 创建一个 ingress</span><br><span class="line"></span><br><span class="line">vi dashboard-ingress.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - dashboard.zhdya.cn</span><br><span class="line">    secretName: dashboard-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: dashboard.zhdya.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kubernetes-dashboard</span><br><span class="line">          servicePort: 443</span><br><span class="line"></span><br><span class="line"># 创建配置文件</span><br><span class="line">[root@master1 src]# kubectl apply -f dashboard-ingress.yaml</span><br><span class="line">ingress.extensions/kubernetes-dashboard created</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl get ingress -n kube-system</span><br><span class="line">NAME                   HOSTS                ADDRESS   PORTS     AGE</span><br><span class="line">kubernetes-dashboard   dashboard.zhdya.cn             80, 443   37s</span><br></pre></td></tr></table></figure><h4 id="测试访问"><a href="#测试访问" class="headerlink" title="测试访问"></a>测试访问</h4><p><img src="http://myimage.okay686.cn/okay686cn/180925/laCAm1Jb43.png?imageslim" srcset="/img/loading.gif" alt="mark"></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 登录认证</span><br><span class="line"></span><br><span class="line"># 首先创建一个 dashboard rbac 超级用户</span><br><span class="line"></span><br><span class="line">vi dashboard-admin-rbac.yaml</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line"># 导入配置文件</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl apply -f dashboard-admin-rbac.yaml</span><br><span class="line">serviceaccount/kubernetes-dashboard-admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created</span><br><span class="line"></span><br><span class="line"># 查看超级用户的 token 名称</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl -n kube-system get secret | grep kubernetes-dashboard-admin</span><br><span class="line">kubernetes-dashboard-admin-token-kq27d   kubernetes.io/service-account-token   3         38s</span><br><span class="line"></span><br><span class="line"># 查看 token 部分</span><br><span class="line"></span><br><span class="line">[root@master1 src]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-kq27d</span><br></pre></td></tr></table></figure><h4 id="然后我们登录-web-ui-选择-令牌登录"><a href="#然后我们登录-web-ui-选择-令牌登录" class="headerlink" title="然后我们登录 web ui 选择 令牌登录"></a>然后我们登录 web ui 选择 令牌登录</h4><p>然后就发现了还是那熟悉的味道：</p><p><img src="http://myimage.okay686.cn/okay686cn/180925/iaIfdiBega.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes 1.11.2整理Ⅱ</title>
    <link href="/2019/02/09/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A1/"/>
    <url>/2019/02/09/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A1/</url>
    
    <content type="html"><![CDATA[<h3 id="配置-kubelet-认证"><a href="#配置-kubelet-认证" class="headerlink" title="配置 kubelet 认证"></a>配置 kubelet 认证</h3><p>kubelet 授权 kube-apiserver 的一些操作 exec run logs 等</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># RBAC 只需创建一次就可以</span><br><span class="line"></span><br><span class="line">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</span><br></pre></td></tr></table></figure><h4 id="创建-bootstrap-kubeconfig-文件"><a href="#创建-bootstrap-kubeconfig-文件" class="headerlink" title="创建 bootstrap kubeconfig 文件"></a>创建 bootstrap kubeconfig 文件</h4><p>++注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token++</p><h4 id="创建-集群所有-kubelet-的-token"><a href="#创建-集群所有-kubelet-的-token" class="headerlink" title="创建 集群所有 kubelet 的 token"></a>创建 集群所有 kubelet 的 token</h4><p>==注意修改hostname==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master1 --kubeconfig ~/.kube/config</span><br><span class="line">of2phx.v39lq3ofeh0w6f3m</span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master2 --kubeconfig ~/.kube/config</span><br><span class="line">b3stk9.edz2iylppqjo5qbc</span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master3 --kubeconfig ~/.kube/config</span><br><span class="line">ck2uqr.upeu75jzjj1ko901</span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node1 --kubeconfig ~/.kube/config</span><br><span class="line">1ocjm9.7qa3rd5byuft9gwr</span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node2 --kubeconfig ~/.kube/config</span><br><span class="line">htsqn3.z9z6579gxw5jdfzd</span><br></pre></td></tr></table></figure></p><h4 id="查看生成的-token"><a href="#查看生成的-token" class="headerlink" title="查看生成的 token"></a>查看生成的 token</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# kubeadm token list --kubeconfig ~/.kube/config</span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS</span><br><span class="line">1ocjm9.7qa3rd5byuft9gwr   23h       2018-09-02T16:06:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node1</span><br><span class="line"></span><br><span class="line">b3stk9.edz2iylppqjo5qbc   23h       2018-09-02T16:03:46+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:master2</span><br><span class="line"></span><br><span class="line">ck2uqr.upeu75jzjj1ko901   23h       2018-09-02T16:05:16+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:master3</span><br><span class="line"></span><br><span class="line">htsqn3.z9z6579gxw5jdfzd   23h       2018-09-02T16:06:34+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node2</span><br><span class="line"></span><br><span class="line">of2phx.v39lq3ofeh0w6f3m   23h       2018-09-02T16:03:40+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:master1</span><br></pre></td></tr></table></figure><p>以下为了区分 会先生成 hostname 名称加 bootstrap.kubeconfig</p><h4 id="生成-master1-的-bootstrap-kubeconfig"><a href="#生成-master1-的-bootstrap-kubeconfig" class="headerlink" title="生成 master1 的 bootstrap.kubeconfig"></a>生成 master1 的 bootstrap.kubeconfig</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=master1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=of2phx.v39lq3ofeh0w6f3m \</span><br><span class="line">  --kubeconfig=master1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=master1-bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=master1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 拷贝生成的 master1-bootstrap.kubeconfig 文件</span><br><span class="line"></span><br><span class="line">mv master1-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><h4 id="生成-master2-的-bootstrap-kubeconfig"><a href="#生成-master2-的-bootstrap-kubeconfig" class="headerlink" title="生成 master2 的 bootstrap.kubeconfig"></a>生成 master2 的 bootstrap.kubeconfig</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=master2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=b3stk9.edz2iylppqjo5qbc \</span><br><span class="line">  --kubeconfig=master2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=master2-bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=master2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝生成的 master2-bootstrap.kubeconfig 文件</span><br><span class="line"></span><br><span class="line">scp master2-bootstrap.kubeconfig 192.168.161.162:/etc/kubernetes/bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><h4 id="生成-master3-的-bootstrap-kubeconfig"><a href="#生成-master3-的-bootstrap-kubeconfig" class="headerlink" title="生成 master3 的 bootstrap.kubeconfig"></a>生成 master3 的 bootstrap.kubeconfig</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=master3-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=ck2uqr.upeu75jzjj1ko901 \</span><br><span class="line">  --kubeconfig=master3-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=master3-bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=master3-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝生成的 master3-bootstrap.kubeconfig 文件</span><br><span class="line"></span><br><span class="line">scp master3-bootstrap.kubeconfig 192.168.161.163:/etc/kubernetes/bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><h4 id="生成-node1-的-bootstrap-kubeconfig"><a href="#生成-node1-的-bootstrap-kubeconfig" class="headerlink" title="生成 node1 的 bootstrap.kubeconfig"></a>生成 node1 的 bootstrap.kubeconfig</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=node1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=1ocjm9.7qa3rd5byuft9gwr \</span><br><span class="line">  --kubeconfig=node1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=node1-bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=node1-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝生成的 node1-bootstrap.kubeconfig 文件</span><br><span class="line"></span><br><span class="line">scp node1-bootstrap.kubeconfig 192.168.161.77:/etc/kubernetes/bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><h4 id="生成-node2-的-bootstrap-kubeconfig"><a href="#生成-node2-的-bootstrap-kubeconfig" class="headerlink" title="生成 node2 的 bootstrap.kubeconfig"></a>生成 node2 的 bootstrap.kubeconfig</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=node2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=htsqn3.z9z6579gxw5jdfzd \</span><br><span class="line">  --kubeconfig=node2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=node2-bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=node2-bootstrap.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝生成的 node2-bootstrap.kubeconfig 文件</span><br><span class="line"></span><br><span class="line">scp node2-bootstrap.kubeconfig 192.168.161.78:/etc/kubernetes/bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="配置-bootstrap-RBAC-权限"><a href="#配置-bootstrap-RBAC-权限" class="headerlink" title="配置 bootstrap RBAC 权限"></a>配置 bootstrap RBAC 权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 否则报如下错误</span><br><span class="line"></span><br><span class="line">failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:1jezb7&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope</span><br></pre></td></tr></table></figure><h4 id="创建自动批准相关-CSR-请求的-ClusterRole"><a href="#创建自动批准相关-CSR-请求的-ClusterRole" class="headerlink" title="创建自动批准相关 CSR 请求的 ClusterRole"></a>创建自动批准相关 CSR 请求的 ClusterRole</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/kubernetes/tls-instructs-csr.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;certificates.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建 yaml 文件</span><br><span class="line">[root@master1 kubernetes]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver created</span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line">Name:         system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ClusterRole&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;system:certificates.k8s.io:certificatesigningreq...</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                                                      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                                                      -----------------  --------------  -----</span><br><span class="line">  certificatesigningrequests.certificates.k8s.io/selfnodeserver  []                 []              [create]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#  将 ClusterRole 绑定到适当的用户组</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span><br><span class="line"></span><br><span class="line">kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span><br><span class="line"></span><br><span class="line">kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span><br><span class="line"></span><br><span class="line">kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</span><br></pre></td></tr></table></figure><hr><h2 id="Node-端"><a href="#Node-端" class="headerlink" title="Node 端"></a>Node 端</h2><p>单 Node 部分 需要部署的组件有<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker， calico， kubelet， kube-proxy</span><br></pre></td></tr></table></figure></p><p>这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；</span><br><span class="line"></span><br><span class="line">在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server;</span><br><span class="line"></span><br><span class="line">node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口;</span><br><span class="line"></span><br><span class="line">当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA;</span><br></pre></td></tr></table></figure><p><img src="http://myimage.okay686.cn/okay686cn/180901/d4da31h4lB.jpg?imageslim" srcset="/img/loading.gif" alt="mark"></p><p>++这种模式和我之前所接触的不太一样，之前所做的架构是基于KUBE-APISERVER 的负载均衡，所有的node节点都会去连接负载均衡的虚拟VIP。++</p><h3 id="创建Nginx-代理"><a href="#创建Nginx-代理" class="headerlink" title="创建Nginx 代理"></a>创建Nginx 代理</h3><p>在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建配置目录</span><br><span class="line">mkdir -p /etc/nginx</span><br><span class="line"></span><br><span class="line"># 写入代理配置</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conf</span><br><span class="line">error_log stderr notice;</span><br><span class="line"></span><br><span class="line">worker_processes auto;</span><br><span class="line">events &#123;</span><br><span class="line">  multi_accept on;</span><br><span class="line">  use epoll;</span><br><span class="line">  worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stream &#123;</span><br><span class="line">    upstream kube_apiserver &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server 192.168.161.161:6443;</span><br><span class="line">        server 192.168.161.162:6443;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen        0.0.0.0:6443;</span><br><span class="line">        proxy_pass    kube_apiserver;</span><br><span class="line">        proxy_timeout 10m;</span><br><span class="line">        proxy_connect_timeout 1s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 更新权限</span><br><span class="line">chmod +r /etc/nginx/nginx.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=kubernetes apiserver docker wrapper</span><br><span class="line">Wants=docker.socket</span><br><span class="line">After=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=root</span><br><span class="line">PermissionsStartOnly=true</span><br><span class="line">ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\</span><br><span class="line">                              -v /etc/nginx:/etc/nginx \\</span><br><span class="line">                              --name nginx-proxy \\</span><br><span class="line">                              --net=host \\</span><br><span class="line">                              --restart=on-failure:5 \\</span><br><span class="line">                              --memory=512M \\</span><br><span class="line">                              nginx:1.13.7-alpine</span><br><span class="line">ExecStartPre=-/usr/bin/docker rm -f nginx-proxy</span><br><span class="line">ExecStop=/usr/bin/docker stop nginx-proxy</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=15s</span><br><span class="line">TimeoutStartSec=30s</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="启动-Nginx"><a href="#启动-Nginx" class="headerlink" title="启动 Nginx"></a>启动 Nginx</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start nginx-proxy</span><br><span class="line">systemctl enable nginx-proxy</span><br><span class="line">systemctl status nginx-proxy</span><br><span class="line"></span><br><span class="line">journalctl  -u nginx-proxy -f   ##查看实时日志</span><br><span class="line"></span><br><span class="line">9月 01 17:34:55 node1 docker[4032]: 1.13.7-alpine: Pulling from library/nginx</span><br><span class="line">9月 01 17:34:57 node1 docker[4032]: 128191993b8a: Pulling fs layer</span><br><span class="line">9月 01 17:34:57 node1 docker[4032]: 655cae3ea06e: Pulling fs layer</span><br><span class="line">9月 01 17:34:57 node1 docker[4032]: dbc72c3fd216: Pulling fs layer</span><br><span class="line">9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Pulling fs layer</span><br><span class="line">9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Waiting</span><br><span class="line">9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Verifying Checksum</span><br><span class="line">9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Download complete</span><br><span class="line">9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Verifying Checksum</span><br><span class="line">9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Download complete</span><br><span class="line">9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Verifying Checksum</span><br><span class="line">9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Download complete</span><br><span class="line">9月 01 17:35:17 node1 docker[4032]: 128191993b8a: Pull complete</span><br><span class="line">9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Verifying Checksum</span><br><span class="line">9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Download complete</span><br><span class="line">9月 01 17:35:51 node1 docker[4032]: 655cae3ea06e: Pull complete</span><br><span class="line">9月 01 17:35:51 node1 docker[4032]: dbc72c3fd216: Pull complete</span><br><span class="line">9月 01 17:35:51 node1 docker[4032]: f391a4589e37: Pull complete</span><br><span class="line">9月 01 17:35:51 node1 docker[4032]: Digest: sha256:34aa80bb22c79235d466ccbbfa3659ff815100ed21eddb1543c6847292010c4d</span><br><span class="line">9月 01 17:35:51 node1 docker[4032]: Status: Downloaded newer image for nginx:1.13.7-alpine</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: using the &quot;epoll&quot; event method</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: nginx/1.13.7</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: built by gcc 6.2.1 20160822 (Alpine 6.2.1)</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: OS: Linux 3.10.0-514.el7.x86_64</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker processes</span><br><span class="line">9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker process 5</span><br></pre></td></tr></table></figure><h3 id="创建-kubelet-service-文件"><a href="#创建-kubelet-service-文件" class="headerlink" title="创建 kubelet.service 文件"></a>创建 kubelet.service 文件</h3><p>==注意修改节点的hostname↓==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 kubelet 目录</span><br><span class="line"></span><br><span class="line">mkdir -p /var/lib/kubelet</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/kubelet.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line">ExecStart=/usr/local/bin/kubelet \</span><br><span class="line">  --hostname-override=node1 \</span><br><span class="line">  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 \</span><br><span class="line">  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  --config=/etc/kubernetes/kubelet.config.json \</span><br><span class="line">  --cert-dir=/etc/kubernetes/ssl \</span><br><span class="line">  --logtostderr=true \</span><br><span class="line">  --v=2</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p><h4 id="创建-kubelet-config-配置文件"><a href="#创建-kubelet-config-配置文件" class="headerlink" title="创建 kubelet config 配置文件"></a>创建 kubelet config 配置文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/kubernetes/kubelet.config.json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;authentication&quot;: &#123;</span><br><span class="line">    &quot;x509&quot;: &#123;</span><br><span class="line">      &quot;clientCAFile&quot;: &quot;/etc/kubernetes/ssl/ca.pem&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;webhook&quot;: &#123;</span><br><span class="line">      &quot;enabled&quot;: true,</span><br><span class="line">      &quot;cacheTTL&quot;: &quot;2m0s&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;anonymous&quot;: &#123;</span><br><span class="line">      &quot;enabled&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;authorization&quot;: &#123;</span><br><span class="line">    &quot;mode&quot;: &quot;Webhook&quot;,</span><br><span class="line">    &quot;webhook&quot;: &#123;</span><br><span class="line">      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,</span><br><span class="line">      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;address&quot;: &quot;192.168.161.77&quot;,</span><br><span class="line">  &quot;port&quot;: 10250,</span><br><span class="line">  &quot;readOnlyPort&quot;: 0,</span><br><span class="line">  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,</span><br><span class="line">  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,</span><br><span class="line">  &quot;serializeImagePulls&quot;: false,</span><br><span class="line">  &quot;RotateCertificates&quot;: true,</span><br><span class="line">  &quot;featureGates&quot;: &#123;</span><br><span class="line">    &quot;RotateKubeletClientCertificate&quot;: true,</span><br><span class="line">    &quot;RotateKubeletServerCertificate&quot;: true</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;MaxPods&quot;: &quot;512&quot;,</span><br><span class="line">  &quot;failSwapOn&quot;: false,</span><br><span class="line">  &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;,</span><br><span class="line">  &quot;containerLogMaxFiles&quot;: 5,</span><br><span class="line">  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,</span><br><span class="line">  &quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">##其它node节点记得修改如上的IP地址</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如上配置:</span><br><span class="line">node1    本机hostname</span><br><span class="line">10.254.0.2       预分配的 dns 地址</span><br><span class="line">cluster.local.   为 kubernetes 集群的 domain</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1  这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。</span><br><span class="line">&quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;] 可配置多个 dns地址，逗号可开, 可配置宿主机dns.</span><br></pre></td></tr></table></figure><p>++<strong>同理修改其它node节点</strong>++</p><p>启动 kubelet<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl status kubelet</span><br><span class="line"></span><br><span class="line">journalctl -u kubelet -f</span><br></pre></td></tr></table></figure></p><h3 id="创建-kube-proxy-证书"><a href="#创建-kube-proxy-证书" class="headerlink" title="创建 kube-proxy 证书"></a>创建 kube-proxy 证书</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 证书方面由于我们node端没有装 cfssl</span><br><span class="line"># 我们回到 master 端 机器 去配置证书，然后拷贝过来</span><br><span class="line"></span><br><span class="line">cd /opt/ssl</span><br><span class="line"></span><br><span class="line">vi kube-proxy-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">生成 kube-proxy 证书和私钥</span><br><span class="line">/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  -ca-key=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  -config=/opt/ssl/config.json \</span><br><span class="line">  -profile=kubernetes  kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy</span><br><span class="line">  </span><br><span class="line"># 查看生成</span><br><span class="line">ls kube-proxy*</span><br><span class="line">kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem</span><br><span class="line"></span><br><span class="line"># 拷贝到目录</span><br><span class="line"></span><br><span class="line">cp kube-proxy* /etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line">scp ca.pem kube-proxy* 192.168.161.77:/etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line">scp ca.pem kube-proxy* 192.168.161.78:/etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure><h4 id="创建-kube-proxy-kubeconfig-文件"><a href="#创建-kube-proxy-kubeconfig-文件" class="headerlink" title="创建 kube-proxy kubeconfig 文件"></a>创建 kube-proxy kubeconfig 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置集群</span><br><span class="line"></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443 \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \</span><br><span class="line">  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 配置关联</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置默认关联</span><br><span class="line">kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"># 拷贝到需要的 node 端里</span><br><span class="line"></span><br><span class="line">scp kube-proxy.kubeconfig 192.168.161.77:/etc/kubernetes/</span><br><span class="line"></span><br><span class="line">scp kube-proxy.kubeconfig 192.168.161.78:/etc/kubernetes/</span><br></pre></td></tr></table></figure><h4 id="创建-kube-proxy-service-文件"><a href="#创建-kube-proxy-service-文件" class="headerlink" title="创建 kube-proxy.service 文件"></a>创建 kube-proxy.service 文件</h4><p>1.10 官方 ipvs 已经是默认的配置 <strong>–masquerade-all</strong> 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则</p><p>打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 ==<strong>node</strong>== 中安装<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ipset ipvsadm conntrack-tools.x86_64 -y</span><br></pre></td></tr></table></figure></p><p>yaml 配置文件中的 参数如下:</p><p><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /etc/kubernetes/</span><br><span class="line"></span><br><span class="line">vi  kube-proxy.config.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">bindAddress: 192.168.161.77</span><br><span class="line">clientConnection:</span><br><span class="line">  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig</span><br><span class="line">clusterCIDR: 10.254.64.0/18</span><br><span class="line">healthzBindAddress: 192.168.161.77:10256</span><br><span class="line">hostnameOverride: node1             ##注意修改此处的hostname</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">metricsBindAddress: 192.168.161.77:10249</span><br><span class="line">mode: &quot;ipvs&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 kube-proxy 目录</span><br><span class="line"></span><br><span class="line">mkdir -p /var/lib/kube-proxy</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/kube-proxy.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kube-Proxy Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kube-proxy</span><br><span class="line">ExecStart=/usr/local/bin/kube-proxy \</span><br><span class="line">  --config=/etc/kubernetes/kube-proxy.config.yaml \</span><br><span class="line">  --logtostderr=true \</span><br><span class="line">  --v=1</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="启动-kube-proxy"><a href="#启动-kube-proxy" class="headerlink" title="启动 kube-proxy"></a>启动 kube-proxy</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-proxy</span><br><span class="line">systemctl start kube-proxy</span><br><span class="line">systemctl status kube-proxy</span><br></pre></td></tr></table></figure><h4 id="检查-ipvs-情况"><a href="#检查-ipvs-情况" class="headerlink" title="检查  ipvs  情况"></a>检查  ipvs  情况</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 kubernetes]# ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.161.161:6443         Masq    1      0          0</span><br><span class="line">  -&gt; 192.168.161.162:6443         Masq    1      0          0</span><br></pre></td></tr></table></figure><h2 id="配置-Calico-网络"><a href="#配置-Calico-网络" class="headerlink" title="配置 Calico 网络"></a>配置 Calico 网络</h2><p>官方文档 <a href="https://docs.projectcalico.org/v3.1/introduction" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.1/introduction</a></p><h3 id="下载-Calico-yaml"><a href="#下载-Calico-yaml" class="headerlink" title="下载 Calico yaml"></a>下载 Calico yaml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 yaml 文件</span><br><span class="line"></span><br><span class="line">wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml</span><br><span class="line"></span><br><span class="line">wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml</span><br></pre></td></tr></table></figure><h3 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 镜像</span><br><span class="line"></span><br><span class="line"># 国外镜像 有墙</span><br><span class="line">quay.io/calico/node:v3.1.3</span><br><span class="line">quay.io/calico/cni:v3.1.3</span><br><span class="line">quay.io/calico/kube-controllers:v3.1.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 国内镜像</span><br><span class="line">jicki/node:v3.1.3</span><br><span class="line">jicki/cni:v3.1.3</span><br><span class="line">jicki/kube-controllers:v3.1.3</span><br><span class="line"></span><br><span class="line"># 阿里镜像</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:node_v3.1.3</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cni_v3.1.3</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kube-controllers_v3.1.3</span><br><span class="line"></span><br><span class="line"># 替换镜像</span><br><span class="line">sed -i &apos;s/quay\.io\/calico/jicki/g&apos;  calico.yaml</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi calico.yaml</span><br><span class="line"></span><br><span class="line"># 注意修改如下选项:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># etcd 地址</span><br><span class="line"></span><br><span class="line">  etcd_endpoints: &quot;https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379&quot;</span><br><span class="line">  </span><br><span class="line"> </span><br><span class="line"># etcd 证书路径</span><br><span class="line">  # If you&apos;re using TLS enabled etcd uncomment the following.</span><br><span class="line">  # You must also populate the Secret below with these files. </span><br><span class="line">    etcd_ca: &quot;/calico-secrets/etcd-ca&quot;  </span><br><span class="line">    etcd_cert: &quot;/calico-secrets/etcd-cert&quot;</span><br><span class="line">    etcd_key: &quot;/calico-secrets/etcd-key&quot;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面)</span><br><span class="line"></span><br><span class="line">data:</span><br><span class="line">  etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d &apos;\n&apos;)</span><br><span class="line">  etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d &apos;\n&apos;)</span><br><span class="line">  etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d &apos;\n&apos;)</span><br><span class="line">  </span><br><span class="line">## 如上需要去掉() 只需要填写生成的编码即可</span><br><span class="line">  </span><br><span class="line"># 修改 pods 分配的 IP 段</span><br><span class="line"></span><br><span class="line">            - name: CALICO_IPV4POOL_CIDR</span><br><span class="line">              value: &quot;10.254.64.0/18&quot;</span><br></pre></td></tr></table></figure><h3 id="查看服务"><a href="#查看服务" class="headerlink" title="查看服务"></a>查看服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# kubectl get po -n kube-system -o wide</span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE       IP               NODE      NOMINATED NODE</span><br><span class="line">calico-kube-controllers-79cfd7887-xbsd4   1/1       Running   5          11d       192.168.161.77   node1     &lt;none&gt;</span><br><span class="line">calico-node-2545t                         2/2       Running   0          29m       192.168.161.78   node2     &lt;none&gt;</span><br><span class="line">calico-node-tbptz                         2/2       Running   7          11d       192.168.161.77   node1     &lt;none&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master1 kubernetes]# kubectl get nodes -o wide</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME</span><br><span class="line">node1     Ready     &lt;none&gt;    11d       v1.11.2   192.168.161.77   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64   docker://17.3.2</span><br><span class="line">node2     Ready     &lt;none&gt;    29m       v1.11.2   192.168.161.78   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64   docker://17.3.2</span><br></pre></td></tr></table></figure><h3 id="修改-kubelet-配置"><a href="#修改-kubelet-配置" class="headerlink" title="修改 kubelet 配置"></a>修改 kubelet 配置</h3><p>==两台node节点都需要配置==</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#   kubelet 需要增加 cni 插件    --network-plugin=cni</span><br><span class="line"></span><br><span class="line">vim /etc/systemd/system/kubelet.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  --network-plugin=cni \</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 重新加载配置</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br></pre></td></tr></table></figure><p>检查网络的互通性：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# ifconfig</span><br><span class="line">tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1440</span><br><span class="line">        inet 10.254.102.128  netmask 255.255.255.255</span><br><span class="line">        tunnel   txqueuelen 1  (IPIP Tunnel)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# ifconfig</span><br><span class="line">tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1440</span><br><span class="line">        inet 10.254.75.0  netmask 255.255.255.255</span><br><span class="line">        tunnel   txqueuelen 1  (IPIP Tunnel)</span><br><span class="line">        RX packets 2  bytes 168 (168.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 2  bytes 168 (168.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">直接在node2上面ping：</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# ping 10.254.102.128</span><br><span class="line">PING 10.254.102.128 (10.254.102.128) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.254.102.128: icmp_seq=1 ttl=64 time=72.3 ms</span><br><span class="line">64 bytes from 10.254.102.128: icmp_seq=2 ttl=64 time=0.272 ms</span><br></pre></td></tr></table></figure></p><h2 id="安装-calicoctl"><a href="#安装-calicoctl" class="headerlink" title="安装 calicoctl"></a>安装 calicoctl</h2><p>++calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。++</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 二进制文件</span><br><span class="line"></span><br><span class="line">curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl</span><br><span class="line"></span><br><span class="line">mv calicoctl /usr/local/bin/</span><br><span class="line"></span><br><span class="line">chmod +x /usr/local/bin/calicoctl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建 calicoctl.cfg 配置文件</span><br><span class="line"></span><br><span class="line">mkdir /etc/calico</span><br><span class="line"></span><br><span class="line">vim /etc/calico/calicoctl.cfg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: CalicoAPIConfig</span><br><span class="line">metadata:</span><br><span class="line">spec:</span><br><span class="line">  datastoreType: &quot;kubernetes&quot;</span><br><span class="line">  kubeconfig: &quot;/root/.kube/config&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看 calico 状态</span><br><span class="line"></span><br><span class="line">[root@node1 src]# calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">|  PEER ADDRESS  |     PEER TYPE     | STATE |  SINCE   |    INFO     |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line">| 192.168.161.78 | node-to-node mesh | up    | 06:54:19 | Established |</span><br><span class="line">+----------------+-------------------+-------+----------+-------------+</span><br><span class="line"></span><br><span class="line">IPv6 BGP status</span><br><span class="line">No IPv6 peers found.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 src]# calicoctl get node        ##当然我这边是在node节点操作的，node节点是没有/root/.kube/config 这个文件的，只需要从master节点copy过来即可！！</span><br><span class="line">NAME</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes 1.11.2整理Ⅰ</title>
    <link href="/2019/02/08/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A0/"/>
    <url>/2019/02/08/kubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A0/</url>
    
    <content type="html"><![CDATA[<p>1:服务器信息以及节点介绍</p><p>初次使用 ==<strong>CoreDNS</strong>==， ==<strong>Ingress</strong>==， ==<strong>Calico</strong>==</p><p>系统信息：centos7</p><table><thead><tr><th>主机名称</th><th>IP</th><th>备注</th></tr></thead><tbody><tr><td>master1</td><td>192.168.161.161</td><td>master and etcd</td></tr><tr><td>master2</td><td>192.168.161.162</td><td>master and etcd</td></tr><tr><td>master3</td><td>192.168.161.163</td><td>etcd</td></tr><tr><td>node1</td><td>192.168.161.77</td><td>node1</td></tr><tr><td>node2</td><td>192.168.161.78</td><td>node2</td></tr></tbody></table><p><strong>我这边将数据盘挂载了 /opt 目录下</strong></p><h2 id="一、环境初始化"><a href="#一、环境初始化" class="headerlink" title="一、环境初始化"></a>一、环境初始化</h2><p>1：分别在4台主机设置主机名称<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname master1</span><br><span class="line">hostnamectl set-hostname master2</span><br><span class="line">hostnamectl set-hostname master3</span><br><span class="line">hostnamectl set-hostname node1</span><br><span class="line">hostnamectl set-hostname node2</span><br></pre></td></tr></table></figure></p><p>2:配置主机映射</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.161.161 master1</span><br><span class="line">192.168.161.162 master2</span><br><span class="line">192.168.161.163 master3</span><br><span class="line">192.168.161.77 node1</span><br><span class="line">192.168.161.78 node2</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>3：node01上执行ssh免密码登陆配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.161.XXX</span><br></pre></td></tr></table></figure><p>4：四台主机配置、停防火墙、关闭Swap、关闭Selinux、设置内核、K8S的yum源、安装依赖包、配置ntp（配置完后建议重启一次）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">swapoff -a </span><br><span class="line">sed -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstab</span><br><span class="line"></span><br><span class="line">setenforce  0 </span><br><span class="line">sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux </span><br><span class="line">sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config </span><br><span class="line">sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/sysconfig/selinux </span><br><span class="line">sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/selinux/config  </span><br><span class="line"></span><br><span class="line">modprobe br_netfilter</span><br><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line">ls /proc/sys/net/bridge</span><br><span class="line"></span><br><span class="line">yum install -y epel-release</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim  ntpdate libseccomp libtool-ltdl </span><br><span class="line"></span><br><span class="line">systemctl enable ntpdate.service</span><br><span class="line">echo &apos;*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com &gt;/dev/null 2&gt;&amp;1&apos; &gt; /tmp/crontab2.tmp</span><br><span class="line">crontab /tmp/crontab2.tmp</span><br><span class="line">systemctl start ntpdate.service</span><br><span class="line"> </span><br><span class="line">echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.conf</span><br><span class="line">echo &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.conf</span><br><span class="line">echo &quot;* soft nproc 65536&quot;  &gt;&gt; /etc/security/limits.conf</span><br><span class="line">echo &quot;* hard nproc 65536&quot;  &gt;&gt; /etc/security/limits.conf</span><br><span class="line">echo &quot;* soft  memlock  unlimited&quot;  &gt;&gt; /etc/security/limits.conf</span><br><span class="line">echo &quot;* hard memlock  unlimited&quot;  &gt;&gt; /etc/security/limits.conf</span><br></pre></td></tr></table></figure><h2 id="二、环境说明"><a href="#二、环境说明" class="headerlink" title="二、环境说明"></a>二、环境说明</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler</span><br><span class="line"></span><br><span class="line">这里配置2个Master 2个node, Master-161、Master-162 做 Master + etcd, master3 仅仅etcd， node-01 node-02 只做单纯 Node</span><br></pre></td></tr></table></figure><h3 id="创建-验证"><a href="#创建-验证" class="headerlink" title="创建 验证"></a>创建 验证</h3><p>这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。</p><h3 id="安装-cfssl"><a href="#安装-cfssl" class="headerlink" title="安装 cfssl"></a>安装 cfssl</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/local/cfssl</span><br><span class="line"></span><br><span class="line">cd /opt/local/cfssl</span><br><span class="line"></span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 cfssl</span><br><span class="line"></span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 cfssljson</span><br><span class="line"></span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 cfssl-certinfo</span><br><span class="line"></span><br><span class="line">chmod +x *</span><br></pre></td></tr></table></figure><h3 id="创建-CA-证书配置"><a href="#创建-CA-证书配置" class="headerlink" title="创建 CA 证书配置"></a>创建 CA 证书配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /opt/ssl</span><br><span class="line"></span><br><span class="line">cd /opt/ssl</span><br></pre></td></tr></table></figure><h4 id="config-json-文件"><a href="#config-json-文件" class="headerlink" title="config.json 文件"></a>config.json 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi  config.json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">        &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="csr-json-文件"><a href="#csr-json-文件" class="headerlink" title="csr.json 文件"></a>csr.json 文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生成 CA 证书和私钥</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /opt/ssl/</span><br><span class="line"></span><br><span class="line">/opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca</span><br><span class="line"></span><br><span class="line">[root@master1 ssl]#  ls -lt</span><br><span class="line">总用量 20</span><br><span class="line">-rw-r--r-- 1 root root 1005 9月   1 13:36 ca.csr</span><br><span class="line">-rw------- 1 root root 1679 9月   1 13:36 ca-key.pem</span><br><span class="line">-rw-r--r-- 1 root root 1363 9月   1 13:36 ca.pem</span><br><span class="line">-rw-r--r-- 1 root root  210 9月   1 13:35 csr.json</span><br><span class="line">-rw-r--r-- 1 root root  292 9月   1 13:35 config.json</span><br></pre></td></tr></table></figure><h3 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h3><h4 id="创建证书目录"><a href="#创建证书目录" class="headerlink" title="创建证书目录"></a>创建证书目录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h4 id="拷贝所有文件到目录下"><a href="#拷贝所有文件到目录下" class="headerlink" title="拷贝所有文件到目录下"></a>拷贝所有文件到目录下</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp *.pem /etc/kubernetes/ssl</span><br><span class="line">cp ca.csr /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h4 id="这里要将文件拷贝到所有的k8s机器上"><a href="#这里要将文件拷贝到所有的k8s机器上" class="headerlink" title="这里要将文件拷贝到所有的k8s机器上"></a>这里要将文件拷贝到所有的k8s机器上</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp *.pem *.csr 192.168.161.162:/etc/kubernetes/ssl/</span><br><span class="line">scp *.pem *.csr 192.168.161.163:/etc/kubernetes/ssl/</span><br><span class="line">scp *.pem *.csr 192.168.161.77:/etc/kubernetes/ssl/</span><br><span class="line">scp *.pem *.csr 192.168.161.78:/etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure><h2 id="三、安装-docker"><a href="#三、安装-docker" class="headerlink" title="三、安装 docker"></a>三、安装 docker</h2><p>所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 导入 yum 源</span><br><span class="line"></span><br><span class="line"># 安装 yum-config-manager</span><br><span class="line"></span><br><span class="line">yum -y install yum-utils</span><br><span class="line"></span><br><span class="line"># 导入</span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"># 更新 repo</span><br><span class="line">yum makecache</span><br><span class="line"></span><br><span class="line"># 查看yum 版本</span><br><span class="line"></span><br><span class="line">yum list docker-ce.x86_64  --showduplicates |sort -r</span><br><span class="line"></span><br><span class="line"># 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux</span><br><span class="line"></span><br><span class="line">wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</span><br><span class="line"></span><br><span class="line">rpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum -y install docker-ce-17.03.2.ce</span><br><span class="line"></span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="更改docker-配置"><a href="#更改docker-配置" class="headerlink" title="更改docker 配置"></a>更改docker 配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加配置</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=http://docs.docker.com</span><br><span class="line">After=network.target docker-storage-setup.service</span><br><span class="line">Wants=docker-storage-setup.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">Environment=GOTRACEBACK=crash</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">ExecStart=/usr/bin/dockerd \</span><br><span class="line">          $DOCKER_OPTS \</span><br><span class="line">          $DOCKER_STORAGE_OPTIONS \</span><br><span class="line">          $DOCKER_NETWORK_OPTIONS \</span><br><span class="line">          $DOCKER_DNS_OPTIONS \</span><br><span class="line">          $INSECURE_REGISTRY</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNPROC=1048576</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TimeoutStartSec=1min</span><br><span class="line">Restart=on-abnormal</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="修改其他配置"><a href="#修改其他配置" class="headerlink" title="修改其他配置"></a>修改其他配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 低版本内核， kernel 3.10.x  配置使用 overlay2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/systemd/system/docker.service.d/</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/docker.service.d/docker-options.conf</span><br><span class="line"></span><br><span class="line"># 添加如下 :   (注意 environment 必须在同一行，如果出现换行会无法加载)</span><br><span class="line"></span><br><span class="line"># docker 版本 17.03.2 之前配置为 --graph=/opt/docker</span><br><span class="line"></span><br><span class="line"># docker 版本 17.04.x 之后配置为 --data-root=/opt/docker </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \</span><br><span class="line">    --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/docker.service.d/docker-dns.conf</span><br><span class="line"></span><br><span class="line"># 添加如下 : </span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;DOCKER_DNS_OPTIONS=\</span><br><span class="line">    --dns 10.254.0.2 --dns 114.114.114.114  \</span><br><span class="line">    --dns-search default.svc.cluster.local --dns-search svc.cluster.local  \</span><br><span class="line">    --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot;</span><br></pre></td></tr></table></figure><h4 id="重新读取配置，启动-docker"><a href="#重新读取配置，启动-docker" class="headerlink" title="重新读取配置，启动 docker"></a>重新读取配置，启动 docker</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure><h4 id="如果报错-请使用"><a href="#如果报错-请使用" class="headerlink" title="如果报错 请使用"></a>如果报错 请使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status docker -l  或 journalctl -u docker 来定位问题</span><br></pre></td></tr></table></figure><h3 id="etcd-集群"><a href="#etcd-集群" class="headerlink" title="etcd 集群"></a>etcd 集群</h3><p>etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.2 etcd 支持最新版本为 v3.2.18</p><h3 id="安装-etcd"><a href="#安装-etcd" class="headerlink" title="安装 etcd"></a>安装 etcd</h3><p>官方地址 <a href="https://github.com/coreos/etcd/releases" target="_blank" rel="noopener">https://github.com/coreos/etcd/releases</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载 二进制文件（3台master机器都需要）</span><br><span class="line"></span><br><span class="line">wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar zxvf etcd-v3.2.18-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cd etcd-v3.2.18-linux-amd64</span><br><span class="line"></span><br><span class="line">mv etcd  etcdctl /usr/bin/</span><br></pre></td></tr></table></figure><h3 id="创建-etcd-证书"><a href="#创建-etcd-证书" class="headerlink" title="创建 etcd 证书"></a>创建 etcd 证书</h3><p>etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP <strong>请多预留几个</strong>，以备后续添加能通过认证，不需要重新签发。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /opt/ssl/</span><br><span class="line"></span><br><span class="line">vi etcd-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.161.161&quot;,</span><br><span class="line">    &quot;192.168.161.162&quot;,</span><br><span class="line">    &quot;192.168.161.163&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="生成-etcd-密钥"><a href="#生成-etcd-密钥" class="headerlink" title="生成 etcd   密钥"></a>生成 etcd   密钥</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \</span><br><span class="line">  -ca-key=/opt/ssl/ca-key.pem \</span><br><span class="line">  -config=/opt/ssl/config.json \</span><br><span class="line">  -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看生成</span><br><span class="line"></span><br><span class="line">[root@master1 ssl]# ls etcd*</span><br><span class="line">etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查证书</span><br><span class="line"></span><br><span class="line">[root@master1 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝到etcd服务器</span><br><span class="line"></span><br><span class="line"># etcd-1 </span><br><span class="line">cp etcd*.pem /etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line"># etcd-2</span><br><span class="line">scp etcd*.pem 192.168.161.162:/etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line"># etcd-3</span><br><span class="line">scp etcd*.pem 192.168.161.163:/etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 如果 etcd 非 root 用户，读取证书会提示没权限</span><br><span class="line"></span><br><span class="line">chmod 644 /etc/kubernetes/ssl/etcd-key.pem</span><br></pre></td></tr></table></figure><h3 id="修改-etcd-配置"><a href="#修改-etcd-配置" class="headerlink" title="修改 etcd 配置"></a>修改 etcd 配置</h3><p>由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中</p><h4 id="创建-etcd-data-目录，-并授权"><a href="#创建-etcd-data-目录，-并授权" class="headerlink" title="创建 etcd data 目录， 并授权"></a>创建 etcd data 目录， 并授权</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd etcd</span><br><span class="line"></span><br><span class="line">mkdir -p /opt/etcd</span><br><span class="line"></span><br><span class="line">chown -R etcd:etcd /opt/etcd</span><br></pre></td></tr></table></figure><h3 id="etcd-1"><a href="#etcd-1" class="headerlink" title="etcd-1"></a>etcd-1</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/systemd/system/etcd.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/opt/etcd/</span><br><span class="line">User=etcd</span><br><span class="line"># set GOMAXPROCS to number of processors</span><br><span class="line">ExecStart=/usr/bin/etcd \</span><br><span class="line">  --name=etcd1 \</span><br><span class="line">  --cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --initial-advertise-peer-urls=https://192.168.161.161:2380 \</span><br><span class="line">  --listen-peer-urls=https://192.168.161.161:2380 \</span><br><span class="line">  --listen-client-urls=https://192.168.161.161:2379,http://127.0.0.1:2379 \</span><br><span class="line">  --advertise-client-urls=https://192.168.161.161:2379 \</span><br><span class="line">  --initial-cluster-token=k8s-etcd-cluster \</span><br><span class="line">  --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \</span><br><span class="line">  --initial-cluster-state=new \</span><br><span class="line">  --data-dir=/opt/etcd/</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="etcd-2"><a href="#etcd-2" class="headerlink" title="etcd-2"></a>etcd-2</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/systemd/system/etcd.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/opt/etcd/</span><br><span class="line">User=etcd</span><br><span class="line"># set GOMAXPROCS to number of processors</span><br><span class="line">ExecStart=/usr/bin/etcd \</span><br><span class="line">  --name=etcd2 \</span><br><span class="line">  --cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --initial-advertise-peer-urls=https://192.168.161.162:2380 \</span><br><span class="line">  --listen-peer-urls=https://192.168.161.162:2380 \</span><br><span class="line">  --listen-client-urls=https://192.168.161.162:2379,http://127.0.0.1:2379 \</span><br><span class="line">  --advertise-client-urls=https://192.168.161.162:2379 \</span><br><span class="line">  --initial-cluster-token=k8s-etcd-cluster \</span><br><span class="line">  --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \</span><br><span class="line">  --initial-cluster-state=new \</span><br><span class="line">  --data-dir=/opt/etcd</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="etcd-3"><a href="#etcd-3" class="headerlink" title="etcd-3"></a>etcd-3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/systemd/system/etcd.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/opt/etcd/</span><br><span class="line">User=etcd</span><br><span class="line"># set GOMAXPROCS to number of processors</span><br><span class="line">ExecStart=/usr/bin/etcd \</span><br><span class="line">  --name=etcd3 \</span><br><span class="line">  --cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --initial-advertise-peer-urls=https://192.168.161.163:2380 \</span><br><span class="line">  --listen-peer-urls=https://192.168.161.163:2380 \</span><br><span class="line">  --listen-client-urls=https://192.168.161.163:2379,http://127.0.0.1:2379 \</span><br><span class="line">  --advertise-client-urls=https://192.168.161.163:2379 \</span><br><span class="line">  --initial-cluster-token=k8s-etcd-cluster \</span><br><span class="line">  --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \</span><br><span class="line">  --initial-cluster-state=new \</span><br><span class="line">  --data-dir=/opt/etcd/</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="启动-etcd"><a href="#启动-etcd" class="headerlink" title="启动 etcd"></a>启动 etcd</h3><p>分别启动 所有节点的 etcd 服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl status etcd</span><br><span class="line"></span><br><span class="line">journalctl -u etcd -f       ##用此命令来动态查看具体日志</span><br></pre></td></tr></table></figure><h3 id="验证-etcd-集群状态"><a href="#验证-etcd-集群状态" class="headerlink" title="验证 etcd 集群状态"></a>验证 etcd 集群状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\</span><br><span class="line">        --cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">        --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">        --key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">        cluster-health</span><br><span class="line">        </span><br><span class="line">member 60ce394098258c3 is healthy: got healthy result from https://192.168.161.163:2379</span><br><span class="line">member afe2d07db38fa5e2 is healthy: got healthy result from https://192.168.161.162:2379</span><br><span class="line">member ba8a716d98dac47b is healthy: got healthy result from https://192.168.161.161:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure><h3 id="查看-etcd-集群成员："><a href="#查看-etcd-集群成员：" class="headerlink" title="查看 etcd 集群成员："></a>查看 etcd 集群成员：</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\</span><br><span class="line">        --cert-file=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">        --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">        --key-file=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">        member list</span><br><span class="line"></span><br><span class="line">60ce394098258c3: name=etcd3 peerURLs=https://192.168.161.163:2380 clientURLs=https://192.168.161.163:2379 isLeader=false</span><br><span class="line">afe2d07db38fa5e2: name=etcd2 peerURLs=https://192.168.161.162:2380 clientURLs=https://192.168.161.162:2379 isLeader=false</span><br><span class="line">ba8a716d98dac47b: name=etcd1 peerURLs=https://192.168.161.161:2380 clientURLs=https://192.168.161.161:2379 isLeader=true</span><br></pre></td></tr></table></figure><h2 id="配置-Kubernetes-集群"><a href="#配置-Kubernetes-集群" class="headerlink" title="配置 Kubernetes 集群"></a>配置 Kubernetes 集群</h2><p>kubectl 安装在所有需要进行操作的机器上</p><h3 id="Master-and-Node"><a href="#Master-and-Node" class="headerlink" title="Master and Node"></a>Master and Node</h3><p>Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。<br>kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。</p><h3 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 从github 上下载版本   (在两台master上节点执行)</span><br><span class="line"></span><br><span class="line">cd /usr/local/src</span><br><span class="line"></span><br><span class="line">wget https://dl.k8s.io/v1.11.2/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">tar -xzvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">cd kubernetes</span><br><span class="line"></span><br><span class="line">cp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm&#125; /usr/local/bin/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scp server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm&#125; 192.168.161.162:/usr/local/bin/</span><br><span class="line"></span><br><span class="line">scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.77:/usr/local/bin/</span><br><span class="line">scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.78:/usr/local/bin/</span><br></pre></td></tr></table></figure><h3 id="创建-admin-证书"><a href="#创建-admin-证书" class="headerlink" title="创建 admin 证书"></a>创建 admin 证书</h3><p>kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /opt/ssl/</span><br><span class="line"></span><br><span class="line">vi admin-csr.json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成 admin 证书和私钥</span><br><span class="line">cd /opt/ssl/</span><br><span class="line"></span><br><span class="line">/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  -ca-key=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  -config=/opt/ssl/config.json \</span><br><span class="line">  -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看生成</span><br><span class="line"></span><br><span class="line">[root@master1 ssl]# ls admin*</span><br><span class="line">admin.csr  admin-csr.json  admin-key.pem  admin.pem</span><br><span class="line"></span><br><span class="line">cp admin*.pem /etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line">scp admin*.pem 192.168.161.162:/etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure><h3 id="生成-kubernetes-配置文件"><a href="#生成-kubernetes-配置文件" class="headerlink" title="生成 kubernetes 配置文件"></a>生成 kubernetes 配置文件</h3><p>生成证书相关的配置文件存储与 /root/.kube 目录中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置 kubernetes 集群</span><br><span class="line"></span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://127.0.0.1:6443</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 配置 客户端认证</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials admin \</span><br><span class="line">  --client-certificate=/etc/kubernetes/ssl/admin.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --client-key=/etc/kubernetes/ssl/admin-key.pem</span><br><span class="line">  </span><br><span class="line">kubectl config set-context kubernetes \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=admin</span><br><span class="line"></span><br><span class="line">kubectl config use-context kubernetes</span><br></pre></td></tr></table></figure><h3 id="创建-kubernetes-证书"><a href="#创建-kubernetes-证书" class="headerlink" title="创建 kubernetes 证书"></a>创建 kubernetes 证书</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /opt/ssl</span><br><span class="line"></span><br><span class="line">vi kubernetes-csr.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.161.161&quot;,</span><br><span class="line">    &quot;192.168.161.162&quot;,</span><br><span class="line">    &quot;192.168.161.163&quot;,</span><br><span class="line">    &quot;192.168.161.77&quot;,</span><br><span class="line">    &quot;192.168.161.78&quot;,</span><br><span class="line">    &quot;10.254.0.1&quot;,</span><br><span class="line">    &quot;kubernetes&quot;,</span><br><span class="line">    &quot;kubernetes.default&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;ShenZhen&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 192.168.161.161 和 172.16.161.162 为 Master 的IP，多个Master需要写多个。  10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用   kubectl get svc ， 就可以查看到</span><br></pre></td></tr></table></figure><h3 id="生成-kubernetes-证书和私钥"><a href="#生成-kubernetes-证书和私钥" class="headerlink" title="生成 kubernetes 证书和私钥"></a>生成 kubernetes 证书和私钥</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  -ca-key=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  -config=/opt/ssl/config.json \</span><br><span class="line">  -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes</span><br><span class="line"></span><br><span class="line"># 查看生成</span><br><span class="line"></span><br><span class="line">[root@master1 ssl]# ls -lt kubernetes*</span><br><span class="line">-rw-r--r-- 1 root root 1277 9月   1 15:31 kubernetes.csr</span><br><span class="line">-rw------- 1 root root 1679 9月   1 15:31 kubernetes-key.pem</span><br><span class="line">-rw-r--r-- 1 root root 1651 9月   1 15:31 kubernetes.pem</span><br><span class="line">-rw-r--r-- 1 root root  531 9月   1 15:31 kubernetes-csr.json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝到目录</span><br><span class="line">cp kubernetes*.pem /etc/kubernetes/ssl/</span><br><span class="line"></span><br><span class="line">scp kubernetes*.pem 192.168.161.162:/etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure><h3 id="配置-kube-apiserver"><a href="#配置-kube-apiserver" class="headerlink" title="配置 kube-apiserver"></a>配置 kube-apiserver</h3><p>kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成 token</span><br><span class="line"></span><br><span class="line">[root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;</span><br><span class="line">97606de41d5ee3c3392aae432eb3143d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建 encryption-config.yaml 配置</span><br><span class="line"></span><br><span class="line">cat &gt; encryption-config.yaml &lt;&lt;EOF</span><br><span class="line">kind: EncryptionConfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">resources:</span><br><span class="line">  - resources:</span><br><span class="line">      - secrets</span><br><span class="line">    providers:</span><br><span class="line">      - aescbc:</span><br><span class="line">          keys:</span><br><span class="line">            - name: key1</span><br><span class="line">              secret: 97606de41d5ee3c3392aae432eb3143d</span><br><span class="line">      - identity: &#123;&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝</span><br><span class="line"></span><br><span class="line">cp encryption-config.yaml /etc/kubernetes/</span><br><span class="line"></span><br><span class="line">scp encryption-config.yaml 192.168.161.162:/etc/kubernetes/</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成高级审核配置文件</span><br><span class="line"></span><br><span class="line">&gt; 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/</span><br><span class="line">&gt;</span><br><span class="line">&gt; 如下为最低限度的日志审核</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd /etc/kubernetes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF</span><br><span class="line"># Log all requests at the Metadata level.</span><br><span class="line">apiVersion: audit.k8s.io/v1beta1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">- level: Metadata</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 拷贝</span><br><span class="line"></span><br><span class="line">scp audit-policy.yaml 192.168.161.162:/etc/kubernetes/</span><br></pre></td></tr></table></figure><h3 id="创建-kube-apiserver-service-文件"><a href="#创建-kube-apiserver-service-文件" class="headerlink" title="创建 kube-apiserver.service 文件"></a>创建 kube-apiserver.service 文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下</span><br><span class="line"># 配置为 各自的本地 IP</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/kube-apiserver.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=root</span><br><span class="line">ExecStart=/usr/local/bin/kube-apiserver \</span><br><span class="line">  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \</span><br><span class="line">  --anonymous-auth=false \</span><br><span class="line">  --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \</span><br><span class="line">  --advertise-address=192.168.161.161 \</span><br><span class="line">  --allow-privileged=true \</span><br><span class="line">  --apiserver-count=3 \</span><br><span class="line">  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><br><span class="line">  --audit-log-maxage=30 \</span><br><span class="line">  --audit-log-maxbackup=3 \</span><br><span class="line">  --audit-log-maxsize=100 \</span><br><span class="line">  --audit-log-path=/var/log/kubernetes/audit.log \</span><br><span class="line">  --authorization-mode=Node,RBAC \</span><br><span class="line">  --bind-address=0.0.0.0 \</span><br><span class="line">  --secure-port=6443 \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --enable-swagger-ui=true \</span><br><span class="line">  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \</span><br><span class="line">  --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \</span><br><span class="line">  --etcd-servers=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379 \</span><br><span class="line">  --event-ttl=1h \</span><br><span class="line">  --kubelet-https=true \</span><br><span class="line">  --insecure-bind-address=127.0.0.1 \</span><br><span class="line">  --insecure-port=8080 \</span><br><span class="line">  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/18 \</span><br><span class="line">  --service-node-port-range=30000-32000 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --enable-bootstrap-token-auth \</span><br><span class="line">  --v=1</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"># --experimental-encryption-provider-config ，替代之前 token.csv 文件</span><br><span class="line"># 这里面要注意的是 --service-node-port-range=30000-32000</span><br><span class="line"># 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。</span><br><span class="line"></span><br><span class="line">记得在另外一台master上修改IP地址</span><br></pre></td></tr></table></figure><h3 id="启动-kube-apiserver"><a href="#启动-kube-apiserver" class="headerlink" title="启动 kube-apiserver"></a>启动 kube-apiserver</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">systemctl status kube-apiserver</span><br></pre></td></tr></table></figure><h4 id="查看启动端口"><a href="#查看启动端口" class="headerlink" title="查看启动端口"></a>查看启动端口</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# netstat -lntp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.161.161:2379    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 192.168.161.161:2380    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      3844/kube-apiserver</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1715/sshd</span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2066/master</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      3844/kube-apiserver</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1715/sshd</span><br><span class="line">tcp6       0      0 ::1:25                  :::*                    LISTEN      2066/master</span><br></pre></td></tr></table></figure><h3 id="配置-kube-controller-manager"><a href="#配置-kube-controller-manager" class="headerlink" title="配置 kube-controller-manager"></a>配置 kube-controller-manager</h3><p>两台master都需要配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">新增几个配置，用于自动 续期证书</span><br><span class="line"></span><br><span class="line">–feature-gates=RotateKubeletServerCertificate=true</span><br><span class="line">–experimental-cluster-signing-duration=86700h0m0s</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 kube-controller-manager.service 文件</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/kube-controller-manager.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/local/bin/kube-controller-manager \</span><br><span class="line">  --address=0.0.0.0 \</span><br><span class="line">  --master=http://127.0.0.1:8080 \</span><br><span class="line">  --allocate-node-cidrs=true \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/18 \</span><br><span class="line">  --cluster-cidr=10.254.64.0/18 \</span><br><span class="line">  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --feature-gates=RotateKubeletServerCertificate=true \</span><br><span class="line">  --controllers=*,tokencleaner,bootstrapsigner \</span><br><span class="line">  --experimental-cluster-signing-duration=86700h0m0s \</span><br><span class="line">  --cluster-name=kubernetes \</span><br><span class="line">  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --leader-elect=true \</span><br><span class="line">  --node-monitor-grace-period=40s \</span><br><span class="line">  --node-monitor-period=5s \</span><br><span class="line">  --pod-eviction-timeout=5m0s \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="启动-kube-controller-manager"><a href="#启动-kube-controller-manager" class="headerlink" title="启动 kube-controller-manager"></a>启动 kube-controller-manager</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">systemctl status kube-controller-manager</span><br></pre></td></tr></table></figure><h5 id="查看启动端口-1"><a href="#查看启动端口-1" class="headerlink" title="查看启动端口"></a>查看启动端口</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# netstat -lntp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.161.161:2379    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 192.168.161.161:2380    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      3844/kube-apiserver</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1715/sshd</span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2066/master</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      3844/kube-apiserver</span><br><span class="line">tcp6       0      0 :::10252                :::*                    LISTEN      3970/kube-controlle</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1715/sshd</span><br><span class="line">tcp6       0      0 ::1:25                  :::*                    LISTEN      2066/master</span><br></pre></td></tr></table></figure><h3 id="配置-kube-scheduler"><a href="#配置-kube-scheduler" class="headerlink" title="配置 kube-scheduler"></a>配置 kube-scheduler</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 kube-cheduler.service 文件</span><br><span class="line"></span><br><span class="line">vi /etc/systemd/system/kube-scheduler.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/local/bin/kube-scheduler \</span><br><span class="line">  --address=0.0.0.0 \</span><br><span class="line">  --master=http://127.0.0.1:8080 \</span><br><span class="line">  --leader-elect=true \</span><br><span class="line">  --v=1</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="启动-kube-scheduler"><a href="#启动-kube-scheduler" class="headerlink" title="启动 kube-scheduler"></a>启动 kube-scheduler</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler</span><br><span class="line">systemctl start kube-scheduler</span><br><span class="line">systemctl status kube-scheduler</span><br></pre></td></tr></table></figure><h4 id="查看启动端口-2"><a href="#查看启动端口-2" class="headerlink" title="查看启动端口"></a>查看启动端口</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# netstat -lntp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.161.161:2379    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 192.168.161.161:2380    0.0.0.0:*               LISTEN      3605/etcd</span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      3844/kube-apiserver</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1715/sshd</span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2066/master</span><br><span class="line">tcp6       0      0 :::10251                :::*                    LISTEN      4023/kube-scheduler</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      3844/kube-apiserver</span><br><span class="line">tcp6       0      0 :::10252                :::*                    LISTEN      3970/kube-controlle</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1715/sshd</span><br><span class="line">tcp6       0      0 ::1:25                  :::*                    LISTEN      2066/master</span><br></pre></td></tr></table></figure><h3 id="验证-Master-节点"><a href="#验证-Master-节点" class="headerlink" title="验证 Master 节点"></a>验证 Master 节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master1 kubernetes]# kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">etcd-2               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">etcd-1               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master2 bin]# kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">etcd-1               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">etcd-2               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>K8s</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernets</tag>
      
      <tag>K8S</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/02/07/hello-world/"/>
    <url>/2019/02/07/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome @all!!</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
