{"meta":{"title":"结果不会给你开玩笑","subtitle":null,"description":"Tough times never last, but tough people do.","author":"Zhdya","url":"http://zhdya.okay686.cn"},"pages":[{"title":"","date":"2019-02-09T06:26:38.268Z","updated":"2019-02-09T05:41:45.173Z","comments":true,"path":"404/index.html","permalink":"http://zhdya.okay686.cn/404/index.html","excerpt":"","text":""},{"title":"about","date":"2019-02-09T09:54:56.000Z","updated":"2019-02-09T11:28:05.840Z","comments":true,"path":"about/index.html","permalink":"http://zhdya.okay686.cn/about/index.html","excerpt":"","text":"关于我@zhdya Zhdya，男，江苏人，出生于公元1991年02月15日，来自农村，是一位早当家的大青年。对我的life总是充满期待并乐观的追求人生价值的最大化。从小就看到了父母的不容易，家人的拼搏，感叹人生苦短，最害怕自己平庸一生，不求对得起自己，只求对得起家人，挚爱，朋友！！！2011年开始学IT技术，2014年走上社会，回首瞰望自己的大学life，只有一张优秀毕业生值得； ☑90后 ☑没房 ☑没车 ☑没钱 ☑没相貌 ☑没身材 ☑没口才 ☑没经验 ☑没身份 ☑没背景 ☑没死 不想就那么碌碌无为的过着！不想10年后的再次回首，没有任何值得的回忆。 别人能做的我为什么不可以？ English for me are difficult. but i think now it’s quite easy, just a matter of time! 我会啥：CCNA，Network Skills（my major），English，Linux，Python+？？。不多，但不代表着停滞不前@fighting…"}],"posts":[{"title":"CMDB-前端框架AdminLTE（九）","slug":"9、前端框架AdminLTE","date":"2019-03-12T16:00:00.000Z","updated":"2019-03-30T12:10:29.888Z","comments":true,"path":"2019/03/13/9、前端框架AdminLTE/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/13/9、前端框架AdminLTE/","excerpt":"","text":"作为CMDB资产管理项目，必须有一个丰富、直观、酷炫的前端页面。 适合运维平台的前端框架有很多，开源的也不少，这里选用的是AdminLTE。 AdminLTE托管在GitHub上，可以通过下面的地址下载： https://github.com/almasaeed2010/AdminLTE/releases AdminLTE自带JQuery和Bootstrap3框架，无需另外下载。 AdminLTE自带多种配色皮肤，可根据需要实时调整。 AdminLTE是移动端自适应的，无需单独考虑。 AdminLTE自带大量插件，比如表格、Charts等等，可根据需要载入。 但是AdminLTE的源文件包内，缺少font-awesome-4.6.3和ionicons-2.0.1这两个图标插件，它是通过CDN的形式加载的，如果网络不太好，加载可能比较困难或者缓慢，最好用本地静态文件的形式。教程在Github的包内附带上了这两个插件，可以直接使用，当然你自己下载安装也行。 一、创建base.htmlAdminLTE源文件包里有个index.html页面文件，可以利用它修改出我们CMDB项目需要的基本框架。 在项目的根目录cmdb下新建static目录，在settings文件中添加下面的配置： 12345STATIC_URL = &apos;/static/&apos;STATICFILES_DIRS = [ os.path.join(BASE_DIR, &quot;mycmdb/static&quot;),] 为了以后扩展的方便，将AdminLTE源文件包里的 bootstrap、dist 和 plugins 三个文件夹，全部拷贝到 static 目录中，这样做的话文件会比较大，比较多，但可以防止出现引用文件找不到、插件缺失等情况的发生，等以后对AdminLTE非常熟悉了，可以对static中无用的文件进行删减。 在cmdb根目录下的templates目录下，新建base.html文件，将AdminLTE源文件包中的index.html中的内容拷贝过去。然后，根据我们项目的具体情况修改文件引用、页面框架、title、CSS、主体和script块。 undefined 这是一个适合当前CMDB的精简版本。 二、创建路由、视图这里设计了三个视图和页面，分别是： dashboard：仪表盘，图形化的数据展示 index：资产总表，表格的形式展示资产信息 detail：单个资产的详细信息页面 将assets/urls.py修改成下面的样子：123456789101112from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;), url(r&apos;^dashboard/&apos;, views.dashboard, name=&apos;dashboard&apos;), url(r&apos;^index/&apos;, views.index, name=&apos;index&apos;), url(r&apos;^detail/(?P&lt;asset_id&gt;[0-9]+)/$&apos;, views.detail, name=&quot;detail&quot;), url(r&apos;^$&apos;, views.dashboard),] 在 assets/views.py 中，增加下面三个视图：12345678910111213141516171819202122from django.shortcuts import get_object_or_404def index(request): assets = models.Asset.objects.all() return render(request, &apos;assets/index.html&apos;, locals())def dashboard(request): pass return render(request, &apos;assets/dashboard.html&apos;, locals())def detail(request, asset_id): &quot;&quot;&quot; 以显示服务器类型资产详细为例，安全设备、存储设备、网络设备等参照此例。 :param request: :param asset_id: :return: &quot;&quot;&quot; asset = get_object_or_404(models.Asset, id=asset_id) return render(request, &apos;assets/detail.html&apos;, locals()) 注意需要提前1from django.shortcuts import get_object_or_404 导入get_object_or_404()方法，这是一个非常常用的内置方法。 三、创建模版1.dashboard.html在assets目录下创建 templates/assets/dashboard.html 文件，写入下面的代码： undefined 2.index.html在assets目录下创建 templates/assets/index.html 文件，写入下面的代码： undefined 3.detail.html在assets目录下创建 templates/assets/detail.html 文件，写入下面的代码： undefined 以上三个模板都很简单，就是下面的流程： - extends继承‘base.html’； - {% load staticfiles %}载入静态文件； - {% block title %}资产详细{% endblock %}，定制title; - {% block css %}，载入当前页面的专用CSS文件； - {% block script %}，载入当前页面的专用js文件； - 最后在{% block content %}中，编写一个当前页面的面包屑导航； - 页面的主体内容在后面的章节进行充实。 四、访问页面重启CMDB服务器，访问http://192.168.1.3:8000/assets/dashboard/，可以看到下面的页面。","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"urllib.parse.urlencode转换get请求参数","slug":"urllib.parse.urlencode转换get请求参数","date":"2019-03-12T16:00:00.000Z","updated":"2019-03-12T23:53:11.325Z","comments":true,"path":"2019/03/13/urllib.parse.urlencode转换get请求参数/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/13/urllib.parse.urlencode转换get请求参数/","excerpt":"","text":"浏览器地址栏搜索 刘若英1https://www.baidu.com/s?word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 但是复制到文件中是这样的：1https://www.baidu.com/s?word=%E5%88%98%E8%8B%A5%E8%8B%B1&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 这是因为浏览器对中文请求参数进行了转码用代码访问网站所发的请求中如果有中文也必须是转码之后的。这里需要用到1urllib.parse.urlencode 方法。这个方法的作用就是将字典里面所有的键值转化为query-string格式（key=value&amp;key=value），并且将中文转码。 1234567891011121314151617181920212223242526272829303132333435363738import urllib.requestimport urllib.parseimport osurl = &apos;http://www.baidu.com/s?&apos;wd = input(&apos;请输入要搜索关键字： &apos;)&quot;&quot;&quot;word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8&quot;&quot;&quot;data = &#123; &apos;word&apos;: wd, &apos;tn&apos;: &apos;71069079_1_hao_pg&apos;, &apos;ie&apos;: &apos;utf-8&apos;&#125;query_string = urllib.parse.urlencode(data)# 拼接获取完整urlurl += query_string# 发起请求，获取响应response = urllib.request.urlopen(url=url)filename = wd + &apos;.html&apos;dirname = &apos;./html&apos;if not os.path.exists(dirname): os.mkdir(dirname)filepath = dirname + &apos;/&apos; + filename# 以二进制写入文件# with open(filepath, &apos;wb&apos;) as fp:# fp.write(response.read())# 或者以utf8编码写入文件with open (filepath, &apos;w&apos;, encoding=&apos;utf8&apos;) as fp: fp.write(response.read().decode(&apos;utf8&apos;))","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"}]},{"title":"CMDB-已上线资产信息更新（八）","slug":"8、已上线资产信息更新","date":"2019-03-10T16:00:00.000Z","updated":"2019-03-30T10:04:24.252Z","comments":true,"path":"2019/03/11/8、已上线资产信息更新/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/11/8、已上线资产信息更新/","excerpt":"","text":"前面，我们已经实现了资产进入待审批区、更新待审批区的资产信息以及审批资产上线三个主要功能，还剩下一个最主要的实时更新已上线资产信息的功能。 在assets/views.py中的report视图，目前是把已上线资产的数据更新流程‘pass’了，现在将其替换成下面的语句： 1update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) report视图变成了下面的样子：views.py 12345678910111213141516171819202122232425262728293031323334353637383940from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) 然后，进入assets/asset_handler.py模块，修改log()方法，增加UpdateAsset类，最终的asset_handler.py如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\\n%s&quot; % msg event.user = request.user elif log_type == &quot;update&quot;: event.name = &quot;%s &lt;%s&gt; ： 数据更新！&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新成功！&quot; elif log_type == &quot;update_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 更新失败&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新失败！\\n%s&quot; % msg # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;create_manufacturer--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete()class UpdateAsset: &quot;&quot;&quot; 自动更新已上线的资产。 如果想让记录的日志更详细，可以逐条对比数据项，将更新过的项目记录到log信息中。 &quot;&quot;&quot; def __init__(self, request, asset, report_data): self.request = request self.asset = asset self.report_data = report_data # 此处的数据是由客户端发送过来的整个数据字符串 self.asset_update() def asset_update(self): # 为以后的其它类型资产扩展留下接口 func = getattr(self, &quot;_%s_update&quot; % self.report_data[&apos;asset_type&apos;]) func() def _server_update(self): try: self._update_manufacturer() # 更新厂商 self._update_server() # 更新服务器 self._update_CPU() # 更新CPU self._update_RAM() # 更新内存 self._update_disk() # 更新硬盘 self._update_nic() # 更新网卡 self.asset.save() except Exception as e: log(&apos;update_failed&apos;, msg=e, asset=self.asset, request=self.request) print(e) else: # 添加日志 log(&quot;update&quot;, asset=self.asset) print(&quot;资产数据被更新!&quot;) def _update_manufacturer(self): &quot;&quot;&quot; 更新厂商 &quot;&quot;&quot; m = self.report_data.get(&apos;manufacturer&apos;) if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) self.asset.manufacturer = manufacturer_obj else: self.asset.manufacturer = None self.asset.manufacturer.save() def _update_server(self): &quot;&quot;&quot; 更新服务器 &quot;&quot;&quot; self.asset.server.model = self.report_data.get(&apos;model&apos;) self.asset.server.os_type = self.report_data.get(&apos;os_type&apos;) self.asset.server.os_distribution = self.report_data.get(&apos;os_distribution&apos;) self.asset.server.os_release = self.report_data.get(&apos;os_release&apos;) self.asset.server.save() def _update_CPU(self): &quot;&quot;&quot; 更新CPU信息 :return: &quot;&quot;&quot; self.asset.cpu.cpu_model = self.report_data.get(&apos;cpu_model&apos;) self.asset.cpu.cpu_count = self.report_data.get(&apos;cpu_count&apos;) self.asset.cpu.cpu_core_count = self.report_data.get(&apos;cpu_core_count&apos;) self.asset.cpu.save() def _update_RAM(self): &quot;&quot;&quot; 更新内存信息。 使用集合数据类型中差的概念，处理不同的情况。 如果新数据有，但原数据没有，则新增； 如果新数据没有，但原数据有，则删除原来多余的部分； 如果新的和原数据都有，则更新。 在原则上，下面的代码应该写成一个复用的函数， 但是由于内存、硬盘、网卡在某些方面的差别，导致很难提取出重用的代码。 :return: &quot;&quot;&quot; # 获取已有内存信息，并转成字典格式 old_rams = models.RAM.objects.filter(asset=self.asset) old_rams_dict = dict() if old_rams: for ram in old_rams: old_rams_dict[ram.slot] = ram # 获取新数据中的内存信息，并转成字典格式 new_rams_list = self.report_data[&apos;ram&apos;] new_rams_dict = dict() if new_rams_list: for item in new_rams_list: new_rams_dict[item[&apos;slot&apos;]] = item # 利用set类型的差集功能，获得需要删除的内存数据对象 need_deleted_keys = set(old_rams_dict.keys()) - set(new_rams_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_rams_dict[key].delete() # 需要新增或更新的 if new_rams_dict: for key in new_rams_dict: defaults = &#123; &apos;sn&apos;: new_rams_dict[key].get(&apos;sn&apos;), &apos;model&apos;: new_rams_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_rams_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_rams_dict[key].get(&apos;capacity&apos;, 0), &#125; models.RAM.objects.update_or_create(asset=self.asset, slot=key, defaults=defaults) def _update_disk(self): &quot;&quot;&quot; 更新硬盘信息。类似更新内存。 &quot;&quot;&quot; old_disks = models.Disk.objects.filter(asset=self.asset) old_disks_dict = dict() if old_disks: for disk in old_disks: old_disks_dict[disk.sn] = disk new_disks_list = self.report_data[&apos;physical_disk_driver&apos;] new_disks_dict = dict() if new_disks_list: for item in new_disks_list: new_disks_dict[item[&apos;sn&apos;]] = item # 需要删除的 need_deleted_keys = set(old_disks_dict.keys()) - set(new_disks_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_disks_dict[key].delete() # 需要新增或更新的 if new_disks_dict: for key in new_disks_dict: interface_type = new_disks_dict[key].get(&apos;iface_type&apos;, &apos;unknown&apos;) if interface_type not in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: interface_type = &apos;unknown&apos; defaults = &#123; &apos;slot&apos;: new_disks_dict[key].get(&apos;slot&apos;), &apos;model&apos;: new_disks_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_disks_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_disks_dict[key].get(&apos;capacity&apos;, 0), &apos;interface_type&apos;: interface_type, &#125; models.Disk.objects.update_or_create(asset=self.asset, sn=key, defaults=defaults) def _update_nic(self): &quot;&quot;&quot; 更新网卡信息。类似更新内存。 &quot;&quot;&quot; old_nics = models.NIC.objects.filter(asset=self.asset) old_nics_dict = dict() if old_nics: for nic in old_nics: old_nics_dict[nic.model+nic.mac] = nic new_nics_list = self.report_data[&apos;nic&apos;] new_nics_dict = dict() if new_nics_list: for item in new_nics_list: new_nics_dict[item[&apos;model&apos;]+item[&apos;mac&apos;]] = item # 需要删除的 need_deleted_keys = set(old_nics_dict.keys()) - set(new_nics_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_nics_dict[key].delete() # 需要新增或更新的 if new_nics_dict: for key in new_nics_dict: if new_nics_dict[key].get(&apos;net_mask&apos;) and len(new_nics_dict[key].get(&apos;net_mask&apos;)) &gt; 0: net_mask = new_nics_dict[key].get(&apos;net_mask&apos;)[0] else: net_mask = &quot;&quot; defaults = &#123; &apos;name&apos;: new_nics_dict[key].get(&apos;name&apos;), &apos;ip_address&apos;: new_nics_dict[key].get(&apos;ip_address&apos;), &apos;net_mask&apos;: net_mask, &#125; models.NIC.objects.update_or_create(asset=self.asset, model=new_nics_dict[key][&apos;model&apos;], mac=new_nics_dict[key][&apos;mac&apos;], defaults=defaults) print(&apos;更新成功！&apos;) 对于log()函数，只是增加了两种数据更新的日志类型，分别记录不同的日志情况，没什么特别的。 对于UpdateAsset类，类似前面的ApproveAsset类： 首先初始化动作，自动执行asset_update()方法； 依然是通过反射，决定要调用的更新方法； 教程实现了主要的服务器类型资产的更新，对于网络设备、安全设备等请自行完善，基本类似； _server_update(self)方法中，分别更新厂商、服务器本身、CPU、内存、网卡、硬盘等信息。然后保存数据，这些事务应该是原子性的，所以要抓取异常； 不管成功还是失败，都要记录日志。 最主要的，对于_update_CPU(self)等方法，以内存为例，由于内存可能有多条，新的数据中可能出现三种情况，拔除、新增、信息变更，因此要分别对待和处理。 首先，获取已有内存信息，并转成字典格式； 其次，获取新数据中的内存信息，并转成字典格式； 利用set类型的差集功能，获得需要删除的内存数据对象 对要删除的对象，执行delete()方法； 对于需要新增或更新的内存对象，首先生成defaults数据字典； 然后，使用update_or_create(asset=self.asset, slot=key, defaults=defaults)方法，一次性完成新增或者更新数据的操作，不用写两个方法的代码； 硬盘和网卡的操作类同内存的操作。 数据更新完毕后，需要保存asset对象，也就是self.asset.save()，否则前面的工作无法关联保存下来。 现在，可以测试一下资产数据的更新了。重启CMDB，然后转到Client/report_assetss.py脚本，直接运行： 修改其中的一些数据，删除或增加一些内存、硬盘、网卡的条目。注意数据格式必须正确，sn必须不能变。 再次运行脚本，报告数据。进入admin中查看相关内容，可以看到数据已经得到更新了。 至此，CMDB自动资产管理系统的后台部分已经完成了。","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB-审批新资产（七）","slug":"7、审批新资产","date":"2019-03-09T16:00:00.000Z","updated":"2019-03-18T14:19:53.443Z","comments":true,"path":"2019/03/10/7、审批新资产/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/10/7、审批新资产/","excerpt":"","text":"一、自定义admin的actions需要有专门的审批员来审批新资产，对资产的合法性、健全性、可用性等更多方面进行审核，如果没有问题，那么就批准上线。 批准上线这一操作是通过admin的自定义actions来实现的。 Django的admin默认有一个delete操作的action，所有在admin中的模型都有这个action，更多的就需要我们自己编写了。 修改/assets/admin.py的代码，新的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from django.contrib import admin# Register your models here.from assets import modelsfrom . import asset_handlerclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,) actions = [&apos;approve_selected_new_assets&apos;] def approve_selected_new_assets(self, request, queryset): # 获得被打钩的checkbox对应的资产 selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME) success_upline_number = 0 for asset_id in selected: obj = asset_handler.ApproveAsset(request, asset_id) ret = obj.asset_upline() if ret: success_upline_number += 1 # 顶部绿色提示信息 self.message_user(request, &quot;成功批准 %s 条新资产上线！&quot; % success_upline_number) approve_selected_new_assets.short_description = &quot;批准选择的新资产&quot;class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &quot;m_time&quot;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 说明： 通过actions = [‘approve_selected_new_assets’]定义当前模型的新acitons列表； approve_selected_new_assets()方法包含具体的动作逻辑； 自定义的action接收至少三个参数，第一个是self，第二个是request即请求，第三个是被选中的数据对象集合queryset。 首先通过request.POST.getlist()方法获取被打钩的checkbox对应的资产； 可能同时有多个资产被选择，所以这是个批量操作，需要进行循环； selected是一个包含了被选中资产的id值的列表； 对于每一个资产，创建一个asset_handler.ApproveAsset()的实例，然后调用实例的asset_upline()方法，并获取返回值。如果返回值为True，说明该资产被成功批准，那么success_upline_number变量+1，保存成功批准的资产数； 最后，在admin中给与提示信息。 approve_selected_new_assets.short_description = “批准选择的新资产”用于在admin界面中为action提供中文显示。你可以尝试去掉这条，看看效果。 重新启动CMDB，进入admin的待审批资产区，查看上方的acitons动作条，如下所示： 二、创建测试用例由于没有真实的服务器供测试，这里需要手动创建一些虚假的服务器用例，方便后面的使用和展示。 首先，将先前的所有资产条目全部从admin中删除，确保数据库内没有任何数据。 然后，在Client/bin/目录下新建一个report_assets脚本，其内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonimport osimport sysimport urllib.requestimport urllib.parseBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from conf import settingsdef update_test(data): &quot;&quot;&quot; 创建测试用例 :return: &quot;&quot;&quot; # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(data)&#125; # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() ##转码 response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\\033[31;1m发送完毕！\\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\\033[31;1m发送失败，%s\\033[0m&quot; % e)if __name__ == &apos;__main__&apos;: windows_data = &#123; &quot;os_type&quot;: &quot;Windows&quot;, &quot;os_release&quot;: &quot;7 64bit 6.1.7601 &quot;, &quot;os_distribution&quot;: &quot;Microsoft&quot;, &quot;asset_type&quot;: &quot;server&quot;, &quot;cpu_count&quot;: 2, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;cpu_core_count&quot;: 8, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &quot;model&quot;: &quot;Physical Memory&quot;, &quot;manufacturer&quot;: &quot;kingstone &quot;, &quot;sn&quot;: &quot;456&quot; &#125;, ], &quot;manufacturer&quot;: &quot;Intel&quot;, &quot;model&quot;: &quot;P67X-UD3R-B3&quot;, &quot;wake_up_type&quot;: 6, &quot;sn&quot;: &quot;00426-OEM-8992662-111111&quot;, &quot;physical_disk_driver&quot;: [ &#123; &quot;iface_type&quot;: &quot;unknown&quot;, &quot;slot&quot;: 0, &quot;sn&quot;: &quot;3830414130423230343234362020202020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 128 &#125;, &#123; &quot;iface_type&quot;: &quot;SATA&quot;, &quot;slot&quot;: 1, &quot;sn&quot;: &quot;383041413042323023234362020102020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 2048 &#125;, ], &quot;nic&quot;: [ &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000011] Realtek RTL8192CU Wireless LAN 802.11n USB 2.0 Network Adapter&quot;, &quot;name&quot;: 11, &quot;ip_address&quot;: &quot;192.168.1.110&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;0A:01:27:00:00:00&quot;, &quot;model&quot;: &quot;[00000013] VirtualBox Host-Only Ethernet Adapter&quot;, &quot;name&quot;: 13, &quot;ip_address&quot;: &quot;192.168.56.1&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000017] Microsoft Virtual WiFi Miniport Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;Intel Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;192.1.1.1&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, ] &#125; linux_data = &#123; &quot;asset_type&quot;: &quot;server&quot;, &quot;manufacturer&quot;: &quot;innotek GmbH&quot;, &quot;sn&quot;: &quot;00001&quot;, &quot;model&quot;: &quot;VirtualBox&quot;, &quot;uuid&quot;: &quot;E8DE611C-4279-495C-9B58-502B6FCED076&quot;, &quot;wake_up_type&quot;: &quot;Power Switch&quot;, &quot;os_distribution&quot;: &quot;Ubuntu&quot;, &quot;os_release&quot;: &quot;Ubuntu 16.04.3 LTS&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &quot;cpu_count&quot;: &quot;2&quot;, &quot;cpu_core_count&quot;: &quot;4&quot;, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &#125; ], &quot;ram_size&quot;: 3.858997344970703, &quot;nic&quot;: [], &quot;physical_disk_driver&quot;: [ &#123; &quot;model&quot;: &quot;VBOX HARDDISK&quot;, &quot;size&quot;: &quot;50&quot;, &quot;sn&quot;: &quot;VBeee1ba73-09085302&quot; &#125; ] &#125; update_test(linux_data) update_test(windows_data) 该脚本的作用很简单，人为虚构了两台服务器（一台windows，一台Linux）的信息，并发送给CMDB。单独执行该脚本，在admin的新资产待审批区可以看到添加了两条新资产信息。 要添加更多的资产，只需修改脚本中windows_data和linux_data的数据即可。但是要注意的是，如果不修改sn，那么会变成资产数据更新，而不是增加新资产，这一点一定要注意。 三、批准资产上线在/assets/asset_handler.py中添加下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\\n%s&quot; % msg event.user = request.user # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete() 核心就是增加了一个记录日志的log()函数以及审批资产的ApproveAsset类。 log()函数很简单，根据日志类型的不同，保存日志需要的各种信息，比如日志名称、关联的资产对象、日志详细内容和审批人员等等。所有的日志都被保存在数据库中，可以在admin中查看。 对于关键的ApproveAsset类，说明如下： 初始化方法接收reqeust和待审批资产的id； 分别提前获取资产对象和所有数据data； asset_upline()是入口方法，通过反射，获取一个类似_server_upline的方法。之所以这么做，是为后面的网络设别、安全设备、存储设备等更多类型资产的审批留下扩展接口。本教程里只实现了服务器类型资产的审批方法，更多的请自行完善，过程基本类似。 _server_upline()是服务器类型资产上线的核心方法： 它首先新建了一个Asset资产对象（注意要和待审批区的资产区分开）； 然后利用该对象，分别创建了对应的厂商、服务器、CPU、内存、硬盘和网卡，并删除待审批区的对应资产； 在实际的生产环境中，上面的操作应该是原子性的整体事务，任何一步出现异常，所有操作都要回滚； 如果任何一步出现错误，上面的操作全部撤销，也就是asset.delete()。记录错误日志，返回False； 如果没问题，那么记录正确日志，返回True。 对于_create_asset(self)方法，利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 对于_create_manufacturer(self, asset)方法，先判断厂商数据是否存在，再决定是获取还是创建。 对于_create_CPU(self, asset)等方法，教程这里对数据采取了最大限度的容忍，实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。这里的业务逻辑非常复杂，不可能面面俱到。后面的内存、硬盘和网卡也是一样的。 对于_delete_original_asset(self)方法，这里的逻辑是已经审批上线的资产，就从待审批区删除。也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示，不过这样可能导致待审批区越来越大。 四、测试资产上线功能运行 report_assets.py 脚本： 在admin的新资产待审批区选择刚才的3条资产，然后选择上线action并点击‘执行’按钮，稍等片刻，显示成功批准 3 条新资产上线！的绿色提示信息，同时新资产也从待审批区被删除了，如下图所示： 往后，如果我们再次发送这3个服务器资产的信息，那就不是在待审批区了，而是已上线资产了。","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB-新资产待审批区（六）","slug":"6、新资产待审批区","date":"2019-03-08T16:00:00.000Z","updated":"2019-03-09T02:52:40.594Z","comments":true,"path":"2019/03/09/6、新资产待审批区/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/09/6、新资产待审批区/","excerpt":"","text":"一、启用admin前面，我们已经完成了数据收集客户端的编写和测试，下面我们就可以在admin中展示和管理资产数据了。 首先，通过1python manage.py createsuperuser 创建一个管理员账户。 然后，进入/assets/admin.py文件，写入下面的代码：12345678910111213141516171819202122232425262728293031from django.contrib import admin# Register your models here.from assets import modelsclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,)class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &apos;m_time&apos;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 利用刚才创建的管理员用户，登录admin站点： 这里略微对admin界面做了些简单地配置，但目前还没有数据。 二、创建新资产前面我们只是在Pycharm中获取并打印数据，并没有将数据保存到数据库里。下面我们来实现这一功能。 修改/assets/views.py文件，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 pass return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) report视图的逻辑是这样的： sn是标识一个资产的唯一字段，必须携带，不能重复！ 从POST中获取发送过来的数据； 使用json转换数据类型； 进行各种数据检查（比如身份验证等等，请自行完善）； 判断数据是否为空，空则返回错误信息，结束视图； 判断data的类型是否字典类型，否则返回错误信息； 之所以要对data的类型进行判断是因为后面要大量的使用字典的get方法和中括号操作； 如果没有携带sn号，返回错误信息； 当前面都没问题时，进入下面的流程： 首先，利用sn值尝试在已上线的资产进行查找，如果有，则进入已上线资产的更新流程，具体实现，这里暂且跳过; 如果没有，说明这是个新资产，需要添加到新资产区； 这里又分两种情况，一种是彻底的新资产，那没得说，需要新增；另一种是新资产区已经有了，但是审批员还没来得及审批，资产数据的后续报告就已经到达了，那么需要更新数据。 创建一个asset_handler.NewAsset()对象，然后调用它的obj.add_to_new_assets_zone()方法，进行数据保存，并接收返回结果； asset_handler是下面我们要新建的资产处理模块，NewAsset是其中的一个类。 为了不让views.py文件过于庞大，通常会建立新的py文件，专门处理一些核心业务。 在assets下新建asset_handler.py文件，并写入下面的代码：12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;ram_size&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos; NewAsset类接收两个参数，request和data，分别封装了请求和资产数据，它的唯一方法：1obj.add_to_new_assets_zone() 首先构造了一个defaults字典，分别将资产数据包的各种数据打包进去，然后利用Django中特别好用的update_or_create()方法，进行数据保存！ update_or_create()方法的机制：如果数据库内没有该数据，那么新增，如果有，则更新，这就大大减少了我们的代码量，不用写两个方法。该方法的参数必须为一些用于查询的指定字段（这里是sn），以及需要新增或者更新的defaults字典。而其返回值，则是一个查询对象和是否新建对象布尔值的二元元组。 三、测试数据重启CMDB，在linux中给Client下的main.py客户端，添加一个report_data的运行参数，然后运行main.py，发送一个资产数据给CMDB服务器，结果如下： 再进入admin后台，查看新资产待审批区，可以看到资产已经成功进入待审批区： 这里我们显示了资产的汇报和更新日期，过几分钟后，重新汇报该资产数据，然后刷新admin中的页面，可以看到，待审批区的资产数据也一并被更新了。","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB-Linux下收集数据（五）","slug":"5、Linux下收集数据","date":"2019-03-03T16:00:00.000Z","updated":"2019-03-07T00:18:33.814Z","comments":true,"path":"2019/03/04/5、Linux下收集数据/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/04/5、Linux下收集数据/","excerpt":"","text":"Linux下收集数据就有很多命令和工具了，比Windows方便多了。 但是要在Python的进程中运行操作系统级别的命令，我们通常需要使用subprocess模块。这个模块的具体用法，请查看Python教程中相关部分的内容。 下面，我们在Client/plugins下创建一个linux包，再到包里创建一个sys_info.py文件，写入下面的代码： 前提需要现在被收集的虚机上面安装必须的组件：1yum install -y lsb 关于disk的获取与原著有差别，更容易理解：1234567891011##获取厂商：[root@python_master pythontest]# dmidecode -s system-manufacturerVMware, Inc.##获取型号：[root@python_master pythontest]# dmidecode -s system-product-nameVMware Virtual Platform##获取sn：[root@python_master pythontest]# dmidecode -s system-serial-numberVMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-3 13:16# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport subprocessdef collect(): filter_keys = [&apos;Manufacturer&apos;, &apos;Serial Number&apos;, &apos;Product Name&apos;, &apos;UUID&apos;, &apos;Wake-up Type&apos;] raw_data = &#123;&#125; for key in filter_keys: try: res = subprocess.Popen(&quot;sudo dmidecode -t system|grep &apos;%s&apos;&quot; % key, stdout=subprocess.PIPE, shell=True) result = res.stdout.read().decode() data_list = result.split(&apos;:&apos;) if len(data_list) &gt; 1: raw_data[key] = data_list[1].strip() else: raw_data[key] = -1 except Exception as e: print(e) raw_data[key] = -2 data = dict() data[&apos;asset_type&apos;] = &apos;server&apos; data[&apos;manufacturer&apos;] = raw_data[&apos;Manufacturer&apos;] data[&apos;sn&apos;] = raw_data[&apos;Serial Number&apos;] data[&apos;model&apos;] = raw_data[&apos;Product Name&apos;] data[&apos;uuid&apos;] = raw_data[&apos;UUID&apos;] data[&apos;wake_up_type&apos;] = raw_data[&apos;Wake-up Type&apos;] data.update(get_os_info()) data.update(get_cpu_info()) data.update(get_ram_info()) data.update(get_nic_info()) data.update(get_disk_info()) return datadef get_os_info(): &quot;&quot;&quot; 获取操作系统信息 :return: &quot;&quot;&quot; distributor = subprocess.Popen(&quot;lsb_release -a|grep &apos;Distributor ID&apos;&quot;, stdout=subprocess.PIPE, shell=True) distributor = distributor.stdout.read().decode().split(&quot;:&quot;) release = subprocess.Popen(&quot;lsb_release -a|grep &apos;Description&apos;&quot;, stdout=subprocess.PIPE, shell=True) release = release.stdout.read().decode().split(&quot;:&quot;) data_dic = &#123; &quot;os_distribution&quot;: distributor[1].strip() if len(distributor) &gt; 1 else &quot;&quot;, &quot;os_release&quot;: release[1].strip() if len(release) &gt; 1 else &quot;&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &#125; return data_dicdef get_cpu_info(): &quot;&quot;&quot; 获取cpu信息 :return: &quot;&quot;&quot; base_cmd = &apos;cat /proc/cpuinfo&apos; raw_data = &#123; &apos;cpu_model&apos;: &quot;%s |grep &apos;model name&apos; |head -1 &quot; % base_cmd, &apos;cpu_count&apos;: &quot;%s |grep &apos;processor&apos;|wc -l &quot; % base_cmd, &apos;cpu_core_count&apos;: &quot;%s |grep &apos;cpu cores&apos; |awk -F: &apos;&#123;SUM +=$2&#125; END &#123;print SUM&#125;&apos;&quot; % base_cmd, &#125; for key, cmd in raw_data.items(): try: cmd_res = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True) raw_data[key] = cmd_res.stdout.read().decode().strip() except ValueError as e: print(e) raw_data[key] = &quot;&quot; data = &#123; &quot;cpu_count&quot;: raw_data[&quot;cpu_count&quot;], &quot;cpu_core_count&quot;: raw_data[&quot;cpu_core_count&quot;] &#125; cpu_model = raw_data[&quot;cpu_model&quot;].split(&quot;:&quot;) if len(cpu_model) &gt; 1: data[&quot;cpu_model&quot;] = cpu_model[1].strip() else: data[&quot;cpu_model&quot;] = -1 return datadef get_ram_info(): &quot;&quot;&quot; 获取内存信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;sudo dmidecode -t memory&quot;, stdout=subprocess.PIPE, shell=True) raw_list = raw_data.stdout.read().decode().split(&quot;\\n&quot;) raw_ram_list = [] item_list = [] for line in raw_list: if line.startswith(&quot;Memory Device&quot;): raw_ram_list.append(item_list) item_list = [] else: item_list.append(line.strip()) ram_list = [] for item in raw_ram_list: item_ram_size = 0 ram_item_to_dic = &#123;&#125; for i in item: data = i.split(&quot;:&quot;) if len(data) == 2: key, v = data if key == &apos;Size&apos;: if v.strip() != &quot;No Module Installed&quot;: ram_item_to_dic[&apos;capacity&apos;] = v.split()[0].strip() item_ram_size = round(float(v.split()[0])) else: ram_item_to_dic[&apos;capacity&apos;] = 0 if key == &apos;Type&apos;: ram_item_to_dic[&apos;model&apos;] = v.strip() if key == &apos;Manufacturer&apos;: ram_item_to_dic[&apos;manufacturer&apos;] = v.strip() if key == &apos;Serial Number&apos;: ram_item_to_dic[&apos;sn&apos;] = v.strip() if key == &apos;Asset Tag&apos;: ram_item_to_dic[&apos;asset_tag&apos;] = v.strip() if key == &apos;Locator&apos;: ram_item_to_dic[&apos;slot&apos;] = v.strip() if item_ram_size == 0: pass else: ram_list.append(ram_item_to_dic) raw_total_size = subprocess.Popen(&quot;cat /proc/meminfo|grep MemTotal &quot;, stdout=subprocess.PIPE, shell=True) raw_total_size = raw_total_size.stdout.read().decode().split(&quot;:&quot;) ram_data = &#123;&apos;ram&apos;: ram_list&#125; if len(raw_total_size) == 2: total_gb_size = int(raw_total_size[1].split()[0]) / 1024**2 ram_data[&apos;ram_size&apos;] = total_gb_size return ram_datadef get_nic_info(): &quot;&quot;&quot; 获取网卡信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;ifconfig -a&quot;, stdout=subprocess.PIPE, shell=True) raw_data = raw_data.stdout.read().decode().split(&quot;\\n&quot;) nic_dic = dict() next_ip_line = False last_mac_addr = None for line in raw_data: if next_ip_line: next_ip_line = False nic_name = last_mac_addr.split()[0] mac_addr = last_mac_addr.split(&quot;HWaddr&quot;)[1].strip() raw_ip_addr = line.split(&quot;inet addr:&quot;) raw_bcast = line.split(&quot;Bcast:&quot;) raw_netmask = line.split(&quot;Mask:&quot;) if len(raw_ip_addr) &gt; 1: ip_addr = raw_ip_addr[1].split()[0] network = raw_bcast[1].split()[0] netmask = raw_netmask[1].split()[0] else: ip_addr = None network = None netmask = None if mac_addr not in nic_dic: nic_dic[mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 0, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; else: if &apos;%s_bonding_addr&apos; % (mac_addr,) not in nic_dic: random_mac_addr = &apos;%s_bonding_addr&apos; % (mac_addr,) else: random_mac_addr = &apos;%s_bonding_addr2&apos; % (mac_addr,) nic_dic[random_mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: random_mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 1, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; if &quot;HWaddr&quot; in line: next_ip_line = True last_mac_addr = line nic_list = [] for k, v in nic_dic.items(): nic_list.append(v) return &#123;&apos;nic&apos;: nic_list&#125;def get_disk_info(): &quot;&quot;&quot; 获取存储信息。 本脚本只针对centos7.6中使用sda2，且只有一块虚拟硬盘的情况。 具体查看硬盘信息的命令，请根据实际情况，实际调整。 如果需要查看Raid信息，可以尝试MegaCli工具。 :return: &quot;&quot;&quot; sn_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-serial-number&quot;, stdout=subprocess.PIPE, shell=True) sn = sn_raw_data.stdout.read().decode() model_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-product-name&quot;, stdout=subprocess.PIPE, shell=True) model = model_raw_data.stdout.read().decode() #size_data = subprocess.Popen(&quot;sudo fdisk -l /dev/sda2 | grep Disk|head -1&quot;, stdout=subprocess.PIPE, shell=True) #size_data = size_data.stdout.read().decode() #size = size_data.split(&quot;:&quot;)[1].strip().split(&quot; &quot;)[0] size_raw_data = subprocess.Popen(&quot;sudo smartctl -a /dev/sda2 |grep Capacity&quot;, stdout=subprocess.PIPE, shell=True) raw_data = size_raw_data.stdout.read().decode() data_list = raw_data.split()[4] size = data_list.split(&apos;[&apos;)[1] result = &#123;&apos;physical_disk_driver&apos;: []&#125; disk_dict = dict() disk_dict[&quot;model&quot;] = model disk_dict[&quot;size&quot;] = size disk_dict[&quot;sn&quot;] = sn result[&apos;physical_disk_driver&apos;].append(disk_dict) return resultif __name__ == &quot;__main__&quot;: # 收集信息功能测试 d = collect() print(d) 先来个输出在 centos7.6虚机上面的测试（可以读出所有数据）：1&#123;&apos;asset_type&apos;: &apos;server&apos;, &apos;manufacturer&apos;: &apos;VMware, Inc.&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60&apos;, &apos;model&apos;: &apos;VMware Virtual Platform&apos;, &apos;uuid&apos;: &apos;68254d56-ee5c-fbdc-a15e-776a5fe76660&apos;, &apos;wake_up_type&apos;: &apos;Power Switch&apos;, &apos;os_distribution&apos;: &apos;CentOS&apos;, &apos;os_release&apos;: &apos;CentOS Linux release 7.6.1810 (Core)&apos;, &apos;os_type&apos;: &apos;Linux&apos;, &apos;cpu_count&apos;: &apos;1&apos;, &apos;cpu_core_count&apos;: &apos;1&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;ram&apos;: [&#123;&apos;capacity&apos;: &apos;1024&apos;, &apos;slot&apos;: &apos;RAM slot #0&apos;, &apos;model&apos;: &apos;DRAM&apos;, &apos;manufacturer&apos;: &apos;Not Specified&apos;, &apos;sn&apos;: &apos;Not Specified&apos;, &apos;asset_tag&apos;: &apos;Not Specified&apos;&#125;], &apos;ram_size&apos;: 0.9497604370117188, &apos;nic&apos;: [], &apos;physical_disk_driver&apos;: [&#123;&apos;model&apos;: &apos;VMware Virtual Platform\\n&apos;, &apos;size&apos;: &apos;32.2&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60\\n&apos;&#125;]&#125; 代码整体没有什么难点，无非就是使用subprocess.Popen()方法执行Linux的命令，然后获取返回值，并以规定的格式打包到data字典里。 需要说明的问题有： 当Linux中存在好几个Python解释器版本时，要注意调用方式，前面已经强调过了； 不同的Linux发行版，有些命令可能没有，需要额外安装； 所使用的查看硬件信息的命令并不一定必须和这里的一样，只要能获得数据就行； 有一些命令在ubuntu中涉及sudo的问题，需要特别对待； 最终数据字典的格式一定要正确。 可以在Linux下配置cronb或其它定时服务，设置定期的数据收集、报告任务。下面，我们在Linux虚拟机上，测试一下客户端。 将Pycharm中的Client客户端文件夹，拷贝到Linux虚拟机中，我这里是centos7.6 进入bin目录，运行：1python3 main.py report_data 一切顺利的话应该能得到如下的反馈：12345正在将数据发送至： [http://10.101.120.34:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22asset_type%22%3A+%22server%22%2C+%22manufacturer%22%3A+%22VMware%2C+Inc.%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%22%2C+%22model%22%3A+%22VMware+Virtual+Platform%22%2C+%22uuid%22%3A+%2268254d56-ee5c-fbdc-a15e-776a5fe76660%22%2C+%22wake_up_type%22%3A+%22Power+Switch%22%2C+%22os_distribution%22%3A+%22CentOS%22%2C+%22os_release%22%3A+%22CentOS+Linux+release+7.6.1810+%28Core%29%22%2C+%22os_type%22%3A+%22Linux%22%2C+%22cpu_count%22%3A+%221%22%2C+%22cpu_core_count%22%3A+%221%22%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22ram%22%3A+%5B%7B%22capacity%22%3A+%221024%22%2C+%22slot%22%3A+%22RAM+slot+%230%22%2C+%22model%22%3A+%22DRAM%22%2C+%22manufacturer%22%3A+%22Not+Specified%22%2C+%22sn%22%3A+%22Not+Specified%22%2C+%22asset_tag%22%3A+%22Not+Specified%22%7D%5D%2C+%22ram_size%22%3A+0.9497604370117188%2C+%22nic%22%3A+%5B%5D%2C+%22physical_disk_driver%22%3A+%5B%7B%22model%22%3A+%22VMware+Virtual+Platform%5Cn%22%2C+%22size%22%3A+%2232.2%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%5Cn%22%7D%5D%7D&apos;发送完毕！返回结果：成功收到数据！日志记录成功！ 然后我们可以在pycharm 页面收到： 规整如下：","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB-Windows下收集数据（四）","slug":"4、Windows下收集数据","date":"2019-03-02T16:00:00.000Z","updated":"2019-03-03T03:27:04.301Z","comments":true,"path":"2019/03/03/4、Windows下收集数据/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/03/4、Windows下收集数据/","excerpt":"","text":"一、windows中收集硬件信息为了收集运行Windows操作系统的服务器的硬件信息，我们需要编写一个专门的脚本。 在Pycharm的Client目录下的plugins包中，新建一个windows包，然后创建一个sys_info.py文件，写入下面的代码： ==&lt;如下打印data的语句，是为了调试查看输出，可以去掉&gt;==123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 16:41# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport platformimport win32comimport wmi&quot;&quot;&quot;本模块基于windows操作系统，依赖wmi和win32com库，需要提前使用pip进行安装，或者下载安装包手动安装。&quot;&quot;&quot;def collect(): data = &#123; &apos;os_type&apos;: platform.system(), &apos;os_release&apos;: &quot;%s %s %s &quot; % (platform.release(), platform.architecture()[0], platform.version()), &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos; &#125; # 分别获取各种硬件信息 win32obj = Win32Info() data.update(win32obj.get_cpu_info()) data.update(win32obj.get_ram_info()) data.update(win32obj.get_motherboard_info()) data.update(win32obj.get_disk_info()) data.update(win32obj.get_nic_info()) # 最后返回一个数据字典 print(&quot;data11&quot;, data) return dataclass Win32Info(object): def __init__(self): # 固定用法，更多内容请参考模块说明 self.wmi_obj = wmi.WMI() self.wmi_service_obj = win32com.client.Dispatch(&quot;WbemScripting.SWbemLocator&quot;) self.wmi_service_connector = self.wmi_service_obj.ConnectServer(&quot;.&quot;, &quot;root\\cimv2&quot;) def get_cpu_info(self): &quot;&quot;&quot; 获取CPU的相关数据，这里只采集了三个数据，实际有更多，请自行选择需要的数据 :return: &quot;&quot;&quot; data = &#123;&#125; cpu_lists = self.wmi_obj.Win32_Processor() cpu_core_count = 0 for cpu in cpu_lists: cpu_core_count += cpu.NumberOfCores cpu_model = cpu_lists[0].Name # CPU型号（所有的CPU型号都是一样的） data[&quot;cpu_count&quot;] = len(cpu_lists) # CPU个数 data[&quot;cpu_model&quot;] = cpu_model data[&quot;cpu_core_count&quot;] = cpu_core_count # CPU总的核数 print(&quot;data22&quot;, data) return data def get_ram_info(self): &quot;&quot;&quot; 收集内存信息 :return: &quot;&quot;&quot; data = [] # 这个模块用SQL语言获取数据 ram_collections = self.wmi_service_connector.ExecQuery(&quot;Select * from Win32_PhysicalMemory&quot;) for item in ram_collections: # 主机中存在很多根内存，要循环所有的内存数据 ram_size = int(int(item.Capacity) / (1024**3)) # 转换内存单位为GB item_data = &#123; &quot;slot&quot;: item.DeviceLocator.strip(), &quot;capacity&quot;: ram_size, &quot;model&quot;: item.Caption, &quot;manufacturer&quot;: item.Manufacturer, &quot;sn&quot;: item. SerialNumber, &#125; data.append(item_data) # 将每条内存的信息，添加到一个列表里 print(&quot;data33&quot;, data) return &#123;&quot;ram&quot;: data&#125; # 再对data列表封装一层，返回一个字典，方便上级方法的调用 def get_motherboard_info(self): &quot;&quot;&quot; 获取主板信息 :return: &quot;&quot;&quot; computer_info = self.wmi_obj.Win32_ComputerSystem()[0] system_info = self.wmi_obj.Win32_OperatingSystem()[0] data = dict() data[&apos;manufacturer&apos;] = computer_info.Manufacturer data[&apos;model&apos;] = computer_info.Model data[&apos;wake_up_type&apos;] = computer_info.WakeUpType data[&apos;sn&apos;] = system_info.SerialNumber print(&quot;data44&quot;, data) return data def get_disk_info(self): &quot;&quot;&quot; 硬盘信息 :return: &quot;&quot;&quot; data = [] for disk in self.wmi_obj.Win32_DiskDrive(): # 每块硬盘都要获取相应信息 item_data = dict() iface_choices = [&quot;SAS&quot;, &quot;SCSI&quot;, &quot;SATA&quot;, &quot;SSD&quot;] for iface in iface_choices: if iface in disk.Model: item_data[&apos;iface_type&apos;] = iface break else: item_data[&apos;iface_type&apos;] = &apos;unknown&apos; item_data[&apos;slot&apos;] = disk.Index item_data[&apos;sn&apos;] = disk.SerialNumber item_data[&apos;model&apos;] = disk.Model item_data[&apos;manufacturer&apos;] = disk.Manufacturer item_data[&apos;capacity&apos;] = int(int(disk.Size) / (1024**3)) data.append(item_data) print(&quot;data55&quot;, data) return &#123;&apos;physical_disk_driver&apos;: data&#125; def get_nic_info(self): &quot;&quot;&quot; 网卡信息 :return: &quot;&quot;&quot; data = [] for nic in self.wmi_obj.Win32_NetworkAdapterConfiguration(): if nic.MACAddress is not None: item_data = dict() item_data[&apos;mac&apos;] = nic.MACAddress item_data[&apos;model&apos;] = nic.Caption item_data[&apos;name&apos;] = nic.Index if nic.IPAddress is not None: item_data[&apos;ip_address&apos;] = nic.IPAddress[0] item_data[&apos;net_mask&apos;] = nic.IPSubnet else: item_data[&apos;ip_address&apos;] = &apos;&apos; item_data[&apos;net_mask&apos;] = &apos;&apos; data.append(item_data) print(&quot;data66&quot;, data) return &#123;&apos;nic&apos;: data&#125;if __name__ == &quot;__main__&quot;: # 测试代码(仅限于在当前页面运行获取本机的信息） dic = collect() print(dic) windows中没有方便的命令可以获取硬件信息，但是有额外的模块可以帮助我们实现目的，这个模块叫做wmi。可以使用1pip install wmi 的方式安装，当前版本是1.4.9。但是wmi安装后，import wmi依然会出错，因为它依赖一个叫做win32com的模块。 我们依然可以通过1pip install pypiwin32 来安装win32com模块，但是不幸的是，据反映，有些机器无法通过pip成功安装。所以，这里我在github中提供了一个手动安装包==pywin32-220.win-amd64-py3.5(配合wmi模块，获取主机信息的模块).exe==，方便大家。12链接：https://pan.baidu.com/s/14ZUKPJnmlwuUHsYLUUApKg 提取码：nq5i 依赖包的问题解决后，我们来看一下==sys_info.py==脚本的代码。 核心在于collect()方法！ 该方法首先通过platform模块获取平台的信息，然后保存到一个data字典中。 然后创建一个Win32Info对象，并调用win32的各种功能方法，分别获取CPU、RAM、主板、硬盘和网卡的信息。 类Win32Info是我们编写的封装了具体数据收集逻辑的类； 该类中有很多方法，每个方法针对一项数据； 其中对Win32模块的调用方式是固定的，有兴趣的可以自行学习这个模块的官方文档 每一类的数据收集完成后都会作为一个新的子字典，update到开始的data字典中，最终形成完整的信息字典。 最后在脚本末尾有一个测试入口。 整个脚本的代码其实很简单，我们只要将Win32的方法调用当作透明的空气，剩下的不过就是将获得的数据，按照我们指定的格式打包成一个数据字典。 ==强调：数据字典的格式和键值是非常重要的，是预设的，不可以随意改变！== 二、信息收集测试单独运行一下该脚本（注意不是运行CMDB项目），查看一下生成的数据：1234567data22 &#123;&apos;cpu_core_count&apos;: 4, &apos;cpu_count&apos;: 1, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;&#125;data33 [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;]data44 &#123;&apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;wake_up_type&apos;: 6&#125;data55 [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;]data66 [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;]data11 &#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125;&#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125; 上面的信息包含操作系统、主板、CPU、内存、硬盘、网卡等各种信息。可以看到我有1条内存，1块SSD硬盘，以及4块网卡。四块网卡有出现mac地址相同的情况，因为那是虚拟机的。 你自己的数据和我的肯定不一样，但是数据格式和键值必须一样，我们后面自动分析数据、填充数据，都依靠这个固定格式的数据字典。 通过测试我们发现数据可以收集到了，那么再测试一下数据能否正常发送到服务器。 三、数据发送测试由于我这里采用了Linux虚拟机作为测试用例，我们的Django服务器就不能再运行在127.0.0.1:8000上面了。 查看一下当前机器的IP，发现是192.168.1.100，修改项目的settings.py文件，将ALLOWED_HOSTS修改如下： 1ALLOWED_HOSTS = [&quot;*&quot;] 这表示接收所有同一局域网内的网络访问。 然后以0.0.0.0:8000的参数启动CMDB，表示对局域网内所有ip开放服务。 回到客户端，进入Client/bin目录，运行1python main.py report_data 可以看到如下结果：1234567正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22cpu_core_count%22%3A+4%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22wake_up...&lt;中间信息省略&gt;...71b+M.2+2280+256GB%22%2C+%22capacity%22%3A+238%7D%5D%2C+%22cpu_count%22%3A+1%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22asset_type%22%3A+%22server%22%7D&apos;?[31;1m发送失败，HTTP Error 404: Not Found?[0m日志记录成功！ 这是一个404错误，表示服务器地址没找到，这是因为我们还没有为Django编写接收数据的视图和路由。 这时，打开log目录下的日志文件，内容如下：1发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 四、接收数据进入==cmdb/urls.py==文件中，编写一个二级路由，将所有++assets相关的数据都转发到assets.urls++中，如下所示：1234567from django.contrib import adminfrom django.conf.urls import url, includeurlpatterns = [ url(&apos;admin/&apos;, admin.site.urls), url(r&apos;^assets/&apos;, include(&apos;assets.urls&apos;)),] 然后，我们在assets中新建一个urls.py文件，写入下面的代码：12345678from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;),] 这样，我们的路由就写好了。 转过头，我们进入++assets/views.py++文件，写一个简单的视图。1234567891011from django.shortcuts import render# Create your views here.from django.shortcuts import render, HttpResponsedef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 代码很简单，接收POST过来的数据，打印出来，然后返回成功的消息。 重新运1行python main.py report_data 可以看到： 1234567正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22ram%22%3A+%5B%7B%22manufacturer%22%3A+%22802C0...&lt;中间部分省略&gt;...%22%3A+%22Latitude+5480%22%2C+%22sn%22%3A+%2200330-80000-00000-AA069%22%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22manufacturer%22%3A+%22Dell+Inc.%22%7D&apos;?[31;1m发送失败，&lt;urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。&gt;?[0m日志记录成功！ 遇到拒绝服务的错误了。 原因在于我们模拟浏览器发送了一个POST请求给Django，但是请求中没有携带Django需要的csrf安全令牌，所以拒绝了请求。 为了解决这个问题，我们需要在这个report视图上忽略csrf验证，可以通过Django的@csrf_exempt装饰器。修改代码如下：1234567891011from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exempt# Create your views here.@csrf_exemptdef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 重启CMDB服务器，再次从客户端报告数据，可以看到返回结果如下：12345678正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22nic%22%3A+%5B%7B%22name%22%3A+1%2C+%22model%22%3A+%22%5B00000001%5D+VMware+Virt...&lt;中间部分省略&gt;...facturer%22%3A+%22802C0000802C%22%2C+%22slot%22%3A+%22DIMM+A%22%2C+%22sn%22%3A+%221B458788%22%2C+%22model%22%3A+%22%5Cu7269%5Cu7406%5Cu5185%5Cu5b58%22%7D%5D%7D&apos;?[31;1m发送完毕！?[0m返回结果：成功收到数据！日志记录成功！ 看下日志记录：1234发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:00:53 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:05:36 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:06:51 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：成功收到数据！ 这表明数据发送成功了。 再看Pycharm中，也打印出了接收到的数据，一切OK！ CSRF验证的问题解决了，但是又带来新的安全问题。我们可以通过增加用户名、密码，或者md5验证或者自定义安全令牌的方式解决，这部分内容就不展开了。 Windows下的客户端已经验证完毕了，然后我们就可以通过各种方式让脚本定时运行、收集和报告数据，一切都自动化。 最后补充：++CMDB系统是部署在A服务器上，那么客户端CLIENT是需要部署在B服务器上的，那就意味着每台需要被采集数据的服务器都要安装PYTHON及所用到的包。++","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB数据收集客户端（三）","slug":"3、数据收集客户端","date":"2019-03-01T16:00:00.000Z","updated":"2019-03-03T03:26:48.573Z","comments":true,"path":"2019/03/02/3、数据收集客户端/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/02/3、数据收集客户端/","excerpt":"","text":"CMDB最主要的管理对象：服务器，其数据信息自然不可能通过手工收集，必须以客户端的方式，定时自动收集并报告给远程的服务器。 下面，让我们暂时忘掉Django，进入Python运维的世界…… 一、客户端程序组织编写客户端，不能一个py脚本包打天下，要有组织有目的，通常我们会采取下面的结构： 在Pycharm下，创建一个Client文件夹，作为客户端的根目录。 在Client下，创建上面的包。注意是包，不是文件夹： bin是客户端启动脚本的所在目录 conf是配置文件目录 core是核心代码目录 log是日志文件保存目录 plugins是插件或工具目录 二、开发数据收集客户端1.程序入口脚本++在bin目录中新建main.py文件++，写入下面的代码： 123456789101112131415#!/usr/bin/env python# -*- coding:utf-8 -*-import osimport sysBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from core import handlerif __name__ == &apos;__main__&apos;: handler.ArgvHandler(sys.argv) ##获取参数，传入到ArgvHandler() 通过os和sys模块的配合，将当前客户端所在目录设置为工作目录，如果不这么做，会无法导入其它模块； handler模块是核心代码模块，在core目录中，我们一会来实现它。 以后调用客户端就只需要执行python main.py 参数就可以了 ++这里有个问题一定要强调一下，那就是Python解释器的调用，执行命令的方式和代码第一行#!/usr/bin/env python的指定方式一定不能冲突，要根据你的实际情况实际操作和修改代码，很多新手连Python本身都没搞明白就上来执行脚本，碰到各种解释器不合法的错误，请回去补足基础！++ 2.主功能模块++在core下，创建handler.py文件++，写入下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 13:51# @Author : zhdya@zhdya.cn# @File : handler.pyimport jsonimport timeimport urllib.parseimport urllib.requestfrom core import info_collectionfrom conf import settingsclass ArgvHandler(object): def __init__(self, args): self.args = args self.parse_args() def parse_args(self): &quot;&quot;&quot; 分析参数，如果有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 :return: &quot;&quot;&quot; if len(self.args) &gt; 1 and hasattr(self, self.args[1]): func = getattr(self, self.args[1]) func() else: self.help_msg() ##如果执行程序没有带参数就会提示如下信息：help_msg() @staticmethod #静态方法 类或实例均可调用,静态方法函数里不传入self，这样如上self.help_msg()就可以调用了 def help_msg(): &quot;&quot;&quot; 帮助说明 :return: &quot;&quot;&quot; msg = &apos;&apos;&apos; collect_data 收集硬件信息 report_data 收集硬件信息并汇报 &apos;&apos;&apos; print(msg) @staticmethod def collect_data(): &quot;&quot;&quot;收集硬件信息,用于测试！&quot;&quot;&quot; info = info_collection.InfoCollection() asset_data = info.collect() print(asset_data) @staticmethod def report_data(): &quot;&quot;&quot; 收集硬件信息，然后发送到服务器。 :return: &quot;&quot;&quot; # 收集信息 info = info_collection.InfoCollection() asset_data = info.collect() # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(asset_data)&#125; print(&quot;handler_data--&gt;&gt;&quot;, data) # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&quot;handler_url--&gt;&gt;&quot;, url) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() print(&quot;handler_data_encode--&gt;&gt;&quot;, data_encode) response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\\033[31;1m发送完毕！\\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\\033[31;1m发送失败，%s\\033[0m&quot; % e) # 记录发送日志 with open(settings.PATH, &apos;ab&apos;) as f: ##a追加,b二进制文件 string = &apos;发送时间：%s \\t 服务器地址：%s \\t 返回结果：%s \\n&apos; % (time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;), url, message) f.write(string.encode()) print(&quot;日志记录成功！&quot;) 说明： handler模块中只有一个ArgvHandler类； 在main模块中也是实例化了一个ArgvHandler类的对象，并将调用参数传递进去； 首先，初始化方法会保存调用参数，然后执行parse_args()方法分析参数； 如果ArgvHandler类有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 目前ArgvHandler类只有两个核心方法：collect_data和report_dataa； 这两个方法一个是收集数据并打印到屏幕，用于测试；report_data方法才会将实际的数据发往服务器。 数据的收集由info_collection.InfoCollection类负责，一会再看； report_data方法会将收集到的数据打包到一个字典内，并转换为json格式； 然后通过settings中的配置，构造发送目的地url； 通过Python内置的urllib.parse对数据进行封装； 通过urllib.request将数据发送到目的url； 接收服务器返回的信息； 将成功或者失败的信息写入日志文件中。 以后，我们要测试数据收集，执行：1python main.py collect_data 要实际往服务器发送收集到的数据，则执行：1python main.py report_data 3.配置文件要将所有可能修改的数据、常量、配置等都尽量以配置文件的形式组织起来，尽量不要在代码中写死任何数据。 ++在conf中，新建settings.py文件++，写入下面的代码：12345678910111213141516171819202122#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:21# @Author : zhdya@zhdya.cn# @File : settings.pyimport os# 远端服务器配置Params = &#123; &quot;server&quot;: &quot;10.10.7.26&quot;, &quot;port&quot;: 8000, &apos;url&apos;: &apos;/assets/report/&apos;, &apos;request_timeout&apos;: 30,&#125;# 日志文件配置PATH = os.path.join(os.path.dirname(os.getcwd()), &apos;log&apos;, &apos;cmdb.log&apos;)print(&quot;conf_settings--&gt;&gt;&quot;, PATH)# 更多配置，请都集中在此文件中 这里，配置了服务器地址、端口、发送的url、请求的超时时间，以及日志文件路径。请根据你的实际情况进行修改。 ==如上server端我是直接启动的django服务 也就是如上10.10.7.26是我笔记本的IP地址，这样默认我笔记本的10.10.7.26:8000就对外开放了！== 4.信息收集模块++在core中新建info_collection.py文件++，写入下面的代码： ==&lt;关于如下：from plugins.linux import sys_info 以及 from plugins.windows import sys_info as win_sys_info 稍后章节我们会建立一系列的目录！！&gt;==1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:48# @Author : zhdya@zhdya.cn# @File : info_collection.pyimport sysimport platformdef linux_sys_info(): from plugins.linux import sys_info return sys_info.collect()def windows_sys_info(): from plugins.windows import sys_info as win_sys_info return win_sys_info.collect()class InfoCollection(object): def collect(self): # 收集平台信息 # 首先判断当前平台，根据平台的不同，执行不同的方法 try: func = getattr(self, platform.system()) info_data = func() formatted_data = self.build_report_data(info_data) return formatted_data except AttributeError: sys.exit(&quot;不支持当前操作系统： [%s]! &quot; % platform.system()) def Linux(self): return linux_sys_info() def Windows(self): return windows_sys_info() def build_report_data(self, data): # 留下一个接口，方便以后增加功能或者过滤数据 return data 该模块的作用很简单： 首先通过Python内置的platform模块获取执行main脚本的操作系统类别，通常是windows和Linux； 根据操作系统的不同，反射获取相应的信息收集方法，并执行 如果是客户端不支持的操作系统，比如苹果系统，则提示并退出客户端。 因为windows和Linux两大操作系统的巨大平台差异，我们必须写两个收集信息的脚本。 到目前未知，我们的客户端结构如下图所示：","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB模型设计（二）","slug":"2、模型设计","date":"2019-02-28T16:00:00.000Z","updated":"2019-03-03T03:26:36.825Z","comments":true,"path":"2019/03/01/2、模型设计/","link":"","permalink":"http://zhdya.okay686.cn/2019/03/01/2、模型设计/","excerpt":"","text":"一、创建项目创建Django项目cmdb，配置好settings中的语言和时区，最后新建一个app，名字就叫做assets。这些基本过程以后就不再赘述了，不熟悉的请参考教程的前面部分。 1django版本：1.11.11 创建成功后，初始状态如下图所示： 二、模型设计说明：本项目依然采用SQLite数据库，等下一个项目再使用Mysql。 模型设计是整个项目的重中之重，其它所有的内容其实都是围绕它展开的。 而我们设计数据模型的原则和参考依据是前一节分析的项目需求和数据分类表。 1.资产共有数据模型打开assets/models.py文件，首先我们要设计一张资产表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from django.db import modelsfrom django.contrib.auth.models import User# Create your models here.class Asset(models.Model): &quot;&quot;&quot; 所有资产的共有数据表 &quot;&quot;&quot; asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;software&apos;, &apos;软件资产&apos;) ) asset_status = ( (0, &apos;在线&apos;), (1, &apos;下线&apos;), (2, &apos;未知&apos;), (3, &apos;故障&apos;), (4, &apos;备用&apos;), ) asset_type = models.CharField(choices=asset_type_choice, max_length=64, default=&apos;server&apos;, verbose_name=&quot;资产类型&quot;) name = models.CharField(max_length=64, unique=True, verbose_name=&quot;资产名称&quot;) ##唯一，不可重复 sn = models.CharField(max_length=128, unique=True, verbose_name=&quot;资产序列号&quot;) ##唯一，不可重复 business_unit = models.ForeignKey(&apos;BusinessUnit&apos;, null=True, blank=True, verbose_name=&quot;所属业务线&quot;) status = models.SmallIntegerField(choices=asset_status, default=0, verbose_name=&quot;设备状态&quot;) manufacturer = models.ForeignKey(&apos;Manufacturer&apos;, null=True, blank=True, verbose_name=&quot;制造商&quot;) manage_ip = models.GenericIPAddressField(null=True, blank=True, verbose_name=&quot;管理IP&quot;) tags = models.ManyToManyField(&apos;Tag&apos;, blank=True, verbose_name=&quot;标签&quot;) admin = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;资产管理员&quot;, related_name=&apos;admin&apos;) idc = models.ForeignKey(&apos;IDC&apos;, null=True, blank=True, verbose_name=&quot;所在机房&quot;) contract = models.ForeignKey(&apos;Contract&apos;, null=True, blank=True, verbose_name=&quot;合同&quot;) purchase_day = models.DateField(null=True, blank=True, verbose_name=&quot;购买日期&quot;) expire_day = models.DateField(null=True, blank=True, verbose_name=&quot;过保日期&quot;) price = models.FloatField(null=True, blank=True, verbose_name=&quot;价格&quot;) approved_by = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;批准人&quot;, related_name=&quot;approved_by&quot;) memo = models.TextField(null=True, blank=True, verbose_name=&quot;备注&quot;) c_time = models.DateTimeField(auto_now_add=True, verbose_name=&quot;批准日期&quot;) ##auto_add_now默认=False：储存当对象被创建时的时间，可以用来存储比如说博客什么时候创建的，后来你再更改博客，它的值也不会变。 m_time = models.DateTimeField(auto_now=True, verbose_name=&quot;更新日期&quot;) ##auto_now默认=False:当对象被存储时自动将对象的时间更新为当前时间 def __str__(self): return &apos;&lt;%s&gt; %s&apos; %(self.get_asset_type_display(), self.name) class Meta: verbose_name = &quot;资产总表&quot; verbose_name_plural = &quot;资产总表&quot; ordering = [&apos;-c_time&apos;] 说明： sn这个数据字段是所有资产都必须有，并且唯一不可重复的！通常来自自动收集的数据中； name和sn一样，也是唯一的； asset_type_choice和asset_status分别设计为两个选择类型 admin和approved_by是分别是当前资产的管理员和将该资产上线的审批员； 导入Django内置的User表，作为我们CMDB项目的用户表，用于保存管理员和审判员等人员信息； asset表中的很多字段内容都无法自动获取，需要我们手动输入，比如合同、备注。 2.服务器模型服务器作为资产的一种，而且是最主要的管理对象，包含了一些主要的信息，其模型结构如下：1234567891011121314151617181920212223242526272829class Server(models.Model): &quot;&quot;&quot;服务器设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;PC服务器&apos;), (1, &apos;刀片型&apos;), (2, &apos;小型机&apos;), ) created_by_choice = ( (&apos;auto&apos;, &apos;自动添加&apos;), (&apos;manual&apos;, &apos;手动录入&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) # 非常关键的一对一关联！ sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=1, verbose_name=&quot;服务器类型&quot;) created_by = models.CharField(choices=created_by_choice, max_length=32, default=&apos;auto&apos;, verbose_name=&quot;添加方式&quot;) hosted_on = models.ForeignKey(&apos;self&apos;, related_name=&apos;hosted_on_server&apos;, blank=True, null=True, verbose_name=&quot;宿主机&quot;) ##虚拟机专用字段 model = models.CharField(max_length=512, blank=True, null=True, verbose_name=&quot;Raid类型&quot;) os_type = models.CharField(&apos;操作系统类型&apos;, max_length=64, blank=True, null=True) os_distribution = models.CharField(&apos;发行版本&apos;, max_length=64, blank=True, null=True) os_release = models.CharField(&apos;操作系统版本&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; %(self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;服务器&apos; verbose_name_plural = &quot;服务器&quot; 说明： 每台服务器都唯一关联着一个资产对象，因此使用OneToOneField构建了一个一对一字段，这非常重要! 服务器又可分为几种子类型，这里定义了三种； 服务器添加的方式可以分为手动和自动； 有些服务器是虚拟机或者docker生成的，没有物理实体，存在于宿主机中，因此需要增加一个hosted_on字段； 服务器有型号信息，如果硬件信息中不包含，那么指的就是主板型号； Raid类型在采用了Raid的时候才有，否则为空; 操作系统相关信息包含类型、发行版本和具体版本。 3.安全、网络、存储设备和软件资产的模型这部分内容不是项目的主要内容，而且数据大多数不能自动收集和报告，很多都需要手工录入。我这里给出了范例，更多的数据字段，可以自行添加。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class SecurityDevice(models.Model): &quot;&quot;&quot;安全设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;防火墙&apos;), (1, &apos;入侵检测设备&apos;), (2, &apos;互联网网关&apos;), (4, &apos;运维审计系统&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;安全设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;安全设备&apos; verbose_name_plural = &quot;安全设备&quot;class StorageDevice(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;磁盘阵列&apos;), (1, &apos;网络存储器&apos;), (2, &apos;磁带库&apos;), (4, &apos;磁带机&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;存储设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;存储设备&apos; verbose_name_plural = &quot;存储设备&quot;class NetworkDevice(models.Model): &quot;&quot;&quot;网络设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;路由器&apos;), (1, &apos;交换机&apos;), (2, &apos;负载均衡&apos;), (4, &apos;VPN设备&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;网络设备类型&quot;) vlan_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;VLanIP&quot;) intranet_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;内网IP&quot;) model = models.CharField(max_length=128, null=True, blank=True, verbose_name=&quot;网络设备型号&quot;) firmware = models.CharField(max_length=128, blank=True, null=True, verbose_name=&quot;设备固件版本&quot;) port_num = models.SmallIntegerField(null=True, blank=True, verbose_name=&quot;端口个数&quot;) device_detail = models.TextField(null=True, blank=True, verbose_name=&quot;详细配置&quot;) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; % (self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;网络设备&apos; verbose_name_plural = &quot;网络设备&quot;class Software(models.Model): &quot;&quot;&quot; 只保存付费购买的软件 &quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;操作系统&apos;), (1, &apos;办公\\开发软件&apos;), (2, &apos;业务软件&apos;), ) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;软件类型&quot;) license_num = models.IntegerField(default=1, verbose_name=&quot;授权数量&quot;) version = models.CharField(max_length=64, unique=True, help_text=&apos;例如: CentOS release 6.7 (Final)&apos;, verbose_name=&apos;软件/系统版本&apos;) def __str__(self): return &apos;%s--%s&apos; % (self.get_sub_asset_type_display(), self.version) class Meta: verbose_name = &apos;软件/系统&apos; verbose_name_plural = &quot;软件/系统&quot; 说明： 每台安全、网络、存储设备都通过一对一的方式唯一关联这一个资产对象。 通过sub_asset_type又细分设备的子类型 对于软件，它没有物理形体，因此无须关联一个资产对象； 软件只管理那些大型的收费软件，关注点是授权数量和软件版本。对于那些开源的或者免费的软件，显然不算公司的资产。 4.机房、制造商、业务线、合同、资产标签等数据模型这一部分是CMDB中相关的内容，数据表建立后，可以通过手动添加。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class IDC(models.Model): &quot;&quot;&quot;机房&quot;&quot;&quot; name = models.CharField(max_length=64, unique=True, verbose_name=&quot;机房名称&quot;) memo = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;备注&apos;) def __str__(self): return self.name class Meta: verbose_name = &apos;机房&apos; verbose_name_plural = &quot;机房&quot;class Manufacturer(models.Model): &quot;&quot;&quot;厂商&quot;&quot;&quot; name = models.CharField(&apos;厂商名称&apos;, max_length=64, unique=True) telephone = models.CharField(&apos;支持电话&apos;, max_length=30, blank=True, null=True) memo = models.CharField(&apos;备注&apos;, max_length=128, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;厂商&apos; verbose_name_plural = &quot;厂商&quot;class BusinessUnit(models.Model): &quot;&quot;&quot;业务线&quot;&quot;&quot; parent_unit = models.ForeignKey(&apos;self&apos;, blank=True, null=True, related_name=&apos;parent_level&apos;) name = models.CharField(&apos;业务线&apos;, max_length=64, unique=True) memo = models.CharField(&apos;备注&apos;, max_length=64, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;业务线&apos; verbose_name_plural = &quot;业务线&quot;class Contract(models.Model): &quot;&quot;&quot;合同&quot;&quot;&quot; sn = models.CharField(&apos;合同号&apos;, max_length=128, unique=True) name = models.CharField(&apos;合同名称&apos;, max_length=64) memo = models.TextField(&apos;备注&apos;, blank=True, null=True) price = models.IntegerField(&apos;合同金额&apos;) detail = models.TextField(&apos;合同详细&apos;, blank=True, null=True) start_day = models.DateField(&apos;开始日期&apos;, blank=True, null=True) end_day = models.DateField(&apos;失效日期&apos;, blank=True, null=True) license_num = models.IntegerField(&apos;license数量&apos;, blank=True, null=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) m_day = models.DateField(&apos;修改日期&apos;, auto_now=True) def __str__(self): return self.name class Meta: verbose_name = &apos;合同&apos; verbose_name_plural = &quot;合同&quot;class Tag(models.Model): &quot;&quot;&quot;标签&quot;&quot;&quot; name = models.CharField(&apos;标签名&apos;, max_length=32, unique=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) def __str__(self): return self.name class Meta: verbose_name = &apos;标签&apos; verbose_name_plural = &quot;标签&quot; 说明： 机房可以有很多其它字段，比如城市、楼号、楼层和未知等等，如有需要可自行添加； 业务线可以有子业务线，因此使用一个外键关联自身模型； 合同模型主要存储财务部门关心的数据； 资产标签模型与资产是多对多的关系。 5.CPU模型通常一台服务器中只能有一种CPU型号，所以这里使用OneToOneField唯一关联一个资产对象，而不是外键关系。服务器上可以有多个物理CPU，它们的型号都是一样的。每个物理CPU又可能包含多核。1234567891011121314class CPU(models.Model): &quot;&quot;&quot;CPU组件&quot;&quot;&quot; asset = models.OneToOneField(&apos;Asset&apos;) # 设备上的cpu肯定都是一样的，所以不需要建立多个cpu数据，一条就可以，因此使用一对一。 cpu_model = models.CharField(&apos;CPU型号&apos;, max_length=128, blank=True, null=True) cpu_count = models.PositiveSmallIntegerField(&apos;物理CPU个数&apos;, default=1) cpu_core_count = models.PositiveSmallIntegerField(&apos;CPU核数&apos;, default=1) def __str__(self): return self.asset.name + &quot;: &quot; + self.cpu_model class Meta: verbose_name = &apos;CPU&apos; verbose_name_plural = &quot;CPU&quot; 6.RAM模型某个资产中可能有多条内存，所以这里必须是外键关系。其次，内存的sn号可能无法获得，就必须通过内存所在的插槽未知来唯一确定每条内存。因此，unique_together = (‘asset’, ‘slot’)这条设置非常关键，相当于内存的主键了，每条内存数据必须包含slot字段，否则就不合法。1234567891011121314151617class RAM(models.Model): &quot;&quot;&quot;内存组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 只能通过外键关联Asset。否则不能同时关联服务器、网络设备等等。 sn = models.CharField(&apos;SN号&apos;, max_length=128, blank=True, null=True) model = models.CharField(&apos;内存型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;内存制造商&apos;, max_length=128, blank=True, null=True) slot = models.CharField(&apos;插槽&apos;, max_length=64) capacity = models.IntegerField(&apos;内存大小(GB)&apos;, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s: %s&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;内存&apos; verbose_name_plural = &quot;内存&quot; unique_together = (&apos;asset&apos;, &apos;slot&apos;) #unique_together，也就是联合唯一，同一资产下的内存，根据插槽slot的不同，必须唯一 7. 硬盘模型与内存相同的是，硬盘也可能有很多块，所以也是外键关系。不同的是，硬盘通常都能获取到sn号，使用sn作为唯一值比较合适，也就是unique_together = (‘asset’, ‘sn’)。硬盘有不同的接口，这里设置了4种以及unknown，可自行添加其它类别。1234567891011121314151617181920212223242526class Disk(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; disk_interface_type_choice = ( (&apos;SATA&apos;, &apos;SATA&apos;), (&apos;SAS&apos;, &apos;SAS&apos;), (&apos;SCSI&apos;, &apos;SCSI&apos;), (&apos;SSD&apos;, &apos;SSD&apos;), (&apos;unknown&apos;, &apos;unknown&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;) sn = models.CharField(&apos;硬盘SN号&apos;, max_length=128) slot = models.CharField(&apos;所在插槽位&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;磁盘型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;磁盘制造商&apos;, max_length=128, blank=True, null=True) capacity = models.FloatField(&apos;磁盘容量(GB)&apos;, blank=True, null=True) interface_type = models.CharField(&apos;接口类型&apos;, max_length=16, choices=disk_interface_type_choice, default=&apos;unknown&apos;) def __str__(self): return &apos;%s: %s: %s: %sGB&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;硬盘&apos; verbose_name_plural = &quot;硬盘&quot; unique_together = (&apos;asset&apos;, &apos;sn&apos;) 8.网卡模型一台设备中可能有很多块网卡，所以网卡与资产也是外键的关系。另外，由于虚拟机的存在，网卡的mac地址可能会发生重复，无法唯一确定某块网卡，因此通过网卡型号加mac地址的方式来唯一确定网卡。123456789101112131415161718class NIC(models.Model): &quot;&quot;&quot;网卡组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 注意要用外键 name = models.CharField(&apos;网卡名称&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;网卡型号&apos;, max_length=128) mac = models.CharField(&apos;MAC地址&apos;, max_length=64) # 虚拟机有可能会出现同样的mac地址 ip_address = models.GenericIPAddressField(&apos;IP地址&apos;, blank=True, null=True) net_mask = models.CharField(&apos;掩码&apos;, max_length=64, blank=True, null=True) bonding = models.CharField(&apos;绑定地址&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s&apos; % (self.asset.name, self.model, self.mac) class Meta: verbose_name = &apos;网卡&apos; verbose_name_plural = &quot;网卡&quot; unique_together = (&apos;asset&apos;, &apos;model&apos;, &apos;mac&apos;) # 资产、型号和mac必须联合唯一。防止虚拟机中的特殊情况发生错误。 9.日志模型CMDB必须记录各种日志，这是毫无疑问的！我们通常要记录事件名称、类型、关联的资产、子事件、事件详情、谁导致的、发生时间。这些都很重要！ 尤其要注意的是，事件日志不能随着关联资产的删除被一并删除，也就是我们设置on_delete=models.SET_NULL的意义！1234567891011121314151617181920212223242526272829303132class EventLog(models.Model): &quot;&quot;&quot; 日志. 在关联对象被删除的时候，不能一并删除，需保留日志。 因此，on_delete=models.SET_NULL &quot;&quot;&quot; name = models.CharField(&apos;事件名称&apos;, max_length=128) event_type_choice = ( (0, &apos;其它&apos;), (1, &apos;硬件变更&apos;), (2, &apos;新增配件&apos;), (3, &apos;设备下线&apos;), (4, &apos;设备上线&apos;), (5, &apos;定期维护&apos;), (6, &apos;业务上线\\更新\\变更&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批成功时有这项数据 new_asset = models.ForeignKey(&apos;NewAssetApprovalZone&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批失败时有这项数据 event_type = models.SmallIntegerField(&apos;事件类型&apos;, choices=event_type_choice, default=4) component = models.CharField(&apos;事件子项&apos;, max_length=256, blank=True, null=True) detail = models.TextField(&apos;事件详情&apos;) date = models.DateTimeField(&apos;事件时间&apos;, auto_now_add=True) user = models.ForeignKey(User, blank=True, null=True, verbose_name=&apos;事件执行人&apos;, on_delete=models.SET_NULL) # 自动更新资产数据时没有执行人 memo = models.TextField(&apos;备注&apos;, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;事件纪录&apos; verbose_name_plural = &quot;事件纪录&quot; 10.新资产待审批区模型新资产的到来，并不能直接加入CMDB数据库中，而是要通过管理员审批后，才可以上线的。这就需要一个新资产的待审批区。在该区中，以资产的sn号作为唯一值，确定不同的资产。除了关键的包含资产所有信息的data字段，为了方便审批员查看信息，我们还设计了一些厂商、型号、内存大小、CPU类型等字段。同时，有可能出现资产还未审批，更新数据就已经发过来的情况，所以需要一个数据更新日期字段。1234567891011121314151617181920212223242526272829303132333435363738class NewAssetApprovalZone(models.Model): &quot;&quot;&quot;新资产待审批区&quot;&quot;&quot; sn = models.CharField(&apos;资产SN号&apos;, max_length=128, unique=True) # 此字段必填 asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;IDC&apos;, &apos;机房&apos;), (&apos;software&apos;, &apos;软件资产&apos;), ) asset_type = models.CharField(choices=asset_type_choice, default=&apos;server&apos;, max_length=64, blank=True, null=True, verbose_name=&apos;资产类型&apos;) manufacturer = models.CharField(max_length=64, blank=True, null=True, verbose_name=&apos;生产厂商&apos;) model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;型号&apos;) ram_size = models.PositiveIntegerField(blank=True, null=True, verbose_name=&apos;内存大小&apos;) cpu_model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;CPU型号&apos;) cpu_count = models.PositiveSmallIntegerField(blank=True, null=True) cpu_core_count = models.PositiveSmallIntegerField(blank=True, null=True) os_distribution = models.CharField(max_length=64, blank=True, null=True) os_type = models.CharField(max_length=64, blank=True, null=True) os_release = models.CharField(max_length=64, blank=True, null=True) data = models.TextField(&apos;资产数据&apos;) # 此字段必填 c_time = models.DateTimeField(&apos;汇报日期&apos;, auto_now_add=True) m_time = models.DateTimeField(&apos;数据更新日期&apos;, auto_now=True) approved = models.BooleanField(&apos;是否批准&apos;, default=False) def __str__(self): return self.sn class Meta: verbose_name = &apos;新上线待批准资产&apos; verbose_name_plural = &quot;新上线待批准资产&quot; ordering = [&apos;-c_time&apos;] 11.总结通过前面的内容，我们可以看出CMDB数据模型的设计非常复杂，我们这里还是省略了很多不太重要的部分，就这样总共都有400多行代码。其中每个模型需要保存什么字段、采用什么类型、什么关联关系、定义哪些参数、数据是否可以为空，这些都是踩过各种坑后总结出来的，不是随便就能定义的。所以，请务必详细阅读和揣摩这些模型的内容。 一切没有问题之后，注册app，然后makemigrations以及migrate! 注册app： cmdb/settings.py123456789INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;assets&apos;, ##此处] 创建数据库表单：12python manage.py makemigrationspython manage.py migrate","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"CMDB项目需求分析（一）","slug":"1、项目需求分析","date":"2019-02-27T16:00:00.000Z","updated":"2019-03-03T23:50:34.870Z","comments":true,"path":"2019/02/28/1、项目需求分析/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/28/1、项目需求分析/","excerpt":"","text":"一、CMDB简介CMDB (Configuration Management Database)配置管理数据库: CMDB用于存储与管理企业IT架构中设备的各种配置信息，它与所有服务支持和服务交付流程都紧密相联，支持这些流程的运转、发挥配置信息的价值，同时依赖于相关流程保证数据的准确性。 CMDB是ITIL(Information Technology Infrastructure Library，信息技术基础架构库)的基础，常常被认为是构建其它ITIL流程的先决条件而优先考虑，ITIL项目的成败与是否成功建立CMDB有非常大的关系。 CMDB的核心是对整个公司的IT硬件/软件资源进行自动/手动收集、变更操作，说白了也就是对IT资产进行自动化管理，这也是本项目的重点。 二、项目需求分析本项目不是一个完整的的CMDB系统，重点针对服务器资产的自动数据收集、报告、接收、审批、更新和展示，搭建一个基础的面向运维的主机管理平台。 下面是项目需求的总结： 尽可能存储所有的IT资产数据，但不包括外设、优盘、显示器这种属于行政部门管理的设备； 硬件信息可自动收集、报告、分析、存储和展示； 具有后台管理人员的工作界面； 具有前端可视化展示的界面； 具有日志记录功能； 数据可手动添加、修改和删除。 当然，实际的CMDB项目需求绝对不止这些，还有诸如用户管理、权限管理、API安全认证、REST设计等等。 三、资产分类资产种类众多，不是所有的都需要CMDB管理，也不是什么都是CMDB能管理的。 下面是一个大致的分类，不一定准确、全面： 资产类型包括： 服务器 存储设备 安全设备 网络设备 软件资产 服务器又可分为： 刀片服务器 PC服务器 小型机 大型机 其它 存储设备包括： 磁盘阵列 网络存储器 磁带库 磁带机 其它 安全设备包括： 防火墙 入侵检测设备 互联网网关 漏洞扫描设备 数字签名设备 上网行为管理设备 运维审计设备 加密机 其它 网络设备包括： 路由器 交换器 负载均衡 VPN 流量分析 其它 软件资产包括： 操作系统授权 大型软件授权 数据库授权 其它 其中，服务器是运维部门最关心的，也是CMDB中最主要、最方便进行自动化管理的资产。 服务器又可以包含下面的部件： CPU 硬盘 内存 网卡 除此之外，我们还要考虑下面的一些内容： 机房 业务线 合同 管理员 审批员 资产标签 其它未尽事宜 大概对资产进行了分类之后，就要详细考虑各细分数据条目了。 共有数据条目： 有一些数据条目是所有资产都应该有的，比如： 资产名称 资产sn 所属业务线 设备状态 制造商 管理IP 所在机房 资产管理员 资产标签 合同 价格 购买日期 过保日期 批准人 批准日期 数据更新日期 备注 另外，不同类型的资产还有各自不同的数据条目，例如服务器： 服务器： 服务器类型 添加方式 宿主机 服务器型号 Raid类型 操作系统类型 发行版本 操作系统版本 其实，在开始正式编写CMDB项目代码之前，对项目的需求分析准确与否，数据条目的安排是否合理，是决定整个CMDB项目成败的关键。这一部分工作看似简单其实复杂，看似无用其实关键，做好了，项目基础就牢固，没做好，推到重来好几遍很正常！","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"},{"name":"CMDB","slug":"CMDB","permalink":"http://zhdya.okay686.cn/tags/CMDB/"}]},{"title":"Django基础学习Ⅰ","slug":"Django 基础学习Ⅰ","date":"2019-02-10T16:00:00.000Z","updated":"2019-02-12T06:53:03.388Z","comments":true,"path":"2019/02/11/Django 基础学习Ⅰ/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/11/Django 基础学习Ⅰ/","excerpt":"","text":"一、Django简介 Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的，即是CMS（内容管理系统）软件。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django是一个处理网络请求的webweb应用框架 Django是开源的 Django有四个核心组件： 1.数据模型和数据库之间的媒介ORM 2.基于正则表达式的URL分发器 3.视图处理系统 4.模板系统 MVC： 12345m modules 模型， 和数据库字段对应v views 视图 用来展示给用户的，就是我们所学到的前端c controll url控制， 一个url，对应一个方法或者类 二、Django特点1) 强大的数据库功能：用python的类继承，几行代码就可以拥有一个动态的数据库操作API，如果需要也能执行SQL语句。2) 自带的强大的后台功能：几行代码就让网站拥有一个强大的后台，轻松管理内容。3) 优雅的网址：用正则匹配网址，传递到对应函数。4) 模板系统：强大，易扩展的模板系统，设计简易，代码和样式分开设计，更易管理。5) 缓存系统：与memcached或其它缓存系统联用，表现更出色，加载速度更快。6) 国际化：完全支持多语言应用，允许你定义翻译的字符，轻松翻译成不同国家的语言 三、安装Django使用pip工具来安装Django，直接通过下面命令来安装就可以。1# pip install Django 用一下测试django是否安装成功：123456C:\\Users\\ZHDYA&gt;pythonPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import django&gt;&gt;&gt; print (django.VERSION)(2, 0, 6, &apos;final&apos;, 0) 四、创建项目首先，我们先通过django来创建一个项目，命令如下：1# django-admin startproject firstproject 然后在当前目录下就自动生成了一个firstproject的项目然后就可以启动这个项目了：1# python manage.py runserver 127.0.0.1:8080 默认不写ip绑定的是本机的所以ip地址，端口默认为8000 也可以通过在pycharm中直接创建一个Django项目，就自动创建好了文件，然后配置manage.py脚本的参数。 直接再次运行manage.py文件就好。 然后访问url：1http://127.0.0.1:8080 有一个欢迎的首页 4.1、项目目录结构12345678910111213第一层：DjangoTest 项目名称第二层： DjangoTest目录和__init__.py文件，声明是一个包，表示项目实际的python包，不要随意更改该目录，与配置有关联settings.py 项目的全局（所有项目）配置中心urls.py 项目的url配置中心wsgi.py 项目的wsgi配置中心templates 模板目录manage.py django命令管理脚本 setting.py1234567BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ## 类似于环境变量SECRET_KEY = ##密钥DEBUG = True ##当报错时显示的错误信息ALLOWED_HOSTS = [] ##允许哪些机器可以访问 五、创建app打个比方，jd网站：http://www.jd.com/，上面有各种各样的不同模块，我们划分为不同的app，那我们就需要在django里面创建不通的app啦。接下来，我们就来看看如何创建app如果是命令行模式： 12#python manage.py startapp linux#python manage.py startapp python 如果是pycharm，你就需要点击：1Tools-&gt;&gt;Run manage.py Task 然后出现的交互命令行上输入：1startapp newapp 这样就创建了newapp的app。 六、Django的解析顺序既然我们知道Django是使用的MVC的架构，那我们先来聊聊MVC是什么样的原理，首先，通过MVC中的C就是control，用来接收url请求，对应我们django中的url.py模块，M就代表Model，调用数据库的Model层，就是Django的model.py模块，然后经过业务逻辑层的处理，最后渲染到前端界面。前端就是MVC中的view层，对应django的view模块。 其实所有的参数定义都是以setting.py为准，++首先django先去setting.py中找到ROOT_URLCONF = ‘firstproject.urls’找到总url++。然后在firstproject下的urls.py文件中的urlpatterns列表变量，然后根据里面的URL的正则表达式进行解析，如果解析到，就调用第二个参数，第二个参数对应一个类或者一个函数，或者直接是一个前端页面，在经过类或者函数处理完以后，在展现在前端界面。而前端是单独的html文件，前端界面和后端处理分开，架构更加清晰。 在上面的目录结构中，每一个app都会有一个view.py， model.py，我们自己还要在创建一个url.py，通过include函数，在firstproject项目中的总url.py分出去，把属于各自的app的url分配到不通的APP的urls.py文件中，这样可以降低耦合度，增加代码的健壮性。。 6.1、创建urls.py文件urls作为程序的url入口，支持正则匹配，讲访问的url映射到view中的函数中。为了能调用每个app管理自己的url，我们首先需要在DjangoTest的urls.py文件中做如下修改： 12345678from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^newapp/&apos;, include(&apos;newapp.urls&apos;)),] 注意事项：为了避免和别的app取同样的名字，一般我们会在名字前加一个app名称作为前缀url匹配，主url不需要/反斜杠：==因为django已经给域名加了一个反斜杠，如：http://127.0.0.1/主url后面要加/， app的url前面就不需要加/了，主url后面一般不要加$符号， app的url后面要加$符号== 然后在创建newapp/urls.py文件，编辑如下： 1234567from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&apos;^$&apos;, views.index)] 配置 view.py文件而以上：http://127.0.0.1:8080/newapp的url对应的就是view模块中的index函数，在linux的view.py中定义index函数 1234from django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;) 然后在浏览器上访问：http://127.0.0.1:8080/newapp/，如下图所示： 6.2、如果再次增加内容：urls.py123456789from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index), url(r&apos;newapp/&apos;, views.index), url(r&apos;hello/&apos;, views.hello)] views.py1234567from django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;)def hello(request): return HttpResponse(&quot;&lt;h1 style=&apos;text-align:center&apos;&gt; hheello world!!!&lt;/h1&gt;&quot;) 当访问：http://127.0.0.1:8000/newapp/hello/ 这会出现一个一级标题且居中的字体。 6.3、urls捕获参数（匹配正则表达式）在urls.py中增加如下：1url(r&apos;hello/p1(\\w+)p2(.+)/$&apos;, views.hello, name=hello), 在views.py中增加如下：12def hello(request, p1, p2): return HttpResponse(&quot;hello &#123;0&#125;, hello &#123;1&#125;&quot;.format(p1, p2)) url参数的捕获有两种方式：b 捕获关键字参数：在url函数中，正则表达是用（?P）进行捕获，然后在views.py中定义即可 在urls.py中增加如下：1url(r&apos;keyword/(?P&lt;ip&gt;\\S+)/$&apos;, views.keyword, name=keyword), 在views.py中增加如下：12def keyword(request, ip): return HttpResponse(&quot;the ip is &#123;0&#125;&quot;.format(ip)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/keyword/1.1.1.1/ 七、urls重定向在学习url重定向之前，我们先来看看定义url的函数是怎样一个表达形式。 注意如下，有个参数 name= 这个是必须要写的，类似于起个别名。1def url(regex, view, kwargs=None, name=None): 1234567regex：url匹配的正则字符串view：一个可以调用的类型函数，或者使用include函数kwargs：关键字参数，必须是一个字典，可以通过这个传递更多参数给views.py，views通过kwargs.get(“key”)得到对应的valuename：给URL取得名字，以后可以通过reverse函数进行重定向 对于kwargs如何传递参数，我们来看一个例子： 在urls.py中增加如下：1url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), 在views.py中增加如下： 12def test(request, **kwargs): return HttpResponse(&quot;the name is : &#123;0&#125;&quot;.format(kwargs.get(&quot;name&quot;))) 在浏览器上访问url：http://127.0.0.1:8080/newapp/test/ 既然已经知道name属性的用法，现在我们就来说重定向，重定向常用name属性来进行重定向 修改urls.py12url(r&quot;^$&quot;, views.index, name=&quot;index&quot;),url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;), 修改views.py1234from django.http import HttpResponse, HttpResponseRedirectfrom django.urls import reversedef redirect(request): return HttpResponseRedirect(reverse(&quot;index&quot;)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/redirect，直接跳转到http://127.0.0.1:8080/newapp/， 当然也可以指定返回数据的具体类型，例如：Json格式返回 urls.py1234567891011from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index, name=&quot;index&quot;), # url(r&apos;hello/&apos;, views.hello) url(r&apos;hello/p1(\\w+)p2(.+)/$&apos;, views.hello, name=&quot;hello&quot;), url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;)] 在views.py中修改主页为：1234567def index(request): test = dict() test[&apos;name&apos;] = &quot;zhdya&quot; test[&apos;sex&apos;] = &quot;man&quot; test[&apos;age&apos;] = 28 # return HttpResponse(&quot;This is a test Django index!!!&quot;) return HttpResponse(json.dumps(test))","categories":[{"name":"Python3","slug":"Python3","permalink":"http://zhdya.okay686.cn/categories/Python3/"}],"tags":[{"name":"django","slug":"django","permalink":"http://zhdya.okay686.cn/tags/django/"},{"name":"python","slug":"python","permalink":"http://zhdya.okay686.cn/tags/python/"}]},{"title":"kubernetes 1.11.2整理Ⅲ","slug":"kubernetes 1.11.2整理Ⅲ","date":"2019-02-09T16:00:00.000Z","updated":"2019-03-07T00:55:25.672Z","comments":true,"path":"2019/02/10/kubernetes 1.11.2整理Ⅲ/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/10/kubernetes 1.11.2整理Ⅲ/","excerpt":"","text":"测试集群123456789101112131415161718192021222324252627282930313233# 创建一个 nginx deplymentapiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ---apiVersion: v1 kind: Servicemetadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx 创建testnginx deployment123[root@master1 ~]# kubectl create -f testnginx.yamldeployment.extensions/nginx-dm createdservice/nginx-svc created 1234[root@master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-dm-fff68d674-j7dlk 1/1 Running 0 9m 10.254.108.115 node2 &lt;none&gt;nginx-dm-fff68d674-r5hb6 1/1 Running 0 9m 10.254.102.133 node1 &lt;none&gt; 在 安装了 calico 网络的node节点 里 curl1234567891011121314151617181920212223242526[root@node2 ~]# curl 10.254.102.133&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 查看 ipvs 规则12345678910[root@node2 ssl]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 1 0 -&gt; 192.168.161.162:6443 Masq 1 0 0TCP 10.254.18.37:80 rr -&gt; 10.254.75.1:80 Masq 1 0 0 -&gt; 10.254.102.133:80 Masq 1 0 0 配置 CoreDNS官方 地址 https://coredns.io 下载 yaml 文件123wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedmv coredns.yaml.sed coredns.yaml 修改配置文件中的部分配置：12345678910111213141516171819202122# vi coredns.yaml第一处：...data: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/18 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 &#125;... 第二处：搜索 /clusterIP 即可 clusterIP: 10.254.0.2 配置说明12345678910111213141516171）errors官方没有明确解释，后面研究2）health:健康检查，提供了指定端口（默认为8080）上的HTTP端点，如果实例是健康的，则返回“OK”。3）cluster.local：CoreDNS为kubernetes提供的域，10.254.0.0/18这告诉Kubernetes中间件它负责为反向区域提供PTR请求0.0.254.10.in-addr.arpa ..换句话说，这是允许反向DNS解析服务（我们经常使用到得DNS服务器里面有两个区域，即“正向查找区域”和“反向查找区域”，正向查找区域就是我们通常所说的域名解析，反向查找区域即是这里所说的IP反向解析，它的作用就是通过查询IP地址的PTR记录来得到该IP地址指向的域名，当然，要成功得到域名就必需要有该IP地址的PTR记录。PTR记录是邮件交换记录的一种，邮件交换记录中有A记录和PTR记录，A记录解析名字到地址，而PTR记录解析地址到名字。地址是指一个客户端的IP地址，名字是指一个客户的完全合格域名。通过对PTR记录的查询，达到反查的目的。）4）proxy:这可以配置多个upstream 域名服务器，也可以用于延迟查找 /etc/resolv.conf 中定义的域名服务器5）cache:这允许缓存两个响应结果，一个是肯定结果（即，查询返回一个结果）和否定结果（查询返回“没有这样的域”），具有单独的高速缓存大小和TTLs。# 这里 kubernetes cluster.local 为 创建 svc 的 IP 段kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IPclusterIP: 10.254.0.2 创建coreDNS1234567[root@master1 src]# kubectl apply -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.extensions/coredns createdservice/kube-dns created 查看创建：12345678910[root@master1 src]# kubectl get pod,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 23s 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 23s 10.254.75.21 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 23s k8s-app=kube-dns 检查日志123456[root@master1 src]# kubectl logs coredns-55f86bf584-hsrbp -n kube-system.:532018/09/22 02:03:06 [INFO] CoreDNS-1.2.22018/09/22 02:03:06 [INFO] linux/amd64, go1.11, eb51e8bCoreDNS-1.2.2linux/amd64, go1.11, eb51e8b 验证 dns 服务在验证 dns 之前，在 dns 未部署++之前创建的 pod 与 deployment 等，都必须删除++，重新部署，否则无法解析。 创建一个 pods 来测试一下 dns1234567891011apiVersion: v1kind: Podmetadata: name: alpinespec: containers: - name: alpine image: alpine command: - sleep - &quot;3600&quot; 查看 创建的服务123456789[root@master1 ~]# kubectl get po,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/alpine 1/1 Running 0 52s 10.254.102.141 node1 &lt;none&gt;pod/nginx-dm-fff68d674-fzhqk 1/1 Running 0 3m 10.254.102.140 node1 &lt;none&gt;pod/nginx-dm-fff68d674-h8n79 1/1 Running 0 3m 10.254.75.22 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 20d &lt;none&gt;service/nginx-svc ClusterIP 10.254.10.144 &lt;none&gt; 80/TCP 3m name=nginx 测试12345[root@master1 ~]# kubectl exec -it alpine nslookup nginx-svcnslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: nginx-svcAddress 1: 10.254.10.144 nginx-svc.default.svc.cluster.local 部署 DNS 自动伸缩按照 node 数量 自动伸缩 dns 数量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586vim dns-auto-scaling.yamlkind: ServiceAccountapiVersion: v1metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilerules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;list&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;replicationcontrollers/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;deployments/scale&quot;, &quot;replicasets/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] verbs: [&quot;get&quot;, &quot;create&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilesubjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-systemroleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: &quot;20m&quot; memory: &quot;10Mi&quot; command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params=&#123;&quot;linear&quot;:&#123;&quot;coresPerReplica&quot;:256,&quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true&#125;&#125; - --logtostderr=true - --v=2 tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot; serviceAccountName: kube-dns-autoscaler 导入文件12345[root@master1 ~]# kubectl apply -f dns-auto-scaling.yamlserviceaccount/kube-dns-autoscaler createdclusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler createddeployment.apps/kube-dns-autoscaler created ++如下是上面所用到的镜像，如果不可以下载使用如下的即可++：123registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:coredns-1.2.2registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cluster-proportional-autoscaler-amd64_1.1.2-r2 部署 Ingress 与 Dashboard部署 heapster官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster 下载 heapster 相关 yaml 文件1234567wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ==如上官方镜像一直在更新，修改的时候需要把如下的版本号也修改下↓== 下载 heapster 镜像下载123456789101112131415161718# 官方镜像k8s.gcr.io/heapster-grafana-amd64:v4.4.3k8s.gcr.io/heapster-amd64:v1.5.3k8s.gcr.io/heapster-influxdb-amd64:v1.3.3# 个人的镜像jicki/heapster-grafana-amd64:v4.4.3jicki/heapster-amd64:v1.5.3jicki/heapster-influxdb-amd64:v1.3.3# 备用阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-grafana-amd64-v4.4.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-amd64-v1.5.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-influxdb-amd64-v1.3.3# 替换所有yaml 镜像地址sed -i &apos;s/k8s\\.gcr\\.io/jicki/g&apos; *.yaml 修改 yaml 文件123456789# heapster.yaml 文件#### 修改如下部分 #####因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true 1234567891011121314151617181920212223242526272829303132# heapster-rbac.yaml 文件#### 修改为部分 #####将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapsterroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapstersubjects:- kind: ServiceAccount name: heapster namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapster-kubelet-apiroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-adminsubjects:- kind: ServiceAccount name: heapster namespace: kube-system 创建：12345678910[root@master1 dashboard180922]# kubectl apply -f .deployment.extensions/monitoring-grafana createdservice/monitoring-grafana createdclusterrolebinding.rbac.authorization.k8s.io/heapster createdclusterrolebinding.rbac.authorization.k8s.io/heapster-kubelet-api createdserviceaccount/heapster createddeployment.extensions/heapster createdservice/heapster createddeployment.extensions/monitoring-influxdb createdservice/monitoring-influxdb created 这儿可能需要等待一下，这个取决于自己server的网络情况：12345678910111213[root@node1 ~]# journalctl -u kubelet -f-- Logs begin at 六 2018-09-22 09:07:48 CST. --9月 22 10:34:55 node1 kubelet[2301]: I0922 10:34:55.701016 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=======&gt; ] 7.617MB/50.21MB&quot;9月 22 10:35:05 node1 kubelet[2301]: I0922 10:35:05.700868 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [========&gt; ] 8.633MB/50.21MB&quot;9月 22 10:35:15 node1 kubelet[2301]: I0922 10:35:15.701193 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==========&gt; ] 10.66MB/50.21MB&quot;9月 22 10:35:25 node1 kubelet[2301]: I0922 10:35:25.700980 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [============&gt; ] 12.69MB/50.21MB&quot;9月 22 10:35:35 node1 kubelet[2301]: I0922 10:35:35.700779 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [===============&gt; ] 15.74MB/50.21MB&quot;9月 22 10:35:45 node1 kubelet[2301]: I0922 10:35:45.701359 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================&gt; ] 18.28MB/50.21MB&quot;9月 22 10:35:55 node1 kubelet[2301]: I0922 10:35:55.701618 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [====================&gt; ] 20.82MB/50.21MB&quot;9月 22 10:36:05 node1 kubelet[2301]: I0922 10:36:05.701611 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=========================&gt; ] 25.39MB/50.21MB&quot;9月 22 10:36:15 node1 kubelet[2301]: I0922 10:36:15.700926 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==============================&gt; ] 30.99MB/50.21MB&quot;9月 22 10:36:25 node1 kubelet[2301]: I0922 10:36:25.700931 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot;9月 22 10:36:35 node1 kubelet[2301]: I0922 10:36:35.701950 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot; 查看部署情况1234567891011121314151617[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 44m 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 44m 10.254.75.21 node2 &lt;none&gt;pod/heapster-745d7bc8b7-zk65c 1/1 Running 0 13m 10.254.75.51 node2 &lt;none&gt;pod/kube-dns-autoscaler-66d448df8f-4zvw6 1/1 Running 0 32m 10.254.102.142 node1 &lt;none&gt;pod/monitoring-grafana-558c44f948-m2tzz 1/1 Running 0 1m 10.254.75.6 node2 &lt;none&gt;pod/monitoring-influxdb-f6bcc9795-496jd 1/1 Running 0 13m 10.254.102.147 node1 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/heapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 13m k8s-app=heapsterservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 44m k8s-app=kube-dnsservice/monitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 1m k8s-app=grafanaservice/monitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 13m k8s-app=influxdb 部署 dashboard下载 dashboard 镜像12345678# 官方镜像k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3# 个人的镜像jicki/kubernetes-dashboard-amd64:v1.8.3# 阿里的镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kubernetes-dashboard-amd64-v1.8.3 下载 yaml 文件1curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 导入 yaml 123# 替换所有的 images，注意修改镜像版本号为1.8.3sed -i &apos;s/k8s\\.gcr\\.io/jicki/g&apos; kubernetes-dashboard.yaml 创建dashboard1234567[root@master1 dashboard180922]# kubectl apply -f kubernetes-dashboard.yamlsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created 查看创建的dashboard1234[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wide | grep dashboardpod/kubernetes-dashboard-65666d4586-bb66s 1/1 Running 0 7m 10.254.102.151 node1 &lt;none&gt;service/kubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 7m k8s-app=kubernetes-dashboard 部署 Nginx Ingress++Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。++ 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ 配置 调度 node1234567891011121314151617181920212223242526# ingress 有多种方式 1. deployment 自由调度 replicas2. daemonset 全局调度 分配到所有node里# deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签# 默认如下:[root@master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONnode1 Ready &lt;none&gt; 20d v1.11.2node2 Ready &lt;none&gt; 8d v1.11.2# 对 node1 与 node2 打上 label[root@master1 ~]# kubectl label nodes node1 ingress=proxynode/node1 labeled[root@master1 ~]# kubectl label nodes node2 ingress=proxynode/node2 labeled# 打完标签以后[root@master1 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSnode1 Ready &lt;none&gt; 20d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node1node2 Ready &lt;none&gt; 9d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node2 下载镜像1234567891011# 官方镜像gcr.io/google_containers/defaultbackend:1.4quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2# 国内镜像jicki/defaultbackend:1.4jicki/nginx-ingress-controller:0.16.2# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:defaultbackend-1.4registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:nginx-ingress-controller-0.16.2 下载 yaml 文件部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml# 部署 Ingress RBAC 认证curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml# 部署 Ingress Controller 组件curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml# tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务# 为了更加方便理解，如下两个例子：# tcp 例子apiVersion: v1kind: ConfigMapmetadata: name: tcp-services namespace: ingress-nginxdata: 9000: &quot;default/tomcat:8080&quot; # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中# udp 例子apiVersion: v1kind: ConfigMapmetadata: name: udp-services namespace: ingress-nginxdata: 53: &quot;kube-system/kube-dns:53&quot;# 替换所有的 imagessed -i &apos;s/gcr\\.io\\/google_containers/jicki/g&apos; *sed -i &apos;s/quay\\.io\\/kubernetes-ingress-controller/jicki/g&apos; *# 上面 对 两个 node 打了 label 所以配置 replicas: 2# 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。vim with-rbac.yaml第一处：↓spec: replicas: 2 第二处：↓（搜索 /nginx-ingress-serviceaccount 即可，在其下添加） .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... 第三处：↓ # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 导入 yaml 文件123456789101112131415161718192021222324252627282930313233343536373839[root@master1 ingress-service]# kubectl apply -f namespace.yamlnamespace/ingress-nginx created[root@master1 ingress-service]# kubectl get nsNAME STATUS AGEdefault Active 20dingress-nginx Active 6skube-public Active 20dkube-system Active 20d[root@master1 ingress-service]# kubectl apply -f .configmap/nginx-configuration createddeployment.extensions/default-http-backend createdservice/default-http-backend creatednamespace/ingress-nginx configuredserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createdconfigmap/tcp-services createdconfigmap/udp-services createddeployment.extensions/nginx-ingress-controller created# 查看服务，可以看到这两个 pods 被分别调度到 77 与 78 中[root@master1 ingress-service]# kubectl get pods -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEdefault-http-backend-6b89c8bdcb-vvl9f 1/1 Running 0 9m 10.254.102.163 node1 &lt;none&gt;nginx-ingress-controller-cf8d4564d-5vz7h 1/1 Running 0 9m 10.254.75.16 node2 &lt;none&gt;nginx-ingress-controller-cf8d4564d-z7q4b 1/1 Running 0 9m 10.254.102.158 node1 &lt;none&gt;# 查看我们原有的 svc[root@master1 ingress-service]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEalpine 1/1 Running 3 6h 10.254.102.141 node1 &lt;none&gt;nginx-dm-fff68d674-fzhqk 1/1 Running 0 6h 10.254.102.140 node1 &lt;none&gt;nginx-dm-fff68d674-h8n79 1/1 Running 0 6h 10.254.75.22 node2 &lt;none&gt; 创建一个 基于 nginx-dm 的 ingress12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758vi nginx-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingressspec: rules: - host: nginx.zhdya.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 理解如下:- host指虚拟出来的域名，具体地址(我理解应该是Ingress-controller那台Pod所在的主机的地址)应该加入/etc/hosts中,这样所有去nginx.zhdya.cn的请求都会发到nginx- servicePort主要是定义服务的时候的端口，不是NodePort.# 查看服务[root@master1 ingress-service]# kubectl create -f nginx-ingress.yamlingress.extensions/nginx-ingress created[root@master1 ingress-service]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEnginx-ingress nginx.zhdya.cn 80 10s# 测试访问[root@node1 ~]# curl nginx.zhdya.cn&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 当然如果本地浏览器访问的话 我们也需要绑定hosts 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 创建一个基于 dashboard 的 https 的 ingress# 新版本的 dashboard 默认就是 ssl ,所以这里使用 tcp 代理到 443 端口# 查看 dashboard svc[root@master1 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 2dkube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 3dkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2dmonitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 2dmonitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 2d# 修改 tcp-services-configmap.yaml 文件[root@master1 src]# vim tcp-services-configmap.yamlkind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginxdata: 8888: &quot;kube-system/kubernetes-dashboard:443&quot;# 载入配置文件[root@master1 src]# kubectl apply -f tcp-services-configmap.yamlconfigmap/tcp-services configured# 查看服务[root@master1 src]# kubectl get configmap/tcp-services -n ingress-nginxNAME DATA AGEtcp-services 1 2d[root@master1 src]# kubectl describe configmap/tcp-services -n ingress-nginxName: tcp-servicesNamespace: ingress-nginxLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;8888&quot;:&quot;kube-system/kubernetes-dashboard:443&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;tcp-services&quot;,&quot;namesp...Data====8888:----kube-system/kubernetes-dashboard:443Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 20m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal UPDATE 1m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services# 测试访问[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888curl: (6) Could not resolve host: dashboard.zhdya.cn; 未知的名称或服务当然如上报错很正常，咱们需要绑定下hosts在master 上查询下：[root@master1 src]# kubectl get svc -n kube-system -o wide | grep dashboardkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2d k8s-app=kubernetes-dashboard然后再node端绑定hosts [root@node1 ~]# vim /etc/hosts10.254.3.42 dashboard.zhdya.cn[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888HTTP/1.1 200 OKAccept-Ranges: bytesCache-Control: no-storeContent-Length: 990Content-Type: text/html; charset=utf-8Last-Modified: Tue, 13 Feb 2018 11:17:03 GMTDate: Tue, 25 Sep 2018 02:51:18 GMT 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 配置一个基于域名的 https , ingress# 创建一个 基于 自身域名的 证书[root@master1 dashboard-keys]# openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.zhdya.cn-key.key -out dashboard.zhdya.cn.pem -subj &quot;/CN=dashboard.zhdya.cn&quot;Generating a 2048 bit RSA private key.......+++..............+++writing new private key to &apos;dashboard.zhdya.cn-key.key&apos;-----[root@master1 dashboard-keys]# kubectl create secret tls dashboard-secret --namespace=kube-system --cert dashboard.zhdya.cn.pem --key dashboard.zhdya.cn-key.keysecret/dashboard-secret created# 查看 secret[root@master1 dashboard-keys]# kubectl get secret -n kube-system | grep dashboarddashboard-secret kubernetes.io/tls 2 55skubernetes-dashboard-certs Opaque 0 2dkubernetes-dashboard-key-holder Opaque 2 2dkubernetes-dashboard-token-r98wk kubernetes.io/service-account-token 3 2d# 创建一个 ingressvi dashboard-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubernetes-dashboard namespace: kube-system annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot; nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;spec: tls: - hosts: - dashboard.zhdya.cn secretName: dashboard-secret rules: - host: dashboard.zhdya.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443# 创建配置文件[root@master1 src]# kubectl apply -f dashboard-ingress.yamlingress.extensions/kubernetes-dashboard created[root@master1 src]# kubectl get ingress -n kube-systemNAME HOSTS ADDRESS PORTS AGEkubernetes-dashboard dashboard.zhdya.cn 80, 443 37s 测试访问 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 登录认证# 首先创建一个 dashboard rbac 超级用户vi dashboard-admin-rbac.yaml---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system# 导入配置文件[root@master1 src]# kubectl apply -f dashboard-admin-rbac.yamlserviceaccount/kubernetes-dashboard-admin createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created# 查看超级用户的 token 名称[root@master1 src]# kubectl -n kube-system get secret | grep kubernetes-dashboard-adminkubernetes-dashboard-admin-token-kq27d kubernetes.io/service-account-token 3 38s# 查看 token 部分[root@master1 src]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-kq27d 然后我们登录 web ui 选择 令牌登录然后就发现了还是那熟悉的味道：","categories":[{"name":"K8s","slug":"K8s","permalink":"http://zhdya.okay686.cn/categories/K8s/"}],"tags":[{"name":"Kubernets","slug":"Kubernets","permalink":"http://zhdya.okay686.cn/tags/Kubernets/"},{"name":"K8S","slug":"K8S","permalink":"http://zhdya.okay686.cn/tags/K8S/"}]},{"title":"kubernetes 1.11.2整理Ⅱ","slug":"kubernetes 1.11.2整理Ⅱ","date":"2019-02-08T16:00:00.000Z","updated":"2019-03-07T01:49:01.556Z","comments":true,"path":"2019/02/09/kubernetes 1.11.2整理Ⅱ/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/09/kubernetes 1.11.2整理Ⅱ/","excerpt":"","text":"配置 kubelet 认证kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 123# RBAC 只需创建一次就可以kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes 创建 bootstrap kubeconfig 文件++注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token++ 创建 集群所有 kubelet 的 token==注意修改hostname==1234567891011121314[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master1 --kubeconfig ~/.kube/configof2phx.v39lq3ofeh0w6f3m[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master2 --kubeconfig ~/.kube/configb3stk9.edz2iylppqjo5qbc[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master3 --kubeconfig ~/.kube/configck2uqr.upeu75jzjj1ko901[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node1 --kubeconfig ~/.kube/config1ocjm9.7qa3rd5byuft9gwr[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node2 --kubeconfig ~/.kube/confightsqn3.z9z6579gxw5jdfzd 查看生成的 token1234567891011[root@master1 kubernetes]# kubeadm token list --kubeconfig ~/.kube/configTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS1ocjm9.7qa3rd5byuft9gwr 23h 2018-09-02T16:06:32+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node1b3stk9.edz2iylppqjo5qbc 23h 2018-09-02T16:03:46+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master2ck2uqr.upeu75jzjj1ko901 23h 2018-09-02T16:05:16+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master3htsqn3.z9z6579gxw5jdfzd 23h 2018-09-02T16:06:34+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node2of2phx.v39lq3ofeh0w6f3m 23h 2018-09-02T16:03:40+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master1 以下为了区分 会先生成 hostname 名称加 bootstrap.kubeconfig 生成 master1 的 bootstrap.kubeconfig12345678910111213141516171819202122232425262728# 配置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=master1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \\ --token=of2phx.v39lq3ofeh0w6f3m \\ --kubeconfig=master1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=master1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master1-bootstrap.kubeconfig# 拷贝生成的 master1-bootstrap.kubeconfig 文件mv master1-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 master2 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=master2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \\ --token=b3stk9.edz2iylppqjo5qbc \\ --kubeconfig=master2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=master2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master2-bootstrap.kubeconfig# 拷贝生成的 master2-bootstrap.kubeconfig 文件scp master2-bootstrap.kubeconfig 192.168.161.162:/etc/kubernetes/bootstrap.kubeconfig 生成 master3 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=master3-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \\ --token=ck2uqr.upeu75jzjj1ko901 \\ --kubeconfig=master3-bootstrap.kubeconfig# 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=master3-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master3-bootstrap.kubeconfig# 拷贝生成的 master3-bootstrap.kubeconfig 文件scp master3-bootstrap.kubeconfig 192.168.161.163:/etc/kubernetes/bootstrap.kubeconfig 生成 node1 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=node1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \\ --token=1ocjm9.7qa3rd5byuft9gwr \\ --kubeconfig=node1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=node1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node1-bootstrap.kubeconfig# 拷贝生成的 node1-bootstrap.kubeconfig 文件scp node1-bootstrap.kubeconfig 192.168.161.77:/etc/kubernetes/bootstrap.kubeconfig 生成 node2 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=node2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \\ --token=htsqn3.z9z6579gxw5jdfzd \\ --kubeconfig=node2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=node2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node2-bootstrap.kubeconfig# 拷贝生成的 node2-bootstrap.kubeconfig 文件scp node2-bootstrap.kubeconfig 192.168.161.78:/etc/kubernetes/bootstrap.kubeconfig 配置 bootstrap RBAC 权限123456kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers# 否则报如下错误failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:1jezb7&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope 创建自动批准相关 CSR 请求的 ClusterRole12345678910111213141516171819202122232425vi /etc/kubernetes/tls-instructs-csr.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/selfnodeserver&quot;] verbs: [&quot;create&quot;]# 创建 yaml 文件[root@master1 kubernetes]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yamlclusterrole.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver created[root@master1 kubernetes]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserverName: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ClusterRole&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;system:certificates.k8s.io:certificatesigningreq...PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] 12345678910111213141516# 将 ClusterRole 绑定到适当的用户组# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes Node 端单 Node 部分 需要部署的组件有1docker， calico， kubelet， kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA 1234567# master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server;node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口;当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ++这种模式和我之前所接触的不太一样，之前所做的架构是基于KUBE-APISERVER 的负载均衡，所有的node节点都会去连接负载均衡的虚拟VIP。++ 创建Nginx 代理在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy 1234567891011121314151617181920212223242526272829303132# 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.161.161:6443; server 192.168.161.162:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf 123456789101112131415161718192021222324252627# 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 Nginx12345678910111213141516171819202122232425262728293031323334systemctl daemon-reloadsystemctl start nginx-proxysystemctl enable nginx-proxysystemctl status nginx-proxyjournalctl -u nginx-proxy -f ##查看实时日志9月 01 17:34:55 node1 docker[4032]: 1.13.7-alpine: Pulling from library/nginx9月 01 17:34:57 node1 docker[4032]: 128191993b8a: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: 655cae3ea06e: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: dbc72c3fd216: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Waiting9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Verifying Checksum9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Download complete9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Verifying Checksum9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Download complete9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Verifying Checksum9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Download complete9月 01 17:35:17 node1 docker[4032]: 128191993b8a: Pull complete9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Verifying Checksum9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Download complete9月 01 17:35:51 node1 docker[4032]: 655cae3ea06e: Pull complete9月 01 17:35:51 node1 docker[4032]: dbc72c3fd216: Pull complete9月 01 17:35:51 node1 docker[4032]: f391a4589e37: Pull complete9月 01 17:35:51 node1 docker[4032]: Digest: sha256:34aa80bb22c79235d466ccbbfa3659ff815100ed21eddb1543c6847292010c4d9月 01 17:35:51 node1 docker[4032]: Status: Downloaded newer image for nginx:1.13.7-alpine9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: using the &quot;epoll&quot; event method9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: nginx/1.13.79月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: built by gcc 6.2.1 20160822 (Alpine 6.2.1)9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: OS: Linux 3.10.0-514.el7.x86_649月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:10485769月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker processes9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker process 5 创建 kubelet.service 文件==注意修改节点的hostname↓==123456789101112131415161718192021222324252627# 创建 kubelet 目录mkdir -p /var/lib/kubeletvi /etc/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \\ --hostname-override=node1 \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2[Install]WantedBy=multi-user.target 创建 kubelet config 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445vi /etc/kubernetes/kubelet.config.json&#123; &quot;kind&quot;: &quot;KubeletConfiguration&quot;, &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;, &quot;authentication&quot;: &#123; &quot;x509&quot;: &#123; &quot;clientCAFile&quot;: &quot;/etc/kubernetes/ssl/ca.pem&quot; &#125;, &quot;webhook&quot;: &#123; &quot;enabled&quot;: true, &quot;cacheTTL&quot;: &quot;2m0s&quot; &#125;, &quot;anonymous&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;authorization&quot;: &#123; &quot;mode&quot;: &quot;Webhook&quot;, &quot;webhook&quot;: &#123; &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;, &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot; &#125; &#125;, &quot;address&quot;: &quot;192.168.161.77&quot;, &quot;port&quot;: 10250, &quot;readOnlyPort&quot;: 0, &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;, &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;, &quot;serializeImagePulls&quot;: false, &quot;RotateCertificates&quot;: true, &quot;featureGates&quot;: &#123; &quot;RotateKubeletClientCertificate&quot;: true, &quot;RotateKubeletServerCertificate&quot;: true &#125;, &quot;MaxPods&quot;: &quot;512&quot;, &quot;failSwapOn&quot;: false, &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;, &quot;containerLogMaxFiles&quot;: 5, &quot;clusterDomain&quot;: &quot;cluster.local.&quot;, &quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;]&#125;##其它node节点记得修改如上的IP地址 123456# 如上配置:node1 本机hostname10.254.0.2 预分配的 dns 地址cluster.local. 为 kubernetes 集群的 domainregistry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。&quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ++同理修改其它node节点++ 启动 kubelet123456systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubeletjournalctl -u kubelet -f 创建 kube-proxy 证书123456789101112131415161718192021222324# 证书方面由于我们node端没有装 cfssl# 我们回到 master 端 机器 去配置证书，然后拷贝过来cd /opt/sslvi kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 1234567891011121314151617生成 kube-proxy 证书和私钥/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成ls kube-proxy*kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem# 拷贝到目录cp kube-proxy* /etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.77:/etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.78:/etc/kubernetes/ssl/ 创建 kube-proxy kubeconfig 文件1234567891011121314151617181920212223242526272829303132333435# 配置集群kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig# 配置客户端认证kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 配置默认关联kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig# 拷贝到需要的 node 端里scp kube-proxy.kubeconfig 192.168.161.77:/etc/kubernetes/scp kube-proxy.kubeconfig 192.168.161.78:/etc/kubernetes/ 创建 kube-proxy.service 文件1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 ==node== 中安装1yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go 123456789101112131415cd /etc/kubernetes/vi kube-proxy.config.yamlapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 192.168.161.77clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfigclusterCIDR: 10.254.64.0/18healthzBindAddress: 192.168.161.77:10256hostnameOverride: node1 ##注意修改此处的hostnamekind: KubeProxyConfigurationmetricsBindAddress: 192.168.161.77:10249mode: &quot;ipvs&quot; 1234567891011121314151617181920212223# 创建 kube-proxy 目录mkdir -p /var/lib/kube-proxyvi /etc/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 kube-proxy1234systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 检查 ipvs 情况1234567[root@node1 kubernetes]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 0 0 -&gt; 192.168.161.162:6443 Masq 1 0 0 配置 Calico 网络官方文档 https://docs.projectcalico.org/v3.1/introduction 下载 Calico yaml12345# 下载 yaml 文件wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yamlwget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml 下载镜像1234567891011121314151617181920# 下载 镜像# 国外镜像 有墙quay.io/calico/node:v3.1.3quay.io/calico/cni:v3.1.3quay.io/calico/kube-controllers:v3.1.3# 国内镜像jicki/node:v3.1.3jicki/cni:v3.1.3jicki/kube-controllers:v3.1.3# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:node_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cni_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kube-controllers_v3.1.3# 替换镜像sed -i &apos;s/quay\\.io\\/calico/jicki/g&apos; calico.yaml 修改配置12345678910111213141516171819202122232425262728293031vi calico.yaml# 注意修改如下选项:# etcd 地址 etcd_endpoints: &quot;https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379&quot; # etcd 证书路径 # If you&apos;re using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot; # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面)data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d &apos;\\n&apos;) etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d &apos;\\n&apos;) etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d &apos;\\n&apos;) ## 如上需要去掉() 只需要填写生成的编码即可 # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: &quot;10.254.64.0/18&quot; 查看服务1234567891011[root@master1 kubernetes]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcalico-kube-controllers-79cfd7887-xbsd4 1/1 Running 5 11d 192.168.161.77 node1 &lt;none&gt;calico-node-2545t 2/2 Running 0 29m 192.168.161.78 node2 &lt;none&gt;calico-node-tbptz 2/2 Running 7 11d 192.168.161.77 node1 &lt;none&gt;[root@master1 kubernetes]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEnode1 Ready &lt;none&gt; 11d v1.11.2 192.168.161.77 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2node2 Ready &lt;none&gt; 29m v1.11.2 192.168.161.78 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2 修改 kubelet 配置==两台node节点都需要配置== 12345678910111213# kubelet 需要增加 cni 插件 --network-plugin=cnivim /etc/systemd/system/kubelet.service --network-plugin=cni \\# 重新加载配置systemctl daemon-reloadsystemctl restart kubelet.servicesystemctl status kubelet.service 检查网络的互通性：12345678910111213141516171819202122232425[root@node1 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.102.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@node2 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.75.0 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 直接在node2上面ping：[root@node2 ~]# ping 10.254.102.128PING 10.254.102.128 (10.254.102.128) 56(84) bytes of data.64 bytes from 10.254.102.128: icmp_seq=1 ttl=64 time=72.3 ms64 bytes from 10.254.102.128: icmp_seq=2 ttl=64 time=0.272 ms 安装 calicoctl++calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 下载 二进制文件curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctlmv calicoctl /usr/local/bin/chmod +x /usr/local/bin/calicoctl# 创建 calicoctl.cfg 配置文件mkdir /etc/calicovim /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: datastoreType: &quot;kubernetes&quot; kubeconfig: &quot;/root/.kube/config&quot;# 查看 calico 状态[root@node1 src]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+-------------------+-------+----------+-------------+| 192.168.161.78 | node-to-node mesh | up | 06:54:19 | Established |+----------------+-------------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.[root@node1 src]# calicoctl get node ##当然我这边是在node节点操作的，node节点是没有/root/.kube/config 这个文件的，只需要从master节点copy过来即可！！NAMEnode1node2","categories":[{"name":"K8s","slug":"K8s","permalink":"http://zhdya.okay686.cn/categories/K8s/"}],"tags":[{"name":"Kubernets","slug":"Kubernets","permalink":"http://zhdya.okay686.cn/tags/Kubernets/"},{"name":"K8S","slug":"K8S","permalink":"http://zhdya.okay686.cn/tags/K8S/"}]},{"title":"kubernetes 1.11.2整理Ⅰ","slug":"kubernetes 1.11.2整理Ⅰ","date":"2019-02-07T16:00:00.000Z","updated":"2019-03-07T00:55:36.335Z","comments":true,"path":"2019/02/08/kubernetes 1.11.2整理Ⅰ/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/08/kubernetes 1.11.2整理Ⅰ/","excerpt":"","text":"1:服务器信息以及节点介绍 初次使用 ==CoreDNS==， ==Ingress==， ==Calico== 系统信息：centos7 主机名称 IP 备注 master1 192.168.161.161 master and etcd master2 192.168.161.162 master and etcd master3 192.168.161.163 etcd node1 192.168.161.77 node1 node2 192.168.161.78 node2 我这边将数据盘挂载了 /opt 目录下 一、环境初始化1：分别在4台主机设置主机名称12345hostnamectl set-hostname master1hostnamectl set-hostname master2hostnamectl set-hostname master3hostnamectl set-hostname node1hostnamectl set-hostname node2 2:配置主机映射 123456789cat &lt;&lt;EOF &gt; /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.161.161 master1192.168.161.162 master2192.168.161.163 master3192.168.161.77 node1192.168.161.78 node2EOF 3：node01上执行ssh免密码登陆配置 1ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.161.XXX 4：四台主机配置、停防火墙、关闭Swap、关闭Selinux、设置内核、K8S的yum源、安装依赖包、配置ntp（配置完后建议重启一次） 1234567891011121314151617181920212223242526272829303132333435systemctl stop firewalldsystemctl disable firewalldswapoff -a sed -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstabsetenforce 0 sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/selinux/config modprobe br_netfiltercat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.d/k8s.confls /proc/sys/net/bridgeyum install -y epel-releaseyum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl systemctl enable ntpdate.serviceecho &apos;*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com &gt;/dev/null 2&gt;&amp;1&apos; &gt; /tmp/crontab2.tmpcrontab /tmp/crontab2.tmpsystemctl start ntpdate.service echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft memlock unlimited&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard memlock unlimited&quot; &gt;&gt; /etc/security/limits.conf 二、环境说明123基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler这里配置2个Master 2个node, Master-161、Master-162 做 Master + etcd, master3 仅仅etcd， node-01 node-02 只做单纯 Node 创建 验证这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 安装 cfssl1234567891011121314mkdir -p /opt/local/cfsslcd /opt/local/cfsslwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64mv cfssl_linux-amd64 cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64mv cfssljson_linux-amd64 cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 cfssl-certinfochmod +x * 创建 CA 证书配置123mkdir /opt/sslcd /opt/ssl config.json 文件123456789101112131415161718192021vi config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; csr.json 文件123456789101112131415161718vi csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 CA 证书和私钥 1234567891011cd /opt/ssl//opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca[root@master1 ssl]# ls -lt总用量 20-rw-r--r-- 1 root root 1005 9月 1 13:36 ca.csr-rw------- 1 root root 1679 9月 1 13:36 ca-key.pem-rw-r--r-- 1 root root 1363 9月 1 13:36 ca.pem-rw-r--r-- 1 root root 210 9月 1 13:35 csr.json-rw-r--r-- 1 root root 292 9月 1 13:35 config.json 分发证书创建证书目录1mkdir -p /etc/kubernetes/ssl 拷贝所有文件到目录下12cp *.pem /etc/kubernetes/sslcp ca.csr /etc/kubernetes/ssl 这里要将文件拷贝到所有的k8s机器上1234scp *.pem *.csr 192.168.161.162:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.163:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.77:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.78:/etc/kubernetes/ssl/ 三、安装 docker所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 123456789101112131415161718192021222324252627# 导入 yum 源# 安装 yum-config-manageryum -y install yum-utils# 导入yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo# 更新 repoyum makecache# 查看yum 版本yum list docker-ce.x86_64 --showduplicates |sort -r# 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinuxwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmrpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmyum -y install docker-ce-17.03.2.cedocker version 更改docker 配置12345678910111213141516171819202122232425262728293031# 添加配置vi /etc/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.comAfter=network.target docker-storage-setup.serviceWants=docker-storage-setup.service[Service]Type=notifyEnvironment=GOTRACEBACK=crashExecReload=/bin/kill -s HUP $MAINPIDDelegate=yesKillMode=processExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRYLimitNOFILE=1048576LimitNPROC=1048576LimitCORE=infinityTimeoutStartSec=1minRestart=on-abnormal[Install]WantedBy=multi-user.target 修改其他配置12345678910111213141516171819202122232425262728293031323334353637# 低版本内核， kernel 3.10.x 配置使用 overlay2vi /etc/docker/daemon.json&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;mkdir -p /etc/systemd/system/docker.service.d/vi /etc/systemd/system/docker.service.d/docker-options.conf# 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载)# docker 版本 17.03.2 之前配置为 --graph=/opt/docker# docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service]Environment=&quot;DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5&quot;vi /etc/systemd/system/docker.service.d/docker-dns.conf# 添加如下 : [Service]Environment=&quot;DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot; 重新读取配置，启动 docker123systemctl daemon-reloadsystemctl start dockersystemctl enable docker 如果报错 请使用1systemctl status docker -l 或 journalctl -u docker 来定位问题 etcd 集群etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.2 etcd 支持最新版本为 v3.2.18 安装 etcd官方地址 https://github.com/coreos/etcd/releases 123456789# 下载 二进制文件（3台master机器都需要）wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gztar zxvf etcd-v3.2.18-linux-amd64.tar.gzcd etcd-v3.2.18-linux-amd64mv etcd etcdctl /usr/bin/ 创建 etcd 证书etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发。 1234567891011121314151617181920212223242526cd /opt/ssl/vi etcd-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 etcd 密钥1234/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd 1234567891011121314151617181920212223242526# 查看生成[root@master1 ssl]# ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem# 检查证书[root@master1 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem# 拷贝到etcd服务器# etcd-1 cp etcd*.pem /etc/kubernetes/ssl/# etcd-2scp etcd*.pem 192.168.161.162:/etc/kubernetes/ssl/# etcd-3scp etcd*.pem 192.168.161.163:/etc/kubernetes/ssl/# 如果 etcd 非 root 用户，读取证书会提示没权限chmod 644 /etc/kubernetes/ssl/etcd-key.pem 修改 etcd 配置由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 创建 etcd data 目录， 并授权12345useradd etcdmkdir -p /opt/etcdchown -R etcd:etcd /opt/etcd etcd-1123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://192.168.161.161:2380 \\ --listen-peer-urls=https://192.168.161.161:2380 \\ --listen-client-urls=https://192.168.161.161:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://192.168.161.161:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-2123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://192.168.161.162:2380 \\ --listen-peer-urls=https://192.168.161.162:2380 \\ --listen-client-urls=https://192.168.161.162:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://192.168.161.162:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-3123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://192.168.161.163:2380 \\ --listen-peer-urls=https://192.168.161.163:2380 \\ --listen-client-urls=https://192.168.161.163:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://192.168.161.163:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 etcd分别启动 所有节点的 etcd 服务 123456systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcdjournalctl -u etcd -f ##用此命令来动态查看具体日志 验证 etcd 集群状态12345678910etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 60ce394098258c3 is healthy: got healthy result from https://192.168.161.163:2379member afe2d07db38fa5e2 is healthy: got healthy result from https://192.168.161.162:2379member ba8a716d98dac47b is healthy: got healthy result from https://192.168.161.161:2379cluster is healthy 查看 etcd 集群成员：123456789etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list60ce394098258c3: name=etcd3 peerURLs=https://192.168.161.163:2380 clientURLs=https://192.168.161.163:2379 isLeader=falseafe2d07db38fa5e2: name=etcd2 peerURLs=https://192.168.161.162:2380 clientURLs=https://192.168.161.162:2379 isLeader=falseba8a716d98dac47b: name=etcd1 peerURLs=https://192.168.161.161:2380 clientURLs=https://192.168.161.161:2379 isLeader=true 配置 Kubernetes 集群kubectl 安装在所有需要进行操作的机器上 Master and NodeMaster 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 安装组件1234567891011121314151617# 从github 上下载版本 (在两台master上节点执行)cd /usr/local/srcwget https://dl.k8s.io/v1.11.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetescp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm&#125; /usr/local/bin/scp server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm&#125; 192.168.161.162:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.77:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.78:/usr/local/bin/ 创建 admin 证书kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。12345678910111213141516171819202122cd /opt/ssl/vi admin-csr.json&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 1234567891011121314151617# 生成 admin 证书和私钥cd /opt/ssl//opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin# 查看生成[root@master1 ssl]# ls admin*admin.csr admin-csr.json admin-key.pem admin.pemcp admin*.pem /etc/kubernetes/ssl/scp admin*.pem 192.168.161.162:/etc/kubernetes/ssl/ 生成 kubernetes 配置文件生成证书相关的配置文件存储与 /root/.kube 目录中 1234567891011121314151617181920# 配置 kubernetes 集群kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443# 配置 客户端认证kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=adminkubectl config use-context kubernetes 创建 kubernetes 证书123456789101112131415161718192021222324252627282930313233343536cd /opt/sslvi kubernetes-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot;, &quot;192.168.161.77&quot;, &quot;192.168.161.78&quot;, &quot;10.254.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 192.168.161.161 和 172.16.161.162 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 生成 kubernetes 证书和私钥123456789101112131415161718/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes# 查看生成[root@master1 ssl]# ls -lt kubernetes*-rw-r--r-- 1 root root 1277 9月 1 15:31 kubernetes.csr-rw------- 1 root root 1679 9月 1 15:31 kubernetes-key.pem-rw-r--r-- 1 root root 1651 9月 1 15:31 kubernetes.pem-rw-r--r-- 1 root root 531 9月 1 15:31 kubernetes-csr.json# 拷贝到目录cp kubernetes*.pem /etc/kubernetes/ssl/scp kubernetes*.pem 192.168.161.162:/etc/kubernetes/ssl/ 配置 kube-apiserverkubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。12345678910111213141516171819202122232425262728# 生成 token[root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;97606de41d5ee3c3392aae432eb3143d# 创建 encryption-config.yaml 配置cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 97606de41d5ee3c3392aae432eb3143d - identity: &#123;&#125;EOF# 拷贝cp encryption-config.yaml /etc/kubernetes/scp encryption-config.yaml 192.168.161.162:/etc/kubernetes/ 123456789101112131415161718192021# 生成高级审核配置文件&gt; 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&gt;&gt; 如下为最低限度的日志审核cd /etc/kubernetescat &gt;&gt; audit-policy.yaml &lt;&lt;EOF# Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF# 拷贝scp audit-policy.yaml 192.168.161.162:/etc/kubernetes/ 创建 kube-apiserver.service 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下# 配置为 各自的本地 IPvi /etc/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]User=rootExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --anonymous-auth=false \\ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --advertise-address=192.168.161.161 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --v=1Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target# --experimental-encryption-provider-config ，替代之前 token.csv 文件# 这里面要注意的是 --service-node-port-range=30000-32000# 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。记得在另外一台master上修改IP地址 启动 kube-apiserver1234systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 查看启动端口123456789101112[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-controller-manager两台master都需要配置：1234新增几个配置，用于自动 续期证书–feature-gates=RotateKubeletServerCertificate=true–experimental-cluster-signing-duration=86700h0m0s 12345678910111213141516171819202122232425262728293031323334# 创建 kube-controller-manager.service 文件vi /etc/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,tokencleaner,bootstrapsigner \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-controller-manager1234systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 查看启动端口12345678910111213[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-scheduler1234567891011121314151617181920# 创建 kube-cheduler.service 文件vi /etc/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-scheduler1234systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 查看启动端口1234567891011121314[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::10251 :::* LISTEN 4023/kube-schedulertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 验证 Master 节点12345678910111213141516[root@master1 kubernetes]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;[root@master2 bin]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;","categories":[{"name":"K8s","slug":"K8s","permalink":"http://zhdya.okay686.cn/categories/K8s/"}],"tags":[{"name":"Kubernets","slug":"Kubernets","permalink":"http://zhdya.okay686.cn/tags/Kubernets/"},{"name":"K8S","slug":"K8S","permalink":"http://zhdya.okay686.cn/tags/K8S/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-07T08:19:52.028Z","updated":"2019-02-07T08:19:52.028Z","comments":true,"path":"2019/02/07/hello-world/","link":"","permalink":"http://zhdya.okay686.cn/2019/02/07/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}