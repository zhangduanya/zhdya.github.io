<!DOCTYPE html>
<html lang="zh-CN">





<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="Tough times never last, but tough people do.">
  <meta name="author" content="Zhdya">
  <meta name="keywords" content="">
  <title>生产级k8s二进制v14.1高可用部署 ~ 拼！就对了！</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css">
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css">

<link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css">


</head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>拼！就对了！</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/post.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期二, 六月 18日 2019, 12:00 凌晨
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    10.5k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      59 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><h4 id="1-1、角色划分"><a href="#1-1、角色划分" class="headerlink" title="1.1、角色划分"></a>1.1、角色划分</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.8.13.80   vip  </span><br><span class="line">10.8.13.81   master01  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.82   master02  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.83   master03  haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">10.8.13.84   node01    kubelet、docker、kube_proxy、flanneld</span><br><span class="line">10.8.13.85   node02    kubelet、docker、kube_proxy、flanneld</span><br></pre></td></tr></table></figure>
<h4 id="1-2、各主机ssh互通"><a href="#1-2、各主机ssh互通" class="headerlink" title="1.2、各主机ssh互通"></a>1.2、各主机ssh互通</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#ssh-keygen</span><br><span class="line">#ssh-copy-id 10.8.13.82(83-85)</span><br></pre></td></tr></table></figure>
<h4 id="1-3、环境初始化"><a href="#1-3、环境初始化" class="headerlink" title="1.3、环境初始化"></a>1.3、环境初始化</h4><h5 id="1-3-1、停止iptables"><a href="#1-3-1、停止iptables" class="headerlink" title="1.3.1、停止iptables"></a>1.3.1、停止iptables</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld.service </span><br><span class="line">systemctl disable  firewalld.service</span><br></pre></td></tr></table></figure>
<h5 id="1-3-2、关闭selinux"><a href="#1-3-2、关闭selinux" class="headerlink" title="1.3.2、关闭selinux"></a>1.3.2、关闭selinux</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /etc/selinux/config </span><br><span class="line">SELINUX=disabled</span><br><span class="line"># setenforce 0</span><br></pre></td></tr></table></figure>
<h5 id="1-3-3、设置sysctl，开启路由转发"><a href="#1-3-3、设置sysctl，开启路由转发" class="headerlink" title="1.3.3、设置sysctl，开启路由转发"></a>1.3.3、设置sysctl，开启路由转发</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat /etc/sysctl.conf</span><br><span class="line"> fs.file-max=1000000</span><br><span class="line"> net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line"> net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"> net.ipv4.ip_forward = 1</span><br><span class="line"> vm.swappiness = 0</span><br><span class="line"> net.ipv4.ip_forward = 1</span><br><span class="line"> net.ipv4.tcp_max_tw_buckets = 6000</span><br><span class="line"> net.ipv4.tcp_sack = 1</span><br><span class="line"> net.ipv4.tcp_window_scaling = 1</span><br><span class="line"> net.ipv4.tcp_rmem = 4096 87380 4194304</span><br><span class="line"> net.ipv4.tcp_wmem = 4096 16384 4194304</span><br><span class="line"> net.ipv4.tcp_max_syn_backlog = 16384</span><br><span class="line"> net.core.netdev_max_backlog = 32768</span><br><span class="line"> net.core.somaxconn = 32768</span><br><span class="line"> net.core.wmem_default = 8388608</span><br><span class="line"> net.core.rmem_default = 8388608</span><br><span class="line"> net.core.rmem_max = 16777216</span><br><span class="line"> net.core.wmem_max = 16777216</span><br><span class="line"> net.ipv4.tcp_timestamps = 1</span><br><span class="line"> net.ipv4.tcp_fin_timeout = 20</span><br><span class="line"> net.ipv4.tcp_synack_retries = 2</span><br><span class="line"> net.ipv4.tcp_syn_retries = 2</span><br><span class="line"> net.ipv4.tcp_syncookies = 1</span><br><span class="line"></span><br><span class="line"> net.ipv4.tcp_tw_reuse = 1</span><br><span class="line"> net.ipv4.tcp_mem = 94500000 915000000 927000000</span><br><span class="line"> net.ipv4.tcp_max_orphans = 3276800</span><br><span class="line"> net.ipv4.ip_local_port_range = 1024 65000</span><br><span class="line"> net.nf_conntrack_max = 6553500</span><br><span class="line"> net.netfilter.nf_conntrack_max = 6553500</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class="line"> net.netfilter.nf_conntrack_tcp_timeout_established = 3600</span><br></pre></td></tr></table></figure>
<h5 id="1-3-4、加载ipvs"><a href="#1-3-4、加载ipvs" class="headerlink" title="1.3.4、加载ipvs"></a>1.3.4、加载ipvs</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee /etc/sysconfig/modules/ipvs.modules</span><br><span class="line">#!/bin/bash</span><br><span class="line"> modprobe -- ip_vs</span><br><span class="line"> modprobe -- ip_vs_rr</span><br><span class="line"> modprobe -- ip_vs_wrr</span><br><span class="line"> modprobe -- ip_vs_sh</span><br><span class="line"> modprobe -- nf_conntrack_ipv4</span><br><span class="line"> EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure>
<h3 id="二、集群各功能模块描述"><a href="#二、集群各功能模块描述" class="headerlink" title="二、集群各功能模块描述"></a>二、集群各功能模块描述</h3><p><img src="http://myimage.okay686.cn/okay686cn/180302/B41emjc68G.png?imageslim" srcset="/img/loading.gif" alt="mark"></p>
<h5 id="Master节点："><a href="#Master节点：" class="headerlink" title="Master节点："></a>Master节点：</h5><pre><code>Master节点上面主要由四个模块组成，etcd，APIServer，schedule,controller-manager（haproxy、keepalived高可用后面单独说）
</code></pre><h5 id="etcd："><a href="#etcd：" class="headerlink" title="etcd："></a>etcd：</h5><pre><code>etcd是一个高可用的键值存储系统，kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。
</code></pre><h5 id="APIServer"><a href="#APIServer" class="headerlink" title="APIServer:"></a>APIServer:</h5><pre><code>APIServer负责对外提供Restful的kubernetes API的服务，它是系统管理指令的统一接口，任何对资源的增删该查都要交给APIServer处理后再交给etcd。kubectl(kubernetes提供的客户端工具，该工具内部是对kubernetes API的调用）是直接和APIServer交互的。
</code></pre><h5 id="schedule"><a href="#schedule" class="headerlink" title="schedule:"></a>schedule:</h5><pre><code>schedule负责调度Pod到合适的Node上，如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定。 kubernetes目前提供了调度算法，同样也保留了接口。用户根据自己的需求定义自己的调度算法。
</code></pre><h5 id="controller-manager"><a href="#controller-manager" class="headerlink" title="controller manager:"></a>controller manager:</h5><pre><code>如果APIServer做的是前台的工作的话，那么controller manager就是负责后台的。每一个资源都对应一个控制器。而control manager就是负责管理这些控制器的，比如我们通过APIServer创建了一个Pod，当这个Pod创建成功后，APIServer的任务就算完成了。
</code></pre><h4 id="Node节点："><a href="#Node节点：" class="headerlink" title="Node节点："></a>Node节点：</h4><p>每个Node节点主要由四个模板组成：kublet， kube-proxy，docker，flanneld</p>
<h5 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy:"></a>kube-proxy:</h5><pre><code>该模块实现了kubernetes中的服务发现和反向代理功能。kube-proxy支持TCP和UDP连接转发，默认基Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响，另外，kube-proxy还支持session affinity。
</code></pre><h5 id="kublet："><a href="#kublet：" class="headerlink" title="kublet："></a>kublet：</h5><pre><code>kublet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上的所有容器，但是如果容器不是通过kubernetes创建的，它并不会管理。本质上，它负责使Pod的运行状态与期望的状态一致。
</code></pre><h5 id="flanneld："><a href="#flanneld：" class="headerlink" title="flanneld："></a>flanneld：</h5><pre><code>源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。
</code></pre><h3 id="三、下载链接"><a href="#三、下载链接" class="headerlink" title="三、下载链接"></a>三、下载链接</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Client Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">Server Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">Node Binaries</span><br><span class="line">https://dl.k8s.io/v1.14.1/kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">etcd</span><br><span class="line">https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">flannel</span><br><span class="line">https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="四、Master部署"><a href="#四、Master部署" class="headerlink" title="四、Master部署"></a>四、Master部署</h3><p>以下操作都在<strong>master01</strong>上执行，生成证书之后拷贝到master02和master03</p>
<h4 id="4-1、下载软件"><a href="#4-1、下载软件" class="headerlink" title="4.1、下载软件"></a>4.1、下载软件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">wget https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">wget https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<h4 id="4-2、ssl安装"><a href="#4-2、ssl安装" class="headerlink" title="4.2、ssl安装"></a>4.2、ssl安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /usr/local/bin/cfssl</span><br><span class="line">mv cfssljson_linux-amd64 /usr/local/bin/cfssljson</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo</span><br></pre></td></tr></table></figure>
<h4 id="4-3、创建etcd证书"><a href="#4-3、创建etcd证书" class="headerlink" title="4.3、创建etcd证书"></a>4.3、创建etcd证书</h4><p>在所有节点（master01-03、node01-02）创建此路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /k8s/etcd/&#123;bin,cfg,ssl&#125; -p</span><br><span class="line">mkdir /k8s/kubernetes/&#123;bin,cfg,ssl&#125; -p</span><br></pre></td></tr></table></figure></p>
<p>1)、etcd ca配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /k8s/etcd/ssl/</span><br><span class="line">cat &lt;&lt; EOF | tee ca-config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;etcd&quot;: &#123;</span><br><span class="line">         &quot;expiry&quot;: &quot;87600h&quot;,</span><br><span class="line">         &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>2)、etcd ca证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee ca-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;etcd CA&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>3)、etcd server证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee server-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    &quot;10.8.13.81&quot;,</span><br><span class="line">    &quot;10.8.13.82&quot;,</span><br><span class="line">    &quot;10.8.13.83&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<p>4)、生成etcd ca证书和私钥</p>
<p>初始化ca<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca </span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca-csr.json  server-csr.json</span><br><span class="line">[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca </span><br><span class="line">2019/05/01 16:13:54 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] generate received request</span><br><span class="line">2019/05/01 16:13:54 [INFO] received CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 16:13:54 [INFO] encoded CSR</span><br><span class="line">2019/05/01 16:13:54 [INFO] signed certificate with serial number 144752911121073185391033754516204538929473929443</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server-csr.json</span><br></pre></td></tr></table></figure></p>
<p>生成server证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server</span><br><span class="line">2019/05/01 16:18:53 [INFO] generate received request</span><br><span class="line">2019/05/01 16:18:53 [INFO] received CSR</span><br><span class="line">2019/05/01 16:18:53 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 16:18:54 [INFO] encoded CSR</span><br><span class="line">2019/05/01 16:18:54 [INFO] signed certificate with serial number 388122587040599986639159163167557684970159030057</span><br><span class="line">2019/05/01 16:18:54 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. </span><br><span class="line">For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></figure></p>
<h4 id="4-4、etcd安装"><a href="#4-4、etcd安装" class="headerlink" title="4.4、etcd安装"></a>4.4、etcd安装</h4><p>1）解压缩<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf etcd-v3.3.11-linux-amd64.tar.gz</span><br><span class="line">cd etcd-v3.3.11-linux-amd64/</span><br><span class="line">cp etcd etcdctl /k8s/etcd/bin/</span><br><span class="line">mkdir /data1/etcd</span><br></pre></td></tr></table></figure></p>
<p>2）配置etcd主文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/etcd/cfg/etcd.conf   </span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd01&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.81:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;</span><br><span class="line"> </span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.81:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p>
<p>3）配置etcd启动文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/data1/etcd/</span><br><span class="line">EnvironmentFile=-/k8s/etcd/cfg/etcd.conf</span><br><span class="line"># set GOMAXPROCS to number of processors</span><br><span class="line">ExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /k8s/etcd/bin/etcd --name=\&quot;$&#123;ETCD_NAME&#125;\&quot; --data-dir=\&quot;$&#123;ETCD_DATA_DIR&#125;\&quot; --listen-client-urls=\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\&quot; --listen-peer-urls=\&quot;$&#123;ETCD_LISTEN_PEER_URLS&#125;\&quot; --advertise-client-urls=\&quot;$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\&quot; --initial-cluster-token=\&quot;$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\&quot; --initial-cluster=\&quot;$&#123;ETCD_INITIAL_CLUSTER&#125;\&quot; --initial-cluster-state=\&quot;$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\&quot; --cert-file=\&quot;$&#123;ETCD_CERT_FILE&#125;\&quot; --key-file=\&quot;$&#123;ETCD_KEY_FILE&#125;\&quot; --trusted-ca-file=\&quot;$&#123;ETCD_TRUSTED_CA_FILE&#125;\&quot; --client-cert-auth=\&quot;$&#123;ETCD_CLIENT_CERT_AUTH&#125;\&quot; --peer-cert-file=\&quot;$&#123;ETCD_PEER_CERT_FILE&#125;\&quot; --peer-key-file=\&quot;$&#123;ETCD_PEER_KEY_FILE&#125;\&quot; --peer-trusted-ca-file=\&quot;$&#123;ETCD_PEER_TRUSTED_CA_FILE&#125;\&quot; --peer-client-cert-auth=\&quot;$&#123;ETCD_PEER_CLIENT_CERT_AUTH&#125;\&quot;&quot;</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>4)、拷贝master01etcd的证书、配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/etcd/ssl/* 10.8.13.82:/k8s/etcd/ssl/</span><br><span class="line">scp /k8s/etcd/ssl/* 10.8.13.83:/k8s/etcd/ssl/</span><br><span class="line">scp /k8s/etcd/cfg/* 10.8.13.82:/k8s/etcd/cfg/</span><br><span class="line">scp /k8s/etcd/cfg/* 10.8.13.83:/k8s/etcd/cfg/</span><br><span class="line">scp /k8s/etcd/bin/* 10.8.13.82:/k8s/etcd/bin/</span><br><span class="line">scp /k8s/etcd/bin/* 10.8.13.83:/k8s/etcd/bin/</span><br><span class="line">scp /usr/lib/systemd/system/etcd.service 10.8.13.82:/usr/lib/systemd/system/etcd.service</span><br><span class="line">scp /usr/lib/systemd/system/etcd.service 10.8.13.83:/usr/lib/systemd/system/etcd.service</span><br></pre></td></tr></table></figure></p>
<p>5)、修改master02、master03 etcd的conf配置文件</p>
<p>matser02 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.82</span><br><span class="line">vim /k8s/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd02&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.82:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.82:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p>
<p>matser03 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.83</span><br><span class="line">vim /k8s/etcd/cfg/etcd.conf</span><br><span class="line">#[Member]</span><br><span class="line">ETCD_NAME=&quot;etcd03&quot;</span><br><span class="line">ETCD_DATA_DIR=&quot;/data1/etcd&quot;</span><br><span class="line">ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;</span><br><span class="line"></span><br><span class="line">#[Clustering]</span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;</span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class="line"></span><br><span class="line">#[Security]</span><br><span class="line">ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class="line">ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;</span><br><span class="line">ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;</span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;</span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br></pre></td></tr></table></figure></p>
<p>6)、启动etcd服务，并加入开机自启动(master三个节点都执行)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure></p>
<p>7)、etcd服务检查<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379&quot; cluster-health</span><br><span class="line">以下为输出：</span><br><span class="line">member 262d942ab474feaa is healthy: got healthy result from https://10.8.13.82:2379</span><br><span class="line">member 3e95c59733e7d54f is healthy: got healthy result from https://10.8.13.83:2379</span><br><span class="line">member fe03446cb13e0221 is healthy: got healthy result from https://10.8.13.81:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure></p>
<p>至此etcd安装完成。。。</p>
<hr>
<h4 id="4-5、haproxy安装配置"><a href="#4-5、haproxy安装配置" class="headerlink" title="4.5、haproxy安装配置"></a>4.5、haproxy安装配置</h4><p>1)、master01配置(需要注意的是端口自定义为16443)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install haproxy</span><br></pre></td></tr></table></figure>
<p>master01、master02、master03都安装haproxy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/haproxy/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    log         127.0.0.1 local2</span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">    # turn on stats unix socket</span><br><span class="line">    stats socket /var/lib/haproxy/stats</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class="line"># use if not designated in their block</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  httplog</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option forwardfor       except 127.0.0.0/8</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># kubernetes apiserver frontend which proxys to the backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">frontend kubernetes-apiserver</span><br><span class="line">    mode                 tcp</span><br><span class="line">    bind                 *:16443</span><br><span class="line">    option               tcplog</span><br><span class="line">    default_backend      kubernetes-apiserver</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># round robin balancing between the various backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">backend kubernetes-apiserver</span><br><span class="line">    mode        tcp</span><br><span class="line">    balance     roundrobin</span><br><span class="line">    server      k8s01 10.8.13.81:6443 check</span><br><span class="line">    server      k8s02 10.8.13.82:6443 check</span><br><span class="line">    server      k8s03 10.8.13.83:6443 check</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># collection haproxy statistics message</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">listen stats</span><br><span class="line">    bind                 *:1080</span><br><span class="line">    stats auth           admin:awesomePassword</span><br><span class="line">    stats refresh        5s</span><br><span class="line">    stats realm          HAProxy\ Statistics</span><br><span class="line">    stats uri            /admin?stats</span><br></pre></td></tr></table></figure>
<p>2）拷贝master01的haproxy到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /etc/haproxy/haproxy.cfg 10.8.13.82:/etc/haproxy/haproxy.cfg</span><br><span class="line">scp /etc/haproxy/haproxy.cfg 10.8.13.83:/etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure></p>
<p>3)启动haproxy服务，并加入开机自启动(master三个节点都执行)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable haproxy</span><br><span class="line">systemctl start haproxy</span><br></pre></td></tr></table></figure></p>
<h4 id="4-6、keepalived安装配置"><a href="#4-6、keepalived安装配置" class="headerlink" title="4.6、keepalived安装配置"></a>4.6、keepalived安装配置</h4><p>1）master01配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install keepalived</span><br></pre></td></tr></table></figure>
<p>master01、master02、master03都安装keepalived<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 100</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>2）master02配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 99</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>3）master03配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens160</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 98</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.8.13.80</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>4）启动keepalived服务（vip在master01上）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable keepalived</span><br><span class="line">systemctl start keepalived</span><br><span class="line">[root@master01 ~]# systemctl status keepalived</span><br><span class="line">● keepalived.service - LVS and VRRP High Availability Monitor</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/keepalived.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:33 CST; 3 days ago</span><br><span class="line">  Process: 992 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1115 (keepalived)</span><br><span class="line">   CGroup: /system.slice/keepalived.service</span><br><span class="line">           ├─1115 /usr/sbin/keepalived -D</span><br><span class="line">           ├─1116 /usr/sbin/keepalived -D</span><br><span class="line">           └─1117 /usr/sbin/keepalived -D</span><br><span class="line"></span><br><span class="line">Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</span><br><span class="line">[root@hwzx-test-cmpmaster01 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 00:50:56:90:22:79 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.8.13.81/24 brd 10.8.13.255 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.8.13.80/32 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::6772:8bb6:b50c:57fe/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>5)keepalived配置注意事项<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;1.killall -0 根据进程名称检测进程是否存活，如果服务器没有该命令，请使用yum install psmisc -y安装</span><br><span class="line"></span><br><span class="line">&gt;2.第一个master节点的state为MASTER，其他master节点的state为BACKUP</span><br><span class="line"></span><br><span class="line">&gt;3.priority表示各个节点的优先级，范围：0～250（非强制要求）</span><br></pre></td></tr></table></figure></p>
<h4 id="4-7、生成kubernets证书与私钥"><a href="#4-7、生成kubernets证书与私钥" class="headerlink" title="4.7、生成kubernets证书与私钥"></a>4.7、生成kubernets证书与私钥</h4><p>1）制作kubernetes ca证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /k8s/kubernetes/ssl</span><br><span class="line">cat &lt;&lt; EOF | tee ca-config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">         &quot;expiry&quot;: &quot;87600h&quot;,</span><br><span class="line">         &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee ca-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class="line">[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class="line">2019/05/01 09:47:08 [INFO] generating a new CA key and certificate from CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] generate received request</span><br><span class="line">2019/05/01 09:47:08 [INFO] received CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:47:08 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:47:08 [INFO] signed certificate with serial number 156611735285008649323551446985295933852737436614</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</span><br></pre></td></tr></table></figure>
<p>2）制作apiserver证书</p>
<p>==注意hosts处，所有IP都写进去，包括vip==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee server-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;10.254.0.1&quot;,</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;10.8.13.81&quot;,</span><br><span class="line">      &quot;10.8.13.82&quot;,</span><br><span class="line">      &quot;10.8.13.83&quot;,</span><br><span class="line">      &quot;10.8.13.84&quot;,</span><br><span class="line">      &quot;10.8.13.85&quot;,</span><br><span class="line">      &quot;10.8.13.80&quot;,</span><br><span class="line">      &quot;kubernetes&quot;,</span><br><span class="line">      &quot;kubernetes.default&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">      &quot;kubernetes.default.svc.cluster.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</span><br><span class="line">2019/05/01 09:51:56 [INFO] generate received request</span><br><span class="line">2019/05/01 09:51:56 [INFO] received CSR</span><br><span class="line">2019/05/01 09:51:56 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:51:56 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:51:56 [INFO] signed certificate with serial number 399376216731194654868387199081648887334508501005</span><br><span class="line">2019/05/01 09:51:56 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></figure>
<p>3）制作kube-proxy证书<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF | tee kube-proxy-csr.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">2019/05/01 09:52:40 [INFO] generate received request</span><br><span class="line">2019/05/01 09:52:40 [INFO] received CSR</span><br><span class="line">2019/05/01 09:52:40 [INFO] generating key: rsa-2048</span><br><span class="line">2019/05/01 09:52:40 [INFO] encoded CSR</span><br><span class="line">2019/05/01 09:52:40 [INFO] signed certificate with serial number 633932731787505365511506755558794469389165123417</span><br><span class="line">2019/05/01 09:52:40 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@master01 ssl]# ls</span><br><span class="line">ca-config.json  ca-csr.json  ca.pem          kube-proxy-csr.json  kube-proxy.pem  server-csr.json  server.pem</span><br><span class="line">ca.csr          ca-key.pem   kube-proxy.csr  kube-proxy-key.pem   server.csr      server-key.pem</span><br></pre></td></tr></table></figure>
<h4 id="4-8部署kubernetes-server"><a href="#4-8部署kubernetes-server" class="headerlink" title="4.8部署kubernetes server"></a>4.8部署kubernetes server</h4><p>kubernetes master 节点运行如下组件：<br>kube-apiserver<br>kube-scheduler<br>kube-controller-manager<br>kube-scheduler 和 kube-controller-manager 以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。</p>
<p>1）解压缩文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf kubernetes-server-linux-amd64.tar.gz </span><br><span class="line">cd kubernetes/server/bin/</span><br><span class="line">cp kube-scheduler kube-apiserver kube-controller-manager kubectl /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure>
<p>2）部署kube-apiserver组件（==注意保留此KEY，下面还会需要==）</p>
<p>创建TLS Bootstrapping Token<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 bin]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;</span><br><span class="line">af93a4194e7bcf7f05dc0bab3a6e97cd</span><br><span class="line"> </span><br><span class="line">vim /k8s/kubernetes/cfg/token.csv</span><br><span class="line">af93a4194e7bcf7f05dc0bab3a6e97cd,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><br></pre></td></tr></table></figure></p>
<p>创建Apiserver配置文件</p>
<p><strong>注</strong>：–bind-address=当前节点ip<br>–advertise-address=当前节点ip<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.81 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.81 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p>
<p>创建apiserver systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-apiserver.service </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-apiserver</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>拷贝master01 kubernetes的证书、配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/ssl/* 10.8.13.82:/k8s/kubernetes/ssl/</span><br><span class="line">scp /k8s/kubernetes/ssl/* 10.8.13.83:/k8s/kubernetes/ssl/</span><br><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.82:/k8s/kubernetes/cfg/</span><br><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.83:/k8s/kubernetes/cfg/</span><br><span class="line">scp /k8s/kubernetes/bin/* 10.8.13.82:/k8s/kubernetes/bin/</span><br><span class="line">scp /k8s/kubernetes/bin/* 10.8.13.83:/k8s/kubernetes/bin/</span><br><span class="line">scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.82:/usr/lib/systemd/system</span><br><span class="line">scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.83:/usr/lib/systemd/system</span><br></pre></td></tr></table></figure></p>
<p>5)、修改master02、master03 etcd的conf配置文件</p>
<p>matser02 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.82</span><br><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.82 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.82 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p>
<p>matser03 etcd.conf配置如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 10.8.13.83</span><br><span class="line">vim /k8s/kubernetes/cfg/kube-apiserver </span><br><span class="line">KUBE_APISERVER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \</span><br><span class="line">--bind-address=10.8.13.83 \</span><br><span class="line">--secure-port=6443 \</span><br><span class="line">--advertise-address=10.8.13.83 \</span><br><span class="line">--allow-privileged=true \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \</span><br><span class="line">--authorization-mode=RBAC,Node \</span><br><span class="line">--enable-bootstrap-token-auth \</span><br><span class="line">--token-auth-file=/k8s/kubernetes/cfg/token.csv \</span><br><span class="line">--service-node-port-range=30000-50000 \</span><br><span class="line">--tls-cert-file=/k8s/kubernetes/ssl/server.pem  \</span><br><span class="line">--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \</span><br><span class="line">--client-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \</span><br><span class="line">--etcd-cafile=/k8s/etcd/ssl/ca.pem \</span><br><span class="line">--etcd-certfile=/k8s/etcd/ssl/server.pem \</span><br><span class="line">--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot;</span><br></pre></td></tr></table></figure></p>
<p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-apiserver</span><br><span class="line">systemctl start kube-apiserver</span><br><span class="line">[root@elasticsearch01 bin]# systemctl status kube-apiserver</span><br><span class="line">● kube-apiserver.service - Kubernetes API Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 705 (kube-apiserver)</span><br><span class="line">   CGroup: /system.slice/kube-apiserver.service</span><br><span class="line">           └─705 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --s...</span><br><span class="line"></span><br><span class="line">5月 13 16:00:43 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:43.495504     705 wrap.go:47] GET /api/v1/namespaces/default/endpoints/kubernetes: (3.700854ms) 200 [kube-apiserver/v1.13.1 (linux/amd64) kubernetes/eec55b9 10.8.13.81:56744]</span><br><span class="line">5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.955530     705 wrap.go:47] GET /api/v1/services?resourceVersion=37540&amp;timeout=6m29s&amp;timeoutSeconds=389&amp;watch=true: (6m29.001574609s) 200 [kube-proxy/v1.13.1 (linux/amd64) kub... 10.8.13.81:56844]</span><br><span class="line">5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.958607     705 get.go:247] Starting watch for /api/v1/services, rv=37540 labels= fields= timeout=8m28s</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.323978     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.410282ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.371766     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (3.606335ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.376888     705 wrap.go:47] GET /apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=32859&amp;timeout=5m5s&amp;timeoutSeconds=305&amp;watch=true: (5m5.001015872s) 200 [kube-apiser... 10.8.13.81:56744]</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.377312     705 reflector.go:357] k8s.io/kube-aggregator/pkg/client/informers/internalversion/factory.go:117: Watch close - *apiregistration.APIService total 0 items received</span><br><span class="line">5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.378469     705 get.go:247] Starting watch for /apis/apiregistration.k8s.io/v1/apiservices, rv=32859 labels= fields= timeout=8m12s</span><br><span class="line">5月 13 16:00:49 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:49.206602     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (4.541086ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]</span><br><span class="line">5月 13 16:00:50 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:50.027213     705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.418662ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show in full.</span><br><span class="line"></span><br><span class="line">[root@master01 bin]# ps -ef |grep kube-apiserver</span><br><span class="line">root       705     1  3 5月10 ?       02:35:10 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --service-cluster-ip-range=10.254.0.0/16 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth --token-auth-file=/k8s/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/k8s/kubernetes/ssl/server.pem --tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem --client-ca-file=/k8s/kubernetes/ssl/ca.pem --service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem --etcd-cafile=/k8s/etcd/ssl/ca.pem --etcd-certfile=/k8s/etcd/ssl/server.pem --etcd-keyfile=/k8s/etcd/ssl/server-key.pem</span><br><span class="line">root      7098 24767  0 15:57 pts/0    00:00:00 grep --color=auto kube-apiserver</span><br><span class="line">[root@master01 bin]# netstat -tulpn |grep kube-apiserve</span><br><span class="line">tcp        0      0 10.8.13.81:6443         0.0.0.0:*               LISTEN      705/kube-apiserver  </span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      705/kube-apiserver</span><br></pre></td></tr></table></figure></p>
<p>3）部署kube-scheduler组件<br>创建kube-scheduler配置文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim  /k8s/kubernetes/cfg/kube-scheduler </span><br><span class="line">KUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot;</span><br></pre></td></tr></table></figure></p>
<p>参数备注：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；</span><br><span class="line">--kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</span><br><span class="line">--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</span><br><span class="line">创建kube-scheduler systemd文件</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-scheduler.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>
<p>拷贝master01 kube-scheduler配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.82:/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.83:/k8s/kubernetes/cfg/kube-scheduler</span><br><span class="line">scp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.82:/usr/lib/systemd/system/kube-scheduler.service</span><br><span class="line">scp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.83:/usr/lib/systemd/system/kube-scheduler.service</span><br></pre></td></tr></table></figure></p>
<p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-scheduler.service </span><br><span class="line">systemctl start kube-scheduler.service</span><br><span class="line">[root@master01 bin]# systemctl status kube-scheduler.service</span><br><span class="line">● kube-scheduler.service - Kubernetes Scheduler</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 693 (kube-scheduler)</span><br><span class="line">   CGroup: /system.slice/kube-scheduler.service</span><br><span class="line">           └─693 /k8s/kubernetes/bin/kube-scheduler --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect</span><br><span class="line"></span><br><span class="line">5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024121     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024161     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151743     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151799     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434965     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434999     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571674     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571707     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br><span class="line">5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914369     693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914411     693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler</span><br></pre></td></tr></table></figure></p>
<p>4）部署kube-controller-manager组件</p>
<p>创建kube-controller-manager配置文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">KUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--master=127.0.0.1:8080 \</span><br><span class="line">--leader-elect=true \</span><br><span class="line">--address=127.0.0.1 \</span><br><span class="line">--service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">--cluster-name=kubernetes \</span><br><span class="line">--cluster-signing-cert-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--cluster-signing-key-file=/k8s/kubernetes/ssl/ca-key.pem  \</span><br><span class="line">--root-ca-file=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">--service-account-private-key-file=/k8s/kubernetes/ssl/ca-key.pem&quot;</span><br></pre></td></tr></table></figure></p>
<p>创建kube-controller-manager systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-controller-manager.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/kubernetes/kubernetes</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>拷贝master01 kube-controller-manager配置文件、启动文件到master02和master03对应路径下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.82:/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.83:/k8s/kubernetes/cfg/kube-controller-manager</span><br><span class="line">scp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.82:/usr/lib/systemd/system/kube-controller-manager.service</span><br><span class="line">scp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.83:/usr/lib/systemd/system/kube-controller-manager.service</span><br></pre></td></tr></table></figure></p>
<p>启动服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-controller-manager</span><br><span class="line">systemctl start kube-controller-manager</span><br><span class="line">[root@master01 bin]# systemctl status kube-controller-manager</span><br><span class="line">● kube-controller-manager.service - Kubernetes Controller Manager</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago</span><br><span class="line">     Docs: https://github.com/kubernetes/kubernetes</span><br><span class="line"> Main PID: 685 (kube-controller)</span><br><span class="line">   CGroup: /system.slice/kube-controller-manager.service</span><br><span class="line">           └─685 /k8s/kubernetes/bin/kube-controller-manager --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect=true --address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/k8s/kubernetes/ssl/ca...</span><br><span class="line"></span><br><span class="line">5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539102     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539136     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767187     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767221     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939294     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939329     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212185     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212218     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br><span class="line">5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291399     685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired</span><br><span class="line">5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291430     685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager</span><br></pre></td></tr></table></figure></p>
<h4 id="4-9、验证kubeserver服务"><a href="#4-9、验证kubeserver服务" class="headerlink" title="4.9、验证kubeserver服务"></a>4.9、验证kubeserver服务</h4><p>设置环境变量(==所有服务器都执行此步==)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">PATH=/k8s/kubernetes/bin:$PATH</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>查看master服务状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ~]# kubectl get cs,nodes</span><br><span class="line">NAME                                 STATUS    MESSAGE             ERROR</span><br><span class="line">componentstatus/scheduler            Healthy   ok                  </span><br><span class="line">componentstatus/controller-manager   Healthy   ok                  </span><br><span class="line">componentstatus/etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure></p>
<p>至此master组件安装完毕</p>
<hr>
<h3 id="五、Node部署-node01、node02安装"><a href="#五、Node部署-node01、node02安装" class="headerlink" title="五、Node部署(node01、node02安装)"></a>五、Node部署(node01、node02安装)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubernetes work 节点运行如下组件：</span><br><span class="line">docker</span><br><span class="line">kubelet</span><br><span class="line">kube-proxy</span><br><span class="line">flannel</span><br></pre></td></tr></table></figure>
<h4 id="5-1-Docker环境安装"><a href="#5-1-Docker环境安装" class="headerlink" title="5.1 Docker环境安装"></a>5.1 Docker环境安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">yum list docker-ce --showduplicates | sort -r</span><br><span class="line">yum install docker-ce -y</span><br><span class="line">systemctl start docker &amp;&amp; systemctl enable docker</span><br></pre></td></tr></table></figure>
<h4 id="5-2-部署kubelet组件"><a href="#5-2-部署kubelet组件" class="headerlink" title="5.2 部署kubelet组件"></a>5.2 部署kubelet组件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如exec、run、logs 等;</span><br><span class="line">kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况;</span><br><span class="line">为确保安全，只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如apiserver、heapster)。</span><br></pre></td></tr></table></figure>
<p>1)、安装二进制文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.13.1/kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">tar zxvf kubernetes-node-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes/node/bin/</span><br><span class="line">cp kube-proxy kubelet kubectl /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure></p>
<p>2)、从master01复制相关证书到node01和node02节点<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# cd /k8s/kubernetes/ssl/</span><br><span class="line">[root@master01 ssl]# scp *.pem 10.8.13.84:/k8s/kubernetes/ssl/</span><br><span class="line">root@10.8.13.84&apos;s password: </span><br><span class="line">ca-key.pem                                                                                         100% 1679   914.6KB/s   00:00    </span><br><span class="line">ca.pem                                                                                             100% 1359     1.0MB/s   00:00    </span><br><span class="line">kube-proxy-key.pem                                                                                 100% 1675     1.2MB/s   00:00    </span><br><span class="line">kube-proxy.pem                                                                                     100% 1403     1.1MB/s   00:00    </span><br><span class="line">server-key.pem                                                                                     100% 1679   809.1KB/s   00:00    </span><br><span class="line">server.pem                                                                                         100% 1675     1.2MB/s   00:00</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.84:/k8s/etcd/ssl/</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.84:/k8s/etcd/bin/</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# scp *.pem 10.8.13.85:/k8s/kubernetes/ssl/</span><br><span class="line">root@10.8.13.85&apos;s password: </span><br><span class="line">ca-key.pem                                                                                         100% 1679   914.6KB/s   00:00    </span><br><span class="line">ca.pem                                                                                             100% 1359     1.0MB/s   00:00    </span><br><span class="line">kube-proxy-key.pem                                                                                 100% 1675     1.2MB/s   00:00    </span><br><span class="line">kube-proxy.pem                                                                                     100% 1403     1.1MB/s   00:00    </span><br><span class="line">server-key.pem                                                                                     100% 1679   809.1KB/s   00:00    </span><br><span class="line">server.pem                                                                                         100% 1675     1.2MB/s   00:00</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.85:/k8s/etcd/ssl/</span><br><span class="line">[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.85:/k8s/etcd/bin/</span><br></pre></td></tr></table></figure>
<p>3)、创建kubelet bootstrap kubeconfig文件</p>
<p>通过脚本实现<br>KUBE_APISERVER=vip:haproxy中自定义的端口<br>BOOTSTRAP_TOKEN=部署kube-apiserver中生成的token<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/environment.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">#创建kubelet bootstrapping kubeconfig </span><br><span class="line">BOOTSTRAP_TOKEN=af93a4194e7bcf7f05dc0bab3a6e97cd</span><br><span class="line">KUBE_APISERVER=&quot;https://10.8.13.80:16443&quot;</span><br><span class="line">#设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line">#设置客户端认证参数</span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span><br><span class="line"> </span><br><span class="line">#----------------------</span><br><span class="line"> </span><br><span class="line"># 创建kube-proxy kubeconfig文件</span><br><span class="line"> </span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/k8s/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate=/k8s/kubernetes/ssl/kube-proxy.pem \</span><br><span class="line">  --client-key=/k8s/kubernetes/ssl/kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"> </span><br><span class="line">kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure></p>
<p>执行脚本<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 cfg]# cd /k8s/kubernetes/cfg/</span><br><span class="line">[root@node01 cfg]# sh environment.sh </span><br><span class="line">Cluster &quot;kubernetes&quot; set.</span><br><span class="line">User &quot;kubelet-bootstrap&quot; set.</span><br><span class="line">Context &quot;default&quot; created.</span><br><span class="line">Switched to context &quot;default&quot;.</span><br><span class="line">Cluster &quot;kubernetes&quot; set.</span><br><span class="line">User &quot;kube-proxy&quot; set.</span><br><span class="line">Context &quot;default&quot; created.</span><br><span class="line">Switched to context &quot;default&quot;.</span><br><span class="line">[root@node01 cfg]# ls</span><br><span class="line">bootstrap.kubeconfig  environment.sh  kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure></p>
<p>4)、创建kubelet参数配置模板文件</p>
<p>address:node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet.config</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 10.8.13.84</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">clusterDNS: [&quot;10.254.0.10&quot;]</span><br><span class="line">clusterDomain: cluster.local.</span><br><span class="line">failSwapOn: false</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure></p>
<p>5)、创建kubelet配置文件</p>
<p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet</span><br><span class="line"> </span><br><span class="line">KUBELET_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.84 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \</span><br><span class="line">--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \</span><br><span class="line">--config=/k8s/kubernetes/cfg/kubelet.config \</span><br><span class="line">--cert-dir=/k8s/kubernetes/ssl \</span><br><span class="line">--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;</span><br></pre></td></tr></table></figure></p>
<p>6)、创建kubelet systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kubelet.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=/k8s/kubernetes/cfg/kubelet</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kubelet $KUBELET_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line">KillMode=process</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>7)、将kubelet-bootstrap用户绑定到系统集群角色(==在master01执行==)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kubelet-bootstrap \</span><br><span class="line">  --clusterrole=system:node-bootstrapper \</span><br><span class="line">  --user=kubelet-bootstrap</span><br></pre></td></tr></table></figure></p>
<p>注意这个默认连接localhost:8080端口，可以在master上操作<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl create clusterrolebinding kubelet-bootstrap \</span><br><span class="line">&gt;   --clusterrole=system:node-bootstrapper \</span><br><span class="line">&gt;   --user=kubelet-bootstrap</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created</span><br></pre></td></tr></table></figure></p>
<p>8)、复制node01kubelet配置和启动服务文件到node02相对应路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/* 10.8.13.85:/k8s/kubernetes/cfg/</span><br><span class="line">scp /usr/lib/systemd/system/kubelet.service 10.8.13.85:/usr/lib/systemd/system/kubelet.service</span><br></pre></td></tr></table></figure></p>
<p>9)、修改node02中kubelet.config和kubelet文件中的nodeIP</p>
<p>node02中kubelet.config配置</p>
<p>address:node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet.config</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 10.8.13.85</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">clusterDNS: [&quot;10.254.0.10&quot;]</span><br><span class="line">clusterDomain: cluster.local.</span><br><span class="line">failSwapOn: false</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure></p>
<p>node02中kubelet配置</p>
<p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kubelet</span><br><span class="line"> </span><br><span class="line">KUBELET_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.85 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \</span><br><span class="line">--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \</span><br><span class="line">--config=/k8s/kubernetes/cfg/kubelet.config \</span><br><span class="line">--cert-dir=/k8s/kubernetes/ssl \</span><br><span class="line">--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;</span><br></pre></td></tr></table></figure></p>
<p>10)、启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">systemctl start kubelet</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# systemctl status kubelet</span><br><span class="line">● kubelet.service - Kubernetes Kubelet</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-05-10 20:31:30 CST; 3 days ago</span><br><span class="line"> Main PID: 8583 (kubelet)</span><br><span class="line">   Memory: 45.5M</span><br><span class="line">   CGroup: /system.slice/kubelet.service</span><br><span class="line">           └─8583 /k8s/kubernetes/bin/kubelet --logtostderr=true --v=4 --hostname-override=10.8.13.84 --kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig --config=/k8s/kubernetes/cfg/kubelet.config --cer...</span><br></pre></td></tr></table></figure>
<p>11)、Master接受kubelet CSR请求(master01操作，接受两个node节点)<br>可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书，如下是手动 approve CSR请求操作方法</p>
<p>查看CSR列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl get csr</span><br><span class="line">NAME                                                   AGE    REQUESTOR           CONDITION</span><br><span class="line">node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc   102s   kubelet-bootstrap   Pending</span><br></pre></td></tr></table></figure>
<p>接受node</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl certificate approve node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc approved</span><br></pre></td></tr></table></figure>
<p>再查看CSR</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ssl]# kubectl get csr</span><br><span class="line">NAME                                                   AGE     REQUESTOR           CONDITION</span><br><span class="line">node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc   5m13s   kubelet-bootstrap   Approved,Issued</span><br></pre></td></tr></table></figure>
<h4 id="5-3部署kube-proxy组件-node01执行"><a href="#5-3部署kube-proxy组件-node01执行" class="headerlink" title="5.3部署kube-proxy组件(node01执行)"></a>5.3部署kube-proxy组件(node01执行)</h4><p>kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡</p>
<p>1)、创建 kube-proxy 配置文件</p>
<p>–hostname-override=node节点IP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">KUBE_PROXY_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.84 \</span><br><span class="line">--cluster-cidr=10.254.0.0/16 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot;</span><br></pre></td></tr></table></figure>
<p>2)、创建kube-proxy systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/kube-proxy.service </span><br><span class="line"> </span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Proxy</span><br><span class="line">After=network.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>3)、复制node01kube-proxy配置和服务启动文件到node02相对应路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp /k8s/kubernetes/cfg/kube-proxy 10.8.13.85:/k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">scp /usr/lib/systemd/system/kube-proxy.service 10.8.13.85:/usr/lib/systemd/system/kube-proxy.service</span><br></pre></td></tr></table></figure></p>
<p>4)、修改node02kube-proxy配置文件如下</p>
<p>–hostname-override=node节点IP<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/kube-proxy</span><br><span class="line">KUBE_PROXY_OPTS=&quot;--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=10.8.13.85 \</span><br><span class="line">--cluster-cidr=10.254.0.0/16 \</span><br><span class="line">--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot;</span><br></pre></td></tr></table></figure></p>
<p>5)、启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable kube-proxy</span><br><span class="line">systemctl start kube-proxy</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# systemctl status kube-proxy.service </span><br><span class="line">● kube-proxy.service - Kubernetes Proxy</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-05-10 20:31:31 CST; 3 days ago</span><br><span class="line"> Main PID: 8669 (kube-proxy)</span><br><span class="line">   Memory: 9.9M</span><br><span class="line">   CGroup: /system.slice/kube-proxy.service</span><br><span class="line">           ‣ 8669 /k8s/kubernetes/bin/kube-proxy --logtostderr=true --v=4 --hostname-override=10.8.13.84 --cluster-cidr=10.254.0.0/16 --kubeconfig...</span><br><span class="line"></span><br><span class="line">May 14 09:07:50 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:50.634641    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:51 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:51.365166    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:52 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:52.647317    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:53 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:53.375833    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:54 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:54.658691    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:55 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:55.387881    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:56 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:56.670562    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:57 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:57.398763    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:58 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:58.682049    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br><span class="line">May 14 09:07:59 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:59.411141    8669 config.go:141] Calling handler.OnEndpointsUpdate</span><br></pre></td></tr></table></figure>
<p>6)、查看集群状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES    AGE     VERSION</span><br><span class="line">10.8.13.84   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line">10.8.13.85   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br></pre></td></tr></table></figure></p>
<p>至此node组件安装完成</p>
<hr>
<h3 id="六、Flanneld网络部署-以node01为例，node02同样操作"><a href="#六、Flanneld网络部署-以node01为例，node02同样操作" class="headerlink" title="六、Flanneld网络部署(以node01为例，node02同样操作)"></a>六、Flanneld网络部署(以node01为例，node02同样操作)</h3><p>默认没有flanneld网络，Node节点间的pod不能通信，只能Node内通信，为了部署步骤简洁明了，故flanneld放在后面安装<br>flannel服务需要先于docker启动。flannel服务启动时主要做了以下几步的工作：</p>
<ul>
<li>从etcd中获取network的配置信息</li>
<li>划分subnet，并在etcd中进行注册</li>
<li>将子网信息记录到/run/flannel/subnet.env中</li>
</ul>
<h4 id="6-1-etcd注册网段"><a href="#6-1-etcd注册网段" class="headerlink" title="6.1 etcd注册网段"></a>6.1 etcd注册网段</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# /k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379&quot;  set /k8s/network/config  &apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;</span><br><span class="line">&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；<br>写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；</p>
<h4 id="6-2-flannel安装"><a href="#6-2-flannel安装" class="headerlink" title="6.2 flannel安装"></a>6.2 flannel安装</h4><p>1)、解压安装<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxf flannel-v0.11.0-linux-amd64.tar.gz</span><br><span class="line">mv flanneld mk-docker-opts.sh /k8s/kubernetes/bin/</span><br></pre></td></tr></table></figure></p>
<p>2)、配置flanneld<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /k8s/kubernetes/cfg/flanneld</span><br><span class="line">FLANNEL_OPTIONS=&quot;--etcd-endpoints=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379 -etcd-cafile=/k8s/etcd/ssl/ca.pem -etcd-certfile=/k8s/etcd/ssl/server.pem -etcd-keyfile=/k8s/etcd/ssl/server-key.pem -etcd-prefix=/k8s/network&quot;</span><br></pre></td></tr></table></figure></p>
<p>3)、创建flanneld systemd文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/flanneld.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network-online.target network.target</span><br><span class="line">Before=docker.service</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/k8s/kubernetes/cfg/flanneld</span><br><span class="line">ExecStart=/k8s/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS</span><br><span class="line">ExecStartPost=/k8s/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env</span><br><span class="line">Restart=on-failure</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure></p>
<p>==注意：==</p>
<ul>
<li>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥；</li>
<li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口;</li>
<li>flanneld 运行时需要 root 权限；</li>
</ul>
<p>3）配置Docker启动指定子网</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">添加EnvironmentFile=/run/flannel/subnet.env，修改ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS即可</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/docker.service </span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">After=network-online.target firewalld.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/run/flannel/subnet.env</span><br><span class="line">ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">StartLimitBurst=3</span><br><span class="line">StartLimitInterval=60s</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>
<p>4)、启动服务</p>
<p>注意启动flannel前要关闭docker及相关的kubelet这样flannel才会覆盖docker0网桥<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl stop docker</span><br><span class="line">systemctl start flanneld</span><br><span class="line">systemctl enable flanneld</span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart kube-proxy</span><br></pre></td></tr></table></figure></p>
<p>5)、验证服务<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 bin]# cat /run/flannel/subnet.env </span><br><span class="line">DOCKER_OPT_BIP=&quot;--bip=10.254.88.1/24&quot;</span><br><span class="line">DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;</span><br><span class="line">DOCKER_OPT_MTU=&quot;--mtu=1450&quot;</span><br><span class="line">DOCKER_NETWORK_OPTIONS=&quot; --bip=10.254.88.1/24 --ip-masq=false --mtu=1450&quot;</span><br></pre></td></tr></table></figure></p>
<p>注意查看docker0和flannel是不是属于同一网段<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 00:50:56:90:67:d1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.8.13.84/24 brd 10.8.13.255 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::802:2c0f:a197:38a7/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </span><br><span class="line">    link/ether 02:42:5c:18:5b:93 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.254.88.1/24 brd 10.254.88.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:5cff:fe18:5b93/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </span><br><span class="line">    link/ether 8e:f6:f8:87:47:ee brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.254.88.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::8cf6:f8ff:fe87:47ee/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<p>至此flannel安装完成</p>
<hr>
<h3 id="查看NODE和etcd"><a href="#查看NODE和etcd" class="headerlink" title="查看NODE和etcd"></a>查看NODE和etcd</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@hwzx-test-cmpmaster01 ~]# kubectl get nodes,cs</span><br><span class="line">NAME              STATUS   ROLES    AGE     VERSION</span><br><span class="line">node/10.8.13.84   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line">node/10.8.13.85   Ready    &lt;none&gt;   3d13h   v1.14.1</span><br><span class="line"></span><br><span class="line">NAME                                 STATUS    MESSAGE             ERROR</span><br><span class="line">componentstatus/controller-manager   Healthy   ok                  </span><br><span class="line">componentstatus/scheduler            Healthy   ok                  </span><br><span class="line">componentstatus/etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">componentstatus/etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/K8s">K8s</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/Kubernets">Kubernets</a>
                
                  <a class="hover-with-bg" href="/tags/K8S">K8S</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    
  <!-- 备案信息 -->
  <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">苏ICP备16041225号-2</a>
  



    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/popper/popper.min.js"></script>
<script src="/lib/bootstrap/js/bootstrap.min.js"></script>
<script src="/lib/mdbootstrap/js/mdb.min.js"></script>
<script src="/js/main.js"></script>


  <script src="/js/lazyload.js"></script>



  
    <script src="/lib/tocbot/tocbot.min.js"></script>
  
  <script src="/js/post.js"></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js"></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js"></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  ');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js"></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "生产级k8s二进制v14.1高可用部署&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js"></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js"></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js"></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>





  
  
    <script>
      !function (e, t, a) {
        function r() {
          for (var e = 0; e < s.length; e++) s[e].alpha <= 0 ? (t.body.removeChild(s[e].el), s.splice(e, 1)) : (s[e].y--, s[e].scale += .004, s[e].alpha -= .013, s[e].el.style.cssText = "left:" + s[e].x + "px;top:" + s[e].y + "px;opacity:" + s[e].alpha + ";transform:scale(" + s[e].scale + "," + s[e].scale + ") rotate(45deg);background:" + s[e].color + ";z-index:99999");
          requestAnimationFrame(r)
        }

        function n() {
          var t = "function" == typeof e.onclick && e.onclick;
          e.onclick = function (e) {
            t && t(), o(e)
          }
        }

        function o(e) {
          var a = t.createElement("div");
          a.className = "heart", s.push({
            el: a,
            x: e.clientX - 5,
            y: e.clientY - 5,
            scale: 1,
            alpha: 1,
            color: c()
          }), t.body.appendChild(a)
        }

        function i(e) {
          var a = t.createElement("style");
          a.type = "text/css";
          try {
            a.appendChild(t.createTextNode(e))
          } catch (t) {
            a.styleSheet.cssText = e
          }
          t.getElementsByTagName("head")[0].appendChild(a)
        }

        function c() {
          return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
        }

        var s = [];
        e.requestAnimationFrame = e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function (e) {
          setTimeout(e, 1e3 / 60)
        }, i(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"), n(), r()
      }(window, document);
    </script>
  







</body>
</html>
