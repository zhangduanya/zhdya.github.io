<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>K8S深度理解弹性伸缩 | 拼！就对了！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="laoqi,laoqi's Blog">
  
  <meta name="description" content="二、弹性伸缩2.1 传统弹性伸缩的困境从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。  蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 1、Kubernetes中弹性伸缩存在的问题常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kube">
<meta name="keywords" content="Kubernets,K8S">
<meta property="og:type" content="article">
<meta property="og:title" content="K8S深度理解弹性伸缩">
<meta property="og:url" content="http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/index.html">
<meta property="og:site_name" content="拼！就对了！">
<meta property="og:description" content="二、弹性伸缩2.1 传统弹性伸缩的困境从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。  蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 1、Kubernetes中弹性伸缩存在的问题常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kube">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-vs.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-machine-config.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-up.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-down.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/ansible-node-scaler.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-1.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-2.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/aggergation.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-3.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-4.png">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/prometheus-arch.png">
<meta property="og:image" content="http://myimage.okay686.cn/okay686cn/20191212/fo1z9VXaJWiH.png?imageslim">
<meta property="og:image" content="http://myimage.okay686.cn/okay686cn/20191212/9W3JdYyaEVcC.png?imageslim">
<meta property="og:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-5.png">
<meta property="og:updated_time" content="2019-12-13T02:46:00.045Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="K8S深度理解弹性伸缩">
<meta name="twitter:description" content="二、弹性伸缩2.1 传统弹性伸缩的困境从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。  蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 1、Kubernetes中弹性伸缩存在的问题常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kube">
<meta name="twitter:image" content="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-vs.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  

  
  

</head>
</html>
<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">五谷杂粮，百味人生。</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a href="/">
                        <i class="fa fa-home"></i>
                        <span>主页</span>
                    </a>
                    
                    <a href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>归档</span>
                    </a>
                    
                    <a href="/about">
                        <i class="fa fa-user"></i>
                        <span>关于</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.png" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        五谷杂粮，百味人生。
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        K8S生态，Django，python专列！
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="Github" target="_blank" href="//github.com/zhangduanya">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-K8S深度理解弹性伸缩" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      K8S深度理解弹性伸缩
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/K8s/">K8s</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2019-12-13
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <h3 id="二、弹性伸缩"><a href="#二、弹性伸缩" class="headerlink" title="二、弹性伸缩"></a>二、弹性伸缩</h3><h4 id="2-1-传统弹性伸缩的困境"><a href="#2-1-传统弹性伸缩的困境" class="headerlink" title="2.1 传统弹性伸缩的困境"></a>2.1 传统弹性伸缩的困境</h4><p>从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-vs.png" alt=""></p>
<p>蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。</p>
<p>弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。</p>
<h3 id="1、Kubernetes中弹性伸缩存在的问题"><a href="#1、Kubernetes中弹性伸缩存在的问题" class="headerlink" title="1、Kubernetes中弹性伸缩存在的问题"></a>1、Kubernetes中弹性伸缩存在的问题</h3><p>常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kubernetes中，就会发现如下2个问题。</p>
<ol>
<li><p><strong>机器规格不统一造成机器利用率百分比碎片化</strong></p>
<p>在一个Kubernetes集群中，通常不只包含一种规格的机器，假设集群中存在4C8G与16C32G两种规格的机器，对于10%的资源预留，这两种规格代表的意义是完全不同的。</p>
</li>
</ol>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/scaling-machine-config.png" alt=""></p>
<p>特别是在缩容的场景下，为了保证缩容后集群稳定性，我们一般会一个节点一个节点从集群中摘除，那么如何判断节点是否可以摘除其利用率百分比就是重要的指标。此时如果大规格机器有较低的利用率被判断缩容，那么很有可能会造成节点缩容后，容器重新调度后的争抢。如果优先缩容小规格机器，则可能造成缩容后资源的大量冗余。</p>
<ol start="2">
<li><p><strong>机器利用率不单纯依靠宿主机计算</strong></p>
<p>在大部分生产环境中，资源利用率都不会保持一个高的水位，但从调度来讲，调度应该保持一个比较高的水位，这样才能保障集群稳定性，又不过多浪费资源。</p>
</li>
</ol>
<h3 id="2、弹性伸缩概念的延伸"><a href="#2、弹性伸缩概念的延伸" class="headerlink" title="2、弹性伸缩概念的延伸"></a>2、弹性伸缩概念的延伸</h3><p>不是所有的业务都存在峰值流量，越来越细分的业务形态带来更多成本节省和可用性之间的跳转。</p>
<ol>
<li>在线负载型：微服务、网站、API</li>
<li>离线任务型：离线计算、机器学习</li>
<li>定时任务型：定时批量计算</li>
</ol>
<p>不同类型的负载对于弹性伸缩的要求有所不同，在线负载对弹出时间敏感，离线任务对价格敏感，定时任务对调度敏感。</p>
<h4 id="2-2-kubernetes-弹性伸缩布局"><a href="#2-2-kubernetes-弹性伸缩布局" class="headerlink" title="2.2 kubernetes 弹性伸缩布局"></a>2.2 kubernetes 弹性伸缩布局</h4><p>在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。</p>
<p>有三种弹性伸缩：</p>
<ul>
<li><p>CA（Cluster Autoscaler）：Node级别自动扩/缩容</p>
<p>cluster-autoscaler组件</p>
</li>
<li><p>HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容</p>
</li>
<li><p>VPA（Vertical Pod Autoscaler）：Pod配置自动扩/缩容，主要是CPU、内存</p>
<p>addon-resizer组件</p>
</li>
</ul>
<p>如果在云上建议 HPA 结合 cluster-autoscaler 的方式进行集群的弹性伸缩管理。</p>
<h4 id="2-3-Node-自动扩容-缩容"><a href="#2-3-Node-自动扩容-缩容" class="headerlink" title="2.3 Node 自动扩容/缩容"></a>2.3 Node 自动扩容/缩容</h4><h5 id="1、Cluster-AutoScaler"><a href="#1、Cluster-AutoScaler" class="headerlink" title="1、Cluster AutoScaler"></a>1、Cluster AutoScaler</h5><p><strong>扩容：</strong>Cluster AutoScaler 定期检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-up.png" alt=""></p>
<p><strong>缩容：</strong>Cluster AutoScaler 也会定期监测 Node 的资源使用情况，当一个 Node 长时间资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除。此时，原来的 Pod 会自动调度到其他 Node 上面。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/cluster-autoscaler-down.png" alt=""></p>
<p>支持的云提供商：</p>
<ul>
<li>阿里云：<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md</a></li>
<li>AWS： <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li>
<li>Azure： <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md</a></li>
</ul>
<h5 id="2、Ansible扩容Node"><a href="#2、Ansible扩容Node" class="headerlink" title="2、Ansible扩容Node"></a>2、Ansible扩容Node</h5><p><strong>自动化流程：</strong></p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/ansible-node-scaler.png" alt=""></p>
<ol>
<li>触发新增Node</li>
<li>调用Ansible脚本部署组件</li>
<li>检查服务是否可用</li>
<li>调用API将新Node加入集群或者启用Node自动加入</li>
<li>观察新Node状态</li>
<li>完成Node扩容，接收新Pod</li>
</ol>
<p><strong>扩容</strong></p>
<pre><code># cat hosts 
...
[newnode]
192.168.31.71 node_name=k8s-node3
# ansible-playbook -i hosts add-node.yml -k
</code></pre><p><strong>缩容</strong></p>
<p>如果你想从Kubernetes集群中删除节点，正确流程如下：</p>
<p><strong>1、获取节点列表</strong></p>
<pre><code>kubectl get node
</code></pre><p><strong>2、设置不可调度</strong></p>
<pre><code>kubectl cordon $node_name
</code></pre><p><strong>3、驱逐节点上的Pod</strong></p>
<pre><code>kubectl drain $node_name --ignore-daemonsets
</code></pre><p><strong>4、移除节点</strong></p>
<p>该节点上已经没有任何资源了，可以直接移除节点：</p>
<pre><code>kubectl delete node $node_name
</code></pre><p>这样，我们平滑移除了一个 k8s 节点。</p>
<h4 id="2-4-Pod自动扩容-缩容（HPA）"><a href="#2-4-Pod自动扩容-缩容（HPA）" class="headerlink" title="2.4 Pod自动扩容/缩容（HPA）"></a>2.4 Pod自动扩容/缩容（HPA）</h4><p>Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据资源利用率或者自定义指标自动调整replication controller, deployment 或 replica set，实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适于无法缩放的对象，例如DaemonSet。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-1.png" alt=""></p>
<h5 id="1、HPA基本原理"><a href="#1、HPA基本原理" class="headerlink" title="1、HPA基本原理"></a>1、HPA基本原理</h5><p>Kubernetes 中的 Metrics Server 持续采集所有 Pod 副本的指标数据。HPA 控制器通过 Metrics Server 的 API（Heapster 的 API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod 副本数量。当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。如图所示。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-2.png"></p>
<p>在弹性伸缩中，冷却周期是不能逃避的一个话题， 由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸， 所以在每次做出扩容缩容后，冷却时间是多少。</p>
<p>在 HPA 中，<strong>默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。</strong></p>
<p>可以通过调整kube-controller-manager组件启动参数设置冷却时间：</p>
<ul>
<li>–horizontal-pod-autoscaler-downscale-delay ：扩容冷却</li>
<li>–horizontal-pod-autoscaler-upscale-delay ：缩容冷却</li>
</ul>
<h5 id="2、HPA的演进历程"><a href="#2、HPA的演进历程" class="headerlink" title="2、HPA的演进历程"></a>2、HPA的演进历程</h5><p>目前 HPA 已经支持了 autoscaling/v1、autoscaling/v2beta1和autoscaling/v2beta2  三个大版本 。</p>
<p>目前大多数人比较熟悉是autoscaling/v1，这个版本只支持CPU一个指标的弹性伸缩。</p>
<p>而autoscaling/v2beta1增加了支持自定义指标，autoscaling/v2beta2又额外增加了外部指标支持。</p>
<p>而产生这些变化不得不提的是Kubernetes社区对监控与监控指标的认识与转变。从早期Heapster到Metrics Server再到将指标边界进行划分，一直在丰富监控生态。</p>
<p>示例：</p>
<p>v1版本：</p>
<pre><code>apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache    ##指定对象
  minReplicas: 1        ##指定副本范围
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50        ##指定阈值
</code></pre><p>v2beta2版本：（可以基于resource，pod，object，external等）</p>
<pre><code>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache    ##选择对象
  minReplicas: 1        ##范围
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu     ##基于cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods      ##基于pod
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object    ##基于qps
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
  - type: External
    external:
      metric:
        name: queue_messages_ready
        selector: &quot;queue=worker_tasks&quot;
      target:
        type: AverageValue
        averageValue: 30
</code></pre><h4 id="2-5-基于CPU指标缩放"><a href="#2-5-基于CPU指标缩放" class="headerlink" title="2.5 基于CPU指标缩放"></a>2.5 基于CPU指标缩放</h4><h5 id="1、-Kubernetes-API-Aggregation"><a href="#1、-Kubernetes-API-Aggregation" class="headerlink" title="1、 Kubernetes API Aggregation"></a>1、 Kubernetes API Aggregation</h5><p>什么是k8sapi聚合？</p>
<pre><code>它是允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。
</code></pre><p>当然看如下图↓：你也可以把agg当作一个nginx的反代，它就是代理层。</p>
<p>在 Kubernetes 1.7 版本引入了聚合层，允许第三方应用程序通过将自己注册到kube-apiserver上，仍然通过 API Server 的 HTTP URL 对新的 API 进行访问和操作。为了实现这个机制，Kubernetes 在 kube-apiserver 服务中引入了一个 API 聚合层（API Aggregation Layer），用于将扩展 API 的访问请求转发到用户服务的功能。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/aggergation.png" alt=""></p>
<p>当你访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端 。通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。</p>
<p>如果你使用kubeadm部署的，默认已开启。<strong>如果你使用==二进制方式部署==的话，需要在kube-APIServer中添加启动参数</strong>，增加以下配置：</p>
<pre><code># vi /opt/kubernetes/cfg/kube-apiserver.conf
...
--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \
--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \
--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \
--requestheader-allowed-names=kubernetes \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
--requestheader-group-headers=X-Remote-Group \
--requestheader-username-headers=X-Remote-User \
--enable-aggregator-routing=true \
...
</code></pre><p>  在设置完成重启 kube-apiserver 服务，就启用 API 聚合功能了。  </p>
<h4 id="2、部署-Metrics-Server"><a href="#2、部署-Metrics-Server" class="headerlink" title="2、部署 Metrics Server"></a>2、部署 Metrics Server</h4><p>Metrics Server是一个集群范围的资源使用情况的数据聚合器。作为一个应用部署在集群中。</p>
<p>Metric server从每个节点上Kubelet公开的摘要API收集指标。 </p>
<p>Metrics server通过Kubernetes聚合器注册在Master APIServer中。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-3.png" alt=""></p>
<pre><code># git clone https://github.com/kubernetes-incubator/metrics-server
# cd metrics-server/deploy/1.8+/
# vi metrics-server-deployment.yaml   # 添加2条启动参数   
...
      containers:
      - name: metrics-server
        image: lizhenliang/metrics-server-amd64:v0.3.1
        command:
        - /metrics-server
        - --kubelet-insecure-tls        ##忽略证书的验证
        - --kubelet-preferred-address-types=InternalIP      ##一般node都是用主机名注册的，但是metricserver是通过pod启动的，解析不到hostname，所以需要采集节点的IP
...
# kubectl create -f .
</code></pre><p>可通过Metrics API在Kubernetes中获得资源使用率指标，例如容器CPU和内存使用率。这些度量标准既可以由用户直接访问（例如，通过使用<code>kubectl top</code>命令），也可以由集群中的控制器（例如，Horizontal Pod Autoscaler）用于进行决策。 </p>
<pre><code>[root@k8s-master1 metrics-server]# kubectl get po -n kube-system -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP               NODE          NOMINATED NODE   READINESS GATES
coredns-6d8cfdd59d-pbbbc         1/1     Running   0          131m   10.244.2.2       k8s-node2     &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-4gj8p      1/1     Running   1          142m   192.168.171.11   k8s-master1   &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-k5lfh      1/1     Running   1          75m    192.168.171.14   k8s-node3     &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-kddhv      1/1     Running   1          142m   192.168.171.12   k8s-node1     &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-q8g25      1/1     Running   1          141m   192.168.171.13   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-server-7dbbcf4c7-v5zpm   1/1     Running   0          49s    10.244.2.4       k8s-node2     &lt;none&gt;           &lt;none&gt;
</code></pre><p>查看是否注册到apiservice</p>
<pre><code>[root@k8s-master1 metrics-server]# kubectl get apiservice
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        3m22s
</code></pre><p>测试：</p>
<pre><code>[root@k8s-master1 metrics-server]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes
{&quot;kind&quot;:&quot;NodeMetricsList&quot;,&quot;apiVersion&quot;:&quot;metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:{&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;},&quot;items&quot;:[{&quot;metadata&quot;:{&quot;name&quot;:&quot;k8s-master1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-master1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;},&quot;timestamp&quot;:&quot;2019-12-10T15:40:34Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:{&quot;cpu&quot;:&quot;104718141n&quot;,&quot;memory&quot;:&quot;1158508Ki&quot;}},{&quot;metadata&quot;:{&quot;name&quot;:&quot;k8s-node1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;},&quot;timestamp&quot;:&quot;2019-12-10T15:40:26Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:{&quot;cpu&quot;:&quot;61460193n&quot;,&quot;memory&quot;:&quot;556328Ki&quot;}},{&quot;metadata&quot;:{&quot;name&quot;:&quot;k8s-node2&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node2&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;},&quot;timestamp&quot;:&quot;2019-12-10T15:40:32Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:{&quot;cpu&quot;:&quot;57896643n&quot;,&quot;memory&quot;:&quot;570056Ki&quot;}},{&quot;metadata&quot;:{&quot;name&quot;:&quot;k8s-node3&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node3&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;},&quot;timestamp&quot;:&quot;2019-12-10T15:40:30Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:{&quot;cpu&quot;:&quot;30890872n&quot;,&quot;memory&quot;:&quot;403264Ki&quot;}}]}

[root@k8s-master1 metrics-server]# kubectl top node
NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
k8s-master1   105m         5%     1131Mi          65%
k8s-node1     62m          3%     543Mi           31%
k8s-node2     58m          2%     556Mi           32%
k8s-node3     31m          1%     393Mi           22%

[root@k8s-master1 metrics-server]# kubectl top pod
NAME                  CPU(cores)   MEMORY(bytes)
web-944cddf48-4x6qr   0m           3Mi
web-944cddf48-64d7r   0m           2Mi
web-944cddf48-hjwng   0m           3Mi
web-944cddf48-skh82   0m           3Mi
</code></pre><h4 id="3、autoscaling-v1（CPU指标实践）"><a href="#3、autoscaling-v1（CPU指标实践）" class="headerlink" title="3、autoscaling/v1（CPU指标实践）"></a>3、autoscaling/v1（CPU指标实践）</h4><p>autoscaling/v1版本只支持CPU一个指标。</p>
<p>首先部署一个应用：</p>
<pre><code>[root@k8s-master1 hpa]# kubectl run web --image=nginx --replicas=8 --requests=&quot;cpu=100m,memory=100Mi&quot; --expose 80 --port 80 --dry-run -o yaml &gt;app.yaml

</code></pre><p>创建HPA策略：</p>
<pre><code>[root@k8s-master1 hpa]# kubectl autoscale deployment web --min=2 --max=8 -o yaml --dry-run &gt;hpav1.yaml

[root@k8s-master1 hpa]# kubectl apply -f hpav1.yaml
horizontalpodautoscaler.autoscaling/web created

###cat hpav1.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: web
spec:
  maxReplicas: 8
  minReplicas: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  targetCPUUtilizationPercentage: 60
</code></pre><p>scaleTargetRef：表示当前要伸缩对象是谁</p>
<p>targetCPUUtilizationPercentage：当整体的资源利用率超过50%的时候，会进行扩容。</p>
<p>查看当前状态：</p>
<pre><code>[root@k8s-master1 hpa]# kubectl get hpa
NAME   REFERENCE        TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
web    Deployment/web   0%/60%    2         8         3          24s
</code></pre><p>开启压测：</p>
<pre><code># yum install httpd-tools
# ab -n 100000 -c 100  http://10.1.206.176/status.php
</code></pre><p>10.0.0.147 为ClusterIP。</p>
<p>检查扩容状态：</p>
<pre><code># kubectl get hpa
# kubectl top pods
# kubectl get pods

[root@k8s-master1 hpa]# ab -n 1000000 -c 1000  http://10.0.0.201/index.html    ##压测

关闭压测，过一会检查缩容状态。
[root@k8s-master1 ~]# kubectl top po
NAME                   CPU(cores)   MEMORY(bytes)
web-77cfdb7c6c-dpm5t   116m         5Mi
web-77cfdb7c6c-lwkbj   343m         5Mi
[root@k8s-master1 ~]# kubectl get po
NAME                   READY   STATUS    RESTARTS   AGE
web-77cfdb7c6c-94p6x   1/1     Running   0          7s
web-77cfdb7c6c-9xwbj   1/1     Running   0          23s
web-77cfdb7c6c-dpm5t   1/1     Running   0          33m
web-77cfdb7c6c-gpk6d   1/1     Running   0          7s
web-77cfdb7c6c-l7b4r   1/1     Running   0          23s
web-77cfdb7c6c-lwkbj   1/1     Running   0          33m
web-77cfdb7c6c-w6lz6   1/1     Running   0          7s
web-77cfdb7c6c-wpzb5   1/1     Running   0          7s
</code></pre><p><strong>==工作流程==</strong>：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor)</p>
<h4 id="4、autoscaling-v2beta2（多指标）"><a href="#4、autoscaling-v2beta2（多指标）" class="headerlink" title="4、autoscaling/v2beta2（多指标）"></a>4、autoscaling/v2beta2（多指标）</h4><p>为满足更多的需求， HPA 还有 autoscaling/v2beta1和 autoscaling/v2beta2两个版本。</p>
<p>这两个版本的区别是 autoscaling/v1beta1支持了 Resource Metrics（CPU）和 Custom Metrics（应用程序指标），而在 autoscaling/v2beta2的版本中额外增加了 External Metrics的支持。</p>
<pre><code># kubectl get hpa.v2beta2.autoscaling -o yaml &gt; /tmp/hpa-v2.yaml
</code></pre><pre><code>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: web
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - resource:
    type: Resource
      name: cpu
      target:
        averageUtilization: 60
        type: Utilization
</code></pre><p>与上面v1版本效果一样，只不过这里格式有所变化。</p>
<p>v2还支持其他另种类型的度量指标，：Pods和Object。  </p>
<pre><code>type: Pods
pods:
  metric:
    name: packets-per-second
  target:
    type: AverageValue
    averageValue: 1k
</code></pre><pre><code>type: Object
object:
  metric:
    name: requests-per-second
  describedObject:
    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    name: main-route
  target:
    type: Value
    value: 2k
</code></pre><p>metrics中的type字段有四种类型的值：Object、Pods、Resource、External。 </p>
<ul>
<li><p>Resource：指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。</p>
</li>
<li><p>Object：指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。</p>
</li>
<li><p>Pods：指的是伸缩对象Pods的指标，数据需要第三方的adapter提供，只允许AverageValue类型的目标值。</p>
</li>
<li><p>External：指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。</p>
</li>
</ul>
<pre><code># hpa-v2.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: web
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
</code></pre><p><strong>==工作流程==</strong>：hpa -&gt; apiserver -&gt; kube aggregation  -&gt; prometheus-adapter -&gt; prometheus -&gt; pods</p>
<h3 id="2-6-基于Prometheus自定义指标缩放"><a href="#2-6-基于Prometheus自定义指标缩放" class="headerlink" title="2.6 基于Prometheus自定义指标缩放"></a>2.6 基于Prometheus自定义指标缩放</h3><p>资源指标只包含CPU、内存，一般来说也够了。但如果想根据自定义指标:如请求qps/5xx错误数来实现HPA，就需要使用自定义指标了，目前比较成熟的实现是 Prometheus Custom Metrics。自定义指标由Prometheus来提供，再利用k8s-prometheus-adpater聚合到apiserver，实现和核心指标（metric-server)同样的效果。</p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-4.png" alt=""></p>
<h4 id="1、部署Prometheus"><a href="#1、部署Prometheus" class="headerlink" title="1、部署Prometheus"></a>1、部署Prometheus</h4><p>Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。</p>
<p><strong>Prometheus</strong> <strong>特点：</strong></p>
<ul>
<li><p>自动采集，服务发现；</p>
</li>
<li><p>多维数据模型：由度量名称和键值对标识的时间序列数据；</p>
</li>
<li><p>PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询；</p>
</li>
<li><p>不依赖分布式存储，单个服务器节点可直接工作；</p>
</li>
<li><p>基于HTTP的pull方式采集时间序列数据；</p>
</li>
<li><p>推送时间序列数据通过PushGateway组件支持；</p>
</li>
<li><p>通过服务发现或静态配置发现目标；</p>
</li>
<li><p>多种图形模式及仪表盘支持（grafana）；</p>
</li>
</ul>
<p><strong>Prometheus组成及架构：</strong></p>
<p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/prometheus-arch.png" alt=""></p>
<ul>
<li><p>Prometheus Server：收集指标和存储时间序列数据，并提供查询接口</p>
</li>
<li><p>ClientLibrary：客户端库</p>
</li>
<li><p>Push Gateway：短期存储指标数据。主要用于临时性的任务</p>
</li>
<li><p>Exporters：采集已有的第三方服务监控指标并暴露metrics</p>
</li>
<li><p>Alertmanager：告警</p>
</li>
<li><p>Web UI：简单的Web控制台</p>
</li>
</ul>
<p><strong>部署：</strong></p>
<p>现在node上安装：</p>
<pre><code>[root@k8s-node1 ~]# yum install -y nfs-utils

NFS 配置及使用
我们在服务端创建一个共享目录 /data/share ，作为客户端挂载的远端入口，然后设置权限。

$ mkdir -p /opt/sharedata/
$ chmod 666 /opt/sharedata/

然后，修改 NFS 配置文件 /etc/exports

[root@k8s-node1 ~]# cat /etc/exports
/opt/sharedata 192.168.171.0/24(rw,sync,insecure,no_subtree_check,no_root_squash)

说明一下，这里配置后边有很多参数，每个参数有不同的含义，具体可以参考下边。此处，我配置了将 /data/share 文件目录设置为允许 IP 为该 192.168.171.0/24 区间的客户端挂载，当然，如果客户端 IP 不在该区间也想要挂载的话，可以设置 IP 区间更大或者设置为 * 即允许所有客户端挂载，例如：/home *(ro,sync,insecure,no_root_squash) 设置 /home 目录允许所有客户端只读挂载。

# 启动 NFS 服务
$ service nfs start
# 或者使用如下命令亦可
/bin/systemctl start nfs.service

[root@k8s-node1 ~]# showmount -e localhost
Export list for localhost:
/opt/sharedata 192.168.171.0/24

示例：
挂载远端目录到本地 /share 目录。

$ mount 192.168.171.11:/opt/sharedata /share
$ df -h | grep 192.168.171.11
Filesystem                 Size  Used  Avail Use% Mounted on
192.168.171.11:/opt/sharedata   27G   11G   17G   40%  /share

客户端要卸载 NFS 挂载的话，使用如下命令即可。
$ umount /share
</code></pre><p>现在master上安装：</p>
<pre><code>链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg  提取码：7l3z
从分享包中导入nfs-client.zip

# cd nfs-client
# [root@k8s-master1 nfs-client]# cat deployment.yaml
...省略
serviceAccountName: nfs-client-provisioner
containers:
  - name: nfs-client-provisioner
    image: quay.io/external_storage/nfs-client-provisioner:latest
    volumeMounts:
      - name: nfs-client-root
        mountPath: /persistentvolumes
    env:
      - name: PROVISIONER_NAME
        value: fuseim.pri/ifs
      - name: NFS_SERVER
        value: 192.168.171.12   ##nfs的server地址
      - name: NFS_PATH
        value: /opt/sharedata  ##暴露的目录
volumes:
  - name: nfs-client-root
    nfs:
      server: 192.168.171.12
      path: /opt/sharedata
...省略

[root@k8s-master1 nfs-client]# kubectl apply -f .
storageclass.storage.k8s.io/managed-nfs-storage created
serviceaccount/nfs-client-provisioner created
deployment.apps/nfs-client-provisioner created
serviceaccount/nfs-client-provisioner unchanged
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created

[root@k8s-master1 nfs-client]# kubectl get po
NAME                                    READY   STATUS              RESTARTS   AGE
nfs-client-provisioner-9c784f97-cqzhb   1/1     running   0          2m16s
</code></pre><pre><code>链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg  提取码：7l3z
# cd prometheus
# kubectl apply -f .
[root@k8s-master1 nfs-client]# kubectl get po -o wide -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
coredns-6d8cfdd59d-pbbbc         1/1     Running   2          2d1h    10.244.2.15      k8s-node2     &lt;none&gt;           &lt;none&gt;
kube-flannel-ds-amd64-q8g25      1/1     Running   3          2d1h    192.168.171.13   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-server-7dbbcf4c7-v5zpm   1/1     Running   3          47h     10.244.2.14      k8s-node2     &lt;none&gt;           &lt;none&gt;
prometheus-0                     2/2     Running   0          6m48s   10.244.3.15      k8s-node3     &lt;none&gt;           &lt;none&gt;

[root@k8s-master1 nfs-client]# kubectl get svc -n kube-system
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
kube-dns         ClusterIP   10.0.0.2     &lt;none&gt;        53/UDP,53/TCP    2d1h
metrics-server   ClusterIP   10.0.0.5     &lt;none&gt;        443/TCP          47h
prometheus       NodePort    10.0.0.147   &lt;none&gt;        9090:30090/TCP   7m46s
</code></pre><p>访问Prometheus UI：<a href="http://NdeIP:30090" target="_blank" rel="noopener">http://NdeIP:30090</a></p>
<p><img src="http://myimage.okay686.cn/okay686cn/20191212/fo1z9VXaJWiH.png?imageslim" alt="mark"></p>
<h4 id="2、-部署-Custom-Metrics-Adapter"><a href="#2、-部署-Custom-Metrics-Adapter" class="headerlink" title="2、 部署 Custom Metrics Adapter"></a>2、 部署 Custom Metrics Adapter</h4><p>但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(k8s-prometheus-adpater)，将prometheus的metrics 数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主APIServer中注册，以便直接通过/apis/来访问。</p>
<p> <a href="https://github.com/DirectXMan12/k8s-prometheus-adapter" target="_blank" rel="noopener">https://github.com/DirectXMan12/k8s-prometheus-adapter</a> </p>
<p>该 PrometheusAdapter 有一个稳定的Helm Charts，我们直接使用。</p>
<p>先准备下helm环境：</p>
<pre><code>wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz
tar zxvf helm-v3.0.0-linux-amd64.tar.gz 
mv linux-amd64/helm /usr/bin/
helm repo add stable http://mirror.azure.cn/kubernetes/charts
helm repo update
helm repo list
</code></pre><p>部署prometheus-adapter，指定prometheus地址：</p>
<pre><code># helm install prometheus-adapter stable/prometheus-adapter --namespace kube-system --set prometheus.url=http://prometheus.kube-system,prometheus.port=9090
# helm list -n kube-system
</code></pre><pre><code># kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
prometheus-adapter-77b7b4dd8b-ktsvx   1/1     Running   0          9m
</code></pre><p>确保适配器注册到APIServer：</p>
<pre><code>[root@k8s-master1 ~]# kubectl get apiservices |grep custom
v1beta1.custom.metrics.k8s.io          kube-system/prometheus-adapter   True        87s

[root@k8s-master1 ~]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot;
{&quot;kind&quot;:&quot;APIResourceList&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;groupVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;resources&quot;:[{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;pods/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]},{&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]}]}
</code></pre><h4 id="3、基于QPS指标实践"><a href="#3、基于QPS指标实践" class="headerlink" title="3、基于QPS指标实践"></a>3、基于QPS指标实践</h4><p>部署一个应用：</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: metrics-app
  name: metrics-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: metrics-app
  template:
    metadata:
      labels:
        app: metrics-app
      annotations:
        prometheus.io/scrape: &quot;true&quot;    ##是否可以被采集数据
        prometheus.io/port: &quot;80&quot;        ##采集访问的端口
        prometheus.io/path: &quot;/metrics&quot;  ##采集访问的URL
    spec:
      containers:
      - image: lizhenliang/metrics-app
        name: metrics-app
        ports:
        - name: web
          containerPort: 80
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-app
  labels:
    app: metrics-app
spec:
  ports:
  - name: web
    port: 80
    targetPort: 80
  selector:
    app: metrics-app
</code></pre><pre><code>[root@k8s-master1 hpa]# kubectl get po -o wide
NAME                                     READY   STATUS    RESTARTS   AGE   IP            NODE          NOMINATED NODE   READINESS GATES
metrics-app-7674cfb699-5l72f             1/1     Running   0          19s   10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-btch5             0/1     Running   0          19s   10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-kksjr             0/1     Running   0          19s   10.244.0.15   k8s-master1   &lt;none&gt;           &lt;none&gt;

[root@k8s-master1 hpa]# kubectl get svc
NAME          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes    ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP   2d1h
metrics-app   ClusterIP   10.0.0.163   &lt;none&gt;        80/TCP    39s
</code></pre><p>该metrics-app暴露了一个Prometheus指标接口，可以通过访问service看到：</p>
<pre><code>[root@k8s-master1 hpa]# curl 10.0.0.163/metrics
# HELP http_requests_total The amount of requests in total
# TYPE http_requests_total counter
http_requests_total 20
# HELP http_requests_per_second The amount of requests per second the latest ten seconds
# TYPE http_requests_per_second gauge
http_requests_per_second 0.5

##顺带测试下负载均衡：
[root@k8s-master1 hpa]# curl 10.0.0.163
Hello! My name is metrics-app-7674cfb699-btch5. The last 10 seconds, the average QPS has been 0.5. Total requests served: 35
[root@k8s-master1 hpa]# curl 10.0.0.163
Hello! My name is metrics-app-7674cfb699-5l72f. The last 10 seconds, the average QPS has been 0.5. Total requests served: 38
[root@k8s-master1 hpa]# curl 10.0.0.163
Hello! My name is metrics-app-7674cfb699-kksjr. The last 10 seconds, the average QPS has been 0.5. Total requests served: 37
</code></pre><p>收集到的每个容器被访问的次数：</p>
<p><img src="http://myimage.okay686.cn/okay686cn/20191212/9W3JdYyaEVcC.png?imageslim" alt="mark"></p>
<p>创建HPA策略：</p>
<pre><code># vi app-hpa-v2.yml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-app-hpa 
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics-app
  minReplicas: 1
  maxReplicas: 8
  metrics:
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: 800m   # 800m 即0.8个/秒
</code></pre><pre><code>[root@k8s-master1 hpa]# kubectl get hpa
NAME              REFERENCE                TARGETS          MINPODS   MAXPODS   REPLICAS   AGE
metrics-app-hpa   Deployment/metrics-app   &lt;unknown&gt;/800m   1         8         3          36s
</code></pre><p>这里使用Prometheus提供的指标测试来测试自定义指标（QPS）的自动缩放。</p>
<h4 id="4、配置适配器收集特定的指标"><a href="#4、配置适配器收集特定的指标" class="headerlink" title="4、配置适配器收集特定的指标"></a>4、配置适配器收集特定的指标</h4><p>当创建好HPA还没结束，因为适配器还不知道你要什么指标（http_requests_per_second），HPA也就获取不到Pod提供指标。</p>
<p> ConfigMap在default名称空间中编辑prometheus-adapter ，并seriesQuery在该rules: 部分的顶部添加一个新的： </p>
<pre><code># kubectl edit cm prometheus-adapter -n kube-system
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: prometheus-adapter
    chart: prometheus-adapter-v0.1.2
    heritage: Tiller
    release: prometheus-adapter
  name: prometheus-adapter
data:
  config.yaml: |
    rules:      ##增加如下一段：
    - seriesQuery: &#39;http_requests_total{kubernetes_namespace!=&quot;&quot;,kubernetes_pod_name!=&quot;&quot;}&#39;      ##在prometheus中就可以直接查询到这部分数据
      resources:
        overrides:
          kubernetes_namespace: {resource: &quot;namespace&quot;}
          kubernetes_pod_name: {resource: &quot;pod&quot;}
      name:
        matches: &quot;^(.*)_total&quot;
        as: &quot;${1}_per_second&quot;
      metricsQuery: &#39;sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&#39;
...
</code></pre><p>该规则将http_requests在2分钟的间隔内收集该服务的所有Pod的平均速率。</p>
<p>测试API：</p>
<pre><code>[root@k8s-master1 hpa]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot;
{&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;apiVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:{&quot;selfLink&quot;:&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;},&quot;items&quot;:[{&quot;describedObject&quot;:{&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-5l72f&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;},&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;},{&quot;describedObject&quot;:{&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-btch5&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;},&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;},{&quot;describedObject&quot;:{&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-kksjr&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;},&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;}]}
</code></pre><pre><code>[root@k8s-master1 hpa]# kubectl get hpa
NAME              REFERENCE                TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
metrics-app-hpa   Deployment/metrics-app   416m/800m   1         8         2          20m
</code></pre><p>压测：</p>
<pre><code>ab -n 100000 -c 100  http://10.0.0.163/metrics
</code></pre><p>查看容器扩容的情况：</p>
<pre><code>[root@k8s-master1 ~]# kubectl get po -o wide
NAME                                     READY   STATUS              RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES
metrics-app-7674cfb699-5l72f             1/1     Running             0          48m    10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-6rht6             1/1     Running             0          16s    10.244.0.16   k8s-master1   &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-9ltvr             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-master1   &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-btch5             1/1     Running             0          48m    10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-kft7p             1/1     Running             0          16s    10.244.3.16   k8s-node3     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-plhrp             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-sgvln             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node1     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-wr56r             0/1     ContainerCreating   0          1s     &lt;none&gt;        k8s-node1     &lt;none&gt;           &lt;none&gt;
nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running             0          8m7s   10.244.2.17   k8s-node2     &lt;none&gt;           &lt;none&gt;
[root@k8s-master1 ~]# kubectl get po -o wide
NAME                                     READY   STATUS    RESTARTS   AGE    IP            NODE          NOMINATED NODE   READINESS GATES
metrics-app-7674cfb699-5l72f             1/1     Running   0          48m    10.244.1.13   k8s-node1     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-6rht6             1/1     Running   0          18s    10.244.0.16   k8s-master1   &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-9ltvr             0/1     Running   0          3s     10.244.0.17   k8s-master1   &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-btch5             1/1     Running   0          48m    10.244.2.16   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-kft7p             1/1     Running   0          18s    10.244.3.16   k8s-node3     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-plhrp             0/1     Running   0          3s     10.244.2.18   k8s-node2     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-sgvln             0/1     Running   0          3s     10.244.1.16   k8s-node1     &lt;none&gt;           &lt;none&gt;
metrics-app-7674cfb699-wr56r             0/1     Running   0          3s     10.244.1.17   k8s-node1     &lt;none&gt;           &lt;none&gt;
nfs-client-provisioner-f9fdd5cc9-ffzbd   1/1     Running   0          8m9s   10.244.2.17   k8s-node2     &lt;none&gt;           &lt;none&gt;
</code></pre><p>查看HPA状态：</p>
<pre><code>[root@k8s-master1 ~]# kubectl get hpa
NAME              REFERENCE                TARGETS        MINPODS   MAXPODS   REPLICAS   AGE
metrics-app-hpa   Deployment/metrics-app   414345m/800m   1         8         8          21m

[root@k8s-master1 ~]# kubectl describe hpa metrics-app-hpa
...省略
Metrics:                               ( current / target )
  &quot;http_requests_per_second&quot; on pods:  818994m / 800m
Min replicas:                          1
Max replicas:                          8
Deployment pods:                       8 current / 8 desired
Conditions:
  Type            Status  Reason            Message
  ----            ------  ------            -------
  AbleToScale     True    ReadyForNewScale  recommended size matches current size
  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second
  ScalingLimited  True    TooManyReplicas   the desired replica count is more than the maximum replica count
Events:
  Type     Reason                        Age                   From                       Message
  ----     ------                        ----                  ----                       -------
  Warning  FailedComputeMetricsReplicas  19m (x12 over 22m)    horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get pods metric value: unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods
  Warning  FailedGetPodsMetric           7m18s (x61 over 22m)  horizontal-pod-autoscaler  unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods
  Normal   SuccessfulRescale             88s                   horizontal-pod-autoscaler  New size: 4; reason: pods metric http_requests_per_second above target
</code></pre><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><img src="https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/hpa-5.png" style="zoom:200%;"></p>
<pre><code>1、应用程序暴露/metrics监控指标并且是prometheus数据格式；

2、通过/metrics收集每个Pod的http_request_total指标；

3、prometheus将收集到的信息汇总；

4、APIServer定时从Prometheus查询，获取request_per_second的数据；

5、HPA定期向APIServer查询以判断是否符合配置的autoscaler规则；

6、如果符合autoscaler规则，则修改Deployment的ReplicaSet副本数量进行伸缩。
</code></pre>
            <div class="post-copyright">
    <div class="content">
        <p>最后更新： 2019年12月13日 10:46</p>
        <p>原始链接： <a class="post-url" href="/2019/12/13/K8S深度理解弹性伸缩/" title="K8S深度理解弹性伸缩">http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/</a></p>
        <footer>
            <a href="http://zhdya.okay686.cn">
                <img src="/images/logo.png" alt="Zhdya">
                Zhdya
            </a>
        </footer>
    </div>
</div>

      
        
            
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;">赏</a>
</div>

<div id="reward" class="post-modal reward-lay">
    <a class="close" href="javascript:;" id="reward-close">×</a>
    <span class="reward-title">
        <i class="icon icon-quote-left"></i>
        老板，中午饭可否加餐？
        <i class="icon icon-quote-right"></i>
    </span>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/images/wechat_code.jpg" alt="打赏二维码">
        </div>
        <div class="reward-select">
            
            <label class="reward-select-item checked" data-id="wechat" data-wechat="/images/wechat_code.jpg">
                <img class="reward-select-item-wechat" src="/images/wechat.png" alt="微信">
            </label>
            
            
            <label class="reward-select-item" data-id="alipay" data-alipay="/images/alipay_code.jpg">
                <img class="reward-select-item-alipay" src="/images/alipay.png" alt="支付宝">
            </label>
            
        </div>
    </div>
</div>


        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/&title=《K8S深度理解弹性伸缩》 — 拼！就对了！&pic=/images/k8s.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/&title=《K8S深度理解弹性伸缩》 — 拼！就对了！&source=Tough times never last, but tough people do." data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《K8S深度理解弹性伸缩》 — 拼！就对了！&url=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/&via=http://zhdya.okay686.cn" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=http://zhdya.okay686.cn/2019/12/13/K8S深度理解弹性伸缩/" alt="微信分享二维码">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/Kubernets/" class="color5">Kubernets</a>
      
    <a href="/tags/K8S/" class="color4">K8S</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#二、弹性伸缩"><span class="post-toc-text">二、弹性伸缩</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-1-传统弹性伸缩的困境"><span class="post-toc-text">2.1 传统弹性伸缩的困境</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1、Kubernetes中弹性伸缩存在的问题"><span class="post-toc-text">1、Kubernetes中弹性伸缩存在的问题</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2、弹性伸缩概念的延伸"><span class="post-toc-text">2、弹性伸缩概念的延伸</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-kubernetes-弹性伸缩布局"><span class="post-toc-text">2.2 kubernetes 弹性伸缩布局</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-3-Node-自动扩容-缩容"><span class="post-toc-text">2.3 Node 自动扩容/缩容</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1、Cluster-AutoScaler"><span class="post-toc-text">1、Cluster AutoScaler</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2、Ansible扩容Node"><span class="post-toc-text">2、Ansible扩容Node</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-4-Pod自动扩容-缩容（HPA）"><span class="post-toc-text">2.4 Pod自动扩容/缩容（HPA）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1、HPA基本原理"><span class="post-toc-text">1、HPA基本原理</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2、HPA的演进历程"><span class="post-toc-text">2、HPA的演进历程</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-5-基于CPU指标缩放"><span class="post-toc-text">2.5 基于CPU指标缩放</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1、-Kubernetes-API-Aggregation"><span class="post-toc-text">1、 Kubernetes API Aggregation</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2、部署-Metrics-Server"><span class="post-toc-text">2、部署 Metrics Server</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3、autoscaling-v1（CPU指标实践）"><span class="post-toc-text">3、autoscaling/v1（CPU指标实践）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4、autoscaling-v2beta2（多指标）"><span class="post-toc-text">4、autoscaling/v2beta2（多指标）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-6-基于Prometheus自定义指标缩放"><span class="post-toc-text">2.6 基于Prometheus自定义指标缩放</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1、部署Prometheus"><span class="post-toc-text">1、部署Prometheus</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2、-部署-Custom-Metrics-Adapter"><span class="post-toc-text">2、 部署 Custom Metrics Adapter</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3、基于QPS指标实践"><span class="post-toc-text">3、基于QPS指标实践</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4、配置适配器收集特定的指标"><span class="post-toc-text">4、配置适配器收集特定的指标</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#小结"><span class="post-toc-text">小结</span></a></li></ol></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
    <a href="/2019/12/15/Java参数深入分析/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          记一次java_MetaspaceSize设置不当导致的问题
        
      </span>
    </a>
  
  
    <a href="/2019/12/10/K8S之Ingress-Nginx实现高可用/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">K8S之Ingress-Nginx实现高可用</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
        <div id="SOHUCS" sid="K8S深度理解弹性伸缩"></div>
<script type="text/javascript">
    (function(){
        var appid = 'cyu4TKmA0';
        var conf = '6f0306692caa6c8b75d928a4fc3595c4';
        var width = window.innerWidth || document.documentElement.clientWidth;
        if (width < 960) {
            window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>
    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2020 Zhdya<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "http://zhdya.okay686.cn",
      animate: true,
      isHome: false,
      share: true,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Django2/">Django2</a><a class="category-link" href="/categories/JAVA/">JAVA</a><a class="category-link" href="/categories/K8S/">K8S</a><a class="category-link" href="/categories/K8s/">K8s</a><a class="category-link" href="/categories/K8s-ceph/">K8s, ceph</a><a class="category-link" href="/categories/Python3/">Python3</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/BeautifulSoup/" style="font-size: 11.43px;">BeautifulSoup</a> <a href="/tags/CMDB/" style="font-size: 14.29px;">CMDB</a> <a href="/tags/K8S/" style="font-size: 18.57px;">K8S</a> <a href="/tags/Kubernets/" style="font-size: 17.14px;">Kubernets</a> <a href="/tags/calico/" style="font-size: 10px;">calico</a> <a href="/tags/django/" style="font-size: 15.71px;">django</a> <a href="/tags/django2-0/" style="font-size: 12.86px;">django2.0</a> <a href="/tags/django-blog/" style="font-size: 12.86px;">django_blog</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/python3/" style="font-size: 11.43px;">python3</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a href="/">
                    <i class="fa fa-home"></i><span>主页</span>
                </a>
            </li>
            
            <li>
                <a href="/archives">
                    <i class="fa fa-archive"></i><span>归档</span>
                </a>
            </li>
            
            <li>
                <a href="/about">
                    <i class="fa fa-user"></i><span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/BeautifulSoup/" style="font-size: 11.43px;">BeautifulSoup</a> <a href="/tags/CMDB/" style="font-size: 14.29px;">CMDB</a> <a href="/tags/K8S/" style="font-size: 18.57px;">K8S</a> <a href="/tags/Kubernets/" style="font-size: 17.14px;">Kubernets</a> <a href="/tags/calico/" style="font-size: 10px;">calico</a> <a href="/tags/django/" style="font-size: 15.71px;">django</a> <a href="/tags/django2-0/" style="font-size: 12.86px;">django2.0</a> <a href="/tags/django-blog/" style="font-size: 12.86px;">django_blog</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/python3/" style="font-size: 11.43px;">python3</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
<script src="/js/search.js"></script>
<script src="/js/main.js"></script>


  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  <script src="/js/particles.js"></script>







  <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  <script src="/js/animate.js"></script>


  <script src="/js/pop-img.js"></script>
  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>