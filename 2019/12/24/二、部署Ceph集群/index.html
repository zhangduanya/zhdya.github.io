<!DOCTYPE html>
<html lang="zh-CN">





<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="Tough times never last, but tough people do.">
  <meta name="author" content="Zhdya">
  <meta name="keywords" content="">
  <title>二、部署Ceph集群 ~ 不认命？就拼命！</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css">
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css">

<link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css">


</head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">&nbsp;<strong>不认命？就拼命！</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/post.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期二, 十二月 24日 2019, 12:00 凌晨
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    3.8k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      18 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h3 id="一、Ceph版本选择"><a href="#一、Ceph版本选择" class="headerlink" title="一、Ceph版本选择"></a>一、Ceph版本选择</h3><h4 id="Ceph版本来源介绍"><a href="#Ceph版本来源介绍" class="headerlink" title="Ceph版本来源介绍"></a>Ceph版本来源介绍</h4><p>Ceph 社区最新版本是 14，而 Ceph 12 是市面用的最广的稳定版本。<br>第一个 Ceph 版本是 0.1 ，要回溯到 2008 年 1 月。多年来，版本号方案一直没变，直到 2015 年 4 月 0.94.1 （ Hammer 的第一个修正版）发布后，为了避免 0.99 （以及 0.100 或 1.00 ？），制定了新策略。</p>
<p>x.0.z - 开发版（给早期测试者和勇士们）</p>
<p>x.1.z - 候选版（用于测试集群、高手们）</p>
<p>x.2.z - 稳定、修正版（给用户们）</p>
<p>x 将从 9 算起，它代表 Infernalis （ I 是第九个字母），这样第九个发布周期的第一个开发版就是 9.0.0 ；后续的开发版依次是 9.0.1 、 9.0.2 等等。<br>| 版本名称 | 版本号 | 发布时间 |<br>| —— | —— | —— |<br>| Argonaut | 0.48版本(LTS) | 　　2012年6月3日 |<br>| Bobtail | 0.56版本(LTS) | 　2013年5月7日 |<br>| Cuttlefish | 0.61版本 | 　2013年1月1日 |<br>| Dumpling | 0.67版本(LTS) | 　2013年8月14日 |<br>| Emperor | 0.72版本 | 　　　 2013年11月9 |<br>| Firefly | 0.80版本(LTS) | 　2014年5月 |<br>| Giant | Giant | 　October 2014 - April 2015 |<br>| Hammer | Hammer | 　April 2015 - November 2016|<br>| Infernalis | Infernalis | 　November 2015 - June 2016 |<br>| Jewel | 10.2.9 | 　2016年4月 |<br>| Kraken | 11.2.1 | 　2017年10月 |<br>| Luminous |12.2.12  | 　2017年10月 |<br>| mimic | 13.2.7 | 　2018年5月 |<br>| nautilus | 14.2.5 | 　2019年2月 |</p>
<h6 id="Luminous新版本特性"><a href="#Luminous新版本特性" class="headerlink" title="Luminous新版本特性"></a>Luminous新版本特性</h6><ul>
<li>Bluestore<ul>
<li>ceph-osd的新后端存储BlueStore已经稳定，是新创建的OSD的默认设置。<br>BlueStore通过直接管理物理HDD或SSD而不使用诸如XFS的中间文件系统，来管理每个OSD存储的数据，这提供了更大的性能和功能。</li>
<li>BlueStore支持Ceph存储的所有的完整的数据和元数据校验。</li>
<li>BlueStore内嵌支持使用zlib，snappy或LZ4进行压缩。（Ceph还支持zstd进行RGW压缩，但由于性能原因，不为BlueStore推荐使用zstd）</li>
</ul>
</li>
<li>集群的总体可扩展性有所提高。我们已经成功测试了多达10,000个OSD的集群。</li>
<li>ceph-mgr<ul>
<li>ceph-mgr是一个新的后台进程，这是任何Ceph部署的必须部分。虽然当ceph-mgr停止时，IO可以继续，但是度量不会刷新，并且某些与度量相关的请求（例如，ceph df）可能会被阻止。我们建议您多部署ceph-mgr的几个实例来实现可靠性。</li>
<li>ceph-mgr守护进程daemon包括基于REST的API管理。注：API仍然是实验性质的，目前有一些限制，但未来会成为API管理的基础。</li>
<li>ceph-mgr还包括一个Prometheus插件。</li>
<li>ceph-mgr现在有一个Zabbix插件。使用zabbix_sender，它可以将集群故障事件发送到Zabbix Server主机。这样可以方便地监视Ceph群集的状态，并在发生故障时发送通知。</li>
</ul>
</li>
</ul>
<h3 id="二、安装前准备"><a href="#二、安装前准备" class="headerlink" title="二、安装前准备"></a>二、安装前准备</h3><ol>
<li>安装要求：</li>
</ol>
<ul>
<li>最少三台Centos7系统虚拟机用于部署Ceph集群。硬件配置：2C4G，另外每台机器最少挂载三块硬盘(每块盘5G)  </li>
</ul>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP</th>
</tr>
</thead>
<tbody>
<tr>
<td>cephnode01</td>
<td>192.168.171.135 </td>
</tr>
<tr>
<td>cephnode02</td>
<td>192.168.171.136 </td>
</tr>
<tr>
<td>cephnode03</td>
<td>192.168.171.137 </td>
</tr>
<tr>
<td>cephyumresource01</td>
<td>192.168.171.10（内网yum源服务器）</td>
</tr>
</tbody>
</table>
<ol start="2">
<li>环境准备（在Ceph三台机器上操作）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（1）关闭防火墙：</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">（2）关闭selinux：</span><br><span class="line">sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">（3）关闭NetworkManager</span><br><span class="line">systemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager</span><br><span class="line">（4）添加主机名与IP对应关系：</span><br><span class="line">vim /etc/hosts</span><br><span class="line">192.168.171.135 cephnode01</span><br><span class="line">192.168.171.136 cephnode02</span><br><span class="line">192.168.171.137 cephnode03</span><br><span class="line">（5）设置主机名：</span><br><span class="line">hostnamectl set-hostname cephnode01</span><br><span class="line">hostnamectl set-hostname cephnode02</span><br><span class="line">hostnamectl set-hostname cephnode03</span><br><span class="line">（6）同步网络时间和修改时区</span><br><span class="line">systemctl restart chronyd.service &amp;&amp; systemctl enable chronyd.service</span><br><span class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line">（7）设置文件描述符</span><br><span class="line">echo &quot;ulimit -SHn 102400&quot; &gt;&gt; /etc/rc.local</span><br><span class="line">cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF</span><br><span class="line">* soft nofile 65535</span><br><span class="line">* hard nofile 65535</span><br><span class="line">EOF</span><br><span class="line">（8）内核参数优化</span><br><span class="line">cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF</span><br><span class="line">kernel.pid_max = 4194303</span><br><span class="line">echo &quot;vm.swappiness = 0&quot; /etc/sysctl.conf </span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br><span class="line">（9）在cephnode01上配置免密登录到cephnode02、cephnode03</span><br><span class="line">ssh-copy-id root@cephnode02</span><br><span class="line">ssh-copy-id root@cephnode03</span><br><span class="line">(10)read_ahead,通过数据预读并且记载到随机访问内存方式提高磁盘读操作</span><br><span class="line">echo &quot;8192&quot; &gt; /sys/block/sda/queue/read_ahead_kb</span><br><span class="line">(11) I/O Scheduler，SSD要用noop，SATA/SAS使用deadline</span><br><span class="line">echo &quot;deadline&quot; &gt;/sys/block/sd[x]/queue/scheduler</span><br><span class="line">echo &quot;noop&quot; &gt;/sys/block/sd[x]/queue/scheduler</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="三、安装内网yum源"><a href="#三、安装内网yum源" class="headerlink" title="三、安装内网yum源"></a>三、安装内网yum源</h3><p><u>仅在192.168.171.10操作</u></p>
<p>1、安装httpd、createrepo和epel源<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install httpd createrepo epel-release -y</span><br></pre></td></tr></table></figure></p>
<p>2、编辑yum源文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cephyumresource01 ~]# more /etc/yum.repos.d/ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">或者阿里云yum源：（推荐！！）</span><br><span class="line">[Ceph-SRPMS]</span><br><span class="line">name=Ceph SRPMS packagesno</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"> </span><br><span class="line">[Ceph-x86_64]</span><br><span class="line">name=Ceph x86_64 packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br></pre></td></tr></table></figure></p>
<p>3、下载Ceph安装包<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum --downloadonly --downloaddir=/var/www/html/ceph/rpm-nautilus/el7/x86_64/ install ceph ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<p>4、下载Ceph依赖文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-14.2.4-0.el7.src.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-deploy-2.0.1-0.src.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-cloud-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-local-14.2.4-0.el7.noarch.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-rook-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-ssh-14.2.4-0.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-release-1-1.el7.src.rpm </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-medic-1.0.4-16.g60cf7e9.el7.src.rpm</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/repomd.xml </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/repomd.xml</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/183278bb826f5b8853656a306258643384a1547c497dd8b601ed6af73907bb22-other.sqlite.bz2 </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/52bf459e39c76b2ea2cff2c5340ac1d7b5e17a105270f5f01b454d5a058adbd2-filelists.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/4f3141aec1132a9187ff5d1b4a017685e2f83a761880884d451a288fcedb154e-primary.sqlite.bz2</span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/0c554884aa5600b1311cd8f616aa40d036c1dfc0922e36bcce7fd84e297c5357-other.sqlite.bz2 </span><br><span class="line"> wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/597468b64cddfc386937869f88c2930c8e5fda3dd54977c052bab068d7438fcb-primary.sqlite.bz2</span><br></pre></td></tr></table></figure></p>
<p>5、更新yum源<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">createrepo --update  /var/www/html/ceph/rpm-nautilus</span><br></pre></td></tr></table></figure></p>
<h3 id="四、安装Ceph集群"><a href="#四、安装Ceph集群" class="headerlink" title="四、安装Ceph集群"></a>四、安装Ceph集群</h3><p>1、编辑内网yum源,将yum源同步到其它节点并提前做好yum makecache</p>
<p>当然如果外网没有任何限制，也建议直接配置如上阿里云镜像源即可！<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vim /etc/yum.repos.d/ceph.repo </span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/$basearch</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/srpms</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure></p>
<p>只在cephnode01上执行即可（如下标注：每个节点执行，需要在所有节点执行）<br>2、安装ceph-deploy(确认ceph-deploy版本是否为2.0.1)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum list | grep ceph</span><br><span class="line"># yum install -y ceph-deploy</span><br><span class="line"></span><br><span class="line"># ceph-deploy --version</span><br><span class="line">然后测试一下，发现报错：</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br><span class="line"></span><br><span class="line">原因是缺python-setuptools，安装它即可：</span><br><span class="line"># yum install -y python-setuptools</span><br><span class="line"></span><br><span class="line"># ceph-deploy --version</span><br><span class="line">2.0.1</span><br></pre></td></tr></table></figure></p>
<p>3、创建一个my-cluster目录，<strong>所有命令都需要在此目录下进行</strong>（文件位置和名字可以随意）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkdir /my-cluster</span><br><span class="line"># cd /my-cluster</span><br></pre></td></tr></table></figure></p>
<p>4、创建一个Ceph集群<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy new cephnode01 cephnode02 cephnode03</span><br><span class="line">...省略</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host cephnode03</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor cephnode03 at 192.168.171.137</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [&apos;cephnode01&apos;, &apos;cephnode02&apos;, &apos;cephnode03&apos;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [&apos;192.168.171.135&apos;, &apos;192.168.171.136&apos;, &apos;192.168.171.137&apos;]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure></p>
<p>如果安装异常可以查看：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ls ceph-deploy-ceph.log</span><br></pre></td></tr></table></figure></p>
<p>5、安装Ceph软件（<strong>每个节点执行</strong>）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum -y install epel-release</span><br><span class="line"># yum install -y ceph</span><br><span class="line"># ceph -v</span><br><span class="line">ceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable)</span><br></pre></td></tr></table></figure></p>
<p>6、生成monitor检测集群所使用的的秘钥<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mon create-initial</span><br><span class="line"># ls</span><br><span class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log</span><br><span class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring</span><br></pre></td></tr></table></figure></p>
<p>7、<strong>安装Ceph CLI，方便执行一些管理命令</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy admin cephnode01 cephnode02 cephnode03</span><br></pre></td></tr></table></figure></p>
<p>8、<strong>配置mgr，用于管理集群</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mgr create cephnode01 cephnode02 cephnode03</span><br><span class="line"># more ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = b1f800c7-a4bc-4fc7-87e2-239291f2e4c7</span><br><span class="line">mon_initial_members = cephnode01, cephnode02, cephnode03</span><br><span class="line">mon_host = 192.168.171.135,192.168.171.136,192.168.171.137</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure></p>
<p>9、部署rgw（生产一般都是多台，然后nginx做负载均衡）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum install -y ceph-radosgw</span><br><span class="line"># ceph-deploy rgw create cephnode01</span><br></pre></td></tr></table></figure></p>
<p>10、部署MDS（CephFS）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph-deploy mds create cephnode01 cephnode02 cephnode03</span><br></pre></td></tr></table></figure></p>
<p>11、添加osd<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda      8:0    0   30G  0 disk</span><br><span class="line">├─sda1   8:1    0    4G  0 part</span><br><span class="line">└─sda2   8:2    0   26G  0 part /</span><br><span class="line">sdb      8:16   0    8G  0 disk     ##磁盘仅仅为一块裸盘，没有做任何的初始化和处理；</span><br><span class="line">sr0     11:0    1  4.3G  0 rom</span><br><span class="line"></span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode01   ##会自动的格式化成ceph可以读取的格式；</span><br><span class="line">...省略</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephnode01 is now ready for osd use.</span><br><span class="line"></span><br><span class="line"># ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.00780 root default</span><br><span class="line">-3       0.00780     host cephnode01</span><br><span class="line"> 0   hdd 0.00780         osd.0           up  1.00000 1.00000</span><br><span class="line"></span><br><span class="line">##继续添加其它盘及其它node的盘</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode01   如有，修改sdX即可；</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode01</span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode02</span><br><span class="line">ceph-deploy osd create --data /dev/sdb cephnode03</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode03</span><br><span class="line">ceph-deploy osd create --data /dev/sdX cephnode03</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.02339 root default</span><br><span class="line">-3       0.00780     host cephnode01</span><br><span class="line"> 0   hdd 0.00780         osd.0           up  1.00000 1.00000</span><br><span class="line">-5       0.00780     host cephnode02</span><br><span class="line"> 1   hdd 0.00780         osd.1           up  1.00000 1.00000</span><br><span class="line">-7       0.00780     host cephnode03</span><br><span class="line"> 2   hdd 0.00780         osd.2           up  1.00000 1.00000</span><br></pre></td></tr></table></figure></p>
<p>查看集群状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     b1f800c7-a4bc-4fc7-87e2-239291f2e4c7</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03</span><br><span class="line">    mgr: cephnode01(active), standbys: cephnode02, cephnode03</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line">    rgw: 1 daemon active</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1.09KiB</span><br><span class="line">    usage:   3.01GiB used, 21.0GiB / 24.0GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph health detail</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd df   ##每块盘的使用量</span><br><span class="line">ID CLASS WEIGHT  REWEIGHT SIZE    USE     AVAIL   %USE  VAR  PGS</span><br><span class="line"> 0   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line"> 1   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line"> 2   hdd 0.00780  1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00  32</span><br><span class="line">                    TOTAL 24.0GiB 3.01GiB 21.0GiB 12.55</span><br><span class="line">MIN/MAX VAR: 1.00/1.00  STDDEV: 0</span><br></pre></td></tr></table></figure></p>
<h3 id="五、ceph-conf"><a href="#五、ceph-conf" class="headerlink" title="五、ceph.conf"></a>五、ceph.conf</h3><p>1、该配置文件采用init文件语法，###和;为注释，ceph集群在启动的时候会按照顺序加载所有的conf配置文件。 配置文件分为以下几大块配置。</p>
<pre><code>global：全局配置。
osd：osd专用配置，可以使用osd.N，来表示某一个OSD专用配置，N为osd的编号，如0、2、1等。
mon：mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、ceph-monitor-1等。使用命令 ceph mon dump可以获取节点的名称。
client：客户端专用配置。
</code></pre><p>2、配置文件可以从多个地方进行顺序加载，如果冲突将使用最新加载的配置，其加载顺序为。</p>
<pre><code>$CEPH_CONF环境变量
-c 指定的位置
/etc/ceph/ceph.conf
~/.ceph/ceph.conf
./ceph.conf
</code></pre><p>3、配置文件还可以使用一些元变量应用到配置文件，如。</p>
<pre><code>$cluster：当前集群名。
$type：当前服务类型。
$id：进程的标识符。
$host：守护进程所在的主机名。
$name：值为$type.$id。
</code></pre><p>4、ceph.conf详细参数<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]#全局设置</span><br><span class="line">fsid = xxxxxxxxxxxxxxx                           #集群标识ID </span><br><span class="line">mon host = 10.0.1.1,10.0.1.2,10.0.1.3            #monitor IP 地址</span><br><span class="line">auth cluster required = cephx                    #集群认证</span><br><span class="line">auth service required = cephx                           #服务认证</span><br><span class="line">auth client required = cephx                            #客户端认证</span><br><span class="line">osd pool default size = 3                             #最小副本数 默认是3</span><br><span class="line">osd pool default min size = 1                           #PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数</span><br><span class="line">public network = 10.0.1.0/24                            #公共网络(monitorIP段) </span><br><span class="line">cluster network = 10.0.2.0/24                           #集群网络</span><br><span class="line">max open files = 131072                                 #默认0###如果设置了该选项，Ceph会设置系统的max open fds</span><br><span class="line">mon initial members = node1, node2, node3               #初始monitor (由创建monitor命令而定)</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[mon]</span><br><span class="line">mon data = /var/lib/ceph/mon/ceph-$id</span><br><span class="line">mon clock drift allowed = 1                             #默认值0.05###monitor间的clock drift</span><br><span class="line">mon osd min down reporters = 13                         #默认值1###向monitor报告down的最小OSD数</span><br><span class="line">mon osd down out interval = 600      #默认值300      #标记一个OSD状态为down和out之前ceph等待的秒数</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[osd]</span><br><span class="line">osd data = /var/lib/ceph/osd/ceph-$id</span><br><span class="line">osd mkfs type = xfs                                     #格式化系统类型</span><br><span class="line">osd max write size = 512 #默认值90                   #OSD一次可写入的最大值(MB)</span><br><span class="line">osd client message size cap = 2147483648 #默认值100    #客户端允许在内存中的最大数据(bytes)</span><br><span class="line">osd deep scrub stride = 131072 #默认值524288         #在Deep Scrub时候允许读取的字节数(bytes)</span><br><span class="line">osd op threads = 16 #默认值2                         #并发文件系统操作数</span><br><span class="line">osd disk threads = 4 #默认值1                        #OSD密集型操作例如恢复和Scrubbing时的线程</span><br><span class="line">osd map cache size = 1024 #默认值500                 #保留OSD Map的缓存(MB)</span><br><span class="line">osd map cache bl size = 128 #默认值50                #OSD进程在内存中的OSD Map缓存(MB)</span><br><span class="line">osd mount options xfs = &quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&quot; ###默认值rw,noatime,inode64  ###Ceph OSD xfs Mount选项</span><br><span class="line">osd recovery op priority = 2 #默认值10              #恢复操作优先级，取值1-63，值越高占用资源越高</span><br><span class="line">osd recovery max active = 10 #默认值15              #同一时间内活跃的恢复请求数 </span><br><span class="line">osd max backfills = 4  #默认值10                  #一个OSD允许的最大backfills数</span><br><span class="line">osd min pg log entries = 30000 #默认值3000           #修建PGLog是保留的最大PGLog数</span><br><span class="line">osd max pg log entries = 100000 #默认值10000         #修建PGLog是保留的最大PGLog数</span><br><span class="line">osd mon heartbeat interval = 40 #默认值30            #OSD ping一个monitor的时间间隔（默认30s）</span><br><span class="line">ms dispatch throttle bytes = 1048576000 #默认值 104857600 #等待派遣的最大消息数</span><br><span class="line">objecter inflight ops = 819200 #默认值1024           #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限</span><br><span class="line">osd op log threshold = 50 #默认值5                  #一次显示多少操作的log</span><br><span class="line">osd crush chooseleaf type = 0 #默认值为1              #CRUSH规则用到chooseleaf时的bucket的类型</span><br><span class="line">###########################################################################################################################</span><br><span class="line">[client]</span><br><span class="line">rbd cache = true #默认值 true      #RBD缓存</span><br><span class="line">rbd cache size = 335544320 #默认值33554432           #RBD缓存大小(bytes)</span><br><span class="line">rbd cache max dirty = 134217728 #默认值25165824      #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</span><br><span class="line">rbd cache max dirty age = 30 #默认值1                #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</span><br><span class="line">rbd cache writethrough until flush = false #默认值true  #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写</span><br><span class="line">              #设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。</span><br><span class="line">rbd cache max dirty object = 2 #默认值0              #最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分</span><br><span class="line">      #每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能</span><br><span class="line">rbd cache target dirty = 235544320 #默认值16777216    #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty</span><br></pre></td></tr></table></figure></p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/K8S,%20ceph">K8S, ceph</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/K8S">K8S</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <!--PC和WAP自适应版-->
  <div id="SOHUCS" sid="https://zhdya.okay686.cn/2019/12/24/二、部署Ceph集群/"></div>
  <script type="text/javascript">
    (function () {
      var appid = 'cyu4TKmA0';
      var conf = '';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
        var head = document.getElementsByTagName('head')[0] || document.head || document.documentElement;
        var script = document.createElement('script');
        script.type = 'text/javascript';
        script.charset = 'utf-8';
        script.id = 'changyan_mobile_js';
        script.src = 'https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf;
        head.appendChild(script);
      } else {
        var loadJs = function (d, a) {
          var c = document.getElementsByTagName('head')[0] || document.head || document.documentElement;
          var b = document.createElement('script');
          b.setAttribute('type', 'text/javascript');
          b.setAttribute('charset', 'UTF-8');
          b.setAttribute('src', d);
          if (typeof a === 'function') {
            if (window.attachEvent) {
              b.onreadystatechange = function () {
                var e = b.readyState;
                if (e === 'loaded' || e === 'complete') {
                  b.onreadystatechange = null;
                  a();
                }
              };
            } else {
              b.onload = a;
            }
          }
          c.appendChild(b);
        };
        loadJs('https://changyan.sohu.com/upload/changyan.js', function () {
          window.changyan.api.config({ appid: appid, conf: conf });
        });
      }
    })(); </script>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    
  <!-- 备案信息 -->
  <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">苏ICP备16041225号-2</a>
  



    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/popper/popper.min.js"></script>
<script src="/lib/bootstrap/js/bootstrap.min.js"></script>
<script src="/lib/mdbootstrap/js/mdb.min.js"></script>
<script src="/js/main.js"></script>


  <script src="/js/lazyload.js"></script>



  
    <script src="/lib/tocbot/tocbot.min.js"></script>
  
  <script src="/js/post.js"></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js"></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js"></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  ');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js"></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "二、部署Ceph集群&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js"></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js"></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js"></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>





  
  
    <script>
      !function (e, t, a) {
        function r() {
          for (var e = 0; e < s.length; e++) s[e].alpha <= 0 ? (t.body.removeChild(s[e].el), s.splice(e, 1)) : (s[e].y--, s[e].scale += .004, s[e].alpha -= .013, s[e].el.style.cssText = "left:" + s[e].x + "px;top:" + s[e].y + "px;opacity:" + s[e].alpha + ";transform:scale(" + s[e].scale + "," + s[e].scale + ") rotate(45deg);background:" + s[e].color + ";z-index:99999");
          requestAnimationFrame(r)
        }

        function n() {
          var t = "function" == typeof e.onclick && e.onclick;
          e.onclick = function (e) {
            t && t(), o(e)
          }
        }

        function o(e) {
          var a = t.createElement("div");
          a.className = "heart", s.push({
            el: a,
            x: e.clientX - 5,
            y: e.clientY - 5,
            scale: 1,
            alpha: 1,
            color: c()
          }), t.body.appendChild(a)
        }

        function i(e) {
          var a = t.createElement("style");
          a.type = "text/css";
          try {
            a.appendChild(t.createTextNode(e))
          } catch (t) {
            a.styleSheet.cssText = e
          }
          t.getElementsByTagName("head")[0].appendChild(a)
        }

        function c() {
          return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
        }

        var s = [];
        e.requestAnimationFrame = e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function (e) {
          setTimeout(e, 1e3 / 60)
        }, i(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"), n(), r()
      }(window, document);
    </script>
  



  <script>(function (i, s, o, g, r, a, m) {
      i['DaoVoiceObject'] = r;
      i[r] = i[r] ||
        function () {
          (i[r].q = i[r].q || []).push(arguments);
        };
      i[r].l = 1 * new Date();
      a = s.createElement(o);
      m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      a.charset = 'utf-8';
      m.parentNode.insertBefore(a, m);
    })(window, document, 'script', ('https:' === document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/f304979b.js", 'daovoice');
    daovoice('init', {
      app_id: "f304979b",
    });
    daovoice('update');
  </script>





</body>
</html>
