<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记一次java_MetaspaceSize设置不当导致的问题]]></title>
    <url>%2F2019%2F12%2F15%2FJava%E5%8F%82%E6%95%B0%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[原本就是一个小小的metaspaceSize的问题，找了下自己之前整理的资料，发现不是太详细，也是为了后期自己回顾，就写的详细些吧： 先来了解下： 内存的分配及回收–堆中的新生代和老年代：java堆被分为两部分， 一部分被称为新生代， 一部分称为老年代， 他们的比例通常为 1：2 一、JVM中的堆一般分为三部分，新生代、老年代和永久代。1.1、新生代：主要是用来存放新生的对象。一般占据堆空间的1/3，由于频繁创建对象，所以新生代会频繁触发MinorGC进行垃圾回收。新生代分为Eden区、ServivorFrom、ServivorTo三个区，==比例为 8：1：1==。 Eden区：Java新对象的出生地(如果新创建的对象占用内存很大则直接分配给老年代)。 1.1.1、MinorGC 触发机制： 当Eden区内存不够的时候就会触发一次MinorGc，对新生代区进行一次垃圾回收。 ServivorTo：保留了一次MinorGc过程中的幸存者。 ServivorFrom: 上一次GC的幸存者，作为这一次GC的被扫描者。当JVM无法为新建对象分配内存空间的时候(Eden区满的时候)，JVM触发MinorGc。因此新生代空间占用越低，MinorGc越频繁。 每次对象会存在于Eden区和一个Survivor区， 当内存不够时，触发GC，然后把依然存活的对象复制到另一个空白的Survivor区， 然后直接清空其余新生代内存，然后如此循环。总有一个Survivor区是空白的。每进行一次GC，存活对象的年龄+1， 默认情况下，对象年龄达到15时，就会移动到老年代中。 默认的，Edem : from : to = 8 :1 : 1 ( 可以通过参数–XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。 1.2、老年代：老年代的对象比较稳定，所以MajorGC不会频繁执行。 1.2.1、触发MinorGC的条件： 在进行MajorGC之前，一般都先进行了一次MinorGC，使得有新生代的对象进入老年代，当老年代空间不足时就会触发MajorGC。 当无法找到足够大的连续空间分配给新创建的较大对象时，也会触发MajorGC进行垃圾回收腾出空间。 当老年代也满了装不下的时候，就会抛出OOM。 1.3、永久代：指内存的永久保存区域，主要存放Class和Meta（元数据）的信息。Class在被加载的时候元数据信息会放入永久区域，但是GC不会在主程序运行的时候清除永久代的信息。所以这也导致永久代的信息会随着类加载的增多而膨胀，最终导致OOM。 从JDK1.8开始，就没有永久代了， 永久代被元空间的概念所替代。 三、GC 堆Java 中的堆也是 GC 收集垃圾的主要区域。GC 分为两种：Minor GC、FullGC ( 或称为 Major GC )。 3.1、Minor GCMinor GC 是发生在新生代中的垃圾收集动作，所采用的是复制算法。 当对象在 Eden ( 包括一个 Survivor 区域，这里假设是 from 区域 ) 出生后，在经过一次 Minor GC 后，如 果对象还存活，并且能够被另外一块 Survivor 区域所容纳( 上面已经假设为 from 区域，这里应为 to 区域， 即 to 区域有足够的内存空间来存储 Eden 和 from 区域中存活的对象 )，则使用复制算法将这些仍然还存活的对 象复制到另外一块 Survivor 区域 ( 即 to 区域 ) 中，然后清理所使用过的 Eden 以及 Survivor 区域 ( 即 from 区域 )，并且将这些对象的年龄设置为1，以后对象在 Survivor 区每熬过一次 Minor GC，就将对象的年 龄 + 1，当对象的年龄达到某个值时 ( 默认是 15 岁，可以通过参数 -XX:MaxTenuringThreshold 来设定 )，这些对象就会成为老年代。 3.2、Full GCFull GC 是发生在老年代的垃圾收集动作，所采用的是标记-清除算法。 现实的生活中，老年代的人通常会比新生代的人”早死”。堆内存中的老年代(Old)不同于这个，老年代里面的对象 几乎个个都是在 Survivor 区域中熬过来的，它们是不会那么容易就 “死掉” 了的。因此，Full GC 发生的次数不 会有 Minor GC 那么频繁，并且做一次 Full GC 要比进行一次 Minor GC 的时间更长。 另外，标记-清除算法收集垃圾的时候会产生许多的内存碎片 ( 即不连续的内存空间 )，此后需要为较大的对象 分配内存空间时，若无法找到足够的连续的内存空间，就会提前触发一次 GC 的收集动作。 123456789[root@xhy-18-175-61 dsf]# ps aux | grep javaroot 136 13.7 0.7 18846112 1968564 ? Sl+ 18:28 4:08 java -Duser.timezone=GMT+08 -server -Xms3840m -Xmx3840m -XX:NewSize=1024m -XX:MaxNewSize=1024m -XX:MaxDirectMemorySize=256m -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:MinMetaspaceFreeRatio=40 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_heapDump.hprof -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_gc.log -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Dapm.applicationName=cxgc.java.dsf.zkt.service -Dapm.agentId=172.18.175.61-17851 -Dapm.env=product -classpath -DDSF_HOME=/usr/local/dsf -DDio.netty.leakDetectionLevel=PARANOID -jar /usr/local/dsf/dsfapps/dsf.zkt.service.jar通过jstat -gcutil pid查看M的值为97.37，即Meta区使用率也达到了97.37%：[root@xhy-18-175-61 dsf]# jstat -gcutil 136 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 89.03 35.00 3.79 97.37 97.18 19 4.312 0 0.000 4.312那么-XX:MetaspaceSize=256m的含义到底是什么呢？其实，这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。 这里有几个要点需要明确： 无论-XX:MetaspaceSize配置什么值，Metaspace的初始容量一定是21807104（约20.8m）； Metaspace由于使用不断扩容到-XX:MetaspaceSize参数指定的量，就会发生FGC；且之后每次Metaspace扩容都会发生FGC； 如果Old区配置CMS垃圾回收，那么第2点的FGC也会使用CMS算法进行回收； Meta区容量范围为(20.8m, MaxMetaspaceSize)； 如果MaxMetaspaceSize设置太小，可能会导致频繁FGC，甚至OOM； 最后建议： MetaspaceSize和MaxMetaspaceSize设置一样大； 具体设置多大，建议稳定运行一段时间后通过jstat -gc pid确认且这个值大一些，对于大部分项目256m即可。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S深度理解弹性伸缩]]></title>
    <url>%2F2019%2F12%2F13%2FK8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[二、弹性伸缩2.1 传统弹性伸缩的困境从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。 蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 1、Kubernetes中弹性伸缩存在的问题常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kubernetes中，就会发现如下2个问题。 机器规格不统一造成机器利用率百分比碎片化 在一个Kubernetes集群中，通常不只包含一种规格的机器，假设集群中存在4C8G与16C32G两种规格的机器，对于10%的资源预留，这两种规格代表的意义是完全不同的。 特别是在缩容的场景下，为了保证缩容后集群稳定性，我们一般会一个节点一个节点从集群中摘除，那么如何判断节点是否可以摘除其利用率百分比就是重要的指标。此时如果大规格机器有较低的利用率被判断缩容，那么很有可能会造成节点缩容后，容器重新调度后的争抢。如果优先缩容小规格机器，则可能造成缩容后资源的大量冗余。 机器利用率不单纯依靠宿主机计算 在大部分生产环境中，资源利用率都不会保持一个高的水位，但从调度来讲，调度应该保持一个比较高的水位，这样才能保障集群稳定性，又不过多浪费资源。 2、弹性伸缩概念的延伸不是所有的业务都存在峰值流量，越来越细分的业务形态带来更多成本节省和可用性之间的跳转。 在线负载型：微服务、网站、API 离线任务型：离线计算、机器学习 定时任务型：定时批量计算 不同类型的负载对于弹性伸缩的要求有所不同，在线负载对弹出时间敏感，离线任务对价格敏感，定时任务对调度敏感。 2.2 kubernetes 弹性伸缩布局在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。 有三种弹性伸缩： CA（Cluster Autoscaler）：Node级别自动扩/缩容 cluster-autoscaler组件 HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容 VPA（Vertical Pod Autoscaler）：Pod配置自动扩/缩容，主要是CPU、内存 addon-resizer组件 如果在云上建议 HPA 结合 cluster-autoscaler 的方式进行集群的弹性伸缩管理。 2.3 Node 自动扩容/缩容1、Cluster AutoScaler扩容：Cluster AutoScaler 定期检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。 缩容：Cluster AutoScaler 也会定期监测 Node 的资源使用情况，当一个 Node 长时间资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除。此时，原来的 Pod 会自动调度到其他 Node 上面。 支持的云提供商： 阿里云：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md AWS： https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md Azure： https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md 2、Ansible扩容Node自动化流程： 触发新增Node 调用Ansible脚本部署组件 检查服务是否可用 调用API将新Node加入集群或者启用Node自动加入 观察新Node状态 完成Node扩容，接收新Pod 扩容 12345# cat hosts ...[newnode]192.168.31.71 node_name=k8s-node3# ansible-playbook -i hosts add-node.yml -k 缩容 如果你想从Kubernetes集群中删除节点，正确流程如下： 1、获取节点列表 1kubectl get node 2、设置不可调度 1kubectl cordon $node_name 3、驱逐节点上的Pod 1kubectl drain $node_name --ignore-daemonsets 4、移除节点 该节点上已经没有任何资源了，可以直接移除节点： 1kubectl delete node $node_name 这样，我们平滑移除了一个 k8s 节点。 2.4 Pod自动扩容/缩容（HPA）Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据资源利用率或者自定义指标自动调整replication controller, deployment 或 replica set，实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适于无法缩放的对象，例如DaemonSet。 1、HPA基本原理Kubernetes 中的 Metrics Server 持续采集所有 Pod 副本的指标数据。HPA 控制器通过 Metrics Server 的 API（Heapster 的 API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod 副本数量。当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。如图所示。 在弹性伸缩中，冷却周期是不能逃避的一个话题， 由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸， 所以在每次做出扩容缩容后，冷却时间是多少。 在 HPA 中，默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。 可以通过调整kube-controller-manager组件启动参数设置冷却时间： –horizontal-pod-autoscaler-downscale-delay ：扩容冷却 –horizontal-pod-autoscaler-upscale-delay ：缩容冷却 2、HPA的演进历程目前 HPA 已经支持了 autoscaling/v1、autoscaling/v2beta1和autoscaling/v2beta2 三个大版本 。 目前大多数人比较熟悉是autoscaling/v1，这个版本只支持CPU一个指标的弹性伸缩。 而autoscaling/v2beta1增加了支持自定义指标，autoscaling/v2beta2又额外增加了外部指标支持。 而产生这些变化不得不提的是Kubernetes社区对监控与监控指标的认识与转变。从早期Heapster到Metrics Server再到将指标边界进行划分，一直在丰富监控生态。 示例： v1版本： 12345678910111213apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache ##指定对象 minReplicas: 1 ##指定副本范围 maxReplicas: 10 targetCPUUtilizationPercentage: 50 ##指定阈值 v2beta2版本：（可以基于resource，pod，object，external等） 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache ##选择对象 minReplicas: 1 ##范围 maxReplicas: 10 metrics: - type: Resource resource: name: cpu ##基于cpu target: type: Utilization averageUtilization: 50 - type: Pods ##基于pod pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object ##基于qps object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 10k - type: External external: metric: name: queue_messages_ready selector: &quot;queue=worker_tasks&quot; target: type: AverageValue averageValue: 30 2.5 基于CPU指标缩放1、 Kubernetes API Aggregation什么是k8sapi聚合？ 它是允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。 当然看如下图↓：你也可以把agg当作一个nginx的反代，它就是代理层。 在 Kubernetes 1.7 版本引入了聚合层，允许第三方应用程序通过将自己注册到kube-apiserver上，仍然通过 API Server 的 HTTP URL 对新的 API 进行访问和操作。为了实现这个机制，Kubernetes 在 kube-apiserver 服务中引入了一个 API 聚合层（API Aggregation Layer），用于将扩展 API 的访问请求转发到用户服务的功能。 当你访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端 。通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。 如果你使用kubeadm部署的，默认已开启。如果你使用==二进制方式部署==的话，需要在kube-APIServer中添加启动参数，增加以下配置： 1234567891011# vi /opt/kubernetes/cfg/kube-apiserver.conf...--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \--requestheader-allowed-names=kubernetes \--requestheader-extra-headers-prefix=X-Remote-Extra- \--requestheader-group-headers=X-Remote-Group \--requestheader-username-headers=X-Remote-User \--enable-aggregator-routing=true \... 在设置完成重启 kube-apiserver 服务，就启用 API 聚合功能了。 2、部署 Metrics ServerMetrics Server是一个集群范围的资源使用情况的数据聚合器。作为一个应用部署在集群中。 Metric server从每个节点上Kubelet公开的摘要API收集指标。 Metrics server通过Kubernetes聚合器注册在Master APIServer中。 12345678910111213# git clone https://github.com/kubernetes-incubator/metrics-server# cd metrics-server/deploy/1.8+/# vi metrics-server-deployment.yaml # 添加2条启动参数 ... containers: - name: metrics-server image: lizhenliang/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-insecure-tls ##忽略证书的验证 - --kubelet-preferred-address-types=InternalIP ##一般node都是用主机名注册的，但是metricserver是通过pod启动的，解析不到hostname，所以需要采集节点的IP...# kubectl create -f . 可通过Metrics API在Kubernetes中获得资源使用率指标，例如容器CPU和内存使用率。这些度量标准既可以由用户直接访问（例如，通过使用kubectl top命令），也可以由集群中的控制器（例如，Horizontal Pod Autoscaler）用于进行决策。 12345678[root@k8s-master1 metrics-server]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-pbbbc 1/1 Running 0 131m 10.244.2.2 k8s-node2 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-4gj8p 1/1 Running 1 142m 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-k5lfh 1/1 Running 1 75m 192.168.171.14 k8s-node3 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-kddhv 1/1 Running 1 142m 192.168.171.12 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-q8g25 1/1 Running 1 141m 192.168.171.13 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-server-7dbbcf4c7-v5zpm 1/1 Running 0 49s 10.244.2.4 k8s-node2 &lt;none&gt; &lt;none&gt; 查看是否注册到apiservice12[root@k8s-master1 metrics-server]# kubectl get apiservicev1beta1.metrics.k8s.io kube-system/metrics-server True 3m22s 测试： 12345678910111213141516[root@k8s-master1 metrics-server]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes&#123;&quot;kind&quot;:&quot;NodeMetricsList&quot;,&quot;apiVersion&quot;:&quot;metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;&#125;,&quot;items&quot;:[&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-master1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-master1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:34Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;104718141n&quot;,&quot;memory&quot;:&quot;1158508Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:26Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;61460193n&quot;,&quot;memory&quot;:&quot;556328Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node2&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node2&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:32Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;57896643n&quot;,&quot;memory&quot;:&quot;570056Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node3&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node3&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:30Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;30890872n&quot;,&quot;memory&quot;:&quot;403264Ki&quot;&#125;&#125;]&#125;[root@k8s-master1 metrics-server]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%k8s-master1 105m 5% 1131Mi 65%k8s-node1 62m 3% 543Mi 31%k8s-node2 58m 2% 556Mi 32%k8s-node3 31m 1% 393Mi 22%[root@k8s-master1 metrics-server]# kubectl top podNAME CPU(cores) MEMORY(bytes)web-944cddf48-4x6qr 0m 3Miweb-944cddf48-64d7r 0m 2Miweb-944cddf48-hjwng 0m 3Miweb-944cddf48-skh82 0m 3Mi 3、autoscaling/v1（CPU指标实践）autoscaling/v1版本只支持CPU一个指标。 首先部署一个应用： 1[root@k8s-master1 hpa]# kubectl run web --image=nginx --replicas=8 --requests=&quot;cpu=100m,memory=100Mi&quot; --expose 80 --port 80 --dry-run -o yaml &gt;app.yaml 创建HPA策略： 123456789101112131415161718[root@k8s-master1 hpa]# kubectl autoscale deployment web --min=2 --max=8 -o yaml --dry-run &gt;hpav1.yaml[root@k8s-master1 hpa]# kubectl apply -f hpav1.yamlhorizontalpodautoscaler.autoscaling/web created###cat hpav1.yamlapiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: webspec: maxReplicas: 8 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web targetCPUUtilizationPercentage: 60 scaleTargetRef：表示当前要伸缩对象是谁 targetCPUUtilizationPercentage：当整体的资源利用率超过50%的时候，会进行扩容。 查看当前状态：123[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 0%/60% 2 8 3 24s 开启压测： 12# yum install httpd-tools# ab -n 100000 -c 100 http://10.1.206.176/status.php 10.0.0.147 为ClusterIP。 检查扩容状态： 123456789101112131415161718192021# kubectl get hpa# kubectl top pods# kubectl get pods[root@k8s-master1 hpa]# ab -n 1000000 -c 1000 http://10.0.0.201/index.html ##压测关闭压测，过一会检查缩容状态。[root@k8s-master1 ~]# kubectl top poNAME CPU(cores) MEMORY(bytes)web-77cfdb7c6c-dpm5t 116m 5Miweb-77cfdb7c6c-lwkbj 343m 5Mi[root@k8s-master1 ~]# kubectl get poNAME READY STATUS RESTARTS AGEweb-77cfdb7c6c-94p6x 1/1 Running 0 7sweb-77cfdb7c6c-9xwbj 1/1 Running 0 23sweb-77cfdb7c6c-dpm5t 1/1 Running 0 33mweb-77cfdb7c6c-gpk6d 1/1 Running 0 7sweb-77cfdb7c6c-l7b4r 1/1 Running 0 23sweb-77cfdb7c6c-lwkbj 1/1 Running 0 33mweb-77cfdb7c6c-w6lz6 1/1 Running 0 7sweb-77cfdb7c6c-wpzb5 1/1 Running 0 7s ==工作流程==：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor) 4、autoscaling/v2beta2（多指标）为满足更多的需求， HPA 还有 autoscaling/v2beta1和 autoscaling/v2beta2两个版本。 这两个版本的区别是 autoscaling/v1beta1支持了 Resource Metrics（CPU）和 Custom Metrics（应用程序指标），而在 autoscaling/v2beta2的版本中额外增加了 External Metrics的支持。 1# kubectl get hpa.v2beta2.autoscaling -o yaml &gt; /tmp/hpa-v2.yaml 12345678910111213141516171819apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: web namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web minReplicas: 1 maxReplicas: 10 metrics: - resource: type: Resource name: cpu target: averageUtilization: 60 type: Utilization 与上面v1版本效果一样，只不过这里格式有所变化。 v2还支持其他另种类型的度量指标，：Pods和Object。 1234567type: Podspods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k 1234567891011type: Objectobject: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 2k metrics中的type字段有四种类型的值：Object、Pods、Resource、External。 Resource：指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。 Object：指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。 Pods：指的是伸缩对象Pods的指标，数据需要第三方的adapter提供，只允许AverageValue类型的目标值。 External：指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。 1234567891011121314151617181920212223242526272829303132333435363738# hpa-v2.yamlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: web namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 - type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 10k ==工作流程==：hpa -&gt; apiserver -&gt; kube aggregation -&gt; prometheus-adapter -&gt; prometheus -&gt; pods 2.6 基于Prometheus自定义指标缩放资源指标只包含CPU、内存，一般来说也够了。但如果想根据自定义指标:如请求qps/5xx错误数来实现HPA，就需要使用自定义指标了，目前比较成熟的实现是 Prometheus Custom Metrics。自定义指标由Prometheus来提供，再利用k8s-prometheus-adpater聚合到apiserver，实现和核心指标（metric-server)同样的效果。 1、部署PrometheusPrometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。 Prometheus 特点： 自动采集，服务发现； 多维数据模型：由度量名称和键值对标识的时间序列数据； PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询； 不依赖分布式存储，单个服务器节点可直接工作； 基于HTTP的pull方式采集时间序列数据； 推送时间序列数据通过PushGateway组件支持； 通过服务发现或静态配置发现目标； 多种图形模式及仪表盘支持（grafana）； Prometheus组成及架构： Prometheus Server：收集指标和存储时间序列数据，并提供查询接口 ClientLibrary：客户端库 Push Gateway：短期存储指标数据。主要用于临时性的任务 Exporters：采集已有的第三方服务监控指标并暴露metrics Alertmanager：告警 Web UI：简单的Web控制台 部署： 现在node上安装：12345678910111213141516171819202122232425262728293031323334[root@k8s-node1 ~]# yum install -y nfs-utilsNFS 配置及使用我们在服务端创建一个共享目录 /data/share ，作为客户端挂载的远端入口，然后设置权限。$ mkdir -p /opt/sharedata/$ chmod 666 /opt/sharedata/然后，修改 NFS 配置文件 /etc/exports[root@k8s-node1 ~]# cat /etc/exports/opt/sharedata 192.168.171.0/24(rw,sync,insecure,no_subtree_check,no_root_squash)说明一下，这里配置后边有很多参数，每个参数有不同的含义，具体可以参考下边。此处，我配置了将 /data/share 文件目录设置为允许 IP 为该 192.168.171.0/24 区间的客户端挂载，当然，如果客户端 IP 不在该区间也想要挂载的话，可以设置 IP 区间更大或者设置为 * 即允许所有客户端挂载，例如：/home *(ro,sync,insecure,no_root_squash) 设置 /home 目录允许所有客户端只读挂载。# 启动 NFS 服务$ service nfs start# 或者使用如下命令亦可/bin/systemctl start nfs.service[root@k8s-node1 ~]# showmount -e localhostExport list for localhost:/opt/sharedata 192.168.171.0/24示例：挂载远端目录到本地 /share 目录。$ mount 192.168.171.11:/opt/sharedata /share$ df -h | grep 192.168.171.11Filesystem Size Used Avail Use% Mounted on192.168.171.11:/opt/sharedata 27G 11G 17G 40% /share客户端要卸载 NFS 挂载的话，使用如下命令即可。$ umount /share 现在master上安装：12345678910111213141516171819202122232425262728293031323334353637383940链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg 提取码：7l3z从分享包中导入nfs-client.zip# cd nfs-client# [root@k8s-master1 nfs-client]# cat deployment.yaml...省略serviceAccountName: nfs-client-provisionercontainers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.171.12 ##nfs的server地址 - name: NFS_PATH value: /opt/sharedata ##暴露的目录volumes: - name: nfs-client-root nfs: server: 192.168.171.12 path: /opt/sharedata...省略[root@k8s-master1 nfs-client]# kubectl apply -f .storageclass.storage.k8s.io/managed-nfs-storage createdserviceaccount/nfs-client-provisioner createddeployment.apps/nfs-client-provisioner createdserviceaccount/nfs-client-provisioner unchangedclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner createdrole.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner createdrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created[root@k8s-master1 nfs-client]# kubectl get poNAME READY STATUS RESTARTS AGEnfs-client-provisioner-9c784f97-cqzhb 1/1 running 0 2m16s 123456789101112131415链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg 提取码：7l3z# cd prometheus# kubectl apply -f .[root@k8s-master1 nfs-client]# kubectl get po -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-pbbbc 1/1 Running 2 2d1h 10.244.2.15 k8s-node2 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-q8g25 1/1 Running 3 2d1h 192.168.171.13 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-server-7dbbcf4c7-v5zpm 1/1 Running 3 47h 10.244.2.14 k8s-node2 &lt;none&gt; &lt;none&gt;prometheus-0 2/2 Running 0 6m48s 10.244.3.15 k8s-node3 &lt;none&gt; &lt;none&gt;[root@k8s-master1 nfs-client]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2d1hmetrics-server ClusterIP 10.0.0.5 &lt;none&gt; 443/TCP 47hprometheus NodePort 10.0.0.147 &lt;none&gt; 9090:30090/TCP 7m46s 访问Prometheus UI：http://NdeIP:30090 2、 部署 Custom Metrics Adapter但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(k8s-prometheus-adpater)，将prometheus的metrics 数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主APIServer中注册，以便直接通过/apis/来访问。 https://github.com/DirectXMan12/k8s-prometheus-adapter 该 PrometheusAdapter 有一个稳定的Helm Charts，我们直接使用。 先准备下helm环境： 123456wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/helm repo add stable http://mirror.azure.cn/kubernetes/chartshelm repo updatehelm repo list 部署prometheus-adapter，指定prometheus地址： 12# helm install prometheus-adapter stable/prometheus-adapter --namespace kube-system --set prometheus.url=http://prometheus.kube-system,prometheus.port=9090# helm list -n kube-system 123# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEprometheus-adapter-77b7b4dd8b-ktsvx 1/1 Running 0 9m 确保适配器注册到APIServer： 12345[root@k8s-master1 ~]# kubectl get apiservices |grep customv1beta1.custom.metrics.k8s.io kube-system/prometheus-adapter True 87s[root@k8s-master1 ~]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot;&#123;&quot;kind&quot;:&quot;APIResourceList&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;groupVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;resources&quot;:[&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;pods/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;]&#125; 3、基于QPS指标实践部署一个应用： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556apiVersion: apps/v1kind: Deploymentmetadata: labels: app: metrics-app name: metrics-appspec: replicas: 3 selector: matchLabels: app: metrics-app template: metadata: labels: app: metrics-app annotations: prometheus.io/scrape: &quot;true&quot; ##是否可以被采集数据 prometheus.io/port: &quot;80&quot; ##采集访问的端口 prometheus.io/path: &quot;/metrics&quot; ##采集访问的URL spec: containers: - image: lizhenliang/metrics-app name: metrics-app ports: - name: web containerPort: 80 resources: requests: cpu: 200m memory: 256Mi readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5---apiVersion: v1kind: Servicemetadata: name: metrics-app labels: app: metrics-appspec: ports: - name: web port: 80 targetPort: 80 selector: app: metrics-app 12345678910[root@k8s-master1 hpa]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 19s 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 0/1 Running 0 19s 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kksjr 0/1 Running 0 19s 10.244.0.15 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 hpa]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 2d1hmetrics-app ClusterIP 10.0.0.163 &lt;none&gt; 80/TCP 39s 该metrics-app暴露了一个Prometheus指标接口，可以通过访问service看到： 123456789101112131415[root@k8s-master1 hpa]# curl 10.0.0.163/metrics# HELP http_requests_total The amount of requests in total# TYPE http_requests_total counterhttp_requests_total 20# HELP http_requests_per_second The amount of requests per second the latest ten seconds# TYPE http_requests_per_second gaugehttp_requests_per_second 0.5##顺带测试下负载均衡：[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-btch5. The last 10 seconds, the average QPS has been 0.5. Total requests served: 35[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-5l72f. The last 10 seconds, the average QPS has been 0.5. Total requests served: 38[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-kksjr. The last 10 seconds, the average QPS has been 0.5. Total requests served: 37 收集到的每个容器被访问的次数： 创建HPA策略： 123456789101112131415161718192021# vi app-hpa-v2.ymlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: metrics-app-hpa namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: metrics-app minReplicas: 1 maxReplicas: 8 metrics: - type: Pods pods: metric: name: http_requests_per_second target: type: AverageValue averageValue: 800m # 800m 即0.8个/秒 123[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app &lt;unknown&gt;/800m 1 8 3 36s 这里使用Prometheus提供的指标测试来测试自定义指标（QPS）的自动缩放。 4、配置适配器收集特定的指标当创建好HPA还没结束，因为适配器还不知道你要什么指标（http_requests_per_second），HPA也就获取不到Pod提供指标。 ConfigMap在default名称空间中编辑prometheus-adapter ，并seriesQuery在该rules: 部分的顶部添加一个新的： 1234567891011121314151617181920212223# kubectl edit cm prometheus-adapter -n kube-systemapiVersion: v1kind: ConfigMapmetadata: labels: app: prometheus-adapter chart: prometheus-adapter-v0.1.2 heritage: Tiller release: prometheus-adapter name: prometheus-adapterdata: config.yaml: | rules: ##增加如下一段： - seriesQuery: &apos;http_requests_total&#123;kubernetes_namespace!=&quot;&quot;,kubernetes_pod_name!=&quot;&quot;&#125;&apos; ##在prometheus中就可以直接查询到这部分数据 resources: overrides: kubernetes_namespace: &#123;resource: &quot;namespace&quot;&#125; kubernetes_pod_name: &#123;resource: &quot;pod&quot;&#125; name: matches: &quot;^(.*)_total&quot; as: &quot;$&#123;1&#125;_per_second&quot; metricsQuery: &apos;sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&apos;... 该规则将http_requests在2分钟的间隔内收集该服务的所有Pod的平均速率。 测试API： 12[root@k8s-master1 hpa]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot;&#123;&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;apiVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;&#125;,&quot;items&quot;:[&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-5l72f&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-btch5&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-kksjr&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;]&#125; 123[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 416m/800m 1 8 2 20m 压测： 1ab -n 100000 -c 100 http://10.0.0.163/metrics 查看容器扩容的情况：12345678910111213141516171819202122[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 48m 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-6rht6 1/1 Running 0 16s 10.244.0.16 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-9ltvr 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 1/1 Running 0 48m 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kft7p 1/1 Running 0 16s 10.244.3.16 k8s-node3 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-plhrp 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-sgvln 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-wr56r 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 0 8m7s 10.244.2.17 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 48m 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-6rht6 1/1 Running 0 18s 10.244.0.16 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-9ltvr 0/1 Running 0 3s 10.244.0.17 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 1/1 Running 0 48m 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kft7p 1/1 Running 0 18s 10.244.3.16 k8s-node3 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-plhrp 0/1 Running 0 3s 10.244.2.18 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-sgvln 0/1 Running 0 3s 10.244.1.16 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-wr56r 0/1 Running 0 3s 10.244.1.17 k8s-node1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 0 8m9s 10.244.2.17 k8s-node2 &lt;none&gt; &lt;none&gt; 查看HPA状态： 1234567891011121314151617181920212223[root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 414345m/800m 1 8 8 21m[root@k8s-master1 ~]# kubectl describe hpa metrics-app-hpa...省略Metrics: ( current / target ) &quot;http_requests_per_second&quot; on pods: 818994m / 800mMin replicas: 1Max replicas: 8Deployment pods: 8 current / 8 desiredConditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale recommended size matches current size ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second ScalingLimited True TooManyReplicas the desired replica count is more than the maximum replica countEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedComputeMetricsReplicas 19m (x12 over 22m) horizontal-pod-autoscaler invalid metrics (1 invalid out of 1), first error is: failed to get pods metric value: unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods Warning FailedGetPodsMetric 7m18s (x61 over 22m) horizontal-pod-autoscaler unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods Normal SuccessfulRescale 88s horizontal-pod-autoscaler New size: 4; reason: pods metric http_requests_per_second above target 小结 12345678910111、应用程序暴露/metrics监控指标并且是prometheus数据格式；2、通过/metrics收集每个Pod的http_request_total指标；3、prometheus将收集到的信息汇总；4、APIServer定时从Prometheus查询，获取request_per_second的数据；5、HPA定期向APIServer查询以判断是否符合配置的autoscaler规则；6、如果符合autoscaler规则，则修改Deployment的ReplicaSet副本数量进行伸缩。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S之Ingress-Nginx实现高可用]]></title>
    <url>%2F2019%2F12%2F10%2FK8S%E4%B9%8BIngress-Nginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[承接上文，我们部署好了ingress，为了达到生产级的阈值，我们必须要要配置ingress的高可用： 假定我们在Kubernetes 指定两个worker节点中部署了ingress nginx来为后端的pod做proxy，这时候我们就需要通过keepalived实现高可用，提供对外的VIP，也就是externalLB的upstream只需要绑定此VIP即可。 首先我们要先确保有两个worker节点部署了ingress nginx 在本实验中，环境如下： IP地址 主机名 描述 10.0.0.31 k8s-master01 10.0.0.34 k8s-node02 ingress nginx、keepalived 10.0.0.35 k8s-node03 ingress nginx、keepalived 1、查看ingress nginx状态1234[root@k8s-master01 Ingress]# kubectl get pod -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-85bd8789cd-8c4xh 1/1 Running 0 62s 10.0.0.34 k8s-node02 &lt;none&gt; &lt;none&gt;nginx-ingress-controller-85ff8dfd88-vqkhx 1/1 Running 0 3m56s 10.0.0.35 k8s-node03 &lt;none&gt; &lt;none&gt; 创建一个用于测试环境的namespace 1kubectl create namespace test 2、部署一个Deployment（用于测试）12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apps/v1kind: Deploymentmetadata: name: myweb-deploy # 部署在测试环境 namespace: testspec: replicas: 3 selector: matchLabels: name: myweb type: test template: metadata: labels: name: myweb type: test spec: containers: - name: nginx image: nginx:1.13 imagePullPolicy: IfNotPresent ports: - containerPort: 80---# serviceapiVersion: v1kind: Servicemetadata: name: myweb-svcspec: selector: name: myweb type: test ports: - port: 80 targetPort: 80 protocol: TCP---# ingress 执行kubectl create 创建deployment 1kubectl create -f myweb-demo.yaml 123456查看deployment是否部署成功[root@k8s-master01 Project]# kubectl get pods -n test -o wide | grep &quot;myweb&quot;myweb-deploy-6d586d7db4-2g5ll 1/1 Running 0 23s 10.244.3.240 k8s-node02 &lt;none&gt; &lt;none&gt;myweb-deploy-6d586d7db4-cf7w7 1/1 Running 0 4m2s 10.244.1.132 k8s-node01 &lt;none&gt; &lt;none&gt;myweb-deploy-6d586d7db4-rp5zc 1/1 Running 0 3m59s 10.244.2.5 k8s-node03 &lt;none&gt; 3、在两个worker节点部署keepalived1VIP：10.0.0.130，接口：eth0 3.1、安装keepalived1yum -y install keepalived 3.1.1、k8s-node03节点作为MASTER配置keepalived1234567891011121314151617181920212223242526[root@k8s-node03 ~]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email_from Alexandre.Cassen@firewall.loc router_id k8s-node03 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 51 priority 110 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.130/24 dev eth0 label eth0:1 &#125;&#125; 3.1.2、k8s-node02节点作为BACKUP配置keeplived12345678910111213141516171819202122232425[root@k8s-node02 ~]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id k8s-node02 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.130/24 dev eth0 label eth0:1 &#125;&#125; 两个节点启动keepalived并加入开机启动 12systemctl start keepalived.servicesystemctl enable keepalived.service 启动完成后检查k8s-node03的IP地址是否已有VIP 12[root@k8s-node03 ~]# ip add | grep &quot;130&quot; inet 10.0.0.130/24 scope global secondary eth0:1 然后我们在externalLB上的后端配置此VIP，即可实现预期的效果！！ 效果不展示了！自己实验便是最好的展示！！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之存储卷及pvc]]></title>
    <url>%2F2019%2F12%2F09%2Fk8s%E4%B9%8B%E5%AD%98%E5%82%A8%E5%8D%B7%E5%8F%8Apvc%2F</url>
    <content type="text"><![CDATA[一、概述因为pod是有生命周期的,pod一重启,里面的数据就没了,所以我们需要数据持久化存储,在k8s中,存储卷不属于容器,而是属于pod,也就是说同一个pod中的容器可以共享一个存储卷,存储卷可以是宿主机上的目录,也可以是挂载在宿主机上的外部设备。 1.1、存储卷类型： emptyDIR存储卷：pod一重启,存储卷也删除,这叫emptyDir存储卷,一般用于当做临时空间或缓存关系; hostPath存储卷：宿主机上目录作为存储卷,这种也不是真正意义实现了数据持久性; SAN(iscsi)或NAS(nfs、cifs)：网络存储设备; 分布式存储：ceph,glusterfs,cephfs,rbd； 云存储：亚马逊的EBS,Azure Disk,阿里云,关键数据一定要有异地备份； emptyDIR存储卷：1234567891011121314151617181920212223242526272829303132333435vim podtest/pod-vol-demo.yamlapiVersion: v1kind: Podmetadata: name: pod-demo namespace: default labels: app: myapp tier: frontendspec: containers: - name: myapp image: ikubernetes/myapp:v2 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent volumeMounts: - name: html mountPath: /data/ command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;,&quot;while true;do echo $(date) &gt;&gt; /data/index.html; sleep 10;done&quot;] volumes: - name: html emptyDir: &#123;&#125;volumeMounts:把哪个存储卷挂到pod中的哪个目录下；emptyDir:不设置意味着对这个参数下的两个选项不做限制； hostPath:使用宿主机上目录作为存储卷12345678910111213141516171819202122232425262728kubectl explain pods.spec.volumes.hostPath.typeDirectoryOrCreate:要挂载的路径是一个目录,不存在就创建目录;Directory:宿主机上必须实现存在目录,如果不存在就报错;FileOrCreate:表示挂载的是文件,如果不存在就创建;File:表示要挂载的文件必须事先存在,否则就报错. cat pod-hostpath-vol.yamlapiVersion: v1kind: Podmetadata: name: pod-vol-hostpath namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v2 volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html hostPath: path: /data/pod/volume1 type: DirectoryOrCreate hostPath:宿主机上的目录；volumes的名字可以随便取,这是存储卷的名字,但是上面的volumeMounts指定时,name必须和存储卷的名字一致,这样两者才建立了联系； nfs做共享存储12345678910111213141516171819202122232425262728293031这里为了方便,把master节点当做nfs存储,三个节点均执行：yum -y install nfs-utils # 然后在master上启动nfsmkdir /data/volumescat /etc/exports/data/volumes 10.0.0.0/16(rw,no_root_squash)systemctl start nfs在node1和node2上试挂载mount -t nfs k8s-master:/data/volumes /mntcat pod-vol-nfs.yamlapiVersion: v1kind: Podmetadata: name: pod-vol-nfs namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v2 volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html nfs: path: /data/volumes server: k8s-master kubectl apply -f pod-vol-nfs.yaml此时不管pod被建立在哪个节点上,对应节点上是不存放数据的,数据都在nfs主机上 pvc和pv用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。pvc和pv的关系与pod和node关系类似，前者消耗后者的资源，pvc可以向pv申请指定大小的存储资源并设置访问模式。 在定义pod时,我们只需要说明我们要一个多大的存储卷就行了,pvc存储卷必须与当前namespace的pvc建立直接绑定关系,pvc必须与pv建立绑定关系,而pv是真正的某个存储设备上的空间. 一个pvc和pv是一一对应关系,一旦一个pv被一个pvc绑定了,那么这个pv就不能被其他pvc绑定了,一个pvc是可以被多个pod所访问的,pvc在名称空间中,pv是集群级别的。 将master作为存储节点,创建挂载目录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cd /data/volumes &amp;&amp; mkdir v&#123;1,2,3,4,5&#125;cat /etc/exports/data/volumes/v1 10.0.0.0/16(rw,no_root_squash)/data/volumes/v2 10.0.0.0/16(rw,no_root_squash)/data/volumes/v3 10.0.0.0/16(rw,no_root_squash)exportfs -arvshowmount -ekubectl explain pv.spec.nfsaccessModes模式有:ReadWriteOnce:单路读写,可以简写为RWO;ReadOnlyMany:多路只读,可以简写为ROX;ReadWriteMany:多路读写,可以简写为RWX # 先将存储设备定义为pvcat pv-demo.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv001 # 定义pv时不用加名称空间,因为pv是集群级别 labels: name: pv001spec: nfs: path: /data/volumes/v1 server: k8s-master accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;] capacity: # 分配磁盘空间大小 storage: 3Gi---apiVersion: v1kind: PersistentVolumemetadata: name: pv002 labels: name: pv002spec: nfs: path: /data/volumes/v2 server: k8s-master accessModes: [&quot;ReadWriteOnce&quot;] capacity: storage: 5Gi---apiVersion: v1kind: PersistentVolumemetadata: name: pv003 labels: name: pv003spec: nfs: path: /data/volumes/v3 server: k8s-master accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;] capacity: storage: 8Gi kubectl apply -f pv-demo.yamlkubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS pv001 3Gi RWO,RWX Retain Availablepv002 5Gi RWO Retain Availablepv003 8Gi RWO,RWX Retain Available 回收策略：如果某个pvc在pv里面存数据了,后来pvc删了,那么pv里面的数据怎么处理？ reclaim_policy：即pvc删了,但pv里面的数据不删除,还保留着； recycle：即pvc删了,那么就把pv里面的数据也删了； delete：即pvc删了,那么就把pv也删了； 123456789101112131415161718192021222324252627282930# 创建pvc的清单文件kubectl explain pods.spec.volumes.persistentVolumeClaimcat pod-vol-pvc.yamlapiVersion: v1kind: PersistentVolumeClaim # 简称pvcmetadata: name: mypvc namespace: default # pvc和pod在同一个名称空间spec: accessModes: [&quot;ReadWriteMany&quot;] # 一定是pv策略的子集 resources: requests: storage: 7Gi # 申请一个大小至少为7G的pv---apiVersion: v1kind: Podmetadata: name: pod-vol-pvc namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v1 volumeMounts: - name: html # 使用的存储卷的名字 mountPath: /usr/share/nginx/html/ #挂载路径 volumes: - name: html persistentVolumeClaim: claimName: mypvc # 表示要使用哪个pvc 所以pod的存储卷类型如果是pvc,则:pod指定的pvc需要先匹配一个pv,才能被pod所挂载,在k8s 1.10之后,不能手工从底层删除pv。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S负载均衡之nginx-ingress]]></title>
    <url>%2F2019%2F12%2F07%2FK8S%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E4%B9%8Bnginx-ingress%2F</url>
    <content type="text"><![CDATA[++本篇引自多篇大佬文档整合而来，加上自己所理解整理如下：++ 一、目前状况：k8s有了coreDNS解决了k8s集群内部通过dns域名的方式相互访问容器服务，但是集群内部的域名无法在外部被访问，也没有解决域名7层负载均衡的问题，而nginx-ingress就是为了解决基于k8s的7层负载均衡，nginx-ingress也是已addon方式加入k8s集群，以pod的方式运行，多个副本，高可用。 二、Nginx Ingress 一般有三个组件组成： 1）ingress是kubernetes的一个资源对象，用于编写定义规则。 2）反向代理负载均衡器，通常以Service的Port方式运行，接收并按照ingress定义的规则进行转发，通常为nginx，haproxy，traefik等，本文使用nginx。 3）ingress-controller，监听apiserver，获取服务新增，删除等变化，并结合ingress规则动态更新到反向代理负载均衡器上，并重载配置使其生效。 以上三者有机的协调配合起来，就可以完成 Kubernetes 集群服务的暴露。 来看个图例： Nginx 对后端运行的服务（Service1、Service2）提供反向代理，在配置文件中配置了域名与后端服务 Endpoints 的对应关系。客户端通过使用 DNS 服务或者直接配置本地的 hosts 文件，将域名都映射到 Nginx 代理服务器。当客户端访问 service1.com 时，浏览器会把包含域名的请求发送给 nginx 服务器，nginx 服务器根据传来的域名，选择对应的 Service，这里就是选择 Service 1 后端服务，然后根据一定的负载均衡策略，选择 Service1 中的某个容器接收来自客户端的请求并作出响应。过程很简单，nginx 在整个过程中仿佛是一台根据域名进行请求转发的“路由器”，这也就是7层代理的整体工作流程了！ 对于 Nginx 反向代理做了什么，我们已经大概了解了。在 k8s 系统中，后端服务的变化是十分频繁的，单纯依靠人工来更新nginx 的配置文件几乎不可能，nginx-ingress 由此应运而生。Nginx-ingress 通过监视 k8s 的资源状态变化实现对 nginx 配置文件的自动更新，下面本文就来分析下其工作原理。 2.1、nginx-ingress 工作流程分析首先，上一张整体工作模式架构图（只关注配置同步更新）： 不考虑 nginx 状态收集等附件功能，nginx-ingress 模块在运行时主要包括三个主体：NginxController、Store、SyncQueue。 Store 主要负责从 kubernetes APIServer 收集运行时信息，感知各类资源（如 ingress、service等）的变化，并及时将更新事件消息（event）写入一个环形管道； SyncQueue 协程定期扫描 syncQueue 队列，发现有任务就执行更新操作，即借助 Store 完成最新运行数据的拉取，然后根据一定的规则产生新的 nginx 配置，（有些更新必须reload，就本地写入新配置，执行 reload），然后执行动态更新操作，即构造 POST 数据，向本地 Nginx Lua 服务模块发送 post 请求，实现配置更新； NginxController 作为中间的联系者，监听 updateChannel，一旦收到配置更新事件，就向同步队列 syncQueue 里写入一个更新请求。 大白话描述下： 1、ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置； 2、再写到nginx-ingress-control的pod里，这个Ingress； controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中； 3、然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。 2.2、Ingress 可以解决什么问题1、动态配置服务 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作。 2、减少不必要的端口暴露 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式。 2.3、Pod与Ingress的关系 通过Ingress Controller实现Pod的负载均衡, 支持TCP/UDP 4层和HTTP 7层Ingress 只是定义规则，具体的负载均衡服务是由Ingress controller控制器完成。 访问流程：用户—&gt; Ingress Controller(Node) —&gt;service —&gt; Pod 三、部署nginx-ingress-controller以及定义ingress策略20191205最新版目前： 获取配置文件位置: https://github.com/kubernetes/ingress-nginx/tree/nginx-0.26.1/deploy 3.1、下载部署文件提供了两种方式 ： 默认下载最新的yaml： 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 指定版本号下载对应的yaml； 修改镜像路径image 12[root@localhost src]# grep image mandatory.yaml image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 上面的镜像我没办法pull下来，改成使用阿里的google库 1image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 修改后的yaml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274apiVersion: v1kind: Namespacemetadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; - &quot;networking.k8s.io&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; - &quot;networking.k8s.io&quot; resources: - ingresses/status verbs: - update ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot; # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - &quot;ingress-controller-leader-nginx&quot; verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx --- apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: # wait up to five minutes for the drain of connections terminationGracePeriodSeconds: 300 serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: allowPrivilegeEscalation: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 lifecycle: preStop: exec: command: - /wait-shutdown--- mandatory.yaml这一个yaml中包含了很多资源的创建，包括namespace、ConfigMap、role，ServiceAccount等等所有部署ingress-controller需要的资源，配置太多就不粘出来了，我们重点看下如上deployment部分↑ 可以看到主要使用了“registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1”这个镜像 指定了一些启动参数。同时开放了80与443两个端口，并在10254端口做了健康检查。 然后修改上面mandatory.yaml的deployment部分配置为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 修改api版本及kind# apiVersion: apps/v1# kind: DeploymentapiVersion: apps/v1kind: DaemonSetmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec:# 删除Replicas# replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount # 选择对应标签的node nodeSelector: isIngress: &quot;true&quot; # 使用hostNetwork暴露服务 hostNetwork: true containers: - name: nginx-ingress-controller image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: allowPrivilegeEscalation: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10--- kind: DaemonSet：官方原始文件使用的是deployment，replicate 为 1，这样将会在某一台节点上启动对应的nginx-ingress-controller pod。外部流量访问至该节点，由该节点负载分担至内部的service。测试环境考虑防止单点故障，改为DaemonSet然后删掉replicate ，配合亲和性部署在制定节点上启动nginx-ingress-controller pod，确保有多个节点启动nginx-ingress-controller pod，后续将这些节点加入到外部硬件负载均衡组实现高可用性。 hostNetwork: true：添加该字段，暴露nginx-ingress-controller pod的服务端口（80） nodeSelector: 增加亲和性部署，有isIngress=”true” 标签的节点才会部署该DaemonSet 为需要部署nginx-ingress-controller的节点设置lable，这里测试部署在”k8s-node1，k8s-node2，k8s-node3”这个节点。 123$ kubectl label node k8s-node1 isIngress=&quot;true&quot;$ kubectl label node k8s-node2 isIngress=&quot;true&quot;$ kubectl label node k8s-node3 isIngress=&quot;true&quot; 执行yaml文件部署 12345678910111213[root@k8s-master1 src]# kubectl apply -f mandatory.yaml # 执行结果 namespace/ingress-nginx createdconfigmap/nginx-configuration createdconfigmap/tcp-services createdconfigmap/udp-services createdserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createddaemonset.apps/nginx-ingress-controller created 3.2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）123456[root@k8s-master1 src]# kubectl get daemonset -n ingress-nginxNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEnginx-ingress-controller 1 1 1 1 1 isIngress=true 3m24s[root@k8s-master1 src]# kubectl get po -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-ql4x5 1/1 Running 0 3m39s 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt; 可以看到，nginx-controller的pod已经部署在在k8s-node1上了。 到node-1上看下本地端口：1234567891011[root@k8s-node1 ~]# netstat -lntp | grep nginxtcp 0 0 127.0.0.1:10247 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:8181 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 127.0.0.1:10245 0.0.0.0:* LISTEN 34078/nginx-ingresstcp 0 0 127.0.0.1:10246 0.0.0.0:* LISTEN 34132/nginx: mastertcp6 0 0 :::10254 :::* LISTEN 34078/nginx-ingresstcp6 0 0 :::80 :::* LISTEN 34132/nginx: mastertcp6 0 0 :::8181 :::* LISTEN 34132/nginx: mastertcp6 0 0 :::443 :::* LISTEN 34132/nginx: master 由于配置了hostnetwork，nginx已经在node主机本地监听80/443/8181端口。其中8181是nginx-controller默认配置的一个default backend。这样，只要访问node主机有公网IP，就可以直接映射域名来对外网暴露服务了。如果要nginx高可用的话，可以在多个node上部署，并在前面再搭建一套LVS+keepalive做负载均衡。用hostnetwork的另一个好处是，如果lvs用DR模式的话，是不支持端口映射的，这时候如果用nodeport，暴露非标准的端口，管理起来会很麻烦。 划重点：生产须知123456789将keepalived与ingress关联现状：因为pod可以分配在很多node上，若域名与一个node节点绑定，这一个node服务器出现问题，则这个域名就挂了，不能实现高可用解决：将每个node上装上keepalived服务，设置vip，主master，备用的backup，然后域名 绑定到 vip上就实现高可用（假如10台node，其中1台设置为master，其余9台设置为backup，一旦master挂了，其余9台马上顶替，到时候域名直接绑定虚拟vip即可） 3.3、部署service用于对外提供服务123wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml修改service文件，指定一下nodePort，使用30080端口和30443端口作为nodePort 修改后的配置文件如下 1234567891011121314151617181920212223242526apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP nodePort: 30080 # http请求对外映射30080端口 - name: https port: 443 targetPort: 443 protocol: TCP nodePort: 30443 # https请求对外映射30443端口 selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx--- 3.4、部署一个tomcat用于测试ingress转发功能1234567891011121314151617181920212223242526272829303132333435363738394041424344vim k8s-tomcat-test.yamlapiVersion: v1kind: Servicemetadata: name: tomcat namespace: defaultspec: selector: app: tomcat release: canary ports: - name: http targetPort: 8080 port: 8080 - name: ajp targetPort: 8009 port: 8009 --- apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deploy namespace: defaultspec: replicas: 1 selector: matchLabels: app: tomcat release: canary template: metadata: labels: app: tomcat release: canary spec: containers: - name: tomcat image: tomcat ports: - name: http containerPort: 8080 3.5、定义ingress策略1234567891011121314151617vim k8s-tomcat-test-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-tomcat annotations: kubernets.io/ingress.class: &quot;nginx&quot;spec: rules: - host: myapp.zhdya.com http: paths: - path: backend: serviceName: tomcat servicePort: 8080 手动绑定hosts测试：1192.168.171.188 myapp.zhdya.com 把tomcat service通过ingress发布出去： 在浏览器输入：http://myapp.zhdya.com:30080/ 3.6、下面我们对tomcat服务添加https服务123456789101112131415161718192021222324[root@k8s-master ingress-nginx]# openssl genrsa -out tls.key 2048Generating RSA private key, 2048 bit long modulus.......+++..............................+++e is 65537 (0x10001)[root@k8s-master ingress-nginx]# openssl req -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=myapp.zhdya.com #注意域名要和服务的域名一致 [root@k8s-master ingress-nginx]# kubectl create secret tls tomcat-ingress-secret --cert=tls.crt --key=tls.key #创建secretsecret &quot;tomcat-ingress-secret&quot; created[root@k8s-master ingress-nginx]# kubectl get secretNAME TYPE DATA AGEdefault-token-bf52l kubernetes.io/service-account-token 3 9dtomcat-ingress-secret kubernetes.io/tls 2 7s[root@k8s-master ingress-nginx]# kubectl describe secret tomcat-ingress-secretName: tomcat-ingress-secretNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: kubernetes.io/tlsData====tls.crt: 1294 bytes #base64加密tls.key: 1679 bytes 将证书应用至tomcat服务中123456789101112131415161718192021[root@k8s-master01 ingress]# vim k8s-tomcat-test-ingress-tls.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-tomcat-tls annotations: kubernets.io/ingress.class: &quot;nginx&quot;spec: tls: - hosts: - myapp.zhdya.com #与secret证书的域名需要保持一致 secretName: tomcat-ingress-secret #secret证书的名称 rules: - host: myapp.zhdya.com http: paths: - path: backend: serviceName: tomcat servicePort: 8080[root@k8s-master01 ingress]# kubectl apply -f k8s-tomcat-test-ingress-tls.yaml 再次访问服务： https://myapp.zhdya.com:30443/ 文末彩蛋：从3.3-3.6大家有没有发现，我在测试tomcat的容器的时候手动创建了一个service-nodeport.yaml 这个文件，这个文件刚刚也讲到了，就是为了把容器内部的服务暴露出来，当然我们自己也测试了手动绑定了tomcat pod容器所在node节点的IP1192.168.171.188 myapp.zhdya.com 之前我是为了给大家证明，内部pod是如何把service从pod中暴露出来，但是细心的人肯定发现了，在3.1章节，我们明明已经创建了一个nginx-ingress-controller 这个就完全可以帮我们完成服务暴露啊。对的，非常对！！！ nginx-ingress-controller这个重要的组件具体实现了什么功能看文首！！ 12345678910111213[root@k8s-master1 ~]# kubectl get po,svc,ep --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdefault pod/busybox 1/1 Running 7 8d 10.244.0.16 k8s-node1 &lt;none&gt; &lt;none&gt;default pod/tomcat-deploy-758b795dcc-69gjz 1/1 Running 1 2d6h 10.244.0.17 k8s-node1 &lt;none&gt; &lt;none&gt;default pod/tomcat-deploy-758b795dcc-llcp5 1/1 Running 2 2d6h 10.244.1.17 k8s-node2 &lt;none&gt; &lt;none&gt;default pod/web-d86c95cc9-k9vnf 1/1 Running 4 9d 10.244.1.20 k8s-node2 &lt;none&gt; &lt;none&gt;default pod/web-d86c95cc9-x2wn6 1/1 Running 4 8d 10.244.0.18 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-ql4x5 1/1 Running 0 47m 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/coredns-6d8cfdd59d-gbd2m 1/1 Running 5 8d 10.244.0.19 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-d2gzx 1/1 Running 3 9d 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-lwsnd 1/1 Running 4 9d 192.168.171.137 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-wrkfl 1/1 Running 3 9d 10.244.1.19 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/kubernetes-dashboard-7b5bf5d559-csfwm 1/1 Running 4 9d 10.244.1.18 k8s-node2 &lt;none&gt; &lt;none&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127[root@k8s-master1 src]# kubectl exec -it nginx-ingress-controller-ql4x5 -n ingress-nginx -- cat nginx.conf...start省略不重要的配置...## start server myapp.zhdya.com server &#123; server_name myapp.zhdya.com ; listen 80 ; listen [::]:80 ; listen 443 ssl http2 ; listen [::]:443 ssl http2 ; set $proxy_upstream_name &quot;-&quot;; ssl_certificate_by_lua_block &#123; certificate.call() &#125; location / &#123; set $namespace &quot;default&quot;; set $ingress_name &quot;ingress-tomcat&quot;; set $service_name &quot;tomcat&quot;; set $service_port &quot;8080&quot;; set $location_path &quot;/&quot;; rewrite_by_lua_block &#123; lua_ingress.rewrite(&#123; force_ssl_redirect = false, ssl_redirect = true, force_no_ssl_redirect = false, use_port_in_redirects = false, &#125;) balancer.rewrite() plugins.run() &#125; header_filter_by_lua_block &#123; plugins.run() &#125; body_filter_by_lua_block &#123; &#125; log_by_lua_block &#123; balancer.log() monitor.call() plugins.run() &#125; port_in_redirect off; set $balancer_ewma_score -1; set $proxy_upstream_name &quot;default-tomcat-8080&quot;; set $proxy_host $proxy_upstream_name; set $pass_access_scheme $scheme; set $pass_server_port $server_port; set $best_http_host $http_host; set $pass_port $pass_server_port; set $proxy_alternative_upstream_name &quot;&quot;; client_max_body_size 1m; proxy_set_header Host $best_http_host; # Pass the extracted client certificate to the backend # Allow websocket connections proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_set_header X-Request-ID $req_id; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Host $best_http_host; proxy_set_header X-Forwarded-Port $pass_port; proxy_set_header X-Forwarded-Proto $pass_access_scheme; proxy_set_header X-Scheme $pass_access_scheme; # Pass the original X-Forwarded-For proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for; # mitigate HTTPoxy Vulnerability # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/ proxy_set_header Proxy &quot;&quot;; # Custom headers to proxied server proxy_connect_timeout 5s; proxy_send_timeout 60s; proxy_read_timeout 60s; proxy_buffering off; proxy_buffer_size 4k; proxy_buffers 4 4k; proxy_max_temp_file_size 1024m; proxy_request_buffering on; proxy_http_version 1.1; proxy_cookie_domain off; proxy_cookie_path off; # In case of errors try the next upstream server before returning an error proxy_next_upstream error timeout; proxy_next_upstream_timeout 0; proxy_next_upstream_tries 3; proxy_pass http://upstream_balancer; proxy_redirect off; &#125; &#125; ## end server myapp.zhdya.com...end省略不重要的配置... 建议大家一定要把这个nginx.conf文件细细的看下你就会证实nginx-ingress-controller 这个组件的功劳是多么的强大！！ 然后我们换掉之前手动绑定的hosts，变更为 ingress-nginx pod所在的节点：1192.168.171.136 myapp.zhdya.com 再次访问 是不是就不需要所谓的 30443端口了呢？？？ 再然后，小伙伴们也知道了外层的externalLB改如何操作和绑定了吧？当然还有我文中的重点！！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S集群优化之路由转发：使用IPVS替代iptables]]></title>
    <url>%2F2019%2F12%2F06%2FK8S%E9%9B%86%E7%BE%A4%E4%BC%98%E5%8C%96%E4%B9%8B%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91%EF%BC%9A%E4%BD%BF%E7%94%A8IPVS%E6%9B%BF%E4%BB%A3iptables%2F</url>
    <content type="text"><![CDATA[一、为什么要使用IPVS从k8s的1.8版本开始，kube-proxy引入了IPVS模式，IPVS模式与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 二、具体步骤2.1、开启内核参数1234567cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1EOFsysctl -p 2.2、开启ipvs支持1234567891011121314151617yum -y install ipvsadm ipset# 临时生效modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4# 永久生效cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOFmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF 2.3、配置kube-proxy12345678910111213141516171819202122232425262728# 添加下面两行 --proxy-mode=ipvs \ --masquerade-all=true \# 修改服务文件vim /usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/data/k8s/kube-proxyExecStart=/data/k8s/bin/kube-proxy \ --bind-address=192.168.1.145 \ --hostname-override=192.168.1.145 \ --cluster-cidr=10.254.0.0/16 \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --logtostderr=true \ --proxy-mode=ipvs \ --masquerade-all=true \ --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 2.4、重启kube-proxy123systemctl daemon-reloadsystemctl restart kube-proxysystemctl status kube-proxy 三、测试测试是否生效123456789101112131415161718192021222324[root@k8sNode01 docker]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.1.142:6443 Masq 1 0 0 -&gt; 192.168.1.143:6443 Masq 1 1 0 -&gt; 192.168.1.144:6443 Masq 1 1 0 TCP 10.254.27.38:80 rr -&gt; 172.30.36.4:9090 Masq 1 0 0 TCP 10.254.72.60:80 rr -&gt; 172.30.90.4:8080 Masq 1 0 0 TCP 10.254.72.247:80 rr -&gt; 172.30.36.5:3000 Masq 1 0 0 TCP 127.0.0.1:27841 rr -&gt; 172.30.36.2:80 Masq 1 0 0 -&gt; 172.30.90.2:80 Masq 1 0 0 TCP 127.0.0.1:28453 rr -&gt; 172.30.36.5:3000 Masq 1 0 0 TCP 127.0.0.1:36018 rr -&gt; 172.30.36.4:9090 Masq 1 0 0 TCP 172.30.90.0:27841 rr -&gt; 172.30.36.2:80 Masq 1 0 0 -&gt; 172.30.90.2:80 Masq 1 0 0]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible自动化部署kubernetes-1.16]]></title>
    <url>%2F2019%2F12%2F05%2Fansible%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2kubernetes-1.16%2F</url>
    <content type="text"><![CDATA[概述：集群包含coreDNS、cni、nginx-ingress、HA、flanneld 百度网盘链接：https://pan.baidu.com/s/1KYbpshhpTu62DnQwF1LUnQ 提取码：vi5e 一、单master部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114[root@k8s-ansible1 ~]# tree ansible-install-k8s-masteransible-install-k8s-master├── add-node.yml├── ansible.cfg├── group_vars│ └── all.yml├── hosts├── multi-master-deploy.yml├── multi-master.jpg├── README.md├── roles│ ├── addons│ │ ├── files│ │ │ ├── coredns.yaml│ │ │ ├── ingress-controller.yaml│ │ │ ├── kube-flannel.yaml│ │ │ └── kubernetes-dashboard.yaml│ │ └── tasks│ │ └── main.yml│ ├── common│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ └── hosts.j2│ ├── docker│ │ ├── files│ │ │ ├── daemon.json│ │ │ └── docker.service│ │ └── tasks│ │ └── main.yml│ ├── etcd│ │ ├── files│ │ │ └── etcd_cert│ │ │ ├── ca-key.pem│ │ │ ├── ca.pem│ │ │ ├── server-key.pem│ │ │ └── server.pem│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── etcd.conf.j2│ │ ├── etcd.service.j2│ │ └── etcd.sh.j2│ ├── ha│ │ ├── files│ │ │ └── check_nginx.sh│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── keepalived.conf.j2│ │ └── nginx.conf.j2│ ├── master│ │ ├── files│ │ │ ├── apiserver-to-kubelet-rbac.yaml│ │ │ ├── etcd_cert│ │ │ │ ├── ca.pem│ │ │ │ ├── server-key.pem│ │ │ │ └── server.pem│ │ │ ├── k8s_cert│ │ │ │ ├── admin-key.pem│ │ │ │ ├── admin.pem│ │ │ │ ├── ca-key.pem│ │ │ │ ├── ca.pem│ │ │ │ ├── kube-proxy-key.pem│ │ │ │ ├── kube-proxy.pem│ │ │ │ ├── server-key.pem│ │ │ │ └── server.pem│ │ │ ├── kubelet-bootstrap-rbac.yaml│ │ │ └── token.csv│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── kube-apiserver.conf.j2│ │ ├── kube-apiserver.service.j2│ │ ├── kube-controller-manager.conf.j2│ │ ├── kube-controller-manager.service.j2│ │ ├── kube-scheduler.conf.j2│ │ └── kube-scheduler.service.j2│ ├── node│ │ ├── files│ │ │ └── k8s_cert│ │ │ ├── ca.pem│ │ │ ├── kube-proxy-key.pem│ │ │ └── kube-proxy.pem│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── bootstrap.kubeconfig.j2│ │ ├── kubelet-config.yml.j2│ │ ├── kubelet.conf.j2│ │ ├── kubelet.service.j2│ │ ├── kube-proxy-config.yml.j2│ │ ├── kube-proxy.conf.j2│ │ ├── kube-proxy.kubeconfig.j2│ │ └── kube-proxy.service.j2│ └── tls│ ├── files│ │ ├── generate_etcd_cert.sh│ │ └── generate_k8s_cert.sh│ ├── tasks│ │ └── main.yml│ └── templates│ ├── etcd│ │ ├── ca-config.json.j2│ │ ├── ca-csr.json.j2│ │ └── server-csr.json.j2│ └── k8s│ ├── admin-csr.json.j2│ ├── ca-config.json.j2│ ├── ca-csr.json.j2│ ├── kube-proxy-csr.json.j2│ └── server-csr.json.j2├── single-master-deploy.yml└── single-master.jpg 1.1、解压缩binary_pkg.tar.gz123[root@k8s-ansible1 ~]# lsanaconda-ks.cfg ansible-install-k8s-master ansible-install-k8s-master.zip binary_pkg.tar.gz[root@k8s-ansible1 ~]# tar zxvf binary_pkg.tar.gz 1.2、修改所以节点hosts 及ansible内的hosts12345678910111213141516171819202122232425262728293031323334353637[root@k8s-ansible1 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.171.11 k8s-ansible1 ##ansible机器192.168.171.12 k8s-ansible2 ##master192.168.171.13 k8s-ansible3 ##node1192.168.171.14 k8s-ansible4 ##node2[root@k8s-ansible1 ~]# cd ansible-install-k8s-master[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts[master]# 如果部署单Master，只保留一个Master节点192.168.171.12 node_name=k8s-ansible2#192.168.171.111 node_name=k8s-master2[node]192.168.171.13 node_name=k8s-ansible3192.168.171.14 node_name=k8s-ansible4[etcd]192.168.171.12 etcd_name=k8s-ansible2192.168.171.13 etcd_name=k8s-ansible3192.168.171.14 etcd_name=k8s-ansible4[lb]# 如果部署单Master，该项忽略192.168.31.63 lb_name=lb-master192.168.31.71 lb_name=lb-backup[k8s:children]masternode[newnode]#192.168.31.91 node_name=k8s-node3 1.3、更改全局环境配置1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml# 安装目录software_dir: &apos;/root/binary_pkg&apos;k8s_work_dir: &apos;/opt/kubernetes&apos;etcd_work_dir: &apos;/opt/etcd&apos;tmp_dir: &apos;/tmp/k8s&apos;# 集群网络service_cidr: &apos;10.0.0.0/24&apos;cluster_dns: &apos;10.0.0.2&apos; # 与roles/addons/files/coredns.yaml中IP一致pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: &apos;30000-32767&apos;cluster_domain: &apos;cluster.local&apos;# 高可用，如果部署单Master，该项忽略vip: &apos;192.168.31.88&apos;nic: &apos;ens33&apos;# 自签证书可信任IP列表，为方便扩展，可添加多个预留IPcert_hosts: # 包含所有LB、VIP、Master（多多益善，可以多余出来几个后期扩展用） IP和service_cidr的第一个IP k8s: - 10.0.0.1 - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 - 192.168.171.15 - 192.168.171.16 - 192.168.171.17 - 192.168.171.18 - 192.168.171.19 - 192.168.171.111 # 包含所有etcd节点IP etcd: - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 二、准备部署2.1、单Master版：1ansible-playbook -i hosts single-master-deploy.yml -uroot -k 2.2、历史记录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597[root@k8s-ansible1 ansible-install-k8s-master]# ansible-playbook -i hosts single-master-deploy.yml -uroot -kSSH password:PLAY [0.系统初始化] ********************************************************************************************************************************************************TASK [common : 关闭firewalld] *******************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.12]ok: [192.168.171.13]TASK [common : 关闭selinux] *********************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.13]ok: [192.168.171.12]TASK [common : 关闭swap] ************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [common : 即时生效] **************************************************************************************************************************************************changed: [192.168.171.13]changed: [192.168.171.12]changed: [192.168.171.14]TASK [common : 拷贝时区] **************************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.12]ok: [192.168.171.13]TASK [common : 添加hosts] ***********************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.14]ok: [192.168.171.13]PLAY [1.自签证书] *********************************************************************************************************************************************************TASK [tls : 获取Ansible工作目录] ********************************************************************************************************************************************changed: [localhost]TASK [tls : 创建工作目录] ***************************************************************************************************************************************************ok: [localhost] =&gt; (item=etcd)ok: [localhost] =&gt; (item=k8s)TASK [tls : 准备cfssl工具] ************************************************************************************************************************************************ok: [localhost]TASK [tls : 准备etcd证书请求文件] *********************************************************************************************************************************************ok: [localhost] =&gt; (item=ca-config.json.j2)ok: [localhost] =&gt; (item=ca-csr.json.j2)ok: [localhost] =&gt; (item=server-csr.json.j2)TASK [tls : 准备生成etcd证书脚本] *********************************************************************************************************************************************ok: [localhost]TASK [tls : 生成etcd证书] *************************************************************************************************************************************************changed: [localhost]TASK [tls : 准备k8s证书请求文件] **********************************************************************************************************************************************ok: [localhost] =&gt; (item=ca-config.json.j2)ok: [localhost] =&gt; (item=ca-csr.json.j2)ok: [localhost] =&gt; (item=server-csr.json.j2)ok: [localhost] =&gt; (item=admin-csr.json.j2)ok: [localhost] =&gt; (item=kube-proxy-csr.json.j2)TASK [tls : 准备生成k8s证书脚本] **********************************************************************************************************************************************ok: [localhost]TASK [tls : 生成k8s证书] **************************************************************************************************************************************************changed: [localhost]PLAY [2.部署Docker] *****************************************************************************************************************************************************TASK [docker : 创建临时目录] ************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 分发并解压docker二进制包] ***************************************************************************************************************************************ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)TASK [docker : 移动docker二进制文件] *****************************************************************************************************************************************changed: [192.168.171.13]changed: [192.168.171.12]changed: [192.168.171.14]TASK [docker : 分发service文件] *******************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 创建目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 配置docker] **********************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 启动docker] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [docker : 查看状态] **************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [docker : debug] *************************************************************************************************************************************************ok: [192.168.171.13] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible3&quot;, &quot;ID: O3LF:KDZ3:CXD6:MU6T:3DKL:PS42:6ATX:R4QE:GMI7:QHNO:CVQO:7ZW6&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;ok: [192.168.171.12] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible2&quot;, &quot;ID: DFQT:2YYV:YWY5:IXSS:U6VS:BW7R:6MPH:WCLC:QKOW:Y63I:5TV6:C3HT&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;ok: [192.168.171.14] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible4&quot;, &quot;ID: M3EP:OAKQ:6AMI:RDC6:QX2H:U34M:5GTT:Q2E7:AFP7:C4M3:FUO2:UHS3&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;PLAY [3.部署ETCD集群] *****************************************************************************************************************************************************TASK [etcd : 创建工作目录] **************************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=bin)ok: [192.168.171.13] =&gt; (item=bin)ok: [192.168.171.14] =&gt; (item=bin)ok: [192.168.171.12] =&gt; (item=cfg)ok: [192.168.171.13] =&gt; (item=cfg)ok: [192.168.171.14] =&gt; (item=cfg)ok: [192.168.171.12] =&gt; (item=ssl)ok: [192.168.171.13] =&gt; (item=ssl)ok: [192.168.171.14] =&gt; (item=ssl)TASK [etcd : 创建临时目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [etcd : 分发并解压etcd二进制包] *******************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)TASK [etcd : 移动etcd二进制文件] *********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]TASK [etcd : 分发证书] ****************************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.13] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.13] =&gt; (item=server.pem)changed: [192.168.171.14] =&gt; (item=server.pem)changed: [192.168.171.14] =&gt; (item=server-key.pem)changed: [192.168.171.13] =&gt; (item=server-key.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [etcd : 分发etcd配置文件] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : 分发service文件] *********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : 启动etcd] **************************************************************************************************************************************************changed: [192.168.171.14]changed: [192.168.171.12]changed: [192.168.171.13]TASK [etcd : 分发etcd脚本] ************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]TASK [etcd : 获取etcd集群状态] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : debug] ***************************************************************************************************************************************************ok: [192.168.171.13] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;ok: [192.168.171.12] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;ok: [192.168.171.14] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;PLAY [4.部署K8S Master] *************************************************************************************************************************************************TASK [master : 创建工作目录] ************************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=bin)changed: [192.168.171.12] =&gt; (item=cfg)changed: [192.168.171.12] =&gt; (item=ssl)changed: [192.168.171.12] =&gt; (item=logs)TASK [master : 创建临时目录] ************************************************************************************************************************************************ok: [192.168.171.12]TASK [master : 分发并解压k8s二进制包] ******************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)TASK [master : 移动k8s master二进制文件] *************************************************************************************************************************************changed: [192.168.171.12]TASK [master : 分发k8s证书] ***********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=ca-key.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [master : 分发etcd证书] **********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [master : 分发token文件] *********************************************************************************************************************************************changed: [192.168.171.12]TASK [master : 分发k8s配置文件] *********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-controller-manager.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-scheduler.conf.j2)TASK [master : 分发service文件] *******************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver.service.j2)changed: [192.168.171.12] =&gt; (item=kube-controller-manager.service.j2)changed: [192.168.171.12] =&gt; (item=kube-scheduler.service.j2)TASK [master : 启动k8s master组件] ****************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver)changed: [192.168.171.12] =&gt; (item=kube-controller-manager)changed: [192.168.171.12] =&gt; (item=kube-scheduler)TASK [master : 查看集群状态] ************************************************************************************************************************************************changed: [192.168.171.12]TASK [master : debug] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;cs.stdout_lines&quot;: [ &quot;NAME AGE&quot;, &quot;scheduler &lt;unknown&gt;&quot;, &quot;controller-manager &lt;unknown&gt;&quot;, &quot;etcd-0 &lt;unknown&gt;&quot;, &quot;etcd-1 &lt;unknown&gt;&quot;, &quot;etcd-2 &lt;unknown&gt;&quot; ]&#125;TASK [master : 拷贝RBAC文件] **********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kubelet-bootstrap-rbac.yaml)changed: [192.168.171.12] =&gt; (item=apiserver-to-kubelet-rbac.yaml)TASK [master : 授权APIServer访问Kubelet与授权kubelet bootstrap] **************************************************************************************************************changed: [192.168.171.12]PLAY [5.部署K8S Node] ***************************************************************************************************************************************************TASK [node : 创建工作目录] **************************************************************************************************************************************************changed: [192.168.171.13] =&gt; (item=bin)changed: [192.168.171.14] =&gt; (item=bin)ok: [192.168.171.12] =&gt; (item=bin)changed: [192.168.171.13] =&gt; (item=cfg)changed: [192.168.171.14] =&gt; (item=cfg)ok: [192.168.171.12] =&gt; (item=cfg)changed: [192.168.171.14] =&gt; (item=ssl)changed: [192.168.171.13] =&gt; (item=ssl)ok: [192.168.171.12] =&gt; (item=ssl)changed: [192.168.171.14] =&gt; (item=logs)ok: [192.168.171.12] =&gt; (item=logs)changed: [192.168.171.13] =&gt; (item=logs)TASK [node : 创建cni插件目录] ***********************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=/opt/cni/bin)changed: [192.168.171.13] =&gt; (item=/opt/cni/bin)changed: [192.168.171.12] =&gt; (item=/opt/cni/bin)changed: [192.168.171.13] =&gt; (item=/etc/cni/net.d)changed: [192.168.171.14] =&gt; (item=/etc/cni/net.d)changed: [192.168.171.12] =&gt; (item=/etc/cni/net.d)TASK [node : 创建临时目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [node : 分发并解压k8s二进制包] ********************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)TASK [node : 分发并解压cni插件二进制包] ******************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)TASK [node : 移动k8s node二进制文件] *****************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [node : 分发k8s证书] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.13] =&gt; (item=ca.pem)changed: [192.168.171.14] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=kube-proxy.pem)changed: [192.168.171.14] =&gt; (item=kube-proxy.pem)changed: [192.168.171.13] =&gt; (item=kube-proxy.pem)changed: [192.168.171.12] =&gt; (item=kube-proxy-key.pem)changed: [192.168.171.13] =&gt; (item=kube-proxy-key.pem)changed: [192.168.171.14] =&gt; (item=kube-proxy-key.pem)TASK [node : 分发k8s配置文件] ***********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.13] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.13] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.12] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.12] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.13] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy-config.yml.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy-config.yml.j2)TASK [node : 分发service文件] *********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kubelet.service.j2)changed: [192.168.171.13] =&gt; (item=kubelet.service.j2)changed: [192.168.171.14] =&gt; (item=kubelet.service.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.service.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.service.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.service.j2)TASK [node : 启动k8s node组件] ********************************************************************************************************************************************changed: [192.168.171.13] =&gt; (item=kubelet)changed: [192.168.171.14] =&gt; (item=kubelet)changed: [192.168.171.12] =&gt; (item=kubelet)changed: [192.168.171.14] =&gt; (item=kube-proxy)changed: [192.168.171.13] =&gt; (item=kube-proxy)changed: [192.168.171.12] =&gt; (item=kube-proxy)TASK [node : 分发预准备镜像] *************************************************************************************************************************************************changed: [192.168.171.14]changed: [192.168.171.13]changed: [192.168.171.12]TASK [node : 导入镜像] ****************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]PLAY [6.部署插件] *********************************************************************************************************************************************************TASK [addons : 允许Node加入集群] ********************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 拷贝YAML文件到Master] ***************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/coredns.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/ingress-controller.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kube-flannel.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kubernetes-dashboard.yaml)TASK [addons : 部署Flannel,Dashboard,CoreDNS,Ingress] *******************************************************************************************************************changed: [192.168.171.12]TASK [addons : 替换Dashboard证书] *****************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 查看Pod状态] ***********************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : debug] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;getall.stdout_lines&quot;: [ &quot;NAMESPACE NAME READY STATUS RESTARTS AGE&quot;, &quot;kube-system pod/coredns-6d8cfdd59d-hcfw5 0/1 Pending 0 2s&quot;, &quot;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-nk7t8 0/1 Pending 0 1s&quot;, &quot;kubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-cxgb6 0/1 Pending 0 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&quot;, &quot;default service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 2m19s&quot;, &quot;ingress-nginx service/ingress-nginx ClusterIP 10.0.0.158 &lt;none&gt; 80/TCP,443/TCP 2s&quot;, &quot;kube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2s&quot;, &quot;kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.38 &lt;none&gt; 8000/TCP 1s&quot;, &quot;kubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.180 &lt;none&gt; 443:30001/TCP 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE&quot;, &quot;ingress-nginx daemonset.apps/nginx-ingress-controller 0 0 0 0 0 &lt;none&gt; 2s&quot;, &quot;kube-system daemonset.apps/kube-flannel-ds-amd64 0 0 0 0 0 &lt;none&gt; 2s&quot;, &quot;&quot;, &quot;NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE&quot;, &quot;kube-system deployment.apps/coredns 0/1 1 0 2s&quot;, &quot;kubernetes-dashboard deployment.apps/dashboard-metrics-scraper 0/1 1 0 1s&quot;, &quot;kubernetes-dashboard deployment.apps/kubernetes-dashboard 0/1 1 0 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME DESIRED CURRENT READY AGE&quot;, &quot;kube-system replicaset.apps/coredns-6d8cfdd59d 1 1 0 2s&quot;, &quot;kubernetes-dashboard replicaset.apps/dashboard-metrics-scraper-566cddb686 1 1 0 1s&quot;, &quot;kubernetes-dashboard replicaset.apps/kubernetes-dashboard-c4bc5bd44 1 1 0 1s&quot; ]&#125;TASK [addons : 创建Dashboard管理员令牌] **************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 获取Dashboard管理员令牌] **************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : Kubernetes Dashboard登录信息] ******************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;ui.stdout_lines&quot;: [ &quot;访问地址---&gt;https://NodeIP:30001&quot;, &quot;令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IlhOV0FZU1ZXRU80MU5oRUlYeGsxbExFcVB1R1k0bEEzMDhQQWdWVE5oZG8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tenQ2Y3ciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTU3YjEyYmYtNjNjNC00NzU1LWI4YTAtN2IyY2ZkZmRmNmE3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fKCHvmsmZmIErB9YLHtQrqWQBL_b89W0i_gDa4rwgV9x4UfzVAXskUiZiQs_yAHNmyUaIqPpBdUI64pvAoXilr-6wIk8-R8hpp4BJXLL4OsTtPXxrhIQF4_NP0D-4flg9sHba-I9X9A_2RWskcY53PAPTOjlyOQuldUyTdIT9tXi6jeSgj8CrDBc9O_A3xYWZ1f7RvrdEdU4Kkotc1rsBeGg-OzabU1nNLxWAaDHZJFciYeABtbPoY2fTkdz0JGoIxLpAqcQKoFp9ztGPcoOboCOqeb_hc-caBAmyvVIfbPvBiywdtuidjvb1IazETt_GQlzg7FMBoUpHhJYOTvnAA&quot; ]&#125;PLAY RECAP ************************************************************************************************************************************************************192.168.171.12 : ok=61 changed=40 unreachable=0 failed=0192.168.171.13 : ok=38 changed=23 unreachable=0 failed=0192.168.171.14 : ok=38 changed=23 unreachable=0 failed=0localhost : ok=9 changed=3 unreachable=0 failed=0 2.3、测试1234567891011121314151617181920212223[root@k8s-ansible2 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-ansible2 Ready &lt;none&gt; 117s v1.16.0k8s-ansible3 Ready &lt;none&gt; 117s v1.16.0k8s-ansible4 Ready &lt;none&gt; 117s v1.16.0[root@k8s-ansible2 ~]# kubectl get po --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESingress-nginx nginx-ingress-controller-7g9fh 1/1 Running 0 2m41s 192.168.171.13 k8s-ansible3 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-hc492 1/1 Running 0 2m41s 192.168.171.14 k8s-ansible4 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-s4slm 1/1 Running 0 2m41s 192.168.171.12 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system coredns-6d8cfdd59d-hcfw5 1/1 Running 0 3m8s 10.244.1.2 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-6nbt4 1/1 Running 0 2m51s 192.168.171.12 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-mfksz 1/1 Running 0 2m51s 192.168.171.14 k8s-ansible4 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-vclgg 1/1 Running 0 2m51s 192.168.171.13 k8s-ansible3 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-566cddb686-nk7t8 1/1 Running 0 3m7s 10.244.2.2 k8s-ansible4 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-c4bc5bd44-cxgb6 1/1 Running 0 3m7s 10.244.0.2 k8s-ansible3 &lt;none&gt; &lt;none&gt;[root@k8s-ansible2 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-96KwqYYgTQ0ajISzq2pUc5Dzu07UzjaKRZqRWs31yUk 5m5s kubelet-bootstrap Approved,Issuednode-csr-9PduNyHHpXtDmuNFj3fCkpoNkGcDkO2NPEk3uGQ3kIk 5m6s kubelet-bootstrap Approved,Issuednode-csr-WLFKgflHlDK2f0RFvuTYKlkHcr8hz0iOrzYcp2V50JE 5m6s kubelet-bootstrap Approved,Issued 三、多master部署3.1、hosts1234567891011121314151617181920212223242526[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts[master]# 如果部署单Master，只保留一个Master节点192.168.171.11 node_name=k8s-master1192.168.171.12 node_name=k8s-master2[node]192.168.171.13 node_name=k8s-node1192.168.171.14 node_name=k8s-node2[etcd]192.168.171.11 etcd_name=etcd-1192.168.171.12 etcd_name=etcd-2192.168.171.13 etcd_name=etcd-3[lb]# 如果部署单Master，该项忽略192.168.171.15 lb_name=lb-master192.168.171.16 lb_name=lb-backup[k8s:children]masternode[newnode]#192.168.31.91 node_name=k8s-node3 3.2、全局参数配置12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml# 安装目录software_dir: &apos;/root/binary_pkg&apos;k8s_work_dir: &apos;/opt/kubernetes&apos;etcd_work_dir: &apos;/opt/etcd&apos;tmp_dir: &apos;/tmp/k8s&apos;# 集群网络service_cidr: &apos;10.0.0.0/24&apos;cluster_dns: &apos;10.0.0.2&apos; # 与roles/addons/files/coredns.yaml中IP一致pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: &apos;30000-32767&apos;cluster_domain: &apos;cluster.local&apos;# 高可用，如果部署单Master，该项忽略vip: &apos;192.168.171.88&apos;nic: &apos;ens33&apos;# 自签证书可信任IP列表，为方便扩展，可添加多个预留IPcert_hosts: # 包含所有LB、VIP、Master IP和service_cidr的第一个IP（多多益善，可以多余出来几个后期扩展用） k8s: - 10.0.0.1 - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 - 192.168.171.15 - 192.168.171.16 - 192.168.171.17 - 192.168.171.18 - 192.168.171.19 - 192.168.171.10 - 192.168.171.21 - 192.168.171.88 # 包含所有etcd节点IP etcd: - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 3.3、准备部署123多Master版：ansible-playbook -i hosts multi-master-deploy.yml -uroot -k 3.4、输出历史（重点是结果）1省略... 3.5、测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[root@k8s-ansible1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 85s v1.16.0k8s-master2 Ready &lt;none&gt; 85s v1.16.0k8s-node1 Ready &lt;none&gt; 85s v1.16.0k8s-node2 Ready &lt;none&gt; 85s v1.16.0[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEingress-nginx pod/nginx-ingress-controller-92b8v 1/1 Running 0 90singress-nginx pod/nginx-ingress-controller-dfkp5 1/1 Running 0 90singress-nginx pod/nginx-ingress-controller-hckvr 1/1 Running 0 91singress-nginx pod/nginx-ingress-controller-qckdd 1/1 Running 0 90skube-system pod/coredns-6d8cfdd59d-lsdps 1/1 Running 0 117skube-system pod/kube-flannel-ds-amd64-2mc74 1/1 Running 1 100skube-system pod/kube-flannel-ds-amd64-4hqq7 1/1 Running 0 101skube-system pod/kube-flannel-ds-amd64-dgzrb 1/1 Running 0 100skube-system pod/kube-flannel-ds-amd64-zjtpq 1/1 Running 0 100skubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-9xh7b 1/1 Running 0 116skubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-4f45q 1/1 Running 0 116sNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5m40singress-nginx service/ingress-nginx ClusterIP 10.0.0.170 &lt;none&gt; 80/TCP,443/TCP 117skube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 117skubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.172 &lt;none&gt; 8000/TCP 116skubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.57 &lt;none&gt; 443:30001/TCP 116s[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESingress-nginx pod/nginx-ingress-controller-92b8v 1/1 Running 0 96s 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-dfkp5 1/1 Running 0 96s 192.168.171.12 k8s-master2 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-hckvr 1/1 Running 0 97s 192.168.171.13 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-qckdd 1/1 Running 0 96s 192.168.171.14 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system pod/coredns-6d8cfdd59d-lsdps 1/1 Running 0 2m3s 10.244.1.2 k8s-master2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-2mc74 1/1 Running 1 106s 192.168.171.12 k8s-master2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-4hqq7 1/1 Running 0 107s 192.168.171.13 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-dgzrb 1/1 Running 0 106s 192.168.171.14 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-zjtpq 1/1 Running 0 106s 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-9xh7b 1/1 Running 0 2m2s 10.244.2.2 k8s-master1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-4f45q 1/1 Running 0 2m2s 10.244.3.2 k8s-node2 &lt;none&gt; &lt;none&gt;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORdefault service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5m46s &lt;none&gt;ingress-nginx service/ingress-nginx ClusterIP 10.0.0.170 &lt;none&gt; 80/TCP,443/TCP 2m3s app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginxkube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2m3s k8s-app=kube-dnskubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.172 &lt;none&gt; 8000/TCP 2m2s k8s-app=dashboard-metrics-scraperkubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.57 &lt;none&gt; 443:30001/TCP 2m2s k8s-app=kubernetes-dashboard### 检查高可用的两台机器：[root@k8s-ansible5 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:85:37:e0 brd ff:ff:ff:ff:ff:ff inet 192.168.171.15/24 brd 192.168.171.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.171.88/24 scope global secondary ens33 ##虚拟VIP valid_lft forever preferred_lft forever inet6 fe80::d20b:b903:7edd:b18b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::5d9e:cf1f:ea7f:801f/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::5c0:8885:2874:a77b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever[root@k8s-ansible5 ~]# ps aux | grep nginxroot 7895 0.0 0.0 46356 1168 ? Ss 22:57 0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 7896 0.1 0.1 47184 2308 ? S 22:57 0:00 nginx: worker processnginx 7897 0.0 0.1 46780 1980 ? S 22:57 0:00 nginx: worker processnginx 7898 0.0 0.1 46924 2216 ? S 22:57 0:00 nginx: worker processnginx 7899 0.0 0.1 46876 2220 ? S 22:57 0:00 nginx: worker processroot 11331 0.0 0.0 112728 988 pts/0 S+ 23:07 0:00 grep --color=auto nginx[root@k8s-ansible5 ~]# ps aux | grep keepalivedroot 7975 0.0 0.0 122884 1404 ? Ss 22:57 0:00 /usr/sbin/keepalived -Droot 7976 0.0 0.1 133844 3336 ? S 22:57 0:00 /usr/sbin/keepalived -Droot 7977 0.0 0.1 133784 2892 ? S 22:57 0:00 /usr/sbin/keepalived -Droot 11369 0.0 0.0 112724 992 pts/0 R+ 23:07 0:00 grep --color=auto keepalived]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个生产级K8S高可用集群（2）]]></title>
    <url>%2F2019%2F12%2F01%2F%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[写在前面：此次为了贴合线上的真实情况，此次K8S搭建将不会和咱们网路上的一气呵成相媲美，更多的表现在： 最新版K8S_1.16； 完全基于离线模式的二进制HA搭建（政企）《链接：https://pan.baidu.com/s/1aCmiYdfn5gujnyMutVdaVw提取码：m39k》； 全部组件均采用二进制部署（包含Docker）； 逐一摸索每个组件的配置文件，做到线上有故障能清楚的定位到问题； 既然是分布式，本次安装完全基于： 先单Master到双Master高可用； 新Node如何加到集群； 服务器硬件配置推荐： 生产环境K8S平台规划 – 单Master集群 生产环境K8S平台规划 – 多Master集群（HA） 一、服务器规划 角色 IP 组件 k8s-master1 192.168.171.134 kube-apiserver，kube-controller-manager，kube-scheduler，etcd k8s-master2 192.168.171.135 kube-apiserver，kube-controller-manager，kube-scheduler，etcd k8s-node1 192.168.171.136 kubelet，kube-proxy，docker，etcd k8s-node2 192.168.171.137 kubelet，kube-proxy，docker Load Balancer（Master） 192.168.171.138，192.168.171.188 (VIP) Nginx L4，Keepalived Load Balancer（Backup） 192.168.171.139 Nginx L4，Keepalived 1.1、系统初始化12345678910111213141516171819202122232425262728293031323334353637关闭防火墙：# systemctl stop firewalld# systemctl disable firewalld关闭selinux：# setenforce 0 # 临时# sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/config # 永久关闭swap：# swapoff -a # 临时# vim /etc/fstab # 永久同步系统时间：# ntpdate time.windows.com添加hosts：# vim /etc/hosts192.168.171.134 k8s-master1192.168.171.135 k8s-master2192.168.171.136 k8s-node1192.168.171.137 k8s-node2修改主机名：hostnamectl set-hostname k8s-master1##开启转发cat /etc/sysctl.d/kubernetes.confnet.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1vm.swappiness=0vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100sysctl -p /etc/sysctl.d/kubernetes.conf 二、ETCD集群整个集群中所有的组件均是走的https协议进行交互，所以我们需要配置自签证书到各个服务中； 2.1、将下载好的证书文件上传到K8s-master1中，并解压12345678910111213141516171819202122[root@k8s-master1 ~]# lsanaconda-ks.cfg TLS.tar.gz[root@k8s-master1 ~]# tar zxvf TLS.tar.gzTLS/TLS/cfsslTLS/cfssl-certinfoTLS/cfssljsonTLS/etcd/TLS/etcd/ca-config.jsonTLS/etcd/ca-csr.jsonTLS/etcd/generate_etcd_cert.shTLS/etcd/server-csr.jsonTLS/k8s/TLS/k8s/ca-config.jsonTLS/k8s/ca-csr.jsonTLS/k8s/kube-proxy-csr.jsonTLS/k8s/server-csr.jsonTLS/k8s/generate_k8s_cert.shTLS/cfssl.sh[root@k8s-master1 ~]# cd TLS[root@k8s-master1 TLS]# lscfssl cfssl-certinfo cfssljson cfssl.sh etcd k8s 将超cfssl移动到可执行目录中：运行脚本：（cfssl.sh）《注意脚本中curl原始是被注释掉了》123456[root@k8s-master1 TLS]# cat cfssl.shcurl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfsslcurl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljsoncurl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfocp -rf cfssl cfssl-certinfo cfssljson /usr/local/binchmod +x /usr/local/bin/cfssl* 执行完成脚本后：12[root@k8s-master1 TLS]# ls /usr/local/bin/cfssl cfssl-certinfo cfssljson 1234567891011121314151617181920212223242526[root@k8s-master1 TLS]# lscfssl cfssl-certinfo cfssljson cfssl.sh etcd k8s[root@k8s-master1 TLS]# cd etcd/[root@k8s-master1 etcd]# lsca-config.json ca-csr.json generate_etcd_cert.sh server-csr.json[root@k8s-master1 etcd]# vim server-csr.json[root@k8s-master1 etcd]# cat server-csr.json ###修改如下hosts中的host&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.171.134&quot;, &quot;192.168.171.135&quot;, &quot;192.168.171.136&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125; 执行脚本：123456789101112131415161718[root@k8s-master1 etcd]# sh generate_etcd_cert.sh2019/11/29 20:15:53 [INFO] generating a new CA key and certificate from CSR2019/11/29 20:15:53 [INFO] generate received request2019/11/29 20:15:53 [INFO] received CSR2019/11/29 20:15:53 [INFO] generating key: rsa-20482019/11/29 20:15:53 [INFO] encoded CSR2019/11/29 20:15:53 [INFO] signed certificate with serial number 241029724755122032470009319168181161854241472802019/11/29 20:15:53 [INFO] generate received request2019/11/29 20:15:53 [INFO] received CSR2019/11/29 20:15:53 [INFO] generating key: rsa-20482019/11/29 20:15:53 [INFO] encoded CSR2019/11/29 20:15:53 [INFO] signed certificate with serial number 129361955165654850485179523415464104941810882902019/11/29 20:15:53 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@k8s-master1 etcd]# ls server*server.csr server-csr.json server-key.pem server.pem 至此etcd密钥和证书生成完毕！！ 上传etcd.tar.gz 并解压到k8s-master1中：123456789101112[root@k8s-master1 ~]# tar zxvf etcd.tar.gzetcd/etcd/bin/etcd/bin/etcdetcd/bin/etcdctletcd/cfg/etcd/cfg/etcd.confetcd/ssl/etcd/ssl/ca.pemetcd/ssl/server.pemetcd/ssl/server-key.pemetcd.service 先来了解下etcd.service12345678910111213141516171819202122232425262728293031[root@k8s-master1 ~]# cat etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.conf ##etcd配置文件目录ExecStart=/opt/etcd/bin/etcd \ ##etcd执行文件所在的目录 --name=$&#123;ETCD_NAME&#125; \ --data-dir=$&#123;ETCD_DATA_DIR&#125; \ --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \ --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster-state=new \ --cert-file=/opt/etcd/ssl/server.pem \ --key-file=/opt/etcd/ssl/server-key.pem \ --peer-cert-file=/opt/etcd/ssl/server.pem \ --peer-key-file=/opt/etcd/ssl/server-key.pem \ --trusted-ca-file=/opt/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 12345[root@k8s-master1 etcd]# lsbin cfg ssl[root@k8s-master1 etcd]# cd bin/ ##此目录为etcd的执行文件目录（后期升级可直接下载二进制的可执行文件覆盖升级即可）[root@k8s-master1 bin]# lsetcd etcdctl 再来看下etcd的配置文件目录：1234567891011121314[root@k8s-master1 cfg]# cat etcd.conf#[Member]ETCD_NAME=&quot;etcd-1&quot; ##集群节点的nameETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot; ##数据存放位置ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.134:2380&quot; ##etcd集群内部通讯urlETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot; ##etcd客户端通讯url#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.134:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot; ##集群节点的配置信息ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot; ##集群简单认证的TOKENETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; ##集群的状态（新增的节点要改为existing） copy刚刚生成的etcd证书文件到指定的目录（/root/etcd/ssl）123[root@k8s-master1 etcd]# cp /root/TLS/etcd/&#123;ca,server,server-key&#125;.pem ssl/[root@k8s-master1 etcd]# ls ssl/ca.pem server-key.pem server.pem 然后下发配置etcd和etcd.service到三台集群机器：123456789101112131415161718192021222324252627[root@k8s-master1 ~]# lsanaconda-ks.cfg etcd etcd.service etcd.tar.gz TLS TLS.tar.gz[root@k8s-master1 ~]# scp -r etcd root@192.168.171.134:/opt/etcd 100% 16MB 51.2MB/s 00:00etcdctl 100% 13MB 58.8MB/s 00:00.etcd.conf.swp 100% 12KB 11.8MB/s 00:00etcd.conf 100% 523 634.0KB/s 00:00ca.pem 100% 1265 788.8KB/s 00:00server.pem 100% 1338 1.8MB/s 00:00server-key.pem 100% 1675 1.5MB/s 00:00[root@k8s-master1 ~]# scp -r etcd root@192.168.171.135:/opt/root@192.168.171.135&apos;s password:etcd 100% 16MB 82.4MB/s 00:00etcdctl 100% 13MB 92.3MB/s 00:00.etcd.conf.swp 100% 12KB 7.7MB/s 00:00etcd.conf 100% 523 169.7KB/s 00:00ca.pem 100% 1265 1.3MB/s 00:00server.pem 100% 1338 1.4MB/s 00:00server-key.pem 100% 1675 1.5MB/s 00:00[root@k8s-master1 ~]# scp -r etcd root@192.168.171.136:/opt/etcd 100% 16MB 68.7MB/s 00:00etcdctl 100% 13MB 80.8MB/s 00:00.etcd.conf.swp 100% 12KB 12.5MB/s 00:00etcd.conf 100% 523 385.2KB/s 00:00ca.pem 100% 1265 1.5MB/s 00:00server.pem 100% 1338 2.0MB/s 00:00server-key.pem 100% 1675 2.2MB/s 00 同理copyetcd.service文件：123456[root@k8s-master1 ~]# scp etcd.service root@192.168.171.134:/usr/lib/systemd/system/etcd.service 100% 1078 577.1KB/s 00:00[root@k8s-master1 ~]# scp etcd.service root@192.168.171.135:/usr/lib/systemd/system/etcd.service 100% 1078 780.0KB/s 00:00[root@k8s-master1 ~]# scp etcd.service root@192.168.171.136:/usr/lib/systemd/system/etcd.service 修改另外2台etcd的配置文件： 192.168.171.135中12345678910111213[root@k8s-master2 ~]# cat /opt/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd-2&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.135:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.135:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; 192.168.171.136中12345678910111213[root@k8s-node1 ~]# cat /opt/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd-3&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.136:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.136:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; 启动etcd（第一台启动的时候有些慢是因为在侦听其它节点）1234[root@k8s-master1 ~]# systemctl daemon-reload[root@k8s-master1 ~]# systemctl start etcd[root@k8s-master1 ~]# systemctl enable etcdCreated symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service. 查看etcd集群的日志：1234567891011[root@k8s-master1 ~]# tail /var/log/messages -fNov 29 21:06:20 localhost etcd: set the initial cluster version to 3.0Nov 29 21:06:20 localhost etcd: enabled capabilities for version 3.0Nov 29 21:06:24 localhost etcd: peer 92fcf2aa055d676f became activeNov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message reader)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 reader)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message writer)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 writer)Nov 29 21:06:24 localhost etcd: updating the cluster version from 3.0 to 3.3Nov 29 21:06:24 localhost etcd: updated the cluster version from 3.0 to 3.3Nov 29 21:06:24 localhost etcd: enabled capabilities for version 3.3 查看etcd集群的状态：123456789# /opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints=&quot;https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379&quot; \cluster-healthmember 3530acf25e9921b5 is healthy: got healthy result from https://192.168.171.134:2379member 833528c821fcdcd2 is healthy: got healthy result from https://192.168.171.135:2379member 92fcf2aa055d676f is healthy: got healthy result from https://192.168.171.136:2379cluster is healthy 二、部署Master节点2.1、自签证书12345678[root@k8s-master1 ~]# cd TLS/k8s/[root@k8s-master1 k8s]# pwd/root/TLS/k8s[root@k8s-master1 k8s]# lsca-config.json ca-csr.json generate_k8s_cert.sh kube-proxy-csr.json server-csr.jsonkube-proxy-csr.json：为kube-proxy服务自签的证书ca-config.json，ca-csr.json，server-csr.json：为Api-server服务自签的证书 2.2、划重点（K8S集群内部是用证书进行校验通信） 一定要把和API-SERVER 通信服务的IP写到如下hosts中（master节点，LB，etcd，keepalived，VIP）； 当然这个也是我之前的疑问，如果后期扩展了master 如何加入到当前集群？ 目前得到的验证是先提前多增加IP； 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 k8s]# cat server-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;10.0.0.1&quot;, &quot;127.0.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot;, &quot;192.168.171.134&quot;, &quot;192.168.171.135&quot;, &quot;192.168.171.136&quot;, &quot;192.168.171.137&quot;, &quot;192.168.171.138&quot;, &quot;192.168.171.139&quot;, &quot;192.168.171.188&quot;, &quot;192.168.171.140&quot;, &quot;192.168.171.141&quot;, &quot;192.168.171.142&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 执行脚本生成证书：12345678910111213141516171819202122232425[root@k8s-master1 k8s]# sh generate_k8s_cert.sh2019/11/30 16:08:18 [INFO] generating a new CA key and certificate from CSR2019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:18 [INFO] encoded CSR2019/11/30 16:08:18 [INFO] signed certificate with serial number 3418263221184942457507420707234268862304733819592019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:18 [INFO] encoded CSR2019/11/30 16:08:18 [INFO] signed certificate with serial number 2989165026649416994797859334541381614109130609662019/11/30 16:08:18 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).2019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:19 [INFO] encoded CSR2019/11/30 16:08:19 [INFO] signed certificate with serial number 114546326222977492622969866107478344620111189522019/11/30 16:08:19 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;). 查看生成的证书：12[root@k8s-master1 k8s]# ls *.pemca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem 准备部署master组件：二进制包下载地址：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#v1161 上传包中的k8s-master.tar.gz到/目录 1234567891011121314151617[root@k8s-master1 ~]# tar zxvf k8s-master.tar.gzkubernetes/kubernetes/bin/kubernetes/bin/kubectlkubernetes/bin/kube-apiserverkubernetes/bin/kube-controller-managerkubernetes/bin/kube-schedulerkubernetes/cfg/kubernetes/cfg/token.csvkubernetes/cfg/kube-apiserver.confkubernetes/cfg/kube-controller-manager.confkubernetes/cfg/kube-scheduler.confkubernetes/ssl/kubernetes/logs/kube-apiserver.servicekube-controller-manager.servicekube-scheduler.service copy刚刚生成的证书文件放到当前ssl中：12345[root@k8s-master1 kubernetes]# cp /root/TLS/k8s/*pem ssl/[root@k8s-master1 kubernetes]# lsbin cfg logs ssl[root@k8s-master1 kubernetes]# ls ssl/ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem kube-apiserver12345678910111213141516171819202122232425262728[root@k8s-master1 cfg]# cat kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--logtostderr=false \ ##输出日志--v=2 \ ##日志级别--log-dir=/opt/kubernetes/logs \ ##日志存放目录--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \ ##etcd集群IP--bind-address=192.168.171.134 \ ##绑定IP（可以为外网IP）--secure-port=6443 \ ##安全端口--advertise-address=192.168.171.134 \ ##集群内部通讯地址--allow-privileged=true \ ##允许pod有超级权限--service-cluster-ip-range=10.0.0.0/24 \ ##service的IP范围--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \ ##启动准入控制插件--authorization-mode=RBAC,Node \ ##授权模式--enable-bootstrap-token-auth=true \ ##bootstrap-token认证，自动颁发证书--token-auth-file=/opt/kubernetes/cfg/token.csv \ ##token文件--service-node-port-range=30000-32767 \ ##service的ip范围--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \--audit-log-maxage=30 \ ##如下均为日志的一些策略--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log&quot; kube-controller-manager123456789101112131415[root@k8s-master1 cfg]# cat kube-controller-manager.confKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \ ##日志存放路径--leader-elect=true \ ##选举模式--master=127.0.0.1:8080 \ ##连接本地api-server--address=127.0.0.1 \ ##监听地址--allocate-node-cidrs=true \ ##cni组件--cluster-cidr=10.244.0.0/16 \ ##cni组件IP段--service-cluster-ip-range=10.0.0.0/24 \ ##service范围和api-server中一致--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \--root-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \--experimental-cluster-signing-duration=87600h0m0s&quot; kube-scheduler1234567[root@k8s-master1 cfg]# cat kube-scheduler.confKUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--leader-elect \--master=127.0.0.1:8080 \--address=127.0.0.1&quot; 启动apiserver12345678910[root@k8s-master1 cfg]# cd[root@k8s-master1 ~]# mv kubernetes/ /opt/[root@k8s-master1 ~]# mv *.service /usr/lib/systemd/system/[root@k8s-master1 ~]# systemctl daemon-reload[root@k8s-master1 ~]# systemctl start kube-apiserver[root@k8s-master1 ~]# less /opt/kubernetes/logs/kube-apiserver.INFO ##查看启动日志[root@k8s-master1 ~]# ps aux | grep kuberoot 17717 24.2 18.0 549604 336048 ? Ssl 16:39 0:06 /opt/kubernetes/bin/kube-apiserver --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 --bind-address=192.168.171.134 --secure-port=6443 --advertise-address=192.168.171.134 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth=true --token-auth-file=/opt/kubernetes/cfg/token.csv --service-node-port-range=30000-32767--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem --kubelet-client-key=/opt/kubernetes/ssl/server-key.pem --tls-cert-file=/opt/kubernetes/ssl/server.pem --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem --client-ca-file=/opt/kubernetes/ssl/ca.pem --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem --etcd-cafile=/opt/etcd/ssl/ca.pem --etcd-certfile=/opt/etcd/ssl/server.pem --etcd-keyfile=/opt/etcd/ssl/server-key.pem --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/opt/kubernetes/logs/k8s-audit.logroot 17731 0.0 0.0 112724 988 pts/1 S+ 16:39 0:00 grep --color=auto kube 再次启动kube-controller-manager 及 kube-scheduler12345[root@k8s-master1 ~]# systemctl start kube-controller-manager[root@k8s-master1 ~]# systemctl start kube-scheduler[root@k8s-master1 ~]# systemctl enable kube-apiserver[root@k8s-master1 ~]# systemctl enable kube-controller-manager[root@k8s-master1 ~]# systemctl enable kube-scheduler 移动kubectl到可执行目录12345678910[root@k8s-master1 ~]# mv /opt/kubernetes/bin/kubectl /usr/local/bin/[root@k8s-master1 ~]# kubectl get nodeNo resources found in default namespace.[root@k8s-master1 ~]# kubectl get cs ##经过查看发现了此版本的bugNAME AGEcontroller-manager &lt;unknown&gt;scheduler &lt;unknown&gt;etcd-2 &lt;unknown&gt;etcd-0 &lt;unknown&gt;etcd-1 &lt;unknown&gt; 如上bug：https://segmentfault.com/a/1190000020912684 启用TLS Bootstrapping为kubelet TLS Bootstrapping 授权：1234# cat /opt/kubernetes/cfg/token.csv c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;格式：token,用户,uid,用户组 给kubelet-bootstrap授权： 自动的给kubelet创建证书 123kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap token也可自行生成替换： 1head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos; ==但apiserver配置的token必须要与node节点bootstrap.kubeconfig配置里一致。== 三、部署Worker Node二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ 上传k8s-node.tar.gz到node节点1234567891011121314151617181920[root@k8s-node1 ~]# tar zxvf k8s-node.tar.gzcni-plugins-linux-amd64-v0.8.2.tgzdaemon.jsondocker-18.09.6.tgzdocker.servicekubelet.servicekube-proxy.servicekubernetes/kubernetes/bin/kubernetes/bin/kubeletkubernetes/bin/kube-proxykubernetes/cfg/kubernetes/cfg/kubelet-config.ymlkubernetes/cfg/bootstrap.kubeconfigkubernetes/cfg/kube-proxy.kubeconfigkubernetes/cfg/kube-proxy.confkubernetes/cfg/kubelet.confkubernetes/cfg/kube-proxy-config.ymlkubernetes/ssl/kubernetes/logs/ 3.1、配置并启动Docker123456789101112131415161718# tar zxvf docker-18.09.6.tgz# mv docker/* /usr/bin[root@k8s-node1 ~]# ls /usr/bin/docker dockerd docker-init docker-proxy domainname# mkdir /etc/docker[root@k8s-node1 ~]# cat daemon.json ##配置镜像加速器&#123; &quot;registry-mirrors&quot;: [&quot;http://bc437cce.m.daocloud.io&quot;], &quot;insecure-registries&quot;: [&quot;192.168.171.170&quot;]&#125;# mv daemon.json /etc/docker# mv docker.service /usr/lib/systemd/system# systemctl start docker# systemctl enable docker[root@k8s-node1 ~]# ps aux | grep dockerroot 17326 2.1 1.5 405704 28404 ? Ssl 17:05 0:00 /usr/bin/dockerdroot 17333 1.2 0.8 316224 15048 ? Ssl 17:05 0:00 containerd --config /var/run/docker/containerd/containerd.toml --log-level inforoot 17534 0.0 0.0 112724 988 pts/2 R+ 17:05 0:00 grep --color=auto docker 在查看docker info的时候发现了：12WARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled 解决方法：12345678vim /etc/sysctl.conf添加以下内容net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1最后再执行sysctl -p 3.2、部署kubelet和kube-proxy在master上拷贝证书到Node（有多少node节点就需要scp到多少节点）：123[root@k8s-master1 ~]# cd TLS/k8s/[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.136:/opt/kubernetes/ssl/[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.137:/opt/kubernetes/ssl/ node节点目录123456789101112131415[root@k8s-node1 ~]# cd kubernetes/[root@k8s-node1 kubernetes]# tree ..├── bin│ ├── kubelet│ └── kube-proxy├── cfg│ ├── bootstrap.kubeconfig│ ├── kubelet.conf│ ├── kubelet-config.yml│ ├── kube-proxy.conf│ ├── kube-proxy-config.yml│ └── kube-proxy.kubeconfig├── logs└── ssl 先来看下几个主要的配置文件123456[root@k8s-node1 cfg]# lsbootstrap.kubeconfig kubelet.conf kubelet-config.yml kube-proxy.conf kube-proxy-config.yml kube-proxy.kubeconfigconf：基本配置文件kubeconfig：连接apiserver的配置文件yml：主要配置文件 kubelet.conf1234567891011[root@k8s-node1 cfg]# cat kubelet.confKUBELET_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--hostname-override=k8s-node1 \ ##每个node的name（必须要唯一）--network-plugin=cni \ ##指定网路组件--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \ ##配置文件--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \--config=/opt/kubernetes/cfg/kubelet-config.yml \--cert-dir=/opt/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; ##镜像 bootstrap.kubeconfig（自动为即将要加入集群的node颁发证书）12345678910111213141516171819[root@k8s-node1 cfg]# cat bootstrap.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem ##拿着master的ca证书 server: https://192.168.171.134:6443 ##master的地址 name: kubernetescontexts:- context: cluster: kubernetes user: kubelet-bootstrap name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kubelet-bootstrap user: token: c47ffb939f5ca36231d9e3121a252940 ## 这个token一定要和如上master上token一致 我们也来了解下启动kubelet后如何和apiserver通信的： kubelet 启动带着bootstrap.kubeconfig请求apiserver，apiserver首先会校验所携带的token是否正确，正确则会颁发证书，不正确则会启动失败。 kubelet-config.yml12345678910111213141516171819202122232425262728293031[root@k8s-node1 cfg]# cat kubelet-config.ymlkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 0.0.0.0port: 10250readOnlyPort: 10255cgroupDriver: cgroupfs ##底层驱动（和docker一致）clusterDNS: ##dns- 10.0.0.2clusterDomain: cluster.local ##域failSwapOn: false ##swap关闭authentication: ##认证信息 anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30sevictionHard: ##资源配置 imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%maxOpenFiles: 1000000maxPods: 110 kube-proxy.kubeconfig1234567891011121314151617181920[root@k8s-node1 cfg]# cat kube-proxy.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem server: https://192.168.171.134:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: client-certificate: /opt/kubernetes/ssl/kube-proxy.pem client-key: /opt/kubernetes/ssl/kube-proxy-key.pem kube-proxy-config.yml123456789101112131415[root@k8s-node1 cfg]# vim kube-proxy-config.ymlkind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1address: 0.0.0.0metricsBindAddress: 0.0.0.0:10249clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfighostnameOverride: k8s-node1 ##全node唯一clusterCIDR: 10.0.0.0/24mode: ipvs ##模式ipvs: scheduler: &quot;rr&quot;iptables: masqueradeAll: true 启动kubelet、kube-proxy服务12345678910111213# mv kubernetes /opt# cp kubelet.service kube-proxy.service /usr/lib/systemd/system修改以下三个文件中IP地址：# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.134:6443kubelet.kubeconfig: server: https://192.168.171.134:6443kube-proxy.kubeconfig: server: https://192.168.171.134:6443修改以下两个文件中主机名：# grep hostname *kubelet.conf:--hostname-override=k8s-node1 \kube-proxy-config.yml:hostnameOverride: k8s-node1 123456789101112131415161718[root@k8s-node1 ~]# systemctl start kubelet[root@k8s-node1 ~]# systemctl status kubelet● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled) Active: active (running) since 六 2019-11-30 19:10:01 CST; 11s ago Main PID: 17702 (kubelet) Tasks: 9 Memory: 17.2M CGroup: /system.slice/kubelet.service └─17702 /opt/kubernetes/bin/kubelet --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --hostname-override=k8s-node1 --network-plugin=cni --kubeco...11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a11月 30 19:10:01 k8s-node1 systemd[1]: Stopped Kubernetes Kubelet.11月 30 19:10:01 k8s-node1 systemd[1]: Unit kubelet.service entered failed state.11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service failed.11月 30 19:10:01 k8s-node1 systemd[1]: Started Kubernetes Kubelet.[root@k8s-node1 ~]# systemctl enable kubelet 查看kubelet日志：1234567less /opt/kubernetes/logs/kubelet.INFO其中我们会看到：W1130 19:27:08.379468 17702 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.dE1130 19:27:08.929388 17702 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin isnot ready: cni config uninitialized如上是因为cni的组件没有安装，稍后安装后即可恢复； 然后我们再次回到master节点 查看是否有node节点：1234567891011[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo 2m1s kubelet-bootstrap Pending[root@k8s-master1 k8s]# kubectl certificate approve node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNocertificatesigningrequest.certificates.k8s.io/node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo approved[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo 14m kubelet-bootstrap Approved,Issued[root@k8s-master1 k8s]# kubectl get node ##等待配置完毕cni则会readyNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 25s v1.16.0 启动kube-proxy服务：12[root@k8s-node1 ~]# systemctl start kube-proxy[root@k8s-node1 ~]# systemctl status kube-proxy 查看kube-proxy日志：12345[root@k8s-node1 ~]# tailf /opt/kubernetes/logs/kube-proxy.INFOI1130 19:32:23.692156 18623 proxier.go:1729] Not using `--random-fully` in the MASQUERADE rule for iptables because the local version of iptables does not support it解决方案：https://blog.51cto.com/juestnow/2440260 3.3、部署CNI网络二进制包下载地址：https://github.com/containernetworking/plugins/releases 1234567# mkdir /opt/cni/bin /etc/cni/net.d# tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz –C /opt/cni/bin确保kubelet启用CNI：# cat /opt/kubernetes/cfg/kubelet.conf --network-plugin=cni 3.4、同理增加另外一个node节点1234567891011121314151617181920212223242526272829303157 tar zxvf k8s-node.tar.gz58 mv *.service /usr/lib/systemd/system/59 tar zxvf docker-18.09.6.tgz60 mv docker/* /usr/bin/61 mkdir /etc/docker62 vim daemon.json63 mv daemon.json /etc/docker/64 systemctl start docker65 systemctl enable docker66 systemctl status docker67 mv kubernetes/ /opt/68 cd /opt/kubernetes/69 ls70 cd cfg/71 ls72 vim bootstrap.kubeconfig73 vim kubelet.conf74 vim kubelet-config.yml75 vim kube-proxy.conf76 vim kube-proxy-config.yml77 vim kube-proxy.kubeconfig78 grep 192 *79 grep hostname *80 systemctl start kubelet81 systemctl start kube-proxy82 systemctl enable kubelet83 systemctl enable kube-proxy84 systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy85 mkdir /opt/cni/bin /etc/cni/net.d -p86 cd87 tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/ 虽然如上我只是把node2节点上的操作历史copy了一下，但是足以证明正确的操作步骤就是如上这些步骤，唯一需要注意的地方就是 如上的 kubelet和kube-proxy的配置文件。 3.5、部署flannel组件如要实现cni网路覆盖，我们就必须部署实现这个组件的flannel服务。 在master上操作： 上传kube-flannel.yaml到/目录12345678910111213141516[root@k8s-master1 ~]# cat kube-flannel.yaml ##来看几个主要的信息：1、 net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125;如上的网路信息要和：[root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-controller-manager.conf--cluster-cidr=10.244.0.0/16 \ 一致2、（DaemonSet模式：每个node节点都会自动部署这个服务）apiVersion: apps/v1kind: DaemonSet 在Master执行：123456789[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yaml[root@k8s-master1 ~]# kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-d2gzx 1/1 Running 0 51skube-flannel-ds-amd64-lwsnd 1/1 Running 0 51s[root@k8s-master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 67m v1.16.0k8s-node2 Ready &lt;none&gt; 30m v1.16.0 授权apiserver访问kubelet为提供安全性，kubelet禁止匿名访问，必须授权才可以。 上传apiserver-to-kubelet-rbac.yaml到/目录1234567891011121314151617181920212223242526272829303132333435[root@k8s-master1 ~]# cat apiserver-to-kubelet-rbac.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - &quot;&quot; resources: ##允许直接在master上操作如下的权限 - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics - pods/log verbs: - &quot;*&quot;---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: &quot;&quot;roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes 测试：123456789101112131415161718192021222324252627[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system ##没有权限查看Error from server (Forbidden): Forbidden (user=kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-flannel-ds-amd64-d2gzx)[root@k8s-master1 ~]# kubectl apply -f apiserver-to-kubelet-rbac.yamlclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system ##现在可以查看了I1130 12:30:26.695707 1 main.go:514] Determining IP address of default interfaceI1130 12:30:26.698072 1 main.go:527] Using interface with name ens33 and address 192.168.171.136I1130 12:30:26.698106 1 main.go:544] Defaulting external address to interface address (192.168.171.136)[root@k8s-master1 ~]# kubectl create deployment web --image=nginx ##创建测试deploymentdeployment.apps/web created[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-d86c95cc9-ztx9n 1/1 Running 0 2m49s 10.244.0.2 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl expose deployment web --port=80 --type=NodePort ##创建一个port测试下nginx是否OKservice/web exposed[root@k8s-master1 ~]# kubectl get po,svcNAME READY STATUS RESTARTS AGEpod/web-d86c95cc9-k9vnf 1/1 Running 0 2m34sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4h49mservice/web NodePort 10.0.0.34 &lt;none&gt; 80:32762/TCP 17m 至此单master节点的K8S集群搭建完毕！ 四、部署Web UI和DNS上传yaml/dashboard.yaml12345678910111213141516171819202122232425262728293031323334# vi dashboard.yaml…kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard…[root@k8s-master1 ~]# kubectl apply -f dashboard.yamlnamespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 创建token登录：123456789101112131415161718192021##在创建token之前我们需要先创建service account[root@k8s-master1 ~]# cat dashboard-adminuser.yamlapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 创建service account并绑定默认cluster-admin管理员集群角色：123[root@k8s-master1 ~]# kubectl apply -f dashboard-adminuser.yamlserviceaccount/admin-user createdclusterrolebinding.rbac.authorization.k8s.io/admin-user created 获取token1234567891011121314[root@k8s-master1 ~]# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)Name: admin-user-token-bccwwNamespace: kubernetes-dashboardLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 6e6e1b2d-a0a3-4150-a611-98ce1653b79cType: kubernetes.io/service-account-tokenData====ca.crt: 1359 bytesnamespace: 20 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IktJYmdRdDdkbW1US0dnOHRKemdPMjJ6eUEzTXEtMGQyS0h6cWRpRUVLRE0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWJjY3d3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2ZTZlMWIyZC1hMGEzLTQxNTAtYTYxMS05OGNlMTY1M2I3OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YusDtTl_glNewEO0kMaiZDqOcbSMkRNY6sRT9BQYbzTjdmediGHcEB49wHepo_mXsW0isBnu4Mgpb4KL5y27OkE2hFICwQwQBX5gvHQI2CxuoHaVVi7G8eZn85fR7aKmKi7Uxppv6qOL5icZyl_74_-iQVIm3U59B-x2zoyoUa3tsFgQEpUWvkmbCajD-4sANU-UMyisR3uMdXvnyvz2oCUQBjuqJ5ZqqAupqrvtoJ1L27vHK1t7i_sLgVR_2X8MARrwgynHatEYAODVEsVRMJCBzR4ZW09xcCSbeQ1CopNyGbyPi7o9re_9FyGK18y3q7EmjaEOr2NJ3Yk0MesIyw 部署coreDNS1234567[root@k8s-master1 ~]# kubectl apply -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created 测试是否dns正常1234567891011121314151617181920212223[root@k8s-master1 k8s]# kubectl apply -f bs.yamlpod/busybox created[root@k8s-master1 ~]# kubectl exec -it busybox sh/ # ping 10.0.0.34 ##测试内网IP是否通过PING 10.0.0.34 (10.0.0.34): 56 data bytes64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.086 ms64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.068 ms^C--- 10.0.0.34 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.068/0.077/0.086 ms/ # ping web ##测试dns是否可以解析PING web (10.0.0.34): 56 data bytes64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.049 ms64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.065 ms/ # nslookup kubernetes （均可以解析）Server: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local 五、Master高可用5.1、部署Master组件（与Master1一致）拷贝master1/opt/kubernetes和service文件：1234# scp –r /opt/kubernetes root@192.168.171.135:/opt# scp -r /opt/etcd/ssl/ root@192.168.171.135:/opt/etcd/# scp /usr/lib/systemd/system/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125;.service root@192.168.171.135:/usr/lib/systemd/system# scp /usr/local/bin/kubectl root@192.168.171.135:/usr/local/bin/ 修改apiserver配置文件为本地IP：123456789# cat /opt/kubernetes/cfg/kube-apiserver.conf KUBE_APISERVER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \--bind-address=192.168.171.135 \--secure-port=6443 \--advertise-address=192.168.171.135 \…… 启动kube-apiserver，kube-controller-manager，kube-scheduler123456789[root@k8s-master2 cfg]# systemctl start kube-apiserver[root@k8s-master2 cfg]# systemctl start kube-controller-manager[root@k8s-master2 cfg]# systemctl start kube-scheduler[root@k8s-master2 cfg]# systemctl enable kube-apiserverCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /usr/lib/systemd/system/kube-apiserver.service.[root@k8s-master2 cfg]# systemctl enable kube-controller-managerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.[root@k8s-master2 cfg]# systemctl enable kube-schedulerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service. 在master2上面查看node节点的po12345678910111213[root@k8s-master2 cfg]# kubectl get node -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-node1 Ready &lt;none&gt; 25h v1.16.0 192.168.171.136 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://18.9.6k8s-node2 Ready &lt;none&gt; 25h v1.16.0 192.168.171.137 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://18.9.6[root@k8s-master2 cfg]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-gbd2m 1/1 Running 2 21h 10.244.0.9 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-d2gzx 1/1 Running 1 24h 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-lwsnd 1/1 Running 2 24h 192.168.171.137 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master2 cfg]# kubectl get po -n kubernetes-dashboard -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdashboard-metrics-scraper-566cddb686-wrkfl 1/1 Running 1 23h 10.244.1.8 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard-7b5bf5d559-csfwm 1/1 Running 1 23h 10.244.1.6 k8s-node2 &lt;none&gt; &lt;none&gt; 5.2、部署Nginx负载均衡12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364nginx rpm包：http://nginx.org/packages/rhel/7/x86_64/RPMS/# rpm -vih http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.0-1.el7.ngx.x86_64.rpm[root@localhost ~]# cat /etc/nginx/nginx.confuser nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;####此处↓stream &#123; log_format main &apos;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent&apos;; access_log /var/log/nginx/k8s-access.log main; upstream k8s-apiserver &#123; server 192.168.171.134:6443; server 192.168.171.135:6443; &#125; server &#123; listen 6443; proxy_pass k8s-apiserver; &#125;&#125;####此处↑http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125;# systemctl start nginx# systemctl enable nginx[root@localhost ~]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:6443 0.0.0.0:* LISTEN 7142/nginx: master 5.3、Nginx+KeepAlived高可用主节点（192.168.171.138）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# yum install keepalived# vim /etc/keepalived/keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_MASTER&#125; vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot;&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 100 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.171.188/24 &#125; track_script &#123; check_nginx &#125; &#125;# cat /etc/keepalived/check_nginx.sh #!/bin/bashcount=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)if [ &quot;$count&quot; -eq 0 ];then exit 1else exit 0fi# chmod +x /etc/keepalived/check_nginx.sh# systemctl start keepalived# systemctl enable keepalived 备节点（192.168.171.139）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# cat /etc/keepalived/keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_BACKUP&#125; vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot;&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 90 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.171.188/24 &#125; track_script &#123; check_nginx &#125; &#125;# cat /etc/keepalived/check_nginx.sh #!/bin/bashcount=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)if [ &quot;$count&quot; -eq 0 ];then exit 1else exit 0fi# chmod +x /etc/keepalived/check_nginx.sh# systemctl start keepalived# systemctl enable keepalived 查看虚拟VIP123456789101112131415[root@localhost ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9b:85:86 brd ff:ff:ff:ff:ff:ff inet 192.168.171.138/24 brd 192.168.171.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.171.188/24 scope global secondary ens33 valid_lft forever preferred_lft forever inet6 fe80::d3c5:e3e2:26f6:f6b5/64 scope link noprefixroute valid_lft forever preferred_lft forever 5.4、修改Node连接VIP1234567891011121314151617[root@k8s-node1 ~]# cd /opt/kubernetes/cfg/[root@k8s-node1 cfg]# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.134:6443kubelet.kubeconfig: server: https://192.168.171.134:6443kube-proxy.kubeconfig: server: https://192.168.171.134:6443[root@k8s-node1 cfg]# sed -i &apos;s#192.168.171.134#192.168.171.188#&apos; *[root@k8s-node1 cfg]# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.188:6443kubelet.kubeconfig: server: https://192.168.171.188:6443kube-proxy.kubeconfig: server: https://192.168.171.188:6443[root@k8s-node1 cfg]# systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy同理操作其它node节点 测试VIP是否正常工作：1234567891011121314[root@k8s-node2 cfg]# curl -k --header &quot;Authorization: Bearer c47ffb939f5ca36231d9e3121a252940&quot; https://192.168.171.188:6443/version&#123; &quot;major&quot;: &quot;1&quot;, &quot;minor&quot;: &quot;16&quot;, &quot;gitVersion&quot;: &quot;v1.16.0&quot;, &quot;gitCommit&quot;: &quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, &quot;gitTreeState&quot;: &quot;clean&quot;, &quot;buildDate&quot;: &quot;2019-09-18T14:27:17Z&quot;, &quot;goVersion&quot;: &quot;go1.12.9&quot;, &quot;compiler&quot;: &quot;gc&quot;, &quot;platform&quot;: &quot;linux/amd64&quot;&#125;分别在node1和node2上测试，你会发现nginx会以轮训的方式分别请求apiserver； 至此生产级K8S高可用集群搭建完毕！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个生产级K8S高可用集群（1）]]></title>
    <url>%2F2019%2F11%2F26%2F%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1不多说了，2019年底再开始~~ 一、K8S特性①自我修复在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端的请求，确保服务不中断。 ②弹性伸缩使用命令、UI管控或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行业务。 ③自动部署和回滚K8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不影响业务。 ④服务发现和负载均衡K8S为多个容器提供一个统一的访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。 ⑤机密和配置管理管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。 ⑥存储编排挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网路存储（如NFS，ClusterFS，Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 ⑦批处理提供一次性任务，定时任务；满足批量数据处理和分析的场景。 二、K8S集群架构和组件 Master Master主要负责资源调度，控制副本，和提供统一访问集群的入口。 Node Node由Master管理，并汇报容器状态给Master，同时根据Master要求管理容器生命周期。 Pod Docker最小部署单元是容器，而Kubernetes最小部署单元是Pod，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，一个Pod在同一台Node上运行。 Service Service一个应用服务抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。Service代理Pod集合对外表现是为一个访问入口，分配一个集群IP地址，来自这个IP的请求将负载均衡转发到后端Pod中的容器。用过负载均衡器的朋友可能很好理解，其实Service就是一个抽象的负载均衡器。Service通过Lable Selector选择一组Pod提供服务。 Lable 标签是一个key=value的键值对，附加在某个资源上，每个对象可以有多个标签，然后根据这个lable关联、查询和筛选。就像Service与Pod，当多个Service、多个Pod情况下，访问某个Service怎么就知道转发到指定Pod呢？ Volume 数据卷，挂载宿主机文件、目录或者外部存储到Pod中，为应用服务提供存储，也可以Pod中容器之间共享数据。 Namespace 命名空间将资源对象逻辑上分配到不同Namespace，可以是不同的项目、用户等区分管理，并设定控制策略，从而实现多租户。命名空间也称为虚拟集群。 2.1、Master组件 kube-apiserver Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。 kube-controller-manager 处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。 kube-scheduler 根据调度算法为新创建的Pod选择一个Node节点，可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上。 etcd 分布式键值存储系统。用于保存集群状态数据，比如Pod、Service等对象信息。 2.2、Node组件 kubelet kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。cadvise:监控容器和节点资源 kube-proxy service 在逻辑上代表了后端的多个 Pod，外界通过 service 访问 Pod。service 接收到的请求是如何转发到 Pod 的呢？这就是 kube-proxy 要完成的工作。每个 Node 都会运行 kube-proxy 服务，它负责将访问 service 的 TCP/UPD 数据流转发到后端的容器。如果有多个副本，kube-proxy 会实现负载均衡。 三、下面是更高层次抽象对象： ReplicaSet（确保预期的Pod副本数量） 确保任何给定时间指定的Pod副本数量，并提供声明式更新等功能。 Deployment（无状态应用部署） Deployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。官方建议使用Deployment管理ReplicaSets，而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象，因此Deployment将会是使用最频繁的资源对象。 StatefulSet（有状态应用部署） StatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。典型场景：++Zookeper集群++ DaemonSet（确保所有Node运行同一个Pod） DaemonSet确保所有节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。典型场景：++在每个节点部署日志收集程序（如filebeat），监控程序（agent）++ Job（一次性任务） 一次性任务，运行完成后Pod销毁，不再重新启动新容器。还可以任务定时运行。 Cron Job（定时任务） 定时任务，一个CronJob对象就像一个crontab文件的一行。给定时间定期运行，以Cron格式编写。典型场景：数据库备份，发送邮件]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产级k8s二进制v14.1高可用部署]]></title>
    <url>%2F2019%2F06%2F18%2F%E7%94%9F%E4%BA%A7%E7%BA%A7k8s%E4%BA%8C%E8%BF%9B%E5%88%B6v14.1%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、环境准备1.1、角色划分12345610.8.13.80 vip 10.8.13.81 master01 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.82 master02 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.83 master03 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.84 node01 kubelet、docker、kube_proxy、flanneld10.8.13.85 node02 kubelet、docker、kube_proxy、flanneld 1.2、各主机ssh互通12#ssh-keygen#ssh-copy-id 10.8.13.82(83-85) 1.3、环境初始化1.3.1、停止iptables12systemctl stop firewalld.service systemctl disable firewalld.service 1.3.2、关闭selinux123# cat /etc/selinux/config SELINUX=disabled# setenforce 0 1.3.3、设置sysctl，开启路由转发1234567891011121314151617181920212223242526272829303132333435# cat /etc/sysctl.conf fs.file-max=1000000 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 net.ipv4.ip_forward = 1 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.ipv4.tcp_max_syn_backlog = 16384 net.core.netdev_max_backlog = 32768 net.core.somaxconn = 32768 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_fin_timeout = 20 net.ipv4.tcp_synack_retries = 2 net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_max_orphans = 3276800 net.ipv4.ip_local_port_range = 1024 65000 net.nf_conntrack_max = 6553500 net.netfilter.nf_conntrack_max = 6553500 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60 net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120 net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120 net.netfilter.nf_conntrack_tcp_timeout_established = 3600 1.3.4、加载ipvs123456789cat &lt;&lt; EOF | tee /etc/sysconfig/modules/ipvs.modules#!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 二、集群各功能模块描述 Master节点：Master节点上面主要由四个模块组成，etcd，APIServer，schedule,controller-manager（haproxy、keepalived高可用后面单独说） etcd：etcd是一个高可用的键值存储系统，kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。 APIServer:APIServer负责对外提供Restful的kubernetes API的服务，它是系统管理指令的统一接口，任何对资源的增删该查都要交给APIServer处理后再交给etcd。kubectl(kubernetes提供的客户端工具，该工具内部是对kubernetes API的调用）是直接和APIServer交互的。 schedule:schedule负责调度Pod到合适的Node上，如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定。 kubernetes目前提供了调度算法，同样也保留了接口。用户根据自己的需求定义自己的调度算法。 controller manager:如果APIServer做的是前台的工作的话，那么controller manager就是负责后台的。每一个资源都对应一个控制器。而control manager就是负责管理这些控制器的，比如我们通过APIServer创建了一个Pod，当这个Pod创建成功后，APIServer的任务就算完成了。 Node节点：每个Node节点主要由四个模板组成：kublet， kube-proxy，docker，flanneld kube-proxy:该模块实现了kubernetes中的服务发现和反向代理功能。kube-proxy支持TCP和UDP连接转发，默认基Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响，另外，kube-proxy还支持session affinity。 kublet：kublet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上的所有容器，但是如果容器不是通过kubernetes创建的，它并不会管理。本质上，它负责使Pod的运行状态与期望的状态一致。 flanneld：源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 三、下载链接12345678910Client Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gzServer Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gzNode Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-node-linux-amd64.tar.gzetcdhttps://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gzflannelhttps://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 四、Master部署以下操作都在master01上执行，生成证书之后拷贝到master02和master03 4.1、下载软件1234wget https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gzwget https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gzwget https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gzwget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 4.2、ssl安装1234567wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 4.3、创建etcd证书在所有节点（master01-03、node01-02）创建此路径12mkdir /k8s/etcd/&#123;bin,cfg,ssl&#125; -pmkdir /k8s/kubernetes/&#123;bin,cfg,ssl&#125; -p 1)、etcd ca配置123456789101112131415161718192021cd /k8s/etcd/ssl/cat &lt;&lt; EOF | tee ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;etcd&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF 2)、etcd ca证书12345678910111213141516cat &lt;&lt; EOF | tee ca-csr.json&#123; &quot;CN&quot;: &quot;etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOF 3)、etcd server证书123456789101112131415161718192021cat &lt;&lt; EOF | tee server-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;10.8.13.81&quot;, &quot;10.8.13.82&quot;, &quot;10.8.13.83&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOF 4)、生成etcd ca证书和私钥 初始化ca123456789101112cfssl gencert -initca ca-csr.json | cfssljson -bare ca [root@master01 ssl]# lsca-config.json ca-csr.json server-csr.json[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca 2019/05/01 16:13:54 [INFO] generating a new CA key and certificate from CSR2019/05/01 16:13:54 [INFO] generate received request2019/05/01 16:13:54 [INFO] received CSR2019/05/01 16:13:54 [INFO] generating key: rsa-20482019/05/01 16:13:54 [INFO] encoded CSR2019/05/01 16:13:54 [INFO] signed certificate with serial number 144752911121073185391033754516204538929473929443[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server-csr.json 生成server证书123456789101112cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server2019/05/01 16:18:53 [INFO] generate received request2019/05/01 16:18:53 [INFO] received CSR2019/05/01 16:18:53 [INFO] generating key: rsa-20482019/05/01 16:18:54 [INFO] encoded CSR2019/05/01 16:18:54 [INFO] signed certificate with serial number 3881225870405999866391591631675576849701590300572019/05/01 16:18:54 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server.csr server-csr.json server-key.pem server.pem 4.4、etcd安装1）解压缩1234tar -zxf etcd-v3.3.11-linux-amd64.tar.gzcd etcd-v3.3.11-linux-amd64/cp etcd etcdctl /k8s/etcd/bin/mkdir /data1/etcd 2）配置etcd主文件1234567891011121314151617181920212223vim /k8s/etcd/cfg/etcd.conf #[Member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.81:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot; #[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.81:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; 3）配置etcd启动文件123456789101112131415161718vim /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/data1/etcd/EnvironmentFile=-/k8s/etcd/cfg/etcd.conf# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /k8s/etcd/bin/etcd --name=\&quot;$&#123;ETCD_NAME&#125;\&quot; --data-dir=\&quot;$&#123;ETCD_DATA_DIR&#125;\&quot; --listen-client-urls=\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\&quot; --listen-peer-urls=\&quot;$&#123;ETCD_LISTEN_PEER_URLS&#125;\&quot; --advertise-client-urls=\&quot;$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\&quot; --initial-cluster-token=\&quot;$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\&quot; --initial-cluster=\&quot;$&#123;ETCD_INITIAL_CLUSTER&#125;\&quot; --initial-cluster-state=\&quot;$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\&quot; --cert-file=\&quot;$&#123;ETCD_CERT_FILE&#125;\&quot; --key-file=\&quot;$&#123;ETCD_KEY_FILE&#125;\&quot; --trusted-ca-file=\&quot;$&#123;ETCD_TRUSTED_CA_FILE&#125;\&quot; --client-cert-auth=\&quot;$&#123;ETCD_CLIENT_CERT_AUTH&#125;\&quot; --peer-cert-file=\&quot;$&#123;ETCD_PEER_CERT_FILE&#125;\&quot; --peer-key-file=\&quot;$&#123;ETCD_PEER_KEY_FILE&#125;\&quot; --peer-trusted-ca-file=\&quot;$&#123;ETCD_PEER_TRUSTED_CA_FILE&#125;\&quot; --peer-client-cert-auth=\&quot;$&#123;ETCD_PEER_CLIENT_CERT_AUTH&#125;\&quot;&quot;Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 4)、拷贝master01etcd的证书、配置文件、启动文件到master02和master03对应路径下12345678scp /k8s/etcd/ssl/* 10.8.13.82:/k8s/etcd/ssl/scp /k8s/etcd/ssl/* 10.8.13.83:/k8s/etcd/ssl/scp /k8s/etcd/cfg/* 10.8.13.82:/k8s/etcd/cfg/scp /k8s/etcd/cfg/* 10.8.13.83:/k8s/etcd/cfg/scp /k8s/etcd/bin/* 10.8.13.82:/k8s/etcd/bin/scp /k8s/etcd/bin/* 10.8.13.83:/k8s/etcd/bin/scp /usr/lib/systemd/system/etcd.service 10.8.13.82:/usr/lib/systemd/system/etcd.servicescp /usr/lib/systemd/system/etcd.service 10.8.13.83:/usr/lib/systemd/system/etcd.service 5)、修改master02、master03 etcd的conf配置文件 matser02 etcd.conf配置如下：123456789101112131415161718192021222324ssh 10.8.13.82vim /k8s/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd02&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.82:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.82:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; matser03 etcd.conf配置如下：123456789101112131415161718192021222324ssh 10.8.13.83vim /k8s/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd03&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.83:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.83:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; 6)、启动etcd服务，并加入开机自启动(master三个节点都执行)123systemctl daemon-reloadsystemctl enable etcdsystemctl start etcd 7)、etcd服务检查123456/k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379&quot; cluster-health以下为输出：member 262d942ab474feaa is healthy: got healthy result from https://10.8.13.82:2379member 3e95c59733e7d54f is healthy: got healthy result from https://10.8.13.83:2379member fe03446cb13e0221 is healthy: got healthy result from https://10.8.13.81:2379cluster is healthy 至此etcd安装完成。。。 4.5、haproxy安装配置1)、master01配置(需要注意的是端口自定义为16443) 1yum -y install haproxy master01、master02、master03都安装haproxy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364vim /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# kubernetes apiserver frontend which proxys to the backends#---------------------------------------------------------------------frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------backend kubernetes-apiserver mode tcp balance roundrobin server k8s01 10.8.13.81:6443 check server k8s02 10.8.13.82:6443 check server k8s03 10.8.13.83:6443 check#---------------------------------------------------------------------# collection haproxy statistics message#---------------------------------------------------------------------listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\ Statistics stats uri /admin?stats 2）拷贝master01的haproxy到master02和master03对应路径下12scp /etc/haproxy/haproxy.cfg 10.8.13.82:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg 10.8.13.83:/etc/haproxy/haproxy.cfg 3)启动haproxy服务，并加入开机自启动(master三个节点都执行)123systemctl daemon-reloadsystemctl enable haproxysystemctl start haproxy 4.6、keepalived安装配置1）master01配置 1yum -y install keepalived master01、master02、master03都安装keepalived12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens160 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 2）master02配置12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens160 virtual_router_id 51 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 3）master03配置12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens160 virtual_router_id 51 priority 98 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 4）启动keepalived服务（vip在master01上）123456789101112131415161718192021222324252627282930systemctl daemon-reloadsystemctl enable keepalivedsystemctl start keepalived[root@master01 ~]# systemctl status keepalived● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:33 CST; 3 days ago Process: 992 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 1115 (keepalived) CGroup: /system.slice/keepalived.service ├─1115 /usr/sbin/keepalived -D ├─1116 /usr/sbin/keepalived -D └─1117 /usr/sbin/keepalived -DWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.[root@hwzx-test-cmpmaster01 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:50:56:90:22:79 brd ff:ff:ff:ff:ff:ff inet 10.8.13.81/24 brd 10.8.13.255 scope global ens160 valid_lft forever preferred_lft forever inet 10.8.13.80/32 scope global ens160 valid_lft forever preferred_lft forever inet6 fe80::6772:8bb6:b50c:57fe/64 scope link valid_lft forever preferred_lft forever 5)keepalived配置注意事项12345&gt;1.killall -0 根据进程名称检测进程是否存活，如果服务器没有该命令，请使用yum install psmisc -y安装&gt;2.第一个master节点的state为MASTER，其他master节点的state为BACKUP&gt;3.priority表示各个节点的优先级，范围：0～250（非强制要求） 4.7、生成kubernets证书与私钥1）制作kubernetes ca证书123456789101112131415161718192021cd /k8s/kubernetes/sslcat &lt;&lt; EOF | tee ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF 123456789101112131415161718cat &lt;&lt; EOF | tee ca-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 12345678910cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2019/05/01 09:47:08 [INFO] generating a new CA key and certificate from CSR2019/05/01 09:47:08 [INFO] generate received request2019/05/01 09:47:08 [INFO] received CSR2019/05/01 09:47:08 [INFO] generating key: rsa-20482019/05/01 09:47:08 [INFO] encoded CSR2019/05/01 09:47:08 [INFO] signed certificate with serial number 156611735285008649323551446985295933852737436614[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem 2）制作apiserver证书 ==注意hosts处，所有IP都写进去，包括vip==123456789101112131415161718192021222324252627282930313233cat &lt;&lt; EOF | tee server-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;10.254.0.1&quot;, &quot;127.0.0.1&quot;, &quot;10.8.13.81&quot;, &quot;10.8.13.82&quot;, &quot;10.8.13.83&quot;, &quot;10.8.13.84&quot;, &quot;10.8.13.85&quot;, &quot;10.8.13.80&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 12345678910111213cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server2019/05/01 09:51:56 [INFO] generate received request2019/05/01 09:51:56 [INFO] received CSR2019/05/01 09:51:56 [INFO] generating key: rsa-20482019/05/01 09:51:56 [INFO] encoded CSR2019/05/01 09:51:56 [INFO] signed certificate with serial number 3993762167311946548683871990816488873345085010052019/05/01 09:51:56 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server.csr server-csr.json server-key.pem server.pem 3）制作kube-proxy证书12345678910111213141516171819cat &lt;&lt; EOF | tee kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 1234567891011121314cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy2019/05/01 09:52:40 [INFO] generate received request2019/05/01 09:52:40 [INFO] received CSR2019/05/01 09:52:40 [INFO] generating key: rsa-20482019/05/01 09:52:40 [INFO] encoded CSR2019/05/01 09:52:40 [INFO] signed certificate with serial number 6339327317875053655115067555587944693891651234172019/05/01 09:52:40 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca-csr.json ca.pem kube-proxy-csr.json kube-proxy.pem server-csr.json server.pemca.csr ca-key.pem kube-proxy.csr kube-proxy-key.pem server.csr server-key.pem 4.8部署kubernetes serverkubernetes master 节点运行如下组件：kube-apiserverkube-schedulerkube-controller-managerkube-scheduler 和 kube-controller-manager 以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。 1）解压缩文件 123tar -zxf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin/cp kube-scheduler kube-apiserver kube-controller-manager kubectl /k8s/kubernetes/bin/ 2）部署kube-apiserver组件（==注意保留此KEY，下面还会需要==） 创建TLS Bootstrapping Token12345[root@master01 bin]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;af93a4194e7bcf7f05dc0bab3a6e97cd vim /k8s/kubernetes/cfg/token.csvaf93a4194e7bcf7f05dc0bab3a6e97cd,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; 创建Apiserver配置文件 注：–bind-address=当前节点ip–advertise-address=当前节点ip123456789101112131415161718192021vim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.81 \--secure-port=6443 \--advertise-address=10.8.13.81 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; 创建apiserver systemd文件123456789101112vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-apiserverExecStart=/k8s/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kubernetes的证书、配置文件、启动文件到master02和master03对应路径下12345678scp /k8s/kubernetes/ssl/* 10.8.13.82:/k8s/kubernetes/ssl/scp /k8s/kubernetes/ssl/* 10.8.13.83:/k8s/kubernetes/ssl/scp /k8s/kubernetes/cfg/* 10.8.13.82:/k8s/kubernetes/cfg/scp /k8s/kubernetes/cfg/* 10.8.13.83:/k8s/kubernetes/cfg/scp /k8s/kubernetes/bin/* 10.8.13.82:/k8s/kubernetes/bin/scp /k8s/kubernetes/bin/* 10.8.13.83:/k8s/kubernetes/bin/scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.82:/usr/lib/systemd/systemscp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.83:/usr/lib/systemd/system 5)、修改master02、master03 etcd的conf配置文件 matser02 etcd.conf配置如下：12345678910111213141516171819202122ssh 10.8.13.82vim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.82 \--secure-port=6443 \--advertise-address=10.8.13.82 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; matser03 etcd.conf配置如下：12345678910111213141516171819202122ssh 10.8.13.83vim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.83 \--secure-port=6443 \--advertise-address=10.8.13.83 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; 启动服务123456789101112131415161718192021222324252627282930systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserver[root@elasticsearch01 bin]# systemctl status kube-apiserver● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 705 (kube-apiserver) CGroup: /system.slice/kube-apiserver.service └─705 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --s...5月 13 16:00:43 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:43.495504 705 wrap.go:47] GET /api/v1/namespaces/default/endpoints/kubernetes: (3.700854ms) 200 [kube-apiserver/v1.13.1 (linux/amd64) kubernetes/eec55b9 10.8.13.81:56744]5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.955530 705 wrap.go:47] GET /api/v1/services?resourceVersion=37540&amp;timeout=6m29s&amp;timeoutSeconds=389&amp;watch=true: (6m29.001574609s) 200 [kube-proxy/v1.13.1 (linux/amd64) kub... 10.8.13.81:56844]5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.958607 705 get.go:247] Starting watch for /api/v1/services, rv=37540 labels= fields= timeout=8m28s5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.323978 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.410282ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.371766 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (3.606335ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.376888 705 wrap.go:47] GET /apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=32859&amp;timeout=5m5s&amp;timeoutSeconds=305&amp;watch=true: (5m5.001015872s) 200 [kube-apiser... 10.8.13.81:56744]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.377312 705 reflector.go:357] k8s.io/kube-aggregator/pkg/client/informers/internalversion/factory.go:117: Watch close - *apiregistration.APIService total 0 items received5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.378469 705 get.go:247] Starting watch for /apis/apiregistration.k8s.io/v1/apiservices, rv=32859 labels= fields= timeout=8m12s5月 13 16:00:49 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:49.206602 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (4.541086ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]5月 13 16:00:50 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:50.027213 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.418662ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]Hint: Some lines were ellipsized, use -l to show in full.[root@master01 bin]# ps -ef |grep kube-apiserverroot 705 1 3 5月10 ? 02:35:10 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --service-cluster-ip-range=10.254.0.0/16 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth --token-auth-file=/k8s/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/k8s/kubernetes/ssl/server.pem --tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem --client-ca-file=/k8s/kubernetes/ssl/ca.pem --service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem --etcd-cafile=/k8s/etcd/ssl/ca.pem --etcd-certfile=/k8s/etcd/ssl/server.pem --etcd-keyfile=/k8s/etcd/ssl/server-key.pemroot 7098 24767 0 15:57 pts/0 00:00:00 grep --color=auto kube-apiserver[root@master01 bin]# netstat -tulpn |grep kube-apiservetcp 0 0 10.8.13.81:6443 0.0.0.0:* LISTEN 705/kube-apiserver tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 705/kube-apiserver 3）部署kube-scheduler组件创建kube-scheduler配置文件12vim /k8s/kubernetes/cfg/kube-scheduler KUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot; 参数备注：1234--address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；--kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；创建kube-scheduler systemd文件 12345678910111213vim /usr/lib/systemd/system/kube-scheduler.service [Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-schedulerExecStart=/k8s/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kube-scheduler配置文件、启动文件到master02和master03对应路径下1234scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.82:/k8s/kubernetes/cfg/kube-schedulerscp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.83:/k8s/kubernetes/cfg/kube-schedulerscp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.82:/usr/lib/systemd/system/kube-scheduler.servicescp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.83:/usr/lib/systemd/system/kube-scheduler.service 启动服务12345678910111213141516171819202122systemctl daemon-reloadsystemctl enable kube-scheduler.service systemctl start kube-scheduler.service[root@master01 bin]# systemctl status kube-scheduler.service● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 693 (kube-scheduler) CGroup: /system.slice/kube-scheduler.service └─693 /k8s/kubernetes/bin/kube-scheduler --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024121 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024161 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151743 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151799 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434965 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434999 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571674 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571707 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914369 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914411 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler 4）部署kube-controller-manager组件 创建kube-controller-manager配置文件123456789101112vim /k8s/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \--v=4 \--master=127.0.0.1:8080 \--leader-elect=true \--address=127.0.0.1 \--service-cluster-ip-range=10.254.0.0/16 \--cluster-name=kubernetes \--cluster-signing-cert-file=/k8s/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/k8s/kubernetes/ssl/ca-key.pem \--root-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-private-key-file=/k8s/kubernetes/ssl/ca-key.pem&quot; 创建kube-controller-manager systemd文件12345678910111213vim /usr/lib/systemd/system/kube-controller-manager.service [Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-controller-managerExecStart=/k8s/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kube-controller-manager配置文件、启动文件到master02和master03对应路径下1234scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.82:/k8s/kubernetes/cfg/kube-controller-managerscp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.83:/k8s/kubernetes/cfg/kube-controller-managerscp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.82:/usr/lib/systemd/system/kube-controller-manager.servicescp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.83:/usr/lib/systemd/system/kube-controller-manager.service 启动服务12345678910111213141516171819202122systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-manager[root@master01 bin]# systemctl status kube-controller-manager● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 685 (kube-controller) CGroup: /system.slice/kube-controller-manager.service └─685 /k8s/kubernetes/bin/kube-controller-manager --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect=true --address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/k8s/kubernetes/ssl/ca...5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539102 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539136 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767187 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767221 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939294 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939329 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212185 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212218 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291399 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291430 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager 4.9、验证kubeserver服务设置环境变量(==所有服务器都执行此步==) 123vim /etc/profilePATH=/k8s/kubernetes/bin:$PATHsource /etc/profile 查看master服务状态1234567[root@master01 ~]# kubectl get cs,nodesNAME STATUS MESSAGE ERRORcomponentstatus/scheduler Healthy ok componentstatus/controller-manager Healthy ok componentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 至此master组件安装完毕 五、Node部署(node01、node02安装)12345kubernetes work 节点运行如下组件：dockerkubeletkube-proxyflannel 5.1 Docker环境安装1234yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -ryum install docker-ce -ysystemctl start docker &amp;&amp; systemctl enable docker 5.2 部署kubelet组件123kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如exec、run、logs 等;kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况;为确保安全，只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如apiserver、heapster)。 1)、安装二进制文件1234wget https://dl.k8s.io/v1.13.1/kubernetes-node-linux-amd64.tar.gztar zxvf kubernetes-node-linux-amd64.tar.gzcd kubernetes/node/bin/cp kube-proxy kubelet kubectl /k8s/kubernetes/bin/ 2)、从master01复制相关证书到node01和node02节点1234567891011[root@master01 ssl]# cd /k8s/kubernetes/ssl/[root@master01 ssl]# scp *.pem 10.8.13.84:/k8s/kubernetes/ssl/root@10.8.13.84&apos;s password: ca-key.pem 100% 1679 914.6KB/s 00:00 ca.pem 100% 1359 1.0MB/s 00:00 kube-proxy-key.pem 100% 1675 1.2MB/s 00:00 kube-proxy.pem 100% 1403 1.1MB/s 00:00 server-key.pem 100% 1679 809.1KB/s 00:00 server.pem 100% 1675 1.2MB/s 00:00[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.84:/k8s/etcd/ssl/[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.84:/k8s/etcd/bin/ 12345678910[root@master01 ssl]# scp *.pem 10.8.13.85:/k8s/kubernetes/ssl/root@10.8.13.85&apos;s password: ca-key.pem 100% 1679 914.6KB/s 00:00 ca.pem 100% 1359 1.0MB/s 00:00 kube-proxy-key.pem 100% 1675 1.2MB/s 00:00 kube-proxy.pem 100% 1403 1.1MB/s 00:00 server-key.pem 100% 1679 809.1KB/s 00:00 server.pem 100% 1675 1.2MB/s 00:00[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.85:/k8s/etcd/ssl/[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.85:/k8s/etcd/bin/ 3)、创建kubelet bootstrap kubeconfig文件 通过脚本实现KUBE_APISERVER=vip:haproxy中自定义的端口BOOTSTRAP_TOKEN=部署kube-apiserver中生成的token123456789101112131415161718192021222324252627282930313233343536373839404142434445464748vim /k8s/kubernetes/cfg/environment.sh#!/bin/bash#创建kubelet bootstrapping kubeconfig BOOTSTRAP_TOKEN=af93a4194e7bcf7f05dc0bab3a6e97cdKUBE_APISERVER=&quot;https://10.8.13.80:16443&quot;#设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig #设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig # 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig # 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig #---------------------- # 创建kube-proxy kubeconfig文件 kubectl config set-cluster kubernetes \ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \ --client-certificate=/k8s/kubernetes/ssl/kube-proxy.pem \ --client-key=/k8s/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 执行脚本123456789101112[root@node01 cfg]# cd /k8s/kubernetes/cfg/[root@node01 cfg]# sh environment.sh Cluster &quot;kubernetes&quot; set.User &quot;kubelet-bootstrap&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;.Cluster &quot;kubernetes&quot; set.User &quot;kube-proxy&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;.[root@node01 cfg]# lsbootstrap.kubeconfig environment.sh kube-proxy.kubeconfig 4)、创建kubelet参数配置模板文件 address:node节点IP12345678910111213vim /k8s/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 10.8.13.84port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.254.0.10&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true 5)、创建kubelet配置文件 –hostname-override=node节点IP12345678910vim /k8s/kubernetes/cfg/kubelet KUBELET_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.84 \--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \--config=/k8s/kubernetes/cfg/kubelet.config \--cert-dir=/k8s/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; 6)、创建kubelet systemd文件123456789101112131415vim /usr/lib/systemd/system/kubelet.service [Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service [Service]EnvironmentFile=/k8s/kubernetes/cfg/kubeletExecStart=/k8s/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process [Install]WantedBy=multi-user.target 7)、将kubelet-bootstrap用户绑定到系统集群角色(==在master01执行==)123kubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrap 注意这个默认连接localhost:8080端口，可以在master上操作1234[root@master01 ssl]# kubectl create clusterrolebinding kubelet-bootstrap \&gt; --clusterrole=system:node-bootstrapper \&gt; --user=kubelet-bootstrapclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created 8)、复制node01kubelet配置和启动服务文件到node02相对应路径12scp /k8s/kubernetes/cfg/* 10.8.13.85:/k8s/kubernetes/cfg/scp /usr/lib/systemd/system/kubelet.service 10.8.13.85:/usr/lib/systemd/system/kubelet.service 9)、修改node02中kubelet.config和kubelet文件中的nodeIP node02中kubelet.config配置 address:node节点IP12345678910111213vim /k8s/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 10.8.13.85port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.254.0.10&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true node02中kubelet配置 –hostname-override=node节点IP12345678910vim /k8s/kubernetes/cfg/kubelet KUBELET_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.85 \--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \--config=/k8s/kubernetes/cfg/kubelet.config \--cert-dir=/k8s/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; 10)、启动服务 123systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubelet 12345678[root@node01 ~]# systemctl status kubelet● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2019-05-10 20:31:30 CST; 3 days ago Main PID: 8583 (kubelet) Memory: 45.5M CGroup: /system.slice/kubelet.service └─8583 /k8s/kubernetes/bin/kubelet --logtostderr=true --v=4 --hostname-override=10.8.13.84 --kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig --config=/k8s/kubernetes/cfg/kubelet.config --cer... 11)、Master接受kubelet CSR请求(master01操作，接受两个node节点)可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书，如下是手动 approve CSR请求操作方法 查看CSR列表 123[root@master01 ssl]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc 102s kubelet-bootstrap Pending 接受node 12[root@master01 ssl]# kubectl certificate approve node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdccertificatesigningrequest.certificates.k8s.io/node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc approved 再查看CSR 123[root@master01 ssl]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc 5m13s kubelet-bootstrap Approved,Issued 5.3部署kube-proxy组件(node01执行)kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡 1)、创建 kube-proxy 配置文件 –hostname-override=node节点IP 123456vim /k8s/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.84 \--cluster-cidr=10.254.0.0/16 \--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot; 2)、创建kube-proxy systemd文件12345678910111213vim /usr/lib/systemd/system/kube-proxy.service [Unit]Description=Kubernetes ProxyAfter=network.target [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-proxyExecStart=/k8s/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 3)、复制node01kube-proxy配置和服务启动文件到node02相对应路径12scp /k8s/kubernetes/cfg/kube-proxy 10.8.13.85:/k8s/kubernetes/cfg/kube-proxyscp /usr/lib/systemd/system/kube-proxy.service 10.8.13.85:/usr/lib/systemd/system/kube-proxy.service 4)、修改node02kube-proxy配置文件如下 –hostname-override=node节点IP123456vim /k8s/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.85 \--cluster-cidr=10.254.0.0/16 \--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot; 5)、启动服务 123systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxy 12345678910111213141516171819[root@node01 ~]# systemctl status kube-proxy.service ● kube-proxy.service - Kubernetes Proxy Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2019-05-10 20:31:31 CST; 3 days ago Main PID: 8669 (kube-proxy) Memory: 9.9M CGroup: /system.slice/kube-proxy.service ‣ 8669 /k8s/kubernetes/bin/kube-proxy --logtostderr=true --v=4 --hostname-override=10.8.13.84 --cluster-cidr=10.254.0.0/16 --kubeconfig...May 14 09:07:50 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:50.634641 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:51 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:51.365166 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:52 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:52.647317 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:53 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:53.375833 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:54 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:54.658691 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:55 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:55.387881 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:56 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:56.670562 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:57 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:57.398763 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:58 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:58.682049 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:59 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:59.411141 8669 config.go:141] Calling handler.OnEndpointsUpdate 6)、查看集群状态1234[root@master01 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSION10.8.13.84 Ready &lt;none&gt; 3d13h v1.14.110.8.13.85 Ready &lt;none&gt; 3d13h v1.14.1 至此node组件安装完成 六、Flanneld网络部署(以node01为例，node02同样操作)默认没有flanneld网络，Node节点间的pod不能通信，只能Node内通信，为了部署步骤简洁明了，故flanneld放在后面安装flannel服务需要先于docker启动。flannel服务启动时主要做了以下几步的工作： 从etcd中获取network的配置信息 划分subnet，并在etcd中进行注册 将子网信息记录到/run/flannel/subnet.env中 6.1 etcd注册网段12[root@node01 ~]# /k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379&quot; set /k8s/network/config &apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125; flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致； 6.2 flannel安装1)、解压安装12tar -zxf flannel-v0.11.0-linux-amd64.tar.gzmv flanneld mk-docker-opts.sh /k8s/kubernetes/bin/ 2)、配置flanneld12vim /k8s/kubernetes/cfg/flanneldFLANNEL_OPTIONS=&quot;--etcd-endpoints=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379 -etcd-cafile=/k8s/etcd/ssl/ca.pem -etcd-certfile=/k8s/etcd/ssl/server.pem -etcd-keyfile=/k8s/etcd/ssl/server-key.pem -etcd-prefix=/k8s/network&quot; 3)、创建flanneld systemd文件123456789101112131415vim /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service [Service]Type=notifyEnvironmentFile=/k8s/kubernetes/cfg/flanneldExecStart=/k8s/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/k8s/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure [Install]WantedBy=multi-user.target ==注意：== mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥； flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口; flanneld 运行时需要 root 权限； 3）配置Docker启动指定子网 1添加EnvironmentFile=/run/flannel/subnet.env，修改ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS即可 123456789101112131415161718192021222324vim /usr/lib/systemd/system/docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target [Service]Type=notifyEnvironmentFile=/run/flannel/subnet.envExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s [Install]WantedBy=multi-user.target 4)、启动服务 注意启动flannel前要关闭docker及相关的kubelet这样flannel才会覆盖docker0网桥1234567systemctl daemon-reloadsystemctl stop dockersystemctl start flanneldsystemctl enable flanneldsystemctl start dockersystemctl restart kubeletsystemctl restart kube-proxy 5)、验证服务12345[root@node01 bin]# cat /run/flannel/subnet.env DOCKER_OPT_BIP=&quot;--bip=10.254.88.1/24&quot;DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;DOCKER_OPT_MTU=&quot;--mtu=1450&quot;DOCKER_NETWORK_OPTIONS=&quot; --bip=10.254.88.1/24 --ip-masq=false --mtu=1450&quot; 注意查看docker0和flannel是不是属于同一网段12345678910111213141516171819202122232425[root@node01 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:50:56:90:67:d1 brd ff:ff:ff:ff:ff:ff inet 10.8.13.84/24 brd 10.8.13.255 scope global ens160 valid_lft forever preferred_lft forever inet6 fe80::802:2c0f:a197:38a7/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:5c:18:5b:93 brd ff:ff:ff:ff:ff:ff inet 10.254.88.1/24 brd 10.254.88.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:5cff:fe18:5b93/64 scope link valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 8e:f6:f8:87:47:ee brd ff:ff:ff:ff:ff:ff inet 10.254.88.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::8cf6:f8ff:fe87:47ee/64 scope link valid_lft forever preferred_lft forever 至此flannel安装完成 查看NODE和etcd1234567891011[root@hwzx-test-cmpmaster01 ~]# kubectl get nodes,csNAME STATUS ROLES AGE VERSIONnode/10.8.13.84 Ready &lt;none&gt; 3d13h v1.14.1node/10.8.13.85 Ready &lt;none&gt; 3d13h v1.14.1NAME STATUS MESSAGE ERRORcomponentstatus/controller-manager Healthy ok componentstatus/scheduler Healthy ok componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器网络 calico 基本原理和模拟]]></title>
    <url>%2F2019%2F04%2F22%2F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%20calico%20%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E6%A8%A1%E6%8B%9F%2F</url>
    <content type="text"><![CDATA[摘要在容器网络跨 Host 互联方案中，除了 Flannel 的隧道实现方案，还有一种比较主流的纯三层路由的方案 Calico ，与 Flannel 不同的是 Calico 不使用隧道或 NAT 来实现转发，而是巧妙的把所有二三层流量转换成三层流量，并通过 host 上路由配置完成跨 Host 转发，本文对 Calico 的方案进行基本原理分析和模拟验证。 简介Calico 官方定义如下：12345Calico provides secure network connectivity for containers and virtual machine workloads.Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.Calico also provides dynamic enforcement of network security rules. Using Calico’s simple policy language, you can achieve fine-grained control over communications between containers, virtual machine workloads, and bare metal host endpoints. 总结如下： Calico 为容器和 vm 等提供一个安全的网路互联方法，我们把 VM、Container、白盒等实例统称为 workloads，通过给 workload 分配一个扁平的三层路由可达 IP 地址实现转发，是一种纯三层转发的方案，workload 之间不使用隧道或 NAT 技术，这种方式提供更好的网络性能，提高易维护和可交互性。同时也支持 IPIP 隧道和与 Flannel 集成能力。Calico 提供动态实施的网络安全策略，可使用简单的安全模型语言实现细粒度的安全控制。 相对 Overlay，为什么用 Calico？Calico 是一种 workloads 之间互通的网络方案，并支持以上任意一种场景。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对 workloads 做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。 而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现 workloads 的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。 那么有更好的方案吗？我们在审视了二层方案并思考如何支持大型网络时从 Internet 网络实现中获得了灵感。我们知道在 Internet 的网络中，路由器作为网关连接着自己的子网络，之间通过 BGP 相互学习，并使用防火墙控制不同子网之间安全策略，所有这些子网络共同组成了 Internet 网络，那么，这种方式能否也应用到虚拟化基础平台中呢？ 借鉴这种思路，我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案，整个方案的优势为： 更优的资源利用： 二层网络通讯需要依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，Calico 使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。 另外，二层网络使用 VLAN 隔离技术，天生有 4096 个规格限制，即便可以使用 vxlan 解决，但 vxlan 又带来了隧道开销的新问题。而 Calico 不使用 vlan 或 vxlan 技术，使资源利用率更高。 可扩展性： Calico 使用与 Internet 类似的方案，Internet 的网络比任何数据中心都大，Calico 同样天然具有可扩展性。 简单而更容易 debug： 因为没有隧道，意味着 workloads 之间路径更短更简单，配置更少，在 host 上更容易进行 debug 调试。 更少的依赖： Calico 仅依赖三层路由可达。 可适配性： Calico 较少的依赖性使它能适配所有 VM、Container、白盒或者混合环境场景。 除了以上，还有更多其他优势，因此，如果你在为 OpenStack 或 docker 构建虚拟化网络环境的话，可以好好考虑下 Calico 的方案。 Calico 由 5 部分组件组成，整体构架如下： Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等 Orchestrator Plugin：编排插件，并不是独立运行的某些进程，而是设计与 k8s、OpenStack 等平台集成的插件，如 Neutron’s ML2 plugin 用于用户使用 Neutron API 来管理 Calico，本质是要解决模型和 API 间的兼容性问题。 Etcd：Calico 模型的存储引擎。 BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。 BGP Route Reflector(BIRD)：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。 模拟组网组网如下： guest 配置 169.254.1.1 的默认路由； host 上配置 10.20.2.0/24 和 10.20.1.3/32 路由； 开启 arp proxy 和 ip_forward 能力； 网络连通性测试： 12345# HOST0[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.2.2PING 10.20.2.2 (10.20.2.2) 56(84) bytes of data. bytes from 10.20.2.2: icmp_seq=1 ttl=62 time=0.774 ms bytes from 10.20.2.2: icmp_seq=2 ttl=62 time=0.332 ms 10.20.1.2 与跨 Host 跨子网 10.20.2.2 互通成功123456# HOST0[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.1.3PING 10.20.1.3 (10.20.1.3) 56(84) bytes of data. bytes from 10.20.1.3: icmp_seq=1 ttl=62 time=957 ms bytes from 10.20.1.3: icmp_seq=2 ttl=62 time=0.432 ms bytes from 10.20.1.3: icmp_seq=3 ttl=62 time=0.563 ms 10.20.1.2 与跨 Host 同子网 10.20.1.3 互通成功 1234[root@i-7dlclo08 ~]# ip netns exec ns0 ping 192.168.100.3PING 192.168.100.3 (192.168.100.3) 56(84) bytes of data. bytes from 192.168.100.3: icmp_seq=1 ttl=63 time=1.00 ms bytes from 192.168.100.3: icmp_seq=2 ttl=63 time=0.695 ms 在未做安全策略下，10.20.1.2 与 Host 192.168.100.3 互通成功 转发过程： guest0 本地所有数据包都转发到一个虚假的地址 169.254.1.1，发送 ARP Req。 Host0 的 veth 端收到 ARP Req 时通过开启网卡的 proxy arp 代理功能直接把自己的 MAC 地址返回给 guest0 guest0 发送目的地址为 guest1 的 IP 数据包 因为使用了 169.254.1.1 这样的地址，Host 判断为三层路由转发，查询本地路由 10.20.2.0/24 via 192.168.0.3 dev eth0 发送给对端 host1，如果配置 BGP，这里会看到 proto 协议为 BIRD 在发送之前匹配本地的 iptables 规则进行安全策略控制，这里略 当 host1 收到 10.20.2.2 的数据包时查找本地路由表匹配 10.20.2.2/32 dev veth0 scope link 转发到对应的 veth0 端从而到达 guest1 回程类似，略 整体转发流程简单清晰。因此可以看到，Calico 需要给所有 guest 配置一条特别的路由并利用 veth 的 proxy arp 的能力让 guest 出来的所有转发都变成三层路由转发，再利用 host 的路由表进行转发，这种方式不仅仅实现了同 host 的二三层转发，也能实现跨 host 的转发。 遗留问题 1、租户隔离问题 Calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。 2、路由规模问题 通过路由规则可以看出，路由规模和 guest 分布有关，如果 guest 离散分布在 host 集群中，势必会产生较多的路由项。 3、iptables 规则规模问题 1台 Host 上可能虚拟化十几或几十个容器实例，过多的 iptables 规则造成复杂性和不可调试性，同时也存在性能损耗。 4、跨子网时的网关路由问题 当对端网络不为二层可达时，需要通过三层路由机时，需要网关支持自定义路由配置，即 guest 的目的地址为本网段的网关地址，再由网关进行跨三层转发。 总结： 1、Calico 通过巧妙的引导 workload 所有的流量至一个特殊的网关 169.254.1.1，从而引流到 host 的 calixxx 网络设备上，形成了二三层流量全部转换 host 的三层流量转发。 2、在 Host 上通过开启 arp proxy 的能力实现 arp 代答，arp 广播被抑制在 host 里，arp 记录变成“无效记录”，抑制了广播风暴和不会有 arp 表膨胀的问题。 3、使用 iptables 在 host 做 policy 实现的复杂的安全模型，安全策略应用在每一台虚拟路由器上，最终形成了一个分布式的安全系统。]]></content>
      <categories>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>calico</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：URL与视图函数（九）]]></title>
    <url>%2F2019%2F04%2F13%2F%E4%B9%9D%E3%80%81URL%E4%B8%8E%E8%A7%86%E5%9B%BE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在讲URL与视图函数之前我们先给大家简单介绍一下用户访问网站的流程。我们访问一个网站的时候，一般先打开浏览器，然后在浏览器的地址栏里输入一个网址，也就是URL，然后回车，我们就可以在浏览器里看到这个网址返回的内容。这是我们能看得见的过程，还有一些我们看不见的过程，那就是：++当我们在浏览器里输入网址（URL）时，回车，然后浏览器就会向目标网址发送一个HTTP请求，服务器收到请求之后就会给这个请求做出一个响应，这个响应就是把对应的内容通过浏览器渲染出来，呈现给我们看++。这个过程就是请求与响应。 下图，就是请求响应的过程。 上面我们提到了URL，这个URL在我们的Django中，其实是由我们自己构造的。(这个说法不太严谨，但为了方便大家理解之后的内容，先当这说辞是正确的。) Django中，我们约定URL是在项目同名目录下的urls.py文件里urlpatterns列表构造的。 myblog/myblog/urls.py 表现形式如下： 123456789urlpatterns = [ path(正则表达式, views视图函数，参数，别名),]括号里的参数说明：1、一个正则表达式字符串2、一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串3、可选的要传递给视图函数的默认参数（字典形式）4、一个可选的name参数(别名) 完整的URL应该要这么写：123path(正则表达式, views视图函数，参数，别名)：里面的正则表达式, views视图函数，是必须要写的，而参数，别名是可选的。我们在有特殊需要的时候才写。 通过上面我们可以看到，每个URL都对应一个views视图函数名，视图函数名不能相同，否则会报错。视图函数，Django中约定写在APP应用里的views.py文件里。然后在urls.py文件里通过下面的方式导入：12from APP应用名 import viewsfrom APP应用名.vews import 函数名或类名 视图函数是一个简单的Python 函数，它接受Web请求并且返回Web响应。响应可以是一张网页的HTML内容，一个重定向，一个404错误，一个XML文档，或者一张图片. . . 是任何东西都可以。无论视图本身包含什么逻辑，都要返回响应。这个视图函数代码一般约定是放置在项目或应用程序目录中的名为views.py的文件中。 http请求中产生两个核心对象： 1、http请求—-&gt;HttpRequest对象，用户请求相关的所有信息（对象） 2、http响应—-&gt;HttpResponse对象，响应字符串 首先，打开打开bolg目录下的views.py文件，写一个hello视图函数，在里面输入：12345678from django.http import HttpResponsedef hello(request): &quot;&quot;&quot; 写一个hello函数，通过request接收URL或者说是http请求信息， 然后给这个请求返回一个HttpResponse对象 &quot;&quot;&quot; return HttpResponse(&apos;欢迎使用Django！&apos;) 例子里，我们用到的request，就是HttpRequest对象。HttpResponse(“欢迎使用Django！”)，就是HttpRequest对象，它向http请求响应了一段字符串对象。 我们打开myblog目录下的urls.py文件中先导入视图函数，然后构造一个URL，代码如下： 12345from blog import views #导入视图函数urlpatterns = [ ... path(&apos;&apos;, views.hello), #这个是我们构造的URL] 代码写完之后，启动项目就可以在浏览器里看到视图函数返回的字符串”欢迎使用Django！” 每一个URL都会对应一个视图函数，当一个用户请求访问Django站点的一个页面时，然后就由Django路由系统（URL配置文件）去决定要执行哪个视图函数使用的算法。 通过URL对应关系匹配 -&gt;找到对应的函数（或者类）-&gt;返回字符串(或者读取Html之后返回渲染的字符串）这个过程也就是我们Django请求的生命周期。 视图函数，就是围绕着HttpRequest和HttpResponse这两个对象进行的。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：使用富文本编辑器（八）]]></title>
    <url>%2F2019%2F04%2F12%2F%E5%85%AB%E3%80%81%E4%BD%BF%E7%94%A8%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[在Django admin后台添加数据的时候，文章内容文本框想发布一篇图文并茂的文章需就得手写Html代码，这十分吃力，也没法上传图片和文件。这显然不是我等高大上程序猿想要的。 为提升效率，我们可以使用富文本编辑器添加数据。支持Django的富文本编辑器很多，这里我推荐使用DjangoUeditor，Ueditor是百度开发的一个富文本编辑器，功能强大。下面教大家安装如何使用DjangoUeditor。 1、首先我们先下载DjangoUeditor包点击下面的链接进行下载！下载完成然后解压到项目根目录里。 https://www.django.cn/media/upfile/DjangoUeditor_20181010013851_248.zip 2、settings.py里注册APP在INSTALLED_APPS里添加’DjangoUeditor’ 12345678myblog/settings.yINSTALLED_APPS = [ &apos;django.contrib.admin&apos;, .... &apos;DjangoUeditor&apos;, #注册APP应用]##验证是否已经增加正确，按住 ctrl 点击DjangoUdeitor，只要能跳转到目录即可； 3、myblog/urls.py里添加url。12345678910myblog/urls.py...from django.urls import path, include#留意上面这行比原来多了一个includeurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;&apos;, views.hello), path(&apos;ueditor/&apos;, include(&apos;DjangoUeditor.urls&apos;)), #添加DjangoUeditor的URL] 4、修改blog/models.py里需要使用富文本编辑器渲染的字段。这里面我们要修改的是Article表里的body字段。 把原来的： 123blog/models.pybody = models.TextField() 修改成：12345678blog/models.pyfrom DjangoUeditor.models import UEditorField #头部增加这行代码导入UEditorFieldbody = UEditorField(&apos;内容&apos;, width=800, height=500, toolbars=&quot;full&quot;, imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot;, upload_settings=&#123;&quot;imageMaxSize&quot;: 1204000&#125;, settings=&#123;&#125;, command=None, blank=True ) 留意里面的1imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot; 这两个是图片和文件上传的路径，我们上传文件，++会自动上传到项目根目录media文件夹下对应的upimg和upfile目录里++，这个目录名可以自行定义。有的人问，为什么会上传到media目录里去呢？那是因为之前我们在基础配置文章里，设置了上传文件目录media。 上面步骤完成后，我们启动项目，进入文章发布页面。提示出错： 错误一：123File &quot;C:\Python37\lib\site-packages\django\views\debug.py&quot;, line 332, in get_traceback_html t = DEBUG_ENGINE.from_string(fh.read())UnicodeDecodeError: &apos;gbk&apos; codec can&apos;t decode byte 0xa6 in position 9737: illegal multibyte sequence 查看错误栈最后一行发现是 编码问题找到django 源码 “C:\Python37\lib\site-packages\django\views\debug.py” 332行位置 ,增加utf-8编码 open( encoding=’utf-8’)，问题解决：123456def get_traceback_html(self): &quot;&quot;&quot;Return HTML version of debug 500 HTTP error page.&quot;&quot;&quot; with Path(CURRENT_DIR, &apos;templates&apos;, &apos;technical_500.html&apos;).open( encoding=&apos;utf-8&apos;) as fh: t = DEBUG_ENGINE.from_string(fh.read()) c = Context(self.get_traceback_data(), use_l10n=False) return t.render(c) 错误二：1render() got an unexpected keyword argument &apos;renderer&apos; 我这里使用的是最新版本的Django2.1.1所以报错，解决办法很简单。打开这个文件的93行，注释这行即可。 修改成之后，重新刷新页面，就可以看到我们的富文本编辑器正常显示。 留意，如果我们在富文本编辑器里，上传图片，在编辑器内容里不显示上传的图片。那我们还需要进行如下设置，打开myblog/urls.py文件，在里面输入如下代码：1234567891011121314myblog/urls.py....from django.urls import path, include, re_path#上面这行多加了一个re_pathfrom django.views.static import serve#导入静态文件模块from django.conf import settings#导入配置文件里的文件上传配置urlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), .... re_path(&apos;^media/(?P&lt;path&gt;.*)$&apos;, serve, &#123;&apos;document_root&apos;: settings.MEDIA_ROOT&#125;),#增加此行] 设置好了之后，图片就会正常显示。这样我们就可以用DjangoUeditor富文本编辑器发布图文并茂的文章了。 随便测试了一篇文章：]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：用Admin管理后台数据（七）]]></title>
    <url>%2F2019%2F04%2F11%2F%E4%B8%83%E3%80%81%E7%94%A8Admin%E7%AE%A1%E7%90%86%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[django的admin后台管理它可以让我们快速便捷管理数据，我们可以在各个app目录下的admin.py文件中对其进行控制。想要对APP应用进行管理，最基本的前提是要先在settings里对其进行注册，就是在INSTALLED_APPS里把APP名添加进去。 注册APP应用之后，我们想要在admin后台里对数据库表进行操作，我们还得在应用APP下的admin.py文件里对数据库表先进行注册。我们的APP应用是blog，所以我们需要在blog/admin.py文件里进行注册： 1234567891011121314151617181920212223242526272829303132333435363738blog/admin.pyfrom django.contrib import adminfrom .models import Banner, Category, Tag, Tui, Article, Link #导入需要管理的数据库表@admin.register(Article)class ArticleAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;category&apos;, &apos;title&apos;, &apos;tui&apos;, &apos;user&apos;, &apos;views&apos;, &apos;created_time&apos;) # 文章列表里显示想要显示的字段 list_per_page = 50 # 满50条数据就自动分页 ordering = (&apos;-created_time&apos;,) #后台数据列表排序方式 list_display_links = (&apos;id&apos;, &apos;title&apos;) # 设置哪些字段可以点击进入编辑界面@admin.register(Banner)class BannerAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;text_info&apos;, &apos;img&apos;, &apos;link_url&apos;, &apos;is_active&apos;)@admin.register(Category)class CategoryAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;, &apos;index&apos;)@admin.register(Tag)class TagAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;)@admin.register(Tui)class TuiAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;)@admin.register(Link)class LinkAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;,&apos;linkurl&apos;) 登录管理后台 http://127.0.0.1:8000/admin/ 多出了之前我们在models里创建的表。我们可以在后台里面对这些表进行增、删、改方面的操作。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：创建数据库模型（六）]]></title>
    <url>%2F2019%2F04%2F10%2F%E5%85%AD%E3%80%81%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Django是通过Model操作数据库，不管你数据库的类型是MySql或者Sqlite，Django它自动帮你生成相应数据库类型的SQL语句，所以不需要关注SQL语句和类型，对数据的操作Django帮我们自动完成。只要回写Model就可以了！ django根据代码中定义的类来自动生成数据库表。我们写的类表示数据库的表，如果根据这个类创建的对象是数据库表里的一行数据，对象.id 对象.value是每一行里的数据。 基本的原则如下： 每个模型在Django中的存在形式为一个Python类 每个模型都是django.db.models.Model的子类 模型里的每个类代表数据库中的一个表 模型的每个字段（属性）代表数据表的某一列 Django将自动为你生成数据库访问API 完成博客，我们需要存储六种数据：文章分类、文章、文章标签、幻灯图、推荐位、友情链接。每种数据一个表。 分类表结构设计： 表名：Category、分类名：name 标签表设计： 表名：Tag、标签名：name 文章表结构设计： 表名：Article、标题：title、摘要：excerpt、分类：category、标签：tags、推荐位、内容：body、创建时间：created_time、作者：user、文章封面图片img 幻灯图表结构设计： 表名：Banner、图片文本text_info、图片img、图片链接link_url、图片状态is_active。 推荐位表结构设计： 表名：Tui、推荐位名name。 友情链接表结构设计： 表名：Link、链接名name、链接网址linkurl。 其中： ++文章和分类是一对多的关系，文章和标签是多对多的关系，文章和作者是一对多的关系，文章和推荐位是一对多关系(看自己的需求，也可以设计成多对多)。++ 打开blog/models.py,输入代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788from django.db import modelsfrom django.contrib.auth.models import User #导入Django自带用户模块# 文章分类class Category(models.Model): name = models.CharField(&apos;博客分类&apos;, max_length=100) index = models.IntegerField(default=999, verbose_name=&apos;分类排序&apos;) class Meta: verbose_name = &apos;博客分类&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#文章标签class Tag(models.Model): name = models.CharField(&apos;文章标签&apos;,max_length=100) class Meta: verbose_name = &apos;文章标签&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#推荐位class Tui(models.Model): name = models.CharField(&apos;推荐位&apos;,max_length=100) class Meta: verbose_name = &apos;推荐位&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#文章class Article(models.Model): title = models.CharField(&apos;标题&apos;, max_length=70) excerpt = models.TextField(&apos;摘要&apos;, max_length=200, blank=True) category = models.ForeignKey(Category, on_delete=models.DO_NOTHING, verbose_name=&apos;分类&apos;, blank=True, null=True) #使用外键关联分类表与分类是一对多关系 tags = models.ManyToManyField(Tag,verbose_name=&apos;标签&apos;, blank=True) #使用外键关联标签表与标签是多对多关系 img = models.ImageField(upload_to=&apos;article_img/%Y/%m/%d/&apos;, verbose_name=&apos;文章图片&apos;, blank=True, null=True) body = models.TextField() user = models.ForeignKey(User, on_delete=models.CASCADE, verbose_name=&apos;作者&apos;) &quot;&quot;&quot; 文章作者，这里User是从django.contrib.auth.models导入的。 这里我们通过 ForeignKey 把文章和 User 关联了起来。 &quot;&quot;&quot; views = models.PositiveIntegerField(&apos;阅读量&apos;, default=0) tui = models.ForeignKey(Tui, on_delete=models.DO_NOTHING, verbose_name=&apos;推荐位&apos;, blank=True, null=True) created_time = models.DateTimeField(&apos;发布时间&apos;, auto_now_add=True) modified_time = models.DateTimeField(&apos;修改时间&apos;, auto_now=True) class Meta: verbose_name = &apos;文章&apos; verbose_name_plural = &apos;文章&apos; def __str__(self): return self.title#Bannerclass Banner(models.Model): text_info = models.CharField(&apos;标题&apos;, max_length=50, default=&apos;&apos;) img = models.ImageField(&apos;轮播图&apos;, upload_to=&apos;banner/&apos;) link_url = models.URLField(&apos;图片链接&apos;, max_length=100) is_active = models.BooleanField(&apos;是否是active&apos;, default=False) def __str__(self): return self.text_info class Meta: verbose_name = &apos;轮播图&apos; verbose_name_plural = &apos;轮播图&apos;#友情链接class Link(models.Model): name = models.CharField(&apos;链接名称&apos;, max_length=20) linkurl = models.URLField(&apos;网址&apos;,max_length=100) def __str__(self): return self.name class Meta: verbose_name = &apos;友情链接&apos; verbose_name_plural = &apos;友情链接&apos; 这里面我们多增加了一个img图片封面字段，用于上传文章封面图片的，article_img/为上传目录，%Y/%m/%d/为自动在上传的图片上加上文件上传的时间。 我们已经编写了博客数据库模型的代码，但那还只是 Python 代码而已，Django 还没有把它翻译成数据库语言，因此实际上这些数据库表还没有真正的在数据库中创建。我们需要进行数据库迁移。 在迁移之前，我们先需要设置数据库，如果我们使用默认的sqlite数据库的话，就不需要设置，Django默认使用； sqlite3数据库，如果我们想使用Mysql数据库的话，则需要我们单独配置。我们打开settings.py文件，找到DATABASES，然后把它修改成如下代码： 12345678910111213141516############修改成mysql如下：DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;NAME&apos;: &apos;test&apos;, #你的数据库名称 &apos;USER&apos;: &apos;root&apos;, #你的数据库用户名 &apos;PASSWORD&apos;: &apos;445813&apos;, #你的数据库密码 &apos;HOST&apos;: &apos;&apos;, #你的数据库主机，留空默认为localhost &apos;PORT&apos;: &apos;3306&apos;, #你的数据库端口 &#125;&#125;#由于mysql默认引擎为MySQLdb，在__init__.py文件中添加下面代码#在python3中须替换为pymysql,可在主配置文件（和项目同名的文件下，不是app配置文件）中增加如下代码#import pymysql#pymysql.install_as_MySQLdb()#如果找不到pymysql板块，则通过pip install pymysql进行安装。 数据库设置好之后，我们就依次输入下面的命令进行数据库迁移： 12python manage.py makemigrationspython manage.py migrate 迁移的时候，会有如下提示： 出现这个原因是因为我们的幻灯图使用到图片字段，我们需要引入图片处理包。提示里也给了我们处理方案，输入如下命令，安装Pillow模块即可：1pip install Pillow 然后再次迁移即可；]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：欢迎页面（五）]]></title>
    <url>%2F2019%2F04%2F09%2F%E4%BA%94%E3%80%81%E6%AC%A2%E8%BF%8E%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[基础配置做好了之后，我们就可以先迁移数据到数据库，然后启动我们的项目，感受Django的魅力。 12python manage.py makemigrationspython manage.py migrate 迁移数据之后，网站目录里自动会创建一个数据库文件db.sqlite3，里面存放着我们的数据。 之后输入下面命令创建管理帐号和密码： 1python manage.py createsuperuser 最后，我们设置下启动命令，启动我们的Django项目： 启动后登陆 http://0.0.0.0:8000/ 就可以看到我们的欢迎页。 另外一点，我们当然也可以设置自定义的主页： 首先，打开打开bolg目录下的views.py文件，在里面输入： 123456myblog/blog/views.pyfrom django.http import HttpResponsedef hello(request): return HttpResponse(&apos;Welcome to Django Zone！&apos;) 再打开myblog目录下的urls.py文件，在文件里添加两行代码： 123456789myblog/myblog/urls.pyfrom django.contrib import adminfrom django.urls import pathfrom blog import viewsurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;&apos;, views.hello),] 再次刷新启动后，即可看到新的欢迎页面。 当然，我们在浏览器里面访问：http://127.0.0.1:8000/admin 就可以进入Django自带的后台管理。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：基础配置（四）]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%9B%9B%E3%80%81%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[打开myblog目录下的settings.py文件： 一、设置域名访问权限123456myblog/settings.pyALLOWED_HOSTS = [] #修改前ALLOWED_HOSTS = [&apos;*&apos;]#修改后，表示任何域名都能访问。如果指定域名的话，在&apos;&apos;里放入指定的域名即可 二、设置TEMPLATES里的’DIRS’添加模板目录templates的路径，后面我们做网站模板的时候用得着。 123456789myblog/settings.py#修改前&apos;DIRS&apos;: []#修改后&apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)]注：使用pycharm创建的话会自动添加 三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。四、在INSTALLED_APPS添加APP应用名称。12345678myblog/settings.pyINSTALLED_APPS = [ &apos;django.contrib.admin&apos;, .... &apos;blog.apps.BlogConfig&apos;,#注册APP应用]#使用pycharm创建的话，这里自动添加了，如果是终端命令创建的话，需要手动添加应用名称如&apos;blog&apos;, 五、修改项目语言和时区123456789myblog/settings.py#修改前为英文LANGUAGE_CODE = &apos;en-us&apos;#修改后LANGUAGE_CODE = &apos;zh-hans&apos; #语言修改为中文#时区，修改前TIME_ZONE = &apos;UTC&apos;#修改后TIME_ZONE = &apos;Asia/Shanghai&apos; 六、在项目根目录里创建static和mediastatic用来存放模板CSS、JS、图片等静态资源，media用来存放上传的文件，后面我们在讲解数据库创建的时候有说明。 1234567891011121314myblog/settings.py#设置静态文件目录和名称STATIC_URL = &apos;/static/&apos;#加入下面代码#这个是设置静态文件夹目录的路径STATICFILES_DIRS = ( os.path.join(BASE_DIR, &apos;static&apos;),)#设置文件上传路径，图片上传、文件上传都会存放在此目录里MEDIA_URL = &apos;/media/&apos;MEDIA_ROOT = os.path.join(BASE_DIR, &apos;media&apos;)]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：创建项目（三）]]></title>
    <url>%2F2019%2F04%2F07%2F%E4%B8%89%E3%80%81%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[基础环境： Python3.6 django2.1.1 开发工具为Pycharm 说明：1、为项目保存路径，myblog为项目名。 2、为选择使用的虚拟环境软件，这里选virtualenv。 3、为虚拟环境保存目录，我把它保存在项目里，虚拟环境默认名为env，我系统里有多个项目为了区分出来命名为myblogenv 4、为使用的模板语言，我们默认用django模板语言。 5、为创建项目的时候建立一个模板文件目录，用来存放模板文件。用CMD命令创建项目的话，模板目录需要自己手动创建。 6、为创建一个名为blog的APP应用。同样的用CMD命令创建的话，需要手动通过python manage.py startapp blog命令来进行创建。 点击创建之后，Pycharm自动帮我们完成Django软件下载安装和Django的项目创建。 注意：如果对需要指定Django版本的话，不能直接使用这个方法，这个方法会直接下载最新版本的Django。指定版本的话，请使用CMD通过命令如：1pip install django==2.0.1 第一个黑色的 myblog 为项目文件夹目录。 blog为APP应用目录，也是我们上面设置第6项才创建的。myblog为项目配置目录，myblogvenv为Pycharm创建的虚拟环境目录，与项目无关，不用理会。 目录里的文件含义如下：12345678910111213141516171819blog #APP应用名和目录│ admin.py #对应应用后台管理配置文件。│ apps.py #对应应用的配置文件。│ models.py #数据模块，数据库设计就在此文件中设计。后面重点讲解│ tests.py #自动化测试模块，可在里面编写测试脚本自动化测试│ views.py #视图文件，用来执行响应代码的。你在浏览器所见所得都是它处理的。│ __init__.py│├─migrations #数据迁移、移植文目录，记录数据库操作记录，内容自动生成。│ │ __init__.pymyblog #项目配置目录│ __init__.py #初始化文件，一般情况下不用做任何修改。│ settings.py #项目配置文件，具体如何配置后面有介绍。│ url.py #项目URL设置文件，可理解为路由，可以控制你访问去处。│ wsgi.py #为Python服务器网关接口，是Python与WEB服务器之间的接口。myblogvenv #Pycharm创建的虚拟环境目录，和项目无关，不需要管它。templates #项目模板文件目录，用来存放模板文件manage.py #命令行工具，通过可以与项目与行交互。在终端输入python manege.py help，可以查看功能。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：数据库设计分析（二）]]></title>
    <url>%2F2019%2F04%2F06%2F%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[++从网站需求分析及网站功能、页面设计可以知道，我们的Blog主要以文章内容为主。所以我们在设计数据库的时候，我们主要以文章信息为核心数据，然后逐步向外扩展相关联的数据信息。++ 从如下图片中可以看到，文章有标题、分类、作者、浏览次数、发布时间、文章标签等信息。 这其中，文章与分类的关系是一对多的关系，什么是一对多？就是一篇文章只能有一个分类，而一个分类里可以有多篇文章。文章与标签的关系是多对多的关系，多对多简单理解就是，一篇文章可以有多个标签，一个标签里同样可以有多篇文章。 我们将文章表命名为Article，通过前面的分析得出文章信息表Article的数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 title CharField类型，长度为100 文章标题 category ForeignKey 外键，关联文章分类表 tags ManyToManyField 多对多，关联标签列表 body TextField 文章内容 user ForeignKey 外键，文章作者关联用户模型，系统自带的 views PositiveIntegerField 文章浏览数，正的整数，不能为负 tui ForeignKey 外键，关联推荐位表 created_time DateTimeField 文章发布时间 从文章表里，我们关联了一个分类表，我们把这个分类表命名为category，category表的数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 分类名 文章关联的标签表，我们命名为tag，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 标签名 文章关联的推荐位表，命名为tui，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 标签名 除此之外，我们还有两个独立的表，和文章没有关联的，一个是幻灯图片的表，一个是友情链接的表。 幻灯图表，命名为banner，数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 text_info CharField类型，长度为100 标题，图片文本信息 img ImageField类型 图片类型，保存传图片的路径 link_url URLField类型 图片链接的URL is_active BooleanField布尔类型 有True 和False两个值，意思为是否激活 友情链接表命名为link，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为70 友情链接的名称 linkurl URLField类型 友情链接的URL 至此，我们的数据库构造大致完成，后期如果还有其它的需求，我们可以在这基础上进行增加或者删除。下面我们就开始进行项目的创建与开发。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：项目需求分析（一）]]></title>
    <url>%2F2019%2F04%2F05%2F%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[当我们要开发一个项目的时候，首先需要了解我们项目的具体需求，根据需求类型划分网站功能，并详细了解这些需求的业务流程。然后更具需求和业务流程进行数据库设计。 blog的功能相对比较简单，主要以文章为主。 ==从功能需求来看==，这个Blog的功能分为：网站首页、文章分类、文章内容、幻灯图片、文章推荐、文章排行、热门推荐、文章搜索、友情链接。 1、网站首页：网站首页是整个网站的主界面，也是网站入口界面，里面主要展示Blog的动态信息及Blog功能导。网站动态信息以文章为主，如最新文章、幻灯图片、推荐阅读、文章排行、热门推荐、友情链接等。导航栏主要是将文章的分类的链接展示在首页，方便用户浏览。 2、文章分类：主要展示文章分类信息及链接，方便用户按需查看。文章分类可以在后台添加删除。 3、文章内容：主要展示文章所属分类、文章所属标签、文章内容、作者信息，发布时间信息。可以通过后台增、删、改。 4、幻灯图片：在网站首页，通过图片和文字展示一些重要信息，可以通过后台添加图片、图片描述、图片链接。 5、文章推荐：推荐一些重要的文章，可以在后台进行推荐。 6、文章排行：可根据文章浏览数，按时间段进行查询，然后展示出来。具体可根据自己的需求修改。 7、热门推荐：同样的推荐一些需要推荐的文章，可以在后台按需求或推荐位进行设置。 8、文章搜索：通过关键词搜索文章。 9、友情链接：展示相互链接的网站的名称与链接，可以通过后台添加与删除。 10、单页面：展示网站介绍，作者联系方式等信息，此类信息不经常变动，可以通过后台实现修改，也可以通过修改模板实现。 了解需求之后，就由UI设计师根据网站需求来设计网站页面，然后由前端工程师根据设计好的页面进行切图，实现HTML静态页面，最后由后端根据HTML页面和需求实现数据库构建和网站后台开发。 从设计方面来看，Blog主要分为六个页面，分别是：网站首页、文章分类列表页、文章内容页、搜索列表页、标签列表页、单页面。 1、网站首页：信息聚合的地方，展示多种信息； 2、文章分类列表页：点击分类，进入一个同一分类文章展示的列表页面； 3、文章内容页：文章内容展示页面，对应演示站这个地址； 4、搜索列表页：通过首页搜索按钮，展示出与搜索 词相关的文章列表； 5、标签列表页：展示同一个标签下的所有文章； 6、单页面：展示网站介绍、作者介绍或者联系方式等信息；]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup中find和find_all的用法]]></title>
    <url>%2F2019%2F04%2F04%2FBeautifulSoup%E4%B8%ADfind%E5%92%8Cfind_all%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在爬取网页中有用的信息时，通常是对存在于网页中的文本或各种不同标签的属性值进行查找，Beautiful Soup中内置了一些查找方式，最常用的是 find() 和 find_all() 函数。 同时通过soup.find_all()得到的所有符合条件的结果和soup.select()一样都是列表list，而soup.find()只返回第一个符合条件的结果，所以soup.find()后面可以直接接.text或者get_text()来获得标签中的文本。 一、find()用法1find(name,attrs,recursive,text,**wargs) 例子：12345678910&lt;ul id=&quot;producers&quot;&gt; &lt;li class=&quot;producerlist&quot;&gt; &lt;div class=&quot;name&quot;&gt;plants&lt;/div&gt; &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt; &lt;/li&gt; &lt;li class=&quot;producerlist&quot;&gt; &lt;div class=&quot;name&quot;&gt;algae&lt;/div&gt; &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; (1)ul,li,div这些就是标签； 用法p=soup.find(&apos;ul&apos;) ，那么返回结果是第一个ul标签以及&lt;xx&gt;...&lt;/xx&gt;的所有内容，即上面的代码；注意若用p=soup.find(&apos;ul&apos;).get_text()那么结果不是...的所有内 容，而应该是plants 10000 algae 10000，即...中的标签不算text文本。 (2)…之间的内容就是文本；基于文本内容的查找也可以用soup.find()，但必须用到参数text， 用法p=soup.find(text=&apos;algae&apos;)，print(p)得到的结果就是algae (3)正则表达式后面自己另外去学习； (4)ul id=”producers”&gt;中的id即标签属性，那么我们可以查找具有特定标签的属性； 用法p=soup.find(&apos;ul&apos;, id=&quot;producers&quot;)，那么可以得到&lt;xx&gt;...&lt;/xx&gt;的所有结果，其特点是把标签更一步精确化以便于查找。 对于大多数的情况可以用上面的方法解决，但是有两种情况则要用到参数attrs:一是标签字符中带有-，比如data-custom;二是class不能看作标签属性。解决的办法是在attrs属性用字典进行传递参数： 123soup.find(attrs=&#123;&apos;data-custom&apos;:&apos;xxx&apos;&#125;)以及：soup.find(attrs=&#123;&apos;class&apos;:&apos;xxx&apos;&#125;) 二、find_all()用法应用到find()中的不同过滤参数同理可以用到find_all()中，相比find()，find_all()有个额外的参数limit； 如下所示： 123p=soup.find_all(text=&apos;algae&apos;,limit=2)实际上find()也就是当limit=1时的find_all()。 [参考文档引自]]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python3</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3爬取墨迹天气并发送给微信好友]]></title>
    <url>%2F2019%2F04%2F03%2Fpython3%E7%88%AC%E5%8F%96%E5%A2%A8%E8%BF%B9%E5%A4%A9%E6%B0%94%E5%B9%B6%E5%8F%91%E9%80%81%E7%BB%99%E5%BE%AE%E4%BF%A1%E5%A5%BD%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[需求： 爬取墨迹天气的信息，包括温湿度、风速，生活tips等信息； 可选输入需要查询的城市，自动爬取相应信息； 链接微信，发送给指定好友或群； 思路比较清晰，主要分两块，一是爬虫，二是用python链接微信（非企业版微信） 先随便观察一个城市的墨迹天气，例如苏州市的url 1https://tianqi.moji.com/weather/china/jiangsu/suzhou 多观察几个城市的url可发现共同点就是，前面的都一样，后面的是以省拼音/市拼音结尾的。当然直辖市两者拼音一样。当然还有一些额外情况，比如山西和陕西，后者的拼音是Shaanxi，这个用户输入的时候注意一下； 第一部分： 将汉字转换为拼音； 安装了第三方库xpinyin；12345678910111213# prov = input(&quot;请输入省份：&quot;)# city = input(&quot;请输入城市：&quot;)prov = &quot;江苏&quot;city = &quot;苏州&quot;pin = Pinyin()prov_pin = pin.get_pinyin(prov,&apos;&apos;) #将汉字转为拼音city_pin = pin.get_pinyin(city,&apos;&apos;)url = &quot;https://tianqi.moji.com/weather/china/&quot;weather_url = url+prov_pin+&quot;/&quot;+city_pin# print(weather_url) 第二部分： 定位需要获取的信息； 获取今日天气信息； 使用select筛选的的是class名或者id名，注意同级和下一级的书写形式；find和find_all是查找的标签；123456789101112131415161718192021htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;) #打开页面源代码bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)# print(bs4.prettify())weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;) #定位需要拿到的信息# print(weather)temp1 = weather.find(&apos;em&apos;).get_text()temp2 = weather.find(&apos;b&apos;).get_text()AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()# print(AQI)SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text() #湿度FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text() #风速TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text() #今日天气提示DATE = str(datetime.date.today()) ##今天的日期WEEK = time.strftime(&quot;%w&quot;, time.localtime())INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos; &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPSprint(INFO) 第三部分： 获取明日天气信息；12345678##获取明日天气tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)# print(tomorrow)t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text() ##明日风速t_AQI = tomorrow[-1].get_text().strip() ##明日空气质量t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;print(t_info) 第四部分： 链接微信需要安装第三方库itchat，链接只需要这一句话，很简单。初次链接会弹出二维码，手机扫二维码登陆；123456789101112131415161718192021info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;itchat.auto_login(hotReload=True) #在一段时间内运行不需要扫二维码登陆def sendToPersion(nickName): user = itchat.search_friends(name=nickName) print(user) userName = user[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&apos;send it to HJ succeed&apos;)def sendToRoom(nickName): group = itchat.search_chatrooms(name=nickName) print(group) userName = group[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&quot;send it to Group succeed&quot;)if __name__ == &apos;__main__&apos;: sendToPersion(&quot;微信好友备注名&quot;) sendToRoom(&quot;微信群组备注名&quot;) 12345- 给自己的文件助手filehelper发送信息,此时无需访问通讯录- #itchat.send(&apos;❤来自XXX的天气问候❤&apos;,toUserName=&apos;filehelper&apos;)- #I = itchat.search_friends()# 获取自己的信息，返回自己的属性字典- #friends = itchat.get_friends(update=True)#返回值类型&lt;class &apos;itchat.storage.templates.ContactList&apos;&gt;。可以看做是列表，列表里的每个元素是一个字典，对应一个好友信息 全部代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172from urllib import requestfrom bs4 import BeautifulSoupfrom xpinyin import Pinyinimport timeimport itchat# prov = input(&quot;请输入省份：&quot;)# city = input(&quot;请输入城市：&quot;)prov = &quot;江苏&quot;city = &quot;苏州&quot;pin = Pinyin()prov_pin = pin.get_pinyin(prov,&apos;&apos;) #将汉字转为拼音city_pin = pin.get_pinyin(city,&apos;&apos;)url = &quot;https://tianqi.moji.com/weather/china/&quot;weather_url = url+prov_pin+&quot;/&quot;+city_pin# print(weather_url)htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;) #打开页面源代码bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)# print(bs4.prettify())weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;) #定位需要拿到的信息# print(weather)temp1 = weather.find(&apos;em&apos;).get_text()temp2 = weather.find(&apos;b&apos;).get_text()AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()# print(AQI)SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text() #湿度FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text() #风速TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text() #今日天气提示DATE = str(datetime.date.today()) ##今天的日期WEEK = time.strftime(&quot;%w&quot;, time.localtime())INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos; &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPSprint(INFO)##获取明日天气tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)# print(tomorrow)t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text() ##明日风速t_AQI = tomorrow[-1].get_text().strip() ##明日空气质量t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;print(t_info)info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;itchat.auto_login(hotReload=True) #在一段时间内运行不需要扫二维码登陆def sendToPersion(nickName): user = itchat.search_friends(name=nickName) print(user) userName = user[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&apos;send it to HJ succeed&apos;)def sendToRoom(nickName): group = itchat.search_chatrooms(name=nickName) print(group) userName = group[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&quot;send it to Group succeed&quot;)if __name__ == &apos;__main__&apos;: sendToPersion(&quot;微信好友备注名&quot;) sendToRoom(&quot;微信群组备注名&quot;) 微信好友： 微信群组： 抓取墨迹天气的源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116511661167116811691170117111721173117411751176117711781179118011811182118311841185118611871188118911901191119211931194119511961197119811991200120112021203120412051206120712081209121012111212121312141215121612171218121912201221122212231224122512261227122812291230123112321233123412351236123712381239124012411242124312441245124612471248124912501251125212531254125512561257125812591260126112621263126412651266126712681269127012711272127312741275127612771278127912801281128212831284128512861287128812891290129112921293129412951296129712981299130013011302130313041305130613071308130913101311131213131314131513161317131813191320132113221323132413251326132713281329133013311332133313341335133613371338133913401341134213431344134513461347134813491350135113521353135413551356135713581359136013611362136313641365136613671368&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;meta content=&quot;width=device-width, initial-scale=1&quot; name=&quot;viewport&quot;/&gt; &lt;meta content=&quot;苏州市今天实况：15度 阴，湿度：53%，东南风：2级。白天：16度,阴。 夜间：阴，9度，天气偏凉了，墨迹天气建议您穿上厚些的外套或是保暖的羊毛衫，年老体弱者可以选择保暖的摇粒绒外套。&quot; name=&quot;description&quot;/&gt; &lt;meta content=&quot;苏州市天气预报，苏州市天气查询&quot; name=&quot;keywords&quot;/&gt; &lt;meta content=&quot;Moji Weather Web Dev Team&quot; name=&quot;author&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/favicon.ico&quot; rel=&quot;shortcut icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/custom_icon.png&quot; rel=&quot;apple-touch-icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-60.png&quot; rel=&quot;apple-touch-icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-76.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;76x76&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-retina-120.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;120x120&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-retina-152.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;152x152&quot;/&gt; &lt;meta content=&quot;format=html5;url=https://m.moji.com/weather/china/jiangsu/suzhou&quot; name=&quot;mobile-agent&quot;/&gt; &lt;link href=&quot;https://m.moji.com/weather/china/jiangsu/suzhou&quot; media=&quot;only screen and(max-width: 640px)&quot; rel=&quot;alternate&quot;/&gt; &lt;meta content=&quot;IE=EmulateIE8; charset=UTF-8&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt; &lt;meta content=&quot;IE=edge,chrome=1&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt; &lt;title&gt; 【苏州市天气】_苏州市天气预报_天气查询 - 墨迹天气 &lt;/title&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/reset.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/index.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/chanle.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;head_box&quot;&gt; &lt;div class=&quot;head clearfix&quot;&gt; &lt;a class=&quot;logo&quot; href=&quot;http://www.moji.com/&quot;&gt; &lt;img alt=&quot;墨迹天气&quot; height=&quot;31&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/index/i_logo.png&quot;/&gt; &lt;/a&gt; &lt;div class=&quot;phone&quot;&gt; &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt; &lt;i class=&quot;shake shake-rotate&quot; id=&quot;head_shake&quot;&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/icon/phone.png&quot;/&gt; &lt;/i&gt; &lt;span&gt; 随时随地 想查就查 &lt;/span&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;nav&quot;&gt; &lt;a href=&quot;http://www.moji.com/&quot;&gt; 首页 &lt;/a&gt; &lt;a href=&quot;https://tianqi.moji.com&quot;&gt; 天气 &lt;/a&gt; &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt; 下载 &lt;/a&gt; &lt;!-- &lt;a href=&quot;http://www.moji.com/tob/&quot;&gt;天气服务&lt;sup style=&quot;margin: -32px 0 0 56px;&quot;&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/head_hot.png&quot;&gt;&lt;/sup&gt;--&gt; &lt;a href=&quot;https://tianqi.moji.com/news/index&quot;&gt; 资讯 &lt;/a&gt; &lt;a href=&quot;http://www.moji.com/about/&quot;&gt; 关于墨迹 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-data=&quot;13&quot; data-url=&quot;https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg&quot; id=&quot;skin&quot; style=&quot;background: url(https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg) no-repeat center top;background-size: 100% 100%;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix&quot;&gt; &lt;div class=&quot;comm_box&quot;&gt; &lt;!--面包屑--&gt; &lt;div class=&quot;crumb clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com&quot;&gt; 天气 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china&quot;&gt; 中国 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu&quot;&gt; 江苏省 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; 苏州市 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;search&quot;&gt; &lt;div class=&quot;search&quot;&gt; &lt;div class=&quot;search_default&quot;&gt; &lt;em&gt; 苏州市， 江苏省， 中国 &lt;/em&gt; &lt;strong id=&quot;locate&quot;&gt; &lt;/strong&gt; &lt;b&gt; &lt;!--icon--&gt; &lt;/b&gt; &lt;input placeholder=&quot;输入你要查找的城市&quot; type=&quot;text&quot;/&gt; &lt;i&gt; &lt;/i&gt; &lt;/div&gt; &lt;div class=&quot;search_more&quot;&gt; &lt;a href=&quot;https://tianqi.moji.com/findmycity&quot;&gt; 更多城市 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;search_city&quot; style=&quot;display: none;&quot;&gt; &lt;ul&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix wea_info&quot;&gt; &lt;div class=&quot;left&quot;&gt; &lt;div class=&quot;wea_alert clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu/suzhou&quot;&gt; &lt;span class=&quot;level level_2&quot;&gt; &lt;img alt=&quot;63 良&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/aqi/2.png&quot;/&gt; &lt;/span&gt; &lt;em&gt; 63 良 &lt;/em&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;wea_weather clearfix&quot;&gt; &lt;em&gt; 15 &lt;/em&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; &lt;b&gt; 阴 &lt;/b&gt; &lt;strong class=&quot;info_uptime&quot;&gt; 今天8:45更新 &lt;/strong&gt; &lt;/div&gt; &lt;div class=&quot;wea_about clearfix&quot;&gt; &lt;span&gt; 湿度 53% &lt;/span&gt; &lt;em&gt; 东南风2级 &lt;/em&gt; &lt;/div&gt; &lt;div class=&quot;wea_tips clearfix&quot;&gt; &lt;span&gt; 今日天气提示 &lt;/span&gt; &lt;em&gt; 略微偏凉，还是蛮舒适的。 &lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;right&quot;&gt; &lt;div class=&quot;wea_info_avator&quot;&gt; &lt;img alt=&quot;墨迹天气 小墨哥&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/avator/icon/7.png&quot;/&gt; &lt;div id=&quot;windows_download&quot;&gt; &lt;img alt=&quot;Windows 下载&quot; height=&quot;35&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/avator_windows.png&quot;/&gt; &lt;a href=&quot;http://download.moji001.com/mojiapp/windoz/MoWeatherInstall_1.8.1.1.exe&quot; target=&quot;_blank&quot;&gt; Windows 下载 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix&quot;&gt; &lt;div class=&quot;left&quot;&gt; &lt;div class=&quot;forecast clearfix&quot;&gt; &lt;div class=&quot;g_title&quot;&gt; &lt;span&gt; 预报 &lt;/span&gt; &lt;ul class=&quot;nav&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu/suzhou&quot;&gt; 7天预报 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu/suzhou&quot;&gt; 10天预报 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu/suzhou&quot;&gt; 15天预报 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu/suzhou&quot;&gt; 今天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; 阴 &lt;/li&gt; &lt;li&gt; 9° / 16° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 63 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu/suzhou&quot;&gt; 明天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;多云&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/span&gt; 多云 &lt;/li&gt; &lt;li&gt; 10° / 19° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 62 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu/suzhou&quot;&gt; 后天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; 阴 &lt;/li&gt; &lt;li&gt; 11° / 15° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 68 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;hours&quot;&gt; &lt;div class=&quot;g_title&quot;&gt; &lt;span&gt; 24小时预报 &lt;/span&gt; &lt;ul class=&quot;nav&quot;&gt; &lt;li class=&quot;active&quot;&gt; 温度 &lt;/li&gt; &lt;li&gt; 风力 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;charts clearfix&quot;&gt; &lt;div class=&quot;chart chart_temp clearfix&quot; id=&quot;chart_temp&quot;&gt; &lt;div class=&quot;prev&quot;&gt; &lt;/div&gt; &lt;div class=&quot;next&quot;&gt; &lt;/div&gt; &lt;div class=&quot;num&quot;&gt; &lt;span&gt; 30° &lt;/span&gt; &lt;span&gt; 20° &lt;/span&gt; &lt;span&gt; 10° &lt;/span&gt; &lt;span&gt; 0° &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;canvas&quot;&gt; &lt;div class=&quot;canvas_box&quot;&gt; &lt;canvas height=&quot;300&quot; id=&quot;temp&quot; width=&quot;4000&quot;&gt; &lt;/canvas&gt; &lt;div class=&quot;canvas_point&quot;&gt; &lt;span&gt; &lt;/span&gt; &lt;div&gt; &lt;em&gt; 29° &lt;/em&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;chart chart_wind clearfix&quot; id=&quot;chart_wind&quot; style=&quot;display: none;&quot;&gt; &lt;div class=&quot;prev&quot;&gt; &lt;/div&gt; &lt;div class=&quot;next&quot;&gt; &lt;/div&gt; &lt;div class=&quot;num&quot;&gt; &lt;span&gt; 30° &lt;/span&gt; &lt;span&gt; 20° &lt;/span&gt; &lt;span&gt; 10° &lt;/span&gt; &lt;span&gt; 0° &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;canvas&quot;&gt; &lt;div class=&quot;canvas_box&quot;&gt; &lt;canvas height=&quot;300&quot; id=&quot;wind&quot; width=&quot;4000&quot;&gt; &lt;/canvas&gt; &lt;div class=&quot;canvas_point&quot;&gt; &lt;span&gt; &lt;/span&gt; &lt;div&gt; &lt;em&gt; 29° &lt;/em&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!--生活指数--&gt; &lt;div id=&quot;live_index&quot;&gt; &lt;div class=&quot;live_index_title&quot;&gt; &lt;h2&gt; 生活指数 &lt;/h2&gt; &lt;span&gt; &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;live_index_grid&quot;&gt; &lt;ul class=&quot;clearfix&quot;&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/2.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 适宜 &lt;/dt&gt; &lt;dd&gt; 旅游 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/cold/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/12.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 易发 &lt;/dt&gt; &lt;dd&gt; 感冒 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/fish/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/28.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较适宜 &lt;/dt&gt; &lt;dd&gt; 钓鱼 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/makeup/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/7.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 控油 &lt;/dt&gt; &lt;dd&gt; 化妆 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/sport/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/26.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 不适宜 &lt;/dt&gt; &lt;dd&gt; 运动 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/5.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 很差 &lt;/dt&gt; &lt;dd&gt; 交通 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/car/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/17.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较适宜 &lt;/dt&gt; &lt;dd&gt; 洗车 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/pollution/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/0.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较差 &lt;/dt&gt; &lt;dd&gt; 空气污染扩散 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/dress/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/20.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 温凉 &lt;/dt&gt; &lt;dd&gt; 穿衣 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/uray/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/21.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 最弱 &lt;/dt&gt; &lt;dd&gt; 紫外线 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;/li&gt; &lt;li&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;right&quot;&gt; &lt;!--热门时景--&gt; &lt;div class=&quot;liveview liveview_index&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 热门时景 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81736828&quot;&gt; &lt;span&gt; &lt;img alt=&quot;四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区&quot; data-height=&quot;1301&quot; data-width=&quot;1080&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/06/15072215250.83494900.1182_android.jpg&quot;/&gt; &lt;/span&gt; &lt;h2&gt; 四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区 &lt;/h2&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81749154&quot;&gt; &lt;span&gt; &lt;img alt=&quot;安徽省黄山市休宁县溪口镇詹家山&quot; data-height=&quot;720&quot; data-width=&quot;960&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/08/15074223300.94412000.1764_android.jpg&quot;/&gt; &lt;/span&gt; &lt;h2&gt; 安徽省黄山市休宁县溪口镇詹家山 &lt;/h2&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;near&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 附近地区 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/nearcity/weather/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuzhong-district&quot;&gt; 吴中区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/changshu&quot;&gt; 常熟市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/zhangjiagang&quot;&gt; 张家港市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/kunshan&quot;&gt; 昆山市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wujiang-district&quot;&gt; 吴江区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/taicang&quot;&gt; 太仓市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dongshan-town&quot;&gt; 东山镇 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuxian&quot;&gt; 吴县市（现吴中区、相城区） &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;near&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 附近景点 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/nearscenic/weather/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dong-mountain-scenic-spot&quot;&gt; 东山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/huqiu-mountain-scenic-spot&quot;&gt; 虎丘山风景名胜区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/jinji-lake-scenic-spot&quot;&gt; 金鸡湖景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-lingering-garden&quot;&gt; 留园 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/qionku-mountain-scenic-spot&quot;&gt; 穹窿山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/tianping-mountain-scenic-spot&quot;&gt; 天平山风景名胜区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wang-mountain-scenic-spot&quot;&gt; 旺山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-humble-administrator&apos;s-garden&quot;&gt; 拙政园 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix calendar&quot;&gt; &lt;div class=&quot;g_title clearfix&quot;&gt; &lt;span&gt; 天气日历 &lt;/span&gt; &lt;em&gt; &lt;!--今天8:05更新--&gt; &lt;/em&gt; &lt;/div&gt; &lt;div class=&quot;grid_title clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; 星期日 &lt;/li&gt; &lt;li&gt; 星期一 &lt;/li&gt; &lt;li&gt; 星期二 &lt;/li&gt; &lt;li&gt; 星期三 &lt;/li&gt; &lt;li&gt; 星期四 &lt;/li&gt; &lt;li&gt; 星期五 &lt;/li&gt; &lt;li&gt; 星期六 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;grid clearfix&quot; id=&quot;calendar_grid&quot;&gt; &lt;ul&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 01 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;晴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w0.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 7/18° &lt;/p&gt; &lt;p&gt; 西南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item active&quot;&gt; &lt;em&gt; 02 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/16° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 03 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 10/19° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 04 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/15° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 05 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/22° &lt;/p&gt; &lt;p&gt; 南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 06 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/22° &lt;/p&gt; &lt;p&gt; 南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 07 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/25° &lt;/p&gt; &lt;p&gt; 北风 3-4级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 08 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/15° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 09 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/15° &lt;/p&gt; &lt;p&gt; 东风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 10 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/18° &lt;/p&gt; &lt;p&gt; 东北风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 11 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 12/18° &lt;/p&gt; &lt;p&gt; 东南风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 12 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/23° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 13 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/24° &lt;/p&gt; &lt;p&gt; 东风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 14 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 15/21° &lt;/p&gt; &lt;p&gt; 东南风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 15 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 8/19° &lt;/p&gt; &lt;p&gt; 西北风 5-6级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 16 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 8/19° &lt;/p&gt; &lt;p&gt; 东北风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 17 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 18 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 19 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 20 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 21 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 22 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 23 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 24 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 25 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 26 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 27 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 28 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 29 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 30 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;!--新闻列表--&gt; &lt;input id=&quot;staticdomain&quot; type=&quot;hidden&quot; value=&quot;https://h5tq.moji.com/tianqi&quot;/&gt; &lt;input id=&quot;staticmd5&quot; type=&quot;hidden&quot; value=&quot;&quot;/&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets/scripts/libs/jquery.min.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.charts.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.js&quot;&gt; &lt;/script&gt; &lt;iframe frameborder=&quot;0&quot; height=&quot;0&quot; src=&quot;http://miniweb.cntv.cn/hezuo/mo.html&quot; style=&quot;display: none; overflow: hidden;&quot; width=&quot;0&quot;&gt; &lt;/iframe&gt; &lt;div class=&quot;foot_box clearfix&quot;&gt; &lt;div class=&quot;foot clearfix&quot;&gt; &lt;div class=&quot;related_link&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 今天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china&quot;&gt; 今天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu&quot;&gt; 今天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 明天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china&quot;&gt; 明天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu&quot;&gt; 明天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 后天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china&quot;&gt; 后天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu&quot;&gt; 后天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 7天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china&quot;&gt; 7天预报省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu&quot;&gt; 7天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 10天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china&quot;&gt; 10天预报省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu&quot;&gt; 10天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 15天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china&quot;&gt; 15天预报省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu&quot;&gt; 15天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 空气指数 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china&quot;&gt; 空气指数省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu&quot;&gt; 空气指数城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; pm2.5 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pm/china&quot;&gt; pm2.5省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pm/china/jiangsu&quot;&gt; pm2.5城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 污染指数 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pollution/china&quot;&gt; 污染指数省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pollution/china/jiangsu&quot;&gt; 污染指数城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 时景 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china&quot;&gt; 时景省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu&quot;&gt; 时景城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;foot clearfix&quot;&gt; &lt;div class=&quot;address&quot;&gt; &lt;p&gt; 公司地址：北京市朝阳区酒仙桥路14号兆维华灯大厦A1区3门A216 联系电话：400-880-0599 &lt;/p&gt; &lt;ul class=&quot;f_nav&quot;&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/updata/android/&quot; rel=&quot;nofollow&quot;&gt; 升级日志 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/faq/android/&quot; rel=&quot;nofollow&quot;&gt; 常见问题 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://designer.moji.com/signin&quot; rel=&quot;nofollow&quot;&gt; 设计师平台 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/about/agreement/&quot; rel=&quot;nofollow&quot;&gt; 服务协议 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;copyright&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; var _hmt = _hmt || [];(function() &#123; var hm = document.createElement(&quot;script&quot;); hm.src = &quot;//hm.baidu.com/hm.js?49e9e3e54ae5bf8f8c637e11b3994c74&quot;; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(hm, s);&#125;)(); &lt;/script&gt; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&apos;GoogleAnalyticsObject&apos;]=r;i[r]=i[r]||function()&#123; (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) &#125;)(window,document,&apos;script&apos;,&apos;//www.google-analytics.com/analytics.js&apos;,&apos;ga&apos;); ga(&apos;create&apos;, &apos;UA-49812585-12&apos;, &apos;auto&apos;); ga(&apos;send&apos;, &apos;pageview&apos;); &lt;/script&gt; &lt;script&gt; //自动推送(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)(); &lt;/script&gt; &lt;script&gt; (function()&#123; //手机抖动动画 function phoneAnimate() &#123; $(&quot;#head_shake&quot;).addClass(&quot;shake&quot;); var phone = setTimeout(&apos;$(&quot;#head_shake&quot;).removeClass(&quot;shake&quot;)&apos;,3000); &#125; var d = new Date(); var nowYear = d.getFullYear(); var html = &quot;Copyright© 2009-&quot;+nowYear+&quot; 北京墨迹风云科技股份有限公司 All Rights Reserved&lt;br /&gt;京ICP备10021324号 京公网安备11010502023583&lt;br /&gt; 客服服务热线：400-880-0599 违法和不良信息举报电话：400-880-0599 举报邮箱：AS@moji.com&quot;; $(&quot;.copyright&quot;).html(html); phoneAnimate();&#125;)(); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python3</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Python3爬虫】常见反爬虫措施及解决办法]]></title>
    <url>%2F2019%2F03%2F31%2F%E3%80%90Python3%E7%88%AC%E8%99%AB%E3%80%91%E5%B8%B8%E8%A7%81%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8E%AA%E6%96%BD%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、UserAgentUserAgent中文名为用户代理，它使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本等信息。对于一些网站来说，它会检查我们发送的请求中所携带的UserAgent字段，如果非浏览器，就会被识别为爬虫，一旦被识别出来， 我们的爬虫也就无法正常爬取数据了。这里先看一下在不设置UserAgent字段时该字段的值会是什么：1234import requestsurl = &quot;http://www.baidu.com&quot;res = requests.get(url) 解决办法：1、收集整理常见的UserAgent以供使用123456ua_list = [&quot;Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;, &quot;Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;, &quot;MQQBrowser/25 (Linux; U; 2.3.3; zh-cn; HTC Desire S Build/GRI40;480*800)&quot;, &quot;Mozilla/5.0 (Linux; U; Android 2.3.3; zh-cn; HTC_DesireS_S510e Build/GRI40) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1&quot;, &quot;Mozilla/5.0 (SymbianOS/9.3; U; Series60/3.2 NokiaE75-1 /110.48.125 Profile/MIDP-2.1 Configuration/CLDC-1.1 ) AppleWebKit/413 (KHTML, like Gecko) Safari/413&quot; ...] 2、使用第三方库–fake_useragent使用方法如下：12345from fake_useragent import UserAgentua = UserAgent()for i in range(3): print(ua.random) 剖析：1234print(ua.ie) #随机打印ie浏览器任意版本print(ua.firefox) #随机打印firefox浏览器任意版本print(ua.chrome) #随机打印chrome浏览器任意版本print(ua.random) #随机打印任意厂家的浏览器 二、IP对于一些网站来说，如果某个IP在单位时间里的访问次数超过了某个阈值，那么服务器就会ban掉这个IP了，它就会返回给你一些错误的数据。一般来说，当我们的IP被ban了，我们的爬虫也就无法正常获取数据了，但是用浏览器还是可以正常访问，但是如果用浏览器都无法访问，那就真的GG了。很多网站都会对IP进行检测，比如知乎，如果单个IP访问频率过高就会被封掉。 解决办法：使用代理IP。网上有很多免费代理和付费代理可供选择，免费代理比如：西刺代理、快代理等等，付费代理比如：代理云、阿布云等等。 三、Referer防盗链防盗链主要是针对客户端请求过程中所携带的一些关键信息来验证请求的合法性，而防盗链又有很多种，比如Referer防盗链，还有Cookie防盗链和时间戳防盗链。 Cookie防盗链常见于论坛、社区。当访客请求一个资源的时候，他会检查这个访客的Cookie，如果不是他自己的用户的Cookie，就不会给这个访客正确的资源，也就达到了防盗的目的。时间戳防盗链指的是在他的url后面加上一个时间戳参数，所以如果你直接请求网站的url是无法得到真实的页面的，只有带上时间戳才可以。 这次的例子是天涯社区的图片分社区： 这里我们先打开开发者工具，然后任意选择一张图片，得到这个图片的链接，然后用requests来下载一下这张图片，注意带上Referer字段，看结果如何：12345678910import requestsurl = &quot;http://img3.laibafile.cn/p/l/305989961.jpg&quot;headers = &#123; &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;, &quot;UserAgent&quot;:&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36&quot;&#125;res = requests.get(url)with open(&apos;test.jpg&apos;, &apos;wb&apos;) as f: f.write(res.content) 我们的爬虫正常运行了，也看到生成了一个test.jpg文件，先别急着高兴，打开图片看一下： 解决办法：既然他说仅供天涯社区用户分享，那我们也成为他的用户不就行了吗？二话不说就去注册了个账号，然后登录，再拿到登录后的Cookie： 注意：Cookie是有时效性的，具体多久就会失效我没测试。紧接着把Cookie添加到代码中，然后运行，可以看到成功把图片下载下来了； 搞了这么久才下了一张图片，我们怎么可能就这么满足呢？分析页面可知一个页面上有十五张图片，然后往下拉的时候会看到”正在加载，请稍后”： 我们立马反应过来这是通过AJAX来加载的，于是打开开发者工具查看，可以找到如下内容： 可以看到每个链接“？”前面的部分都是基本一样的，“list_”后面跟的数字表示页数，而“_=”后面这一串数字是什么呢？有经验的人很快就能意识到这是一个时间戳，所以我们来测试一下：1234567import timeimport requestst = time.time()*1000url = &quot;http://pp.tianya.cn/qt/list_4.shtml?_=&#123;&#125;&quot;.format(t)res = requests.get(url)print(res.text) 最后编写程序并运行：12345678910111213141516171819202122232425262728293031323334353637383940414243444546import reimport timeimport requestsfrom fake_useragent import UserAgentua = UserAgent()headers = &#123; &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;, &quot;Cookie&quot;: &quot;user=w=ASD9577&amp;id=139400111&amp;f=1; right=web4=n&amp;portal=n; __u_a=v2.2.4; sso=r=2037194054&amp;sid=&amp;wsid=54DECCFD58BB393C19060A02D963FFFC; temp=k=32072170&amp;s=&amp;t=1553817544&amp;b=bdacde9b28f59113991fd271bdba3f65&amp;ct=1553817544&amp;et=1556409544; temp4=rm=0e74c53c8e0cfe3ec18596583e42c38f; ty_msg=1553817664458_139400111_2_0_0_0_0_0_2_0_0_0_0_0; bbs_msg=1553817664463_139400111_0_0_0_0; time=ct=1553817677.647&quot;, &quot;UserAgent&quot;: ua.random&#125;def crawl(url): res = requests.get(url, headers=headers) res.encoding = &quot;utf-8&quot; print(&quot;status_code:&quot;, res.status_code) # print(&quot;res.text:&quot;, res.text) if res.status_code == 200: # result = re.findall(r&apos;src=&quot;.*?&quot; alt=&quot;.*?&quot;&apos;, res.text) result = re.findall(&apos;&lt;img.+src=&quot;(.*?)&quot; alt=&quot;(.*?)&quot;.+&gt;&apos;, res.text) print(result) for i in result: # print(&quot;iiiiiiiiiii&quot;, i[0], i[1]) download(i[0], i[1]) else: print(&quot;Error Request!&quot;)def download(href, name): try: res = requests.get(href, headers=headers) with open(&apos;&#123;&#125;.jpg&apos;.format(name), &apos;wb&apos;) as f: f.write(res.content) print(&apos;[INFO]&#123;&#125;.jpg已下载！&apos;.format(name)) except: print(&quot;Error Download!&quot;)if __name__ == &apos;__main__&apos;: for num in range(1, 3): # 最大页数2页 time.sleep(2) t = int(time.time() * 1000) # 获取13位时间戳 print(t) page_url = &quot;http://pp.tianya.cn/qt/list_&#123;&#125;.shtml?_=&#123;&#125;&quot;.format(num, t) # 构造链接 crawl(page_url)]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[urllib.parse.urlencode转换get请求参数]]></title>
    <url>%2F2019%2F03%2F22%2Furllib.parse.urlencode%E8%BD%AC%E6%8D%A2get%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[浏览器地址栏搜索 刘若英1https://www.baidu.com/s?word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 但是复制到文件中是这样的：1https://www.baidu.com/s?word=%E5%88%98%E8%8B%A5%E8%8B%B1&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 这是因为浏览器对中文请求参数进行了转码用代码访问网站所发的请求中如果有中文也必须是转码之后的。这里需要用到1urllib.parse.urlencode 方法。这个方法的作用就是将字典里面所有的键值转化为query-string格式（key=value&amp;key=value），并且将中文转码。 1234567891011121314151617181920212223242526272829303132333435363738import urllib.requestimport urllib.parseimport osurl = &apos;http://www.baidu.com/s?&apos;wd = input(&apos;请输入要搜索关键字： &apos;)&quot;&quot;&quot;word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8&quot;&quot;&quot;data = &#123; &apos;word&apos;: wd, &apos;tn&apos;: &apos;71069079_1_hao_pg&apos;, &apos;ie&apos;: &apos;utf-8&apos;&#125;query_string = urllib.parse.urlencode(data)# 拼接获取完整urlurl += query_string# 发起请求，获取响应response = urllib.request.urlopen(url=url)filename = wd + &apos;.html&apos;dirname = &apos;./html&apos;if not os.path.exists(dirname): os.mkdir(dirname)filepath = dirname + &apos;/&apos; + filename# 以二进制写入文件# with open(filepath, &apos;wb&apos;) as fp:# fp.write(response.read())# 或者以utf8编码写入文件with open (filepath, &apos;w&apos;, encoding=&apos;utf8&apos;) as fp: fp.write(response.read().decode(&apos;utf8&apos;))]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-dashboard仪表盘（十二）]]></title>
    <url>%2F2019%2F03%2F15%2F12%E3%80%81dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98%2F</url>
    <content type="text"><![CDATA[对于运维管理平台，一个总览的dashboard仪表盘界面是必须有的，不但提升整体格调，也有利于向老板‘邀功请赏’。 dashboard页面必须酷炫吊炸天，所以界面元素应当美观、丰富、富有冲击力。 完整的dashboard.html文件代码如下：12dashboard.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html 一、资产状态占比图首先，制作一个资产状态百分比表盘，用于显示上线、下线、未知、故障和备用五种资产在总资产中的占比。注意是占比，不是数量！ 按照AdminLTE中提供的示例，在HTML中添加相应的标签，在script中添加相应的JS代码（jQueryKnob）。JS代码基本照抄，不需要改动。对于显示的圆圈，可以修改其颜色、大小、形态、是否只读等属性，可以参照AdminLTE中的范例。 最重要的是，需要从数据库中获取相应的数据，修改assets/views.py中的dashboard视图，最终如下：12345678910111213141516171819def dashboard(request): total = models.Asset.objects.count() upline = models.Asset.objects.filter(status=0).count() offline = models.Asset.objects.filter(status=1).count() unknown = models.Asset.objects.filter(status=2).count() breakdown = models.Asset.objects.filter(status=3).count() backup = models.Asset.objects.filter(status=4).count() up_rate = round(upline/total*100) o_rate = round(offline/total*100) un_rate = round(unknown/total*100) bd_rate = round(breakdown/total*100) bu_rate = round(backup/total*100) server_number = models.Server.objects.count() networkdevice_number = models.NetworkDevice.objects.count() storagedevice_number = models.StorageDevice.objects.count() securitydevice_number = models.SecurityDevice.objects.count() software_number = models.Software.objects.count() return render(request, &apos;assets/dashboard.html&apos;, locals()) 代码很简单，分别获取资产总数量，上线、下线、未知、故障和备用资产的数量，然后计算出各自的占比，例如上线率up_rate。同时获取服务器、网络设备、安全设备和软件设备的数量，后面需要使用。 在dashboard.html中修改各input框的value属性为1value=&quot;&#123;&#123; up_rate &#125;&#125;&quot; （以上线率为例），这是最关键的步骤，前端会根据这个值的大小，决定圆圈的幅度。 完成后的页面如下图所示： 二、不同状态资产数量统计柱状图要绘制柱状图，不可能我们自己一步步从无到有写起，建议使用第三方插件。AdminLTE中内置的是Chartjs插件，但更建议大家使用百度开源的Echarts插件，功能更强大，更容易学习。 百度Echarts的网址，提供插件下载和说明文档、在线帮助等功能。 教程提供了一个echarts.js源文件，当然你也可以自行下载并安装。 使用Echarts的柱状图很简单，首先生成一个用于放置图形的容器：12345678910111213141516171819&lt;div class=&quot;col-md-6&quot;&gt; &lt;!-- BAR CHART --&gt; &lt;div class=&quot;box box-success&quot;&gt; &lt;div class=&quot;box-header with-border&quot;&gt; &lt;h3 class=&quot;box-title&quot;&gt;各状态资产数量统计：&lt;/h3&gt; &lt;div class=&quot;box-tools pull-right&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;collapse&quot;&gt;&lt;i class=&quot;fa fa-minus&quot;&gt;&lt;/i&gt; &lt;/button&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;remove&quot;&gt;&lt;i class=&quot;fa fa-times&quot;&gt;&lt;/i&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;box-body&quot;&gt; &lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;!-- /.box-body --&gt; &lt;/div&gt;&lt;/div&gt; 上面的核心是1&lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; 这句，它指明了图表的id和容器大小。其它的都是AdminLTE框架需要的元素，用于生成表头和折叠、关闭动作按钮。我们的容器是可以折叠和删除的，也是移动端自适应的。 构造了容器后，需要在页面底部首先引入 echarts.js 文件： 1&lt;script src=&quot;&#123;% static &apos;plugins/echarts.js&apos; %&#125;&quot;&gt;&lt;/script&gt; 然后在中，添加初始化的js代码：123456789101112131415161718192021222324252627$(function () &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&apos;barChart&apos;)); // 指定图表的配置项和数据 var option = &#123; color: [&apos;#3398DB&apos;], title: &#123; text: &apos;数量&apos; &#125;, tooltip: &#123;&#125;, legend: &#123; data:[&apos;&apos;] &#125;, xAxis: &#123; data: [&quot;在线&quot;, &quot;下线&quot;,&quot;故障&quot;,&quot;备用&quot;,&quot;未知&quot;] &#125;, yAxis: &#123;&#125;, series: [&#123; name: &apos;数量&apos;, type: &apos;bar&apos;, barWidth: &apos;50%&apos;, data: [&#123;&#123; upline &#125;&#125;, &#123;&#123; offline &#125;&#125;, &#123;&#123; breakdown &#125;&#125;, &#123;&#123; backup &#125;&#125;, &#123;&#123; unknown &#125;&#125;] &#125;] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &#125;); 上面的js代码中，中文文字部分很容易理解，就是x轴的说明文字。还可以设置柱状图的颜色、宽度等特性。关键是series列表，其中的type指定该charts是什么类型，bar表示柱状图，而data就是至关重要的具体数据了，利用模板语言，将从数据库中获取的具体数值传入进来，Echarts插件会根据数值进行动态调整。 三、各类型资产数量统计饼图类似上面的柱状图，在HTML中需要先添加一个容器。不同之处在于初始化的JS代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445//资产类型数量统计 饼图 $(function () &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&apos;donutChart&apos;)); // 指定图表的配置项和数据 option = &#123; title : &#123; x:&apos;center&apos; &#125;, tooltip : &#123; trigger: &apos;item&apos;, formatter: &quot;&#123;a&#125; &lt;br/&gt;&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot; &#125;, legend: &#123; orient: &apos;vertical&apos;, left: &apos;left&apos;, data: [&apos;服务器&apos;,&apos;网络设备&apos;,&apos;存储设备&apos;,&apos;安全设备&apos;,&apos;软件资产&apos;] &#125;, series : [ &#123; name: &apos;资产类型&apos;, type: &apos;pie&apos;, radius : &apos;55%&apos;, center: [&apos;50%&apos;, &apos;60%&apos;], data:[ &#123;value:&#123;&#123; server_number &#125;&#125;, name:&apos;服务器&apos;&#125;, &#123;value:&#123;&#123; networkdevice_number &#125;&#125;, name:&apos;网络设备&apos;&#125;, &#123;value:&#123;&#123; storagedevice_number &#125;&#125;, name:&apos;存储设备&apos;&#125;, &#123;value:&#123;&#123; securitydevice_number &#125;&#125;, name:&apos;安全设备&apos;&#125;, &#123;value:&#123;&#123; software_number &#125;&#125;, name:&apos;软件资产&apos;&#125; ], itemStyle: &#123; emphasis: &#123; shadowBlur: 10, shadowOffsetX: 0, shadowColor: &apos;rgba(0, 0, 0, 0.5)&apos; &#125; &#125; &#125; ] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &#125;); series中的type指定为pie类型表示饼图，data列表动态传入各种资产类型的数量。其它的设置可参考官方文档。 为了展示的方便，我们在admin中新建一些网络设备、安全设备、软件资产等其它类型的资产，然后查看资产总表和饼图。这里我分别添加了一台网络、安全和存储设备和两个软件资产。 查看资产总表如下图所示： 查看dashboard如下图所示： 四、项目总结至此，CMDB项目就基本讲解完毕。 还是要强调的是，这是一个demo版都不能算的教学版，很多内容和细节没有实现，必然存在bug和不足。但不管怎么样，它至少包含CMDB资产管理的主体内容，如果你能从中有点收获，那么教程的目的就达到了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-资产详细页面（十一）]]></title>
    <url>%2F2019%2F03%2F14%2F11%E3%80%81%E8%B5%84%E4%BA%A7%E8%AF%A6%E7%BB%86%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[在资产的详细页面，我们将尽可能地将所有的信息都显示出来，并保持美观、整齐。 教程中实现了主要的服务器资产页面，对于其它类型的资产详细页面，可参照完成，并不复杂。 完整的detail.html页面代码如下：12detail.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html 主要代码全部集中在：1&lt;section class=&quot;content&quot;&gt; 分别用几个表格将概览、服务器、CPU、内存、硬盘和网卡的信息展示出来了。并且，AdminLTE为我们提供了一个折叠的功能，也是非常酷的。 这个HTML文件没有太多需要额外解释的内容，都是一些很基础的模板语言，构造1&lt;table&gt; 然后插入数据。如果没有数据，就以‘N/A’代替。最后在底部添加一个返回资产总表的链接。 下面是一个展示图：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-资产总表（十）]]></title>
    <url>%2F2019%2F03%2F13%2F10%E3%80%81%E8%B5%84%E4%BA%A7%E6%80%BB%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[当前，我们的资产总表如下图所示，还没有任何数据： 这需要我们从数据库中查询数据，然后渲染到前端页面中。 数据的获取很简单，一句：1assets = models.Asset.objects.all() 就搞定。当然，你也可以设置过滤条件，添加分页等等。 而在前端，我们往往需要以表格的形式，规整、美观、可排序的展示出来。这里推荐一个前端插件 datatables，是一个非常好的表格插件，功能强大、配置简单。 其官网为：https://datatables.net/ 中文网站：http://datatables.club/ 在AdminLTE中，集成了datatables插件，无需额外下载和安装，直接引入使用就可以。 下面给出一个完整的index.html模板代码：12index.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html 主要是新增了表格相关的html代码和初始化表格的js代码。 1&lt;table id=&quot;assets_table&quot; class=&quot;table table-bordered table-striped&quot;&gt; id属性非常重要，用于关联相应的初始化js代码。 表格中，循环每一个资产： 首先生成一个排序的列； 再根据资产类型的不同，用不同的颜色生成不同的资产类型名和子类型名； 通过asset.get_asset_type_display的模板语法，拿到资产类型的直观名称，比如‘服务器’，而不是显示呆板的‘server’； 通过asset.server.get_sub_asset_type_display，获取资产对应类型的子类型。这是Django特有的模板语法，非常类似其ORM的语法； 在资产名的栏目，增加了超级链接，用于显示资产的详细内容。这里只实现了服务器类型资产的详细页面，其它类型请自行完善； 根据资产状态的不同，用不同的颜色显示； 利用asset.m_time|date:”Y/m/d [H:m:s]”调整时间的显示格式； 由于资产和tas标签属于多对多的关系，所以需要一个循环，遍历每个tas并打印其名称； 通过asset.tags.all可以获取一个资产对应的多对多字段的全部对象，很类似ORM的做法。 表格的初始化JS代码如下：123456789101112&lt;script&gt; $(function () &#123; $(&apos;#assets_table&apos;).DataTable(&#123; &quot;paging&quot;: true, &lt;!-- 允许分页 --&gt; &quot;lengthChange&quot;: true, &lt;!-- 允许改变每页显示的行数 --&gt; &quot;searching&quot;: true, &lt;!-- 允许内容搜索 --&gt; &quot;ordering&quot;: true, &lt;!-- 允许排序 --&gt; &quot;info&quot;: true, &lt;!-- 显示信息 --&gt; &quot;autoWidth&quot;: false &lt;!-- 固定宽度 --&gt; &#125;); &#125;);&lt;/script&gt; 其中可定义是否允许分页、改变显示的行数、搜索、排序、显示信息、固定宽度等等，通过表格的id进行关联。 下面，我们通过后台admin界面，多增加几个服务器实例，并修改其类型、业务线、状态、厂商、机房、标签，再刷新资产总表，可以看到效果如下： 试着使用一下排序和搜索功能吧！datatables还是相当强大的！ 现在点击资产名称，可以链接到资产详细页面，但没有任何数据显示，在下一节中，我们来实现它。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-前端框架AdminLTE（九）]]></title>
    <url>%2F2019%2F03%2F12%2F9%E3%80%81%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6AdminLTE%2F</url>
    <content type="text"><![CDATA[作为CMDB资产管理项目，必须有一个丰富、直观、酷炫的前端页面。 适合运维平台的前端框架有很多，开源的也不少，这里选用的是AdminLTE。 AdminLTE托管在GitHub上，可以通过下面的地址下载： https://github.com/almasaeed2010/AdminLTE/releases AdminLTE自带JQuery和Bootstrap3框架，无需另外下载。 AdminLTE自带多种配色皮肤，可根据需要实时调整。 AdminLTE是移动端自适应的，无需单独考虑。 AdminLTE自带大量插件，比如表格、Charts等等，可根据需要载入。 但是AdminLTE的源文件包内，缺少font-awesome-4.6.3和ionicons-2.0.1这两个图标插件，它是通过CDN的形式加载的，如果网络不太好，加载可能比较困难或者缓慢，最好用本地静态文件的形式。教程在Github的包内附带上了这两个插件，可以直接使用，当然你自己下载安装也行。 一、创建base.htmlAdminLTE源文件包里有个index.html页面文件，可以利用它修改出我们CMDB项目需要的基本框架。 在项目的根目录cmdb下新建static目录，在settings文件中添加下面的配置： 12345STATIC_URL = &apos;/static/&apos;STATICFILES_DIRS = [ os.path.join(BASE_DIR, &quot;mycmdb/static&quot;),] 为了以后扩展的方便，将AdminLTE源文件包里的 bootstrap、dist 和 plugins 三个文件夹，全部拷贝到 static 目录中，这样做的话文件会比较大，比较多，但可以防止出现引用文件找不到、插件缺失等情况的发生，等以后对AdminLTE非常熟悉了，可以对static中无用的文件进行删减。 在cmdb根目录下的templates目录下，新建base.html文件，将AdminLTE源文件包中的index.html中的内容拷贝过去。然后，根据我们项目的具体情况修改文件引用、页面框架、title、CSS、主体和script块。 12base.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/base.html 这是一个适合当前CMDB的精简版本。 二、创建路由、视图这里设计了三个视图和页面，分别是： dashboard：仪表盘，图形化的数据展示 index：资产总表，表格的形式展示资产信息 detail：单个资产的详细信息页面 将assets/urls.py修改成下面的样子：123456789101112from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;), url(r&apos;^dashboard/&apos;, views.dashboard, name=&apos;dashboard&apos;), url(r&apos;^index/&apos;, views.index, name=&apos;index&apos;), url(r&apos;^detail/(?P&lt;asset_id&gt;[0-9]+)/$&apos;, views.detail, name=&quot;detail&quot;), url(r&apos;^$&apos;, views.dashboard),] 在 assets/views.py 中，增加下面三个视图：12345678910111213141516171819202122from django.shortcuts import get_object_or_404def index(request): assets = models.Asset.objects.all() return render(request, &apos;assets/index.html&apos;, locals())def dashboard(request): pass return render(request, &apos;assets/dashboard.html&apos;, locals())def detail(request, asset_id): &quot;&quot;&quot; 以显示服务器类型资产详细为例，安全设备、存储设备、网络设备等参照此例。 :param request: :param asset_id: :return: &quot;&quot;&quot; asset = get_object_or_404(models.Asset, id=asset_id) return render(request, &apos;assets/detail.html&apos;, locals()) 注意需要提前1from django.shortcuts import get_object_or_404 导入get_object_or_404()方法，这是一个非常常用的内置方法。 三、创建模版1.dashboard.html在assets目录下创建 templates/assets/dashboard.html 文件，写入下面的代码：12dashboard.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html 2.index.html在assets目录下创建 templates/assets/index.html 文件，写入下面的代码：12index.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html 3.detail.html在assets目录下创建 templates/assets/detail.html 文件，写入下面的代码： 12detail.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html 以上三个模板都很简单，就是下面的流程： extends继承‘base.html’； load staticfiles：载入静态文件； block title：资产详细endblock，定制title; block css：载入当前页面的专用CSS文件； block script：载入当前页面的专用js文件； 最后在block content：中，编写一个当前页面的面包屑导航； 页面的主体内容在后面的章节进行充实。 四、访问页面重启CMDB服务器，访问http://192.168.1.3:8000/assets/dashboard/，可以看到下面的页面。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-已上线资产信息更新（八）]]></title>
    <url>%2F2019%2F03%2F11%2F8%E3%80%81%E5%B7%B2%E4%B8%8A%E7%BA%BF%E8%B5%84%E4%BA%A7%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前面，我们已经实现了资产进入待审批区、更新待审批区的资产信息以及审批资产上线三个主要功能，还剩下一个最主要的实时更新已上线资产信息的功能。 在assets/views.py中的report视图，目前是把已上线资产的数据更新流程‘pass’了，现在将其替换成下面的语句： 1update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) report视图变成了下面的样子：views.py 12345678910111213141516171819202122232425262728293031323334353637383940from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) 然后，进入assets/asset_handler.py模块，修改log()方法，增加UpdateAsset类，最终的asset_handler.py如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\n%s&quot; % msg event.user = request.user elif log_type == &quot;update&quot;: event.name = &quot;%s &lt;%s&gt; ： 数据更新！&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新成功！&quot; elif log_type == &quot;update_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 更新失败&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新失败！\n%s&quot; % msg # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;create_manufacturer--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete()class UpdateAsset: &quot;&quot;&quot; 自动更新已上线的资产。 如果想让记录的日志更详细，可以逐条对比数据项，将更新过的项目记录到log信息中。 &quot;&quot;&quot; def __init__(self, request, asset, report_data): self.request = request self.asset = asset self.report_data = report_data # 此处的数据是由客户端发送过来的整个数据字符串 self.asset_update() def asset_update(self): # 为以后的其它类型资产扩展留下接口 func = getattr(self, &quot;_%s_update&quot; % self.report_data[&apos;asset_type&apos;]) func() def _server_update(self): try: self._update_manufacturer() # 更新厂商 self._update_server() # 更新服务器 self._update_CPU() # 更新CPU self._update_RAM() # 更新内存 self._update_disk() # 更新硬盘 self._update_nic() # 更新网卡 self.asset.save() except Exception as e: log(&apos;update_failed&apos;, msg=e, asset=self.asset, request=self.request) print(e) else: # 添加日志 log(&quot;update&quot;, asset=self.asset) print(&quot;资产数据被更新!&quot;) def _update_manufacturer(self): &quot;&quot;&quot; 更新厂商 &quot;&quot;&quot; m = self.report_data.get(&apos;manufacturer&apos;) if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) self.asset.manufacturer = manufacturer_obj else: self.asset.manufacturer = None self.asset.manufacturer.save() def _update_server(self): &quot;&quot;&quot; 更新服务器 &quot;&quot;&quot; self.asset.server.model = self.report_data.get(&apos;model&apos;) self.asset.server.os_type = self.report_data.get(&apos;os_type&apos;) self.asset.server.os_distribution = self.report_data.get(&apos;os_distribution&apos;) self.asset.server.os_release = self.report_data.get(&apos;os_release&apos;) self.asset.server.save() def _update_CPU(self): &quot;&quot;&quot; 更新CPU信息 :return: &quot;&quot;&quot; self.asset.cpu.cpu_model = self.report_data.get(&apos;cpu_model&apos;) self.asset.cpu.cpu_count = self.report_data.get(&apos;cpu_count&apos;) self.asset.cpu.cpu_core_count = self.report_data.get(&apos;cpu_core_count&apos;) self.asset.cpu.save() def _update_RAM(self): &quot;&quot;&quot; 更新内存信息。 使用集合数据类型中差的概念，处理不同的情况。 如果新数据有，但原数据没有，则新增； 如果新数据没有，但原数据有，则删除原来多余的部分； 如果新的和原数据都有，则更新。 在原则上，下面的代码应该写成一个复用的函数， 但是由于内存、硬盘、网卡在某些方面的差别，导致很难提取出重用的代码。 :return: &quot;&quot;&quot; # 获取已有内存信息，并转成字典格式 old_rams = models.RAM.objects.filter(asset=self.asset) old_rams_dict = dict() if old_rams: for ram in old_rams: old_rams_dict[ram.slot] = ram # 获取新数据中的内存信息，并转成字典格式 new_rams_list = self.report_data[&apos;ram&apos;] new_rams_dict = dict() if new_rams_list: for item in new_rams_list: new_rams_dict[item[&apos;slot&apos;]] = item # 利用set类型的差集功能，获得需要删除的内存数据对象 need_deleted_keys = set(old_rams_dict.keys()) - set(new_rams_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_rams_dict[key].delete() # 需要新增或更新的 if new_rams_dict: for key in new_rams_dict: defaults = &#123; &apos;sn&apos;: new_rams_dict[key].get(&apos;sn&apos;), &apos;model&apos;: new_rams_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_rams_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_rams_dict[key].get(&apos;capacity&apos;, 0), &#125; models.RAM.objects.update_or_create(asset=self.asset, slot=key, defaults=defaults) def _update_disk(self): &quot;&quot;&quot; 更新硬盘信息。类似更新内存。 &quot;&quot;&quot; old_disks = models.Disk.objects.filter(asset=self.asset) old_disks_dict = dict() if old_disks: for disk in old_disks: old_disks_dict[disk.sn] = disk new_disks_list = self.report_data[&apos;physical_disk_driver&apos;] new_disks_dict = dict() if new_disks_list: for item in new_disks_list: new_disks_dict[item[&apos;sn&apos;]] = item # 需要删除的 need_deleted_keys = set(old_disks_dict.keys()) - set(new_disks_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_disks_dict[key].delete() # 需要新增或更新的 if new_disks_dict: for key in new_disks_dict: interface_type = new_disks_dict[key].get(&apos;iface_type&apos;, &apos;unknown&apos;) if interface_type not in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: interface_type = &apos;unknown&apos; defaults = &#123; &apos;slot&apos;: new_disks_dict[key].get(&apos;slot&apos;), &apos;model&apos;: new_disks_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_disks_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_disks_dict[key].get(&apos;capacity&apos;, 0), &apos;interface_type&apos;: interface_type, &#125; models.Disk.objects.update_or_create(asset=self.asset, sn=key, defaults=defaults) def _update_nic(self): &quot;&quot;&quot; 更新网卡信息。类似更新内存。 &quot;&quot;&quot; old_nics = models.NIC.objects.filter(asset=self.asset) old_nics_dict = dict() if old_nics: for nic in old_nics: old_nics_dict[nic.model+nic.mac] = nic new_nics_list = self.report_data[&apos;nic&apos;] new_nics_dict = dict() if new_nics_list: for item in new_nics_list: new_nics_dict[item[&apos;model&apos;]+item[&apos;mac&apos;]] = item # 需要删除的 need_deleted_keys = set(old_nics_dict.keys()) - set(new_nics_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_nics_dict[key].delete() # 需要新增或更新的 if new_nics_dict: for key in new_nics_dict: if new_nics_dict[key].get(&apos;net_mask&apos;) and len(new_nics_dict[key].get(&apos;net_mask&apos;)) &gt; 0: net_mask = new_nics_dict[key].get(&apos;net_mask&apos;)[0] else: net_mask = &quot;&quot; defaults = &#123; &apos;name&apos;: new_nics_dict[key].get(&apos;name&apos;), &apos;ip_address&apos;: new_nics_dict[key].get(&apos;ip_address&apos;), &apos;net_mask&apos;: net_mask, &#125; models.NIC.objects.update_or_create(asset=self.asset, model=new_nics_dict[key][&apos;model&apos;], mac=new_nics_dict[key][&apos;mac&apos;], defaults=defaults) print(&apos;更新成功！&apos;) 对于log()函数，只是增加了两种数据更新的日志类型，分别记录不同的日志情况，没什么特别的。 对于UpdateAsset类，类似前面的ApproveAsset类： 首先初始化动作，自动执行asset_update()方法； 依然是通过反射，决定要调用的更新方法； 教程实现了主要的服务器类型资产的更新，对于网络设备、安全设备等请自行完善，基本类似； _server_update(self)方法中，分别更新厂商、服务器本身、CPU、内存、网卡、硬盘等信息。然后保存数据，这些事务应该是原子性的，所以要抓取异常； 不管成功还是失败，都要记录日志。 最主要的，对于_update_CPU(self)等方法，以内存为例，由于内存可能有多条，新的数据中可能出现三种情况，拔除、新增、信息变更，因此要分别对待和处理。 首先，获取已有内存信息，并转成字典格式； 其次，获取新数据中的内存信息，并转成字典格式； 利用set类型的差集功能，获得需要删除的内存数据对象 对要删除的对象，执行delete()方法； 对于需要新增或更新的内存对象，首先生成defaults数据字典； 然后，使用update_or_create(asset=self.asset, slot=key, defaults=defaults)方法，一次性完成新增或者更新数据的操作，不用写两个方法的代码； 硬盘和网卡的操作类同内存的操作。 数据更新完毕后，需要保存asset对象，也就是self.asset.save()，否则前面的工作无法关联保存下来。 现在，可以测试一下资产数据的更新了。重启CMDB，然后转到Client/report_assetss.py脚本，直接运行： 修改其中的一些数据，删除或增加一些内存、硬盘、网卡的条目。注意数据格式必须正确，sn必须不能变。 再次运行脚本，报告数据。进入admin中查看相关内容，可以看到数据已经得到更新了。 至此，CMDB自动资产管理系统的后台部分已经完成了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-审批新资产（七）]]></title>
    <url>%2F2019%2F03%2F10%2F7%E3%80%81%E5%AE%A1%E6%89%B9%E6%96%B0%E8%B5%84%E4%BA%A7%2F</url>
    <content type="text"><![CDATA[一、自定义admin的actions需要有专门的审批员来审批新资产，对资产的合法性、健全性、可用性等更多方面进行审核，如果没有问题，那么就批准上线。 批准上线这一操作是通过admin的自定义actions来实现的。 Django的admin默认有一个delete操作的action，所有在admin中的模型都有这个action，更多的就需要我们自己编写了。 修改/assets/admin.py的代码，新的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from django.contrib import admin# Register your models here.from assets import modelsfrom . import asset_handlerclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,) actions = [&apos;approve_selected_new_assets&apos;] def approve_selected_new_assets(self, request, queryset): # 获得被打钩的checkbox对应的资产 selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME) success_upline_number = 0 for asset_id in selected: obj = asset_handler.ApproveAsset(request, asset_id) ret = obj.asset_upline() if ret: success_upline_number += 1 # 顶部绿色提示信息 self.message_user(request, &quot;成功批准 %s 条新资产上线！&quot; % success_upline_number) approve_selected_new_assets.short_description = &quot;批准选择的新资产&quot;class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &quot;m_time&quot;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 说明： 通过actions = [‘approve_selected_new_assets’]定义当前模型的新acitons列表； approve_selected_new_assets()方法包含具体的动作逻辑； 自定义的action接收至少三个参数，第一个是self，第二个是request即请求，第三个是被选中的数据对象集合queryset。 首先通过request.POST.getlist()方法获取被打钩的checkbox对应的资产； 可能同时有多个资产被选择，所以这是个批量操作，需要进行循环； selected是一个包含了被选中资产的id值的列表； 对于每一个资产，创建一个asset_handler.ApproveAsset()的实例，然后调用实例的asset_upline()方法，并获取返回值。如果返回值为True，说明该资产被成功批准，那么success_upline_number变量+1，保存成功批准的资产数； 最后，在admin中给与提示信息。 approve_selected_new_assets.short_description = “批准选择的新资产”用于在admin界面中为action提供中文显示。你可以尝试去掉这条，看看效果。 重新启动CMDB，进入admin的待审批资产区，查看上方的acitons动作条，如下所示： 二、创建测试用例由于没有真实的服务器供测试，这里需要手动创建一些虚假的服务器用例，方便后面的使用和展示。 首先，将先前的所有资产条目全部从admin中删除，确保数据库内没有任何数据。 然后，在Client/bin/目录下新建一个report_assets脚本，其内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonimport osimport sysimport urllib.requestimport urllib.parseBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from conf import settingsdef update_test(data): &quot;&quot;&quot; 创建测试用例 :return: &quot;&quot;&quot; # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(data)&#125; # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() ##转码 response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\033[31;1m发送完毕！\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e)if __name__ == &apos;__main__&apos;: windows_data = &#123; &quot;os_type&quot;: &quot;Windows&quot;, &quot;os_release&quot;: &quot;7 64bit 6.1.7601 &quot;, &quot;os_distribution&quot;: &quot;Microsoft&quot;, &quot;asset_type&quot;: &quot;server&quot;, &quot;cpu_count&quot;: 2, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;cpu_core_count&quot;: 8, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &quot;model&quot;: &quot;Physical Memory&quot;, &quot;manufacturer&quot;: &quot;kingstone &quot;, &quot;sn&quot;: &quot;456&quot; &#125;, ], &quot;manufacturer&quot;: &quot;Intel&quot;, &quot;model&quot;: &quot;P67X-UD3R-B3&quot;, &quot;wake_up_type&quot;: 6, &quot;sn&quot;: &quot;00426-OEM-8992662-111111&quot;, &quot;physical_disk_driver&quot;: [ &#123; &quot;iface_type&quot;: &quot;unknown&quot;, &quot;slot&quot;: 0, &quot;sn&quot;: &quot;3830414130423230343234362020202020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 128 &#125;, &#123; &quot;iface_type&quot;: &quot;SATA&quot;, &quot;slot&quot;: 1, &quot;sn&quot;: &quot;383041413042323023234362020102020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 2048 &#125;, ], &quot;nic&quot;: [ &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000011] Realtek RTL8192CU Wireless LAN 802.11n USB 2.0 Network Adapter&quot;, &quot;name&quot;: 11, &quot;ip_address&quot;: &quot;192.168.1.110&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;0A:01:27:00:00:00&quot;, &quot;model&quot;: &quot;[00000013] VirtualBox Host-Only Ethernet Adapter&quot;, &quot;name&quot;: 13, &quot;ip_address&quot;: &quot;192.168.56.1&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000017] Microsoft Virtual WiFi Miniport Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;Intel Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;192.1.1.1&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, ] &#125; linux_data = &#123; &quot;asset_type&quot;: &quot;server&quot;, &quot;manufacturer&quot;: &quot;innotek GmbH&quot;, &quot;sn&quot;: &quot;00001&quot;, &quot;model&quot;: &quot;VirtualBox&quot;, &quot;uuid&quot;: &quot;E8DE611C-4279-495C-9B58-502B6FCED076&quot;, &quot;wake_up_type&quot;: &quot;Power Switch&quot;, &quot;os_distribution&quot;: &quot;Ubuntu&quot;, &quot;os_release&quot;: &quot;Ubuntu 16.04.3 LTS&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &quot;cpu_count&quot;: &quot;2&quot;, &quot;cpu_core_count&quot;: &quot;4&quot;, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &#125; ], &quot;ram_size&quot;: 3.858997344970703, &quot;nic&quot;: [], &quot;physical_disk_driver&quot;: [ &#123; &quot;model&quot;: &quot;VBOX HARDDISK&quot;, &quot;size&quot;: &quot;50&quot;, &quot;sn&quot;: &quot;VBeee1ba73-09085302&quot; &#125; ] &#125; update_test(linux_data) update_test(windows_data) 该脚本的作用很简单，人为虚构了两台服务器（一台windows，一台Linux）的信息，并发送给CMDB。单独执行该脚本，在admin的新资产待审批区可以看到添加了两条新资产信息。 要添加更多的资产，只需修改脚本中windows_data和linux_data的数据即可。但是要注意的是，如果不修改sn，那么会变成资产数据更新，而不是增加新资产，这一点一定要注意。 三、批准资产上线在/assets/asset_handler.py中添加下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\n%s&quot; % msg event.user = request.user # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete() 核心就是增加了一个记录日志的log()函数以及审批资产的ApproveAsset类。 log()函数很简单，根据日志类型的不同，保存日志需要的各种信息，比如日志名称、关联的资产对象、日志详细内容和审批人员等等。所有的日志都被保存在数据库中，可以在admin中查看。 对于关键的ApproveAsset类，说明如下： 初始化方法接收reqeust和待审批资产的id； 分别提前获取资产对象和所有数据data； asset_upline()是入口方法，通过反射，获取一个类似_server_upline的方法。之所以这么做，是为后面的网络设别、安全设备、存储设备等更多类型资产的审批留下扩展接口。本教程里只实现了服务器类型资产的审批方法，更多的请自行完善，过程基本类似。 _server_upline()是服务器类型资产上线的核心方法： 它首先新建了一个Asset资产对象（注意要和待审批区的资产区分开）； 然后利用该对象，分别创建了对应的厂商、服务器、CPU、内存、硬盘和网卡，并删除待审批区的对应资产； 在实际的生产环境中，上面的操作应该是原子性的整体事务，任何一步出现异常，所有操作都要回滚； 如果任何一步出现错误，上面的操作全部撤销，也就是asset.delete()。记录错误日志，返回False； 如果没问题，那么记录正确日志，返回True。 对于_create_asset(self)方法，利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 对于_create_manufacturer(self, asset)方法，先判断厂商数据是否存在，再决定是获取还是创建。 对于_create_CPU(self, asset)等方法，教程这里对数据采取了最大限度的容忍，实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。这里的业务逻辑非常复杂，不可能面面俱到。后面的内存、硬盘和网卡也是一样的。 对于_delete_original_asset(self)方法，这里的逻辑是已经审批上线的资产，就从待审批区删除。也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示，不过这样可能导致待审批区越来越大。 四、测试资产上线功能运行 report_assets.py 脚本： 在admin的新资产待审批区选择刚才的3条资产，然后选择上线action并点击‘执行’按钮，稍等片刻，显示成功批准 3 条新资产上线！的绿色提示信息，同时新资产也从待审批区被删除了，如下图所示： 往后，如果我们再次发送这3个服务器资产的信息，那就不是在待审批区了，而是已上线资产了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-新资产待审批区（六）]]></title>
    <url>%2F2019%2F03%2F09%2F6%E3%80%81%E6%96%B0%E8%B5%84%E4%BA%A7%E5%BE%85%E5%AE%A1%E6%89%B9%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[一、启用admin前面，我们已经完成了数据收集客户端的编写和测试，下面我们就可以在admin中展示和管理资产数据了。 首先，通过1python manage.py createsuperuser 创建一个管理员账户。 然后，进入/assets/admin.py文件，写入下面的代码：12345678910111213141516171819202122232425262728293031from django.contrib import admin# Register your models here.from assets import modelsclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,)class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &apos;m_time&apos;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 利用刚才创建的管理员用户，登录admin站点： 这里略微对admin界面做了些简单地配置，但目前还没有数据。 二、创建新资产前面我们只是在Pycharm中获取并打印数据，并没有将数据保存到数据库里。下面我们来实现这一功能。 修改/assets/views.py文件，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 pass return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) report视图的逻辑是这样的： sn是标识一个资产的唯一字段，必须携带，不能重复！ 从POST中获取发送过来的数据； 使用json转换数据类型； 进行各种数据检查（比如身份验证等等，请自行完善）； 判断数据是否为空，空则返回错误信息，结束视图； 判断data的类型是否字典类型，否则返回错误信息； 之所以要对data的类型进行判断是因为后面要大量的使用字典的get方法和中括号操作； 如果没有携带sn号，返回错误信息； 当前面都没问题时，进入下面的流程： 首先，利用sn值尝试在已上线的资产进行查找，如果有，则进入已上线资产的更新流程，具体实现，这里暂且跳过; 如果没有，说明这是个新资产，需要添加到新资产区； 这里又分两种情况，一种是彻底的新资产，那没得说，需要新增；另一种是新资产区已经有了，但是审批员还没来得及审批，资产数据的后续报告就已经到达了，那么需要更新数据。 创建一个asset_handler.NewAsset()对象，然后调用它的obj.add_to_new_assets_zone()方法，进行数据保存，并接收返回结果； asset_handler是下面我们要新建的资产处理模块，NewAsset是其中的一个类。 为了不让views.py文件过于庞大，通常会建立新的py文件，专门处理一些核心业务。 在assets下新建asset_handler.py文件，并写入下面的代码：12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;ram_size&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos; NewAsset类接收两个参数，request和data，分别封装了请求和资产数据，它的唯一方法：1obj.add_to_new_assets_zone() 首先构造了一个defaults字典，分别将资产数据包的各种数据打包进去，然后利用Django中特别好用的update_or_create()方法，进行数据保存！ update_or_create()方法的机制：如果数据库内没有该数据，那么新增，如果有，则更新，这就大大减少了我们的代码量，不用写两个方法。该方法的参数必须为一些用于查询的指定字段（这里是sn），以及需要新增或者更新的defaults字典。而其返回值，则是一个查询对象和是否新建对象布尔值的二元元组。 三、测试数据重启CMDB，在linux中给Client下的main.py客户端，添加一个report_data的运行参数，然后运行main.py，发送一个资产数据给CMDB服务器，结果如下： 再进入admin后台，查看新资产待审批区，可以看到资产已经成功进入待审批区： 这里我们显示了资产的汇报和更新日期，过几分钟后，重新汇报该资产数据，然后刷新admin中的页面，可以看到，待审批区的资产数据也一并被更新了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-Linux下收集数据（五）]]></title>
    <url>%2F2019%2F03%2F04%2F5%E3%80%81Linux%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[Linux下收集数据就有很多命令和工具了，比Windows方便多了。 但是要在Python的进程中运行操作系统级别的命令，我们通常需要使用subprocess模块。这个模块的具体用法，请查看Python教程中相关部分的内容。 下面，我们在Client/plugins下创建一个linux包，再到包里创建一个sys_info.py文件，写入下面的代码： 前提需要现在被收集的虚机上面安装必须的组件：1yum install -y lsb 关于disk的获取与原著有差别，更容易理解：1234567891011##获取厂商：[root@python_master pythontest]# dmidecode -s system-manufacturerVMware, Inc.##获取型号：[root@python_master pythontest]# dmidecode -s system-product-nameVMware Virtual Platform##获取sn：[root@python_master pythontest]# dmidecode -s system-serial-numberVMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-3 13:16# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport subprocessdef collect(): filter_keys = [&apos;Manufacturer&apos;, &apos;Serial Number&apos;, &apos;Product Name&apos;, &apos;UUID&apos;, &apos;Wake-up Type&apos;] raw_data = &#123;&#125; for key in filter_keys: try: res = subprocess.Popen(&quot;sudo dmidecode -t system|grep &apos;%s&apos;&quot; % key, stdout=subprocess.PIPE, shell=True) result = res.stdout.read().decode() data_list = result.split(&apos;:&apos;) if len(data_list) &gt; 1: raw_data[key] = data_list[1].strip() else: raw_data[key] = -1 except Exception as e: print(e) raw_data[key] = -2 data = dict() data[&apos;asset_type&apos;] = &apos;server&apos; data[&apos;manufacturer&apos;] = raw_data[&apos;Manufacturer&apos;] data[&apos;sn&apos;] = raw_data[&apos;Serial Number&apos;] data[&apos;model&apos;] = raw_data[&apos;Product Name&apos;] data[&apos;uuid&apos;] = raw_data[&apos;UUID&apos;] data[&apos;wake_up_type&apos;] = raw_data[&apos;Wake-up Type&apos;] data.update(get_os_info()) data.update(get_cpu_info()) data.update(get_ram_info()) data.update(get_nic_info()) data.update(get_disk_info()) return datadef get_os_info(): &quot;&quot;&quot; 获取操作系统信息 :return: &quot;&quot;&quot; distributor = subprocess.Popen(&quot;lsb_release -a|grep &apos;Distributor ID&apos;&quot;, stdout=subprocess.PIPE, shell=True) distributor = distributor.stdout.read().decode().split(&quot;:&quot;) release = subprocess.Popen(&quot;lsb_release -a|grep &apos;Description&apos;&quot;, stdout=subprocess.PIPE, shell=True) release = release.stdout.read().decode().split(&quot;:&quot;) data_dic = &#123; &quot;os_distribution&quot;: distributor[1].strip() if len(distributor) &gt; 1 else &quot;&quot;, &quot;os_release&quot;: release[1].strip() if len(release) &gt; 1 else &quot;&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &#125; return data_dicdef get_cpu_info(): &quot;&quot;&quot; 获取cpu信息 :return: &quot;&quot;&quot; base_cmd = &apos;cat /proc/cpuinfo&apos; raw_data = &#123; &apos;cpu_model&apos;: &quot;%s |grep &apos;model name&apos; |head -1 &quot; % base_cmd, &apos;cpu_count&apos;: &quot;%s |grep &apos;processor&apos;|wc -l &quot; % base_cmd, &apos;cpu_core_count&apos;: &quot;%s |grep &apos;cpu cores&apos; |awk -F: &apos;&#123;SUM +=$2&#125; END &#123;print SUM&#125;&apos;&quot; % base_cmd, &#125; for key, cmd in raw_data.items(): try: cmd_res = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True) raw_data[key] = cmd_res.stdout.read().decode().strip() except ValueError as e: print(e) raw_data[key] = &quot;&quot; data = &#123; &quot;cpu_count&quot;: raw_data[&quot;cpu_count&quot;], &quot;cpu_core_count&quot;: raw_data[&quot;cpu_core_count&quot;] &#125; cpu_model = raw_data[&quot;cpu_model&quot;].split(&quot;:&quot;) if len(cpu_model) &gt; 1: data[&quot;cpu_model&quot;] = cpu_model[1].strip() else: data[&quot;cpu_model&quot;] = -1 return datadef get_ram_info(): &quot;&quot;&quot; 获取内存信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;sudo dmidecode -t memory&quot;, stdout=subprocess.PIPE, shell=True) raw_list = raw_data.stdout.read().decode().split(&quot;\n&quot;) raw_ram_list = [] item_list = [] for line in raw_list: if line.startswith(&quot;Memory Device&quot;): raw_ram_list.append(item_list) item_list = [] else: item_list.append(line.strip()) ram_list = [] for item in raw_ram_list: item_ram_size = 0 ram_item_to_dic = &#123;&#125; for i in item: data = i.split(&quot;:&quot;) if len(data) == 2: key, v = data if key == &apos;Size&apos;: if v.strip() != &quot;No Module Installed&quot;: ram_item_to_dic[&apos;capacity&apos;] = v.split()[0].strip() item_ram_size = round(float(v.split()[0])) else: ram_item_to_dic[&apos;capacity&apos;] = 0 if key == &apos;Type&apos;: ram_item_to_dic[&apos;model&apos;] = v.strip() if key == &apos;Manufacturer&apos;: ram_item_to_dic[&apos;manufacturer&apos;] = v.strip() if key == &apos;Serial Number&apos;: ram_item_to_dic[&apos;sn&apos;] = v.strip() if key == &apos;Asset Tag&apos;: ram_item_to_dic[&apos;asset_tag&apos;] = v.strip() if key == &apos;Locator&apos;: ram_item_to_dic[&apos;slot&apos;] = v.strip() if item_ram_size == 0: pass else: ram_list.append(ram_item_to_dic) raw_total_size = subprocess.Popen(&quot;cat /proc/meminfo|grep MemTotal &quot;, stdout=subprocess.PIPE, shell=True) raw_total_size = raw_total_size.stdout.read().decode().split(&quot;:&quot;) ram_data = &#123;&apos;ram&apos;: ram_list&#125; if len(raw_total_size) == 2: total_gb_size = int(raw_total_size[1].split()[0]) / 1024**2 ram_data[&apos;ram_size&apos;] = total_gb_size return ram_datadef get_nic_info(): &quot;&quot;&quot; 获取网卡信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;ifconfig -a&quot;, stdout=subprocess.PIPE, shell=True) raw_data = raw_data.stdout.read().decode().split(&quot;\n&quot;) nic_dic = dict() next_ip_line = False last_mac_addr = None for line in raw_data: if next_ip_line: next_ip_line = False nic_name = last_mac_addr.split()[0] mac_addr = last_mac_addr.split(&quot;HWaddr&quot;)[1].strip() raw_ip_addr = line.split(&quot;inet addr:&quot;) raw_bcast = line.split(&quot;Bcast:&quot;) raw_netmask = line.split(&quot;Mask:&quot;) if len(raw_ip_addr) &gt; 1: ip_addr = raw_ip_addr[1].split()[0] network = raw_bcast[1].split()[0] netmask = raw_netmask[1].split()[0] else: ip_addr = None network = None netmask = None if mac_addr not in nic_dic: nic_dic[mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 0, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; else: if &apos;%s_bonding_addr&apos; % (mac_addr,) not in nic_dic: random_mac_addr = &apos;%s_bonding_addr&apos; % (mac_addr,) else: random_mac_addr = &apos;%s_bonding_addr2&apos; % (mac_addr,) nic_dic[random_mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: random_mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 1, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; if &quot;HWaddr&quot; in line: next_ip_line = True last_mac_addr = line nic_list = [] for k, v in nic_dic.items(): nic_list.append(v) return &#123;&apos;nic&apos;: nic_list&#125;def get_disk_info(): &quot;&quot;&quot; 获取存储信息。 本脚本只针对centos7.6中使用sda2，且只有一块虚拟硬盘的情况。 具体查看硬盘信息的命令，请根据实际情况，实际调整。 如果需要查看Raid信息，可以尝试MegaCli工具。 :return: &quot;&quot;&quot; sn_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-serial-number&quot;, stdout=subprocess.PIPE, shell=True) sn = sn_raw_data.stdout.read().decode() model_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-product-name&quot;, stdout=subprocess.PIPE, shell=True) model = model_raw_data.stdout.read().decode() #size_data = subprocess.Popen(&quot;sudo fdisk -l /dev/sda2 | grep Disk|head -1&quot;, stdout=subprocess.PIPE, shell=True) #size_data = size_data.stdout.read().decode() #size = size_data.split(&quot;:&quot;)[1].strip().split(&quot; &quot;)[0] size_raw_data = subprocess.Popen(&quot;sudo smartctl -a /dev/sda2 |grep Capacity&quot;, stdout=subprocess.PIPE, shell=True) raw_data = size_raw_data.stdout.read().decode() data_list = raw_data.split()[4] size = data_list.split(&apos;[&apos;)[1] result = &#123;&apos;physical_disk_driver&apos;: []&#125; disk_dict = dict() disk_dict[&quot;model&quot;] = model disk_dict[&quot;size&quot;] = size disk_dict[&quot;sn&quot;] = sn result[&apos;physical_disk_driver&apos;].append(disk_dict) return resultif __name__ == &quot;__main__&quot;: # 收集信息功能测试 d = collect() print(d) 先来个输出在 centos7.6虚机上面的测试（可以读出所有数据）：1&#123;&apos;asset_type&apos;: &apos;server&apos;, &apos;manufacturer&apos;: &apos;VMware, Inc.&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60&apos;, &apos;model&apos;: &apos;VMware Virtual Platform&apos;, &apos;uuid&apos;: &apos;68254d56-ee5c-fbdc-a15e-776a5fe76660&apos;, &apos;wake_up_type&apos;: &apos;Power Switch&apos;, &apos;os_distribution&apos;: &apos;CentOS&apos;, &apos;os_release&apos;: &apos;CentOS Linux release 7.6.1810 (Core)&apos;, &apos;os_type&apos;: &apos;Linux&apos;, &apos;cpu_count&apos;: &apos;1&apos;, &apos;cpu_core_count&apos;: &apos;1&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;ram&apos;: [&#123;&apos;capacity&apos;: &apos;1024&apos;, &apos;slot&apos;: &apos;RAM slot #0&apos;, &apos;model&apos;: &apos;DRAM&apos;, &apos;manufacturer&apos;: &apos;Not Specified&apos;, &apos;sn&apos;: &apos;Not Specified&apos;, &apos;asset_tag&apos;: &apos;Not Specified&apos;&#125;], &apos;ram_size&apos;: 0.9497604370117188, &apos;nic&apos;: [], &apos;physical_disk_driver&apos;: [&#123;&apos;model&apos;: &apos;VMware Virtual Platform\n&apos;, &apos;size&apos;: &apos;32.2&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60\n&apos;&#125;]&#125; 代码整体没有什么难点，无非就是使用subprocess.Popen()方法执行Linux的命令，然后获取返回值，并以规定的格式打包到data字典里。 需要说明的问题有： 当Linux中存在好几个Python解释器版本时，要注意调用方式，前面已经强调过了； 不同的Linux发行版，有些命令可能没有，需要额外安装； 所使用的查看硬件信息的命令并不一定必须和这里的一样，只要能获得数据就行； 有一些命令在ubuntu中涉及sudo的问题，需要特别对待； 最终数据字典的格式一定要正确。 可以在Linux下配置cronb或其它定时服务，设置定期的数据收集、报告任务。下面，我们在Linux虚拟机上，测试一下客户端。 将Pycharm中的Client客户端文件夹，拷贝到Linux虚拟机中，我这里是centos7.6 进入bin目录，运行：1python3 main.py report_data 一切顺利的话应该能得到如下的反馈：12345正在将数据发送至： [http://10.101.120.34:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22asset_type%22%3A+%22server%22%2C+%22manufacturer%22%3A+%22VMware%2C+Inc.%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%22%2C+%22model%22%3A+%22VMware+Virtual+Platform%22%2C+%22uuid%22%3A+%2268254d56-ee5c-fbdc-a15e-776a5fe76660%22%2C+%22wake_up_type%22%3A+%22Power+Switch%22%2C+%22os_distribution%22%3A+%22CentOS%22%2C+%22os_release%22%3A+%22CentOS+Linux+release+7.6.1810+%28Core%29%22%2C+%22os_type%22%3A+%22Linux%22%2C+%22cpu_count%22%3A+%221%22%2C+%22cpu_core_count%22%3A+%221%22%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22ram%22%3A+%5B%7B%22capacity%22%3A+%221024%22%2C+%22slot%22%3A+%22RAM+slot+%230%22%2C+%22model%22%3A+%22DRAM%22%2C+%22manufacturer%22%3A+%22Not+Specified%22%2C+%22sn%22%3A+%22Not+Specified%22%2C+%22asset_tag%22%3A+%22Not+Specified%22%7D%5D%2C+%22ram_size%22%3A+0.9497604370117188%2C+%22nic%22%3A+%5B%5D%2C+%22physical_disk_driver%22%3A+%5B%7B%22model%22%3A+%22VMware+Virtual+Platform%5Cn%22%2C+%22size%22%3A+%2232.2%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%5Cn%22%7D%5D%7D&apos;发送完毕！返回结果：成功收到数据！日志记录成功！ 然后我们可以在pycharm 页面收到： 规整如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-Windows下收集数据（四）]]></title>
    <url>%2F2019%2F03%2F03%2F4%E3%80%81Windows%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[一、windows中收集硬件信息为了收集运行Windows操作系统的服务器的硬件信息，我们需要编写一个专门的脚本。 在Pycharm的Client目录下的plugins包中，新建一个windows包，然后创建一个sys_info.py文件，写入下面的代码： ==&lt;如下打印data的语句，是为了调试查看输出，可以去掉&gt;==123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 16:41# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport platformimport win32comimport wmi&quot;&quot;&quot;本模块基于windows操作系统，依赖wmi和win32com库，需要提前使用pip进行安装，或者下载安装包手动安装。&quot;&quot;&quot;def collect(): data = &#123; &apos;os_type&apos;: platform.system(), &apos;os_release&apos;: &quot;%s %s %s &quot; % (platform.release(), platform.architecture()[0], platform.version()), &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos; &#125; # 分别获取各种硬件信息 win32obj = Win32Info() data.update(win32obj.get_cpu_info()) data.update(win32obj.get_ram_info()) data.update(win32obj.get_motherboard_info()) data.update(win32obj.get_disk_info()) data.update(win32obj.get_nic_info()) # 最后返回一个数据字典 print(&quot;data11&quot;, data) return dataclass Win32Info(object): def __init__(self): # 固定用法，更多内容请参考模块说明 self.wmi_obj = wmi.WMI() self.wmi_service_obj = win32com.client.Dispatch(&quot;WbemScripting.SWbemLocator&quot;) self.wmi_service_connector = self.wmi_service_obj.ConnectServer(&quot;.&quot;, &quot;root\cimv2&quot;) def get_cpu_info(self): &quot;&quot;&quot; 获取CPU的相关数据，这里只采集了三个数据，实际有更多，请自行选择需要的数据 :return: &quot;&quot;&quot; data = &#123;&#125; cpu_lists = self.wmi_obj.Win32_Processor() cpu_core_count = 0 for cpu in cpu_lists: cpu_core_count += cpu.NumberOfCores cpu_model = cpu_lists[0].Name # CPU型号（所有的CPU型号都是一样的） data[&quot;cpu_count&quot;] = len(cpu_lists) # CPU个数 data[&quot;cpu_model&quot;] = cpu_model data[&quot;cpu_core_count&quot;] = cpu_core_count # CPU总的核数 print(&quot;data22&quot;, data) return data def get_ram_info(self): &quot;&quot;&quot; 收集内存信息 :return: &quot;&quot;&quot; data = [] # 这个模块用SQL语言获取数据 ram_collections = self.wmi_service_connector.ExecQuery(&quot;Select * from Win32_PhysicalMemory&quot;) for item in ram_collections: # 主机中存在很多根内存，要循环所有的内存数据 ram_size = int(int(item.Capacity) / (1024**3)) # 转换内存单位为GB item_data = &#123; &quot;slot&quot;: item.DeviceLocator.strip(), &quot;capacity&quot;: ram_size, &quot;model&quot;: item.Caption, &quot;manufacturer&quot;: item.Manufacturer, &quot;sn&quot;: item. SerialNumber, &#125; data.append(item_data) # 将每条内存的信息，添加到一个列表里 print(&quot;data33&quot;, data) return &#123;&quot;ram&quot;: data&#125; # 再对data列表封装一层，返回一个字典，方便上级方法的调用 def get_motherboard_info(self): &quot;&quot;&quot; 获取主板信息 :return: &quot;&quot;&quot; computer_info = self.wmi_obj.Win32_ComputerSystem()[0] system_info = self.wmi_obj.Win32_OperatingSystem()[0] data = dict() data[&apos;manufacturer&apos;] = computer_info.Manufacturer data[&apos;model&apos;] = computer_info.Model data[&apos;wake_up_type&apos;] = computer_info.WakeUpType data[&apos;sn&apos;] = system_info.SerialNumber print(&quot;data44&quot;, data) return data def get_disk_info(self): &quot;&quot;&quot; 硬盘信息 :return: &quot;&quot;&quot; data = [] for disk in self.wmi_obj.Win32_DiskDrive(): # 每块硬盘都要获取相应信息 item_data = dict() iface_choices = [&quot;SAS&quot;, &quot;SCSI&quot;, &quot;SATA&quot;, &quot;SSD&quot;] for iface in iface_choices: if iface in disk.Model: item_data[&apos;iface_type&apos;] = iface break else: item_data[&apos;iface_type&apos;] = &apos;unknown&apos; item_data[&apos;slot&apos;] = disk.Index item_data[&apos;sn&apos;] = disk.SerialNumber item_data[&apos;model&apos;] = disk.Model item_data[&apos;manufacturer&apos;] = disk.Manufacturer item_data[&apos;capacity&apos;] = int(int(disk.Size) / (1024**3)) data.append(item_data) print(&quot;data55&quot;, data) return &#123;&apos;physical_disk_driver&apos;: data&#125; def get_nic_info(self): &quot;&quot;&quot; 网卡信息 :return: &quot;&quot;&quot; data = [] for nic in self.wmi_obj.Win32_NetworkAdapterConfiguration(): if nic.MACAddress is not None: item_data = dict() item_data[&apos;mac&apos;] = nic.MACAddress item_data[&apos;model&apos;] = nic.Caption item_data[&apos;name&apos;] = nic.Index if nic.IPAddress is not None: item_data[&apos;ip_address&apos;] = nic.IPAddress[0] item_data[&apos;net_mask&apos;] = nic.IPSubnet else: item_data[&apos;ip_address&apos;] = &apos;&apos; item_data[&apos;net_mask&apos;] = &apos;&apos; data.append(item_data) print(&quot;data66&quot;, data) return &#123;&apos;nic&apos;: data&#125;if __name__ == &quot;__main__&quot;: # 测试代码(仅限于在当前页面运行获取本机的信息） dic = collect() print(dic) windows中没有方便的命令可以获取硬件信息，但是有额外的模块可以帮助我们实现目的，这个模块叫做wmi。可以使用1pip install wmi 的方式安装，当前版本是1.4.9。但是wmi安装后，import wmi依然会出错，因为它依赖一个叫做win32com的模块。 我们依然可以通过1pip install pypiwin32 来安装win32com模块，但是不幸的是，据反映，有些机器无法通过pip成功安装。所以，这里我在github中提供了一个手动安装包==pywin32-220.win-amd64-py3.5(配合wmi模块，获取主机信息的模块).exe==，方便大家。12链接：https://pan.baidu.com/s/14ZUKPJnmlwuUHsYLUUApKg 提取码：nq5i 依赖包的问题解决后，我们来看一下==sys_info.py==脚本的代码。 核心在于collect()方法！ 该方法首先通过platform模块获取平台的信息，然后保存到一个data字典中。 然后创建一个Win32Info对象，并调用win32的各种功能方法，分别获取CPU、RAM、主板、硬盘和网卡的信息。 类Win32Info是我们编写的封装了具体数据收集逻辑的类； 该类中有很多方法，每个方法针对一项数据； 其中对Win32模块的调用方式是固定的，有兴趣的可以自行学习这个模块的官方文档 每一类的数据收集完成后都会作为一个新的子字典，update到开始的data字典中，最终形成完整的信息字典。 最后在脚本末尾有一个测试入口。 整个脚本的代码其实很简单，我们只要将Win32的方法调用当作透明的空气，剩下的不过就是将获得的数据，按照我们指定的格式打包成一个数据字典。 ==强调：数据字典的格式和键值是非常重要的，是预设的，不可以随意改变！== 二、信息收集测试单独运行一下该脚本（注意不是运行CMDB项目），查看一下生成的数据：1234567data22 &#123;&apos;cpu_core_count&apos;: 4, &apos;cpu_count&apos;: 1, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;&#125;data33 [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;]data44 &#123;&apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;wake_up_type&apos;: 6&#125;data55 [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;]data66 [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;]data11 &#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125;&#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125; 上面的信息包含操作系统、主板、CPU、内存、硬盘、网卡等各种信息。可以看到我有1条内存，1块SSD硬盘，以及4块网卡。四块网卡有出现mac地址相同的情况，因为那是虚拟机的。 你自己的数据和我的肯定不一样，但是数据格式和键值必须一样，我们后面自动分析数据、填充数据，都依靠这个固定格式的数据字典。 通过测试我们发现数据可以收集到了，那么再测试一下数据能否正常发送到服务器。 三、数据发送测试由于我这里采用了Linux虚拟机作为测试用例，我们的Django服务器就不能再运行在127.0.0.1:8000上面了。 查看一下当前机器的IP，发现是192.168.1.100，修改项目的settings.py文件，将ALLOWED_HOSTS修改如下： 1ALLOWED_HOSTS = [&quot;*&quot;] 这表示接收所有同一局域网内的网络访问。 然后以0.0.0.0:8000的参数启动CMDB，表示对局域网内所有ip开放服务。 回到客户端，进入Client/bin目录，运行1python main.py report_data 可以看到如下结果：1234567正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22cpu_core_count%22%3A+4%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22wake_up...&lt;中间信息省略&gt;...71b+M.2+2280+256GB%22%2C+%22capacity%22%3A+238%7D%5D%2C+%22cpu_count%22%3A+1%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22asset_type%22%3A+%22server%22%7D&apos;?[31;1m发送失败，HTTP Error 404: Not Found?[0m日志记录成功！ 这是一个404错误，表示服务器地址没找到，这是因为我们还没有为Django编写接收数据的视图和路由。 这时，打开log目录下的日志文件，内容如下：1发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 四、接收数据进入==cmdb/urls.py==文件中，编写一个二级路由，将所有++assets相关的数据都转发到assets.urls++中，如下所示：1234567from django.contrib import adminfrom django.conf.urls import url, includeurlpatterns = [ url(&apos;admin/&apos;, admin.site.urls), url(r&apos;^assets/&apos;, include(&apos;assets.urls&apos;)),] 然后，我们在assets中新建一个urls.py文件，写入下面的代码：12345678from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;),] 这样，我们的路由就写好了。 转过头，我们进入++assets/views.py++文件，写一个简单的视图。1234567891011from django.shortcuts import render# Create your views here.from django.shortcuts import render, HttpResponsedef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 代码很简单，接收POST过来的数据，打印出来，然后返回成功的消息。 重新运1行python main.py report_data 可以看到： 1234567正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22ram%22%3A+%5B%7B%22manufacturer%22%3A+%22802C0...&lt;中间部分省略&gt;...%22%3A+%22Latitude+5480%22%2C+%22sn%22%3A+%2200330-80000-00000-AA069%22%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22manufacturer%22%3A+%22Dell+Inc.%22%7D&apos;?[31;1m发送失败，&lt;urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。&gt;?[0m日志记录成功！ 遇到拒绝服务的错误了。 原因在于我们模拟浏览器发送了一个POST请求给Django，但是请求中没有携带Django需要的csrf安全令牌，所以拒绝了请求。 为了解决这个问题，我们需要在这个report视图上忽略csrf验证，可以通过Django的@csrf_exempt装饰器。修改代码如下：1234567891011from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exempt# Create your views here.@csrf_exemptdef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 重启CMDB服务器，再次从客户端报告数据，可以看到返回结果如下：12345678正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22nic%22%3A+%5B%7B%22name%22%3A+1%2C+%22model%22%3A+%22%5B00000001%5D+VMware+Virt...&lt;中间部分省略&gt;...facturer%22%3A+%22802C0000802C%22%2C+%22slot%22%3A+%22DIMM+A%22%2C+%22sn%22%3A+%221B458788%22%2C+%22model%22%3A+%22%5Cu7269%5Cu7406%5Cu5185%5Cu5b58%22%7D%5D%7D&apos;?[31;1m发送完毕！?[0m返回结果：成功收到数据！日志记录成功！ 看下日志记录：1234发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:00:53 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:05:36 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:06:51 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：成功收到数据！ 这表明数据发送成功了。 再看Pycharm中，也打印出了接收到的数据，一切OK！ CSRF验证的问题解决了，但是又带来新的安全问题。我们可以通过增加用户名、密码，或者md5验证或者自定义安全令牌的方式解决，这部分内容就不展开了。 Windows下的客户端已经验证完毕了，然后我们就可以通过各种方式让脚本定时运行、收集和报告数据，一切都自动化。 最后补充：++CMDB系统是部署在A服务器上，那么客户端CLIENT是需要部署在B服务器上的，那就意味着每台需要被采集数据的服务器都要安装PYTHON及所用到的包。++]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB数据收集客户端（三）]]></title>
    <url>%2F2019%2F03%2F02%2F3%E3%80%81%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[CMDB最主要的管理对象：服务器，其数据信息自然不可能通过手工收集，必须以客户端的方式，定时自动收集并报告给远程的服务器。 下面，让我们暂时忘掉Django，进入Python运维的世界…… 一、客户端程序组织编写客户端，不能一个py脚本包打天下，要有组织有目的，通常我们会采取下面的结构： 在Pycharm下，创建一个Client文件夹，作为客户端的根目录。 在Client下，创建上面的包。注意是包，不是文件夹： bin是客户端启动脚本的所在目录 conf是配置文件目录 core是核心代码目录 log是日志文件保存目录 plugins是插件或工具目录 二、开发数据收集客户端1.程序入口脚本++在bin目录中新建main.py文件++，写入下面的代码： 123456789101112131415#!/usr/bin/env python# -*- coding:utf-8 -*-import osimport sysBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from core import handlerif __name__ == &apos;__main__&apos;: handler.ArgvHandler(sys.argv) ##获取参数，传入到ArgvHandler() 通过os和sys模块的配合，将当前客户端所在目录设置为工作目录，如果不这么做，会无法导入其它模块； handler模块是核心代码模块，在core目录中，我们一会来实现它。 以后调用客户端就只需要执行python main.py 参数就可以了 ++这里有个问题一定要强调一下，那就是Python解释器的调用，执行命令的方式和代码第一行#!/usr/bin/env python的指定方式一定不能冲突，要根据你的实际情况实际操作和修改代码，很多新手连Python本身都没搞明白就上来执行脚本，碰到各种解释器不合法的错误，请回去补足基础！++ 2.主功能模块++在core下，创建handler.py文件++，写入下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 13:51# @Author : zhdya@zhdya.cn# @File : handler.pyimport jsonimport timeimport urllib.parseimport urllib.requestfrom core import info_collectionfrom conf import settingsclass ArgvHandler(object): def __init__(self, args): self.args = args self.parse_args() def parse_args(self): &quot;&quot;&quot; 分析参数，如果有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 :return: &quot;&quot;&quot; if len(self.args) &gt; 1 and hasattr(self, self.args[1]): func = getattr(self, self.args[1]) func() else: self.help_msg() ##如果执行程序没有带参数就会提示如下信息：help_msg() @staticmethod #静态方法 类或实例均可调用,静态方法函数里不传入self，这样如上self.help_msg()就可以调用了 def help_msg(): &quot;&quot;&quot; 帮助说明 :return: &quot;&quot;&quot; msg = &apos;&apos;&apos; collect_data 收集硬件信息 report_data 收集硬件信息并汇报 &apos;&apos;&apos; print(msg) @staticmethod def collect_data(): &quot;&quot;&quot;收集硬件信息,用于测试！&quot;&quot;&quot; info = info_collection.InfoCollection() asset_data = info.collect() print(asset_data) @staticmethod def report_data(): &quot;&quot;&quot; 收集硬件信息，然后发送到服务器。 :return: &quot;&quot;&quot; # 收集信息 info = info_collection.InfoCollection() asset_data = info.collect() # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(asset_data)&#125; print(&quot;handler_data--&gt;&gt;&quot;, data) # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&quot;handler_url--&gt;&gt;&quot;, url) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() print(&quot;handler_data_encode--&gt;&gt;&quot;, data_encode) response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\033[31;1m发送完毕！\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e) # 记录发送日志 with open(settings.PATH, &apos;ab&apos;) as f: ##a追加,b二进制文件 string = &apos;发送时间：%s \t 服务器地址：%s \t 返回结果：%s \n&apos; % (time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;), url, message) f.write(string.encode()) print(&quot;日志记录成功！&quot;) 说明： handler模块中只有一个ArgvHandler类； 在main模块中也是实例化了一个ArgvHandler类的对象，并将调用参数传递进去； 首先，初始化方法会保存调用参数，然后执行parse_args()方法分析参数； 如果ArgvHandler类有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 目前ArgvHandler类只有两个核心方法：collect_data和report_dataa； 这两个方法一个是收集数据并打印到屏幕，用于测试；report_data方法才会将实际的数据发往服务器。 数据的收集由info_collection.InfoCollection类负责，一会再看； report_data方法会将收集到的数据打包到一个字典内，并转换为json格式； 然后通过settings中的配置，构造发送目的地url； 通过Python内置的urllib.parse对数据进行封装； 通过urllib.request将数据发送到目的url； 接收服务器返回的信息； 将成功或者失败的信息写入日志文件中。 以后，我们要测试数据收集，执行：1python main.py collect_data 要实际往服务器发送收集到的数据，则执行：1python main.py report_data 3.配置文件要将所有可能修改的数据、常量、配置等都尽量以配置文件的形式组织起来，尽量不要在代码中写死任何数据。 ++在conf中，新建settings.py文件++，写入下面的代码：12345678910111213141516171819202122#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:21# @Author : zhdya@zhdya.cn# @File : settings.pyimport os# 远端服务器配置Params = &#123; &quot;server&quot;: &quot;10.10.7.26&quot;, &quot;port&quot;: 8000, &apos;url&apos;: &apos;/assets/report/&apos;, &apos;request_timeout&apos;: 30,&#125;# 日志文件配置PATH = os.path.join(os.path.dirname(os.getcwd()), &apos;log&apos;, &apos;cmdb.log&apos;)print(&quot;conf_settings--&gt;&gt;&quot;, PATH)# 更多配置，请都集中在此文件中 这里，配置了服务器地址、端口、发送的url、请求的超时时间，以及日志文件路径。请根据你的实际情况进行修改。 ==如上server端我是直接启动的django服务 也就是如上10.10.7.26是我笔记本的IP地址，这样默认我笔记本的10.10.7.26:8000就对外开放了！== 4.信息收集模块++在core中新建info_collection.py文件++，写入下面的代码： ==&lt;关于如下：from plugins.linux import sys_info 以及 from plugins.windows import sys_info as win_sys_info 稍后章节我们会建立一系列的目录！！&gt;==1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:48# @Author : zhdya@zhdya.cn# @File : info_collection.pyimport sysimport platformdef linux_sys_info(): from plugins.linux import sys_info return sys_info.collect()def windows_sys_info(): from plugins.windows import sys_info as win_sys_info return win_sys_info.collect()class InfoCollection(object): def collect(self): # 收集平台信息 # 首先判断当前平台，根据平台的不同，执行不同的方法 try: func = getattr(self, platform.system()) info_data = func() formatted_data = self.build_report_data(info_data) return formatted_data except AttributeError: sys.exit(&quot;不支持当前操作系统： [%s]! &quot; % platform.system()) def Linux(self): return linux_sys_info() def Windows(self): return windows_sys_info() def build_report_data(self, data): # 留下一个接口，方便以后增加功能或者过滤数据 return data 该模块的作用很简单： 首先通过Python内置的platform模块获取执行main脚本的操作系统类别，通常是windows和Linux； 根据操作系统的不同，反射获取相应的信息收集方法，并执行 如果是客户端不支持的操作系统，比如苹果系统，则提示并退出客户端。 因为windows和Linux两大操作系统的巨大平台差异，我们必须写两个收集信息的脚本。 到目前未知，我们的客户端结构如下图所示：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB模型设计（二）]]></title>
    <url>%2F2019%2F03%2F01%2F2%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[一、创建项目创建Django项目cmdb，配置好settings中的语言和时区，最后新建一个app，名字就叫做assets。这些基本过程以后就不再赘述了，不熟悉的请参考教程的前面部分。 1django版本：1.11.11 创建成功后，初始状态如下图所示： 二、模型设计说明：本项目依然采用SQLite数据库，等下一个项目再使用Mysql。 模型设计是整个项目的重中之重，其它所有的内容其实都是围绕它展开的。 而我们设计数据模型的原则和参考依据是前一节分析的项目需求和数据分类表。 1.资产共有数据模型打开assets/models.py文件，首先我们要设计一张资产表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from django.db import modelsfrom django.contrib.auth.models import User# Create your models here.class Asset(models.Model): &quot;&quot;&quot; 所有资产的共有数据表 &quot;&quot;&quot; asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;software&apos;, &apos;软件资产&apos;) ) asset_status = ( (0, &apos;在线&apos;), (1, &apos;下线&apos;), (2, &apos;未知&apos;), (3, &apos;故障&apos;), (4, &apos;备用&apos;), ) asset_type = models.CharField(choices=asset_type_choice, max_length=64, default=&apos;server&apos;, verbose_name=&quot;资产类型&quot;) name = models.CharField(max_length=64, unique=True, verbose_name=&quot;资产名称&quot;) ##唯一，不可重复 sn = models.CharField(max_length=128, unique=True, verbose_name=&quot;资产序列号&quot;) ##唯一，不可重复 business_unit = models.ForeignKey(&apos;BusinessUnit&apos;, null=True, blank=True, verbose_name=&quot;所属业务线&quot;) status = models.SmallIntegerField(choices=asset_status, default=0, verbose_name=&quot;设备状态&quot;) manufacturer = models.ForeignKey(&apos;Manufacturer&apos;, null=True, blank=True, verbose_name=&quot;制造商&quot;) manage_ip = models.GenericIPAddressField(null=True, blank=True, verbose_name=&quot;管理IP&quot;) tags = models.ManyToManyField(&apos;Tag&apos;, blank=True, verbose_name=&quot;标签&quot;) admin = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;资产管理员&quot;, related_name=&apos;admin&apos;) idc = models.ForeignKey(&apos;IDC&apos;, null=True, blank=True, verbose_name=&quot;所在机房&quot;) contract = models.ForeignKey(&apos;Contract&apos;, null=True, blank=True, verbose_name=&quot;合同&quot;) purchase_day = models.DateField(null=True, blank=True, verbose_name=&quot;购买日期&quot;) expire_day = models.DateField(null=True, blank=True, verbose_name=&quot;过保日期&quot;) price = models.FloatField(null=True, blank=True, verbose_name=&quot;价格&quot;) approved_by = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;批准人&quot;, related_name=&quot;approved_by&quot;) memo = models.TextField(null=True, blank=True, verbose_name=&quot;备注&quot;) c_time = models.DateTimeField(auto_now_add=True, verbose_name=&quot;批准日期&quot;) ##auto_add_now默认=False：储存当对象被创建时的时间，可以用来存储比如说博客什么时候创建的，后来你再更改博客，它的值也不会变。 m_time = models.DateTimeField(auto_now=True, verbose_name=&quot;更新日期&quot;) ##auto_now默认=False:当对象被存储时自动将对象的时间更新为当前时间 def __str__(self): return &apos;&lt;%s&gt; %s&apos; %(self.get_asset_type_display(), self.name) class Meta: verbose_name = &quot;资产总表&quot; verbose_name_plural = &quot;资产总表&quot; ordering = [&apos;-c_time&apos;] 说明： sn这个数据字段是所有资产都必须有，并且唯一不可重复的！通常来自自动收集的数据中； name和sn一样，也是唯一的； asset_type_choice和asset_status分别设计为两个选择类型 admin和approved_by是分别是当前资产的管理员和将该资产上线的审批员； 导入Django内置的User表，作为我们CMDB项目的用户表，用于保存管理员和审判员等人员信息； asset表中的很多字段内容都无法自动获取，需要我们手动输入，比如合同、备注。 2.服务器模型服务器作为资产的一种，而且是最主要的管理对象，包含了一些主要的信息，其模型结构如下：1234567891011121314151617181920212223242526272829class Server(models.Model): &quot;&quot;&quot;服务器设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;PC服务器&apos;), (1, &apos;刀片型&apos;), (2, &apos;小型机&apos;), ) created_by_choice = ( (&apos;auto&apos;, &apos;自动添加&apos;), (&apos;manual&apos;, &apos;手动录入&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) # 非常关键的一对一关联！ sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=1, verbose_name=&quot;服务器类型&quot;) created_by = models.CharField(choices=created_by_choice, max_length=32, default=&apos;auto&apos;, verbose_name=&quot;添加方式&quot;) hosted_on = models.ForeignKey(&apos;self&apos;, related_name=&apos;hosted_on_server&apos;, blank=True, null=True, verbose_name=&quot;宿主机&quot;) ##虚拟机专用字段 model = models.CharField(max_length=512, blank=True, null=True, verbose_name=&quot;Raid类型&quot;) os_type = models.CharField(&apos;操作系统类型&apos;, max_length=64, blank=True, null=True) os_distribution = models.CharField(&apos;发行版本&apos;, max_length=64, blank=True, null=True) os_release = models.CharField(&apos;操作系统版本&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; %(self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;服务器&apos; verbose_name_plural = &quot;服务器&quot; 说明： 每台服务器都唯一关联着一个资产对象，因此使用OneToOneField构建了一个一对一字段，这非常重要! 服务器又可分为几种子类型，这里定义了三种； 服务器添加的方式可以分为手动和自动； 有些服务器是虚拟机或者docker生成的，没有物理实体，存在于宿主机中，因此需要增加一个hosted_on字段； 服务器有型号信息，如果硬件信息中不包含，那么指的就是主板型号； Raid类型在采用了Raid的时候才有，否则为空; 操作系统相关信息包含类型、发行版本和具体版本。 3.安全、网络、存储设备和软件资产的模型这部分内容不是项目的主要内容，而且数据大多数不能自动收集和报告，很多都需要手工录入。我这里给出了范例，更多的数据字段，可以自行添加。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class SecurityDevice(models.Model): &quot;&quot;&quot;安全设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;防火墙&apos;), (1, &apos;入侵检测设备&apos;), (2, &apos;互联网网关&apos;), (4, &apos;运维审计系统&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;安全设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;安全设备&apos; verbose_name_plural = &quot;安全设备&quot;class StorageDevice(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;磁盘阵列&apos;), (1, &apos;网络存储器&apos;), (2, &apos;磁带库&apos;), (4, &apos;磁带机&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;存储设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;存储设备&apos; verbose_name_plural = &quot;存储设备&quot;class NetworkDevice(models.Model): &quot;&quot;&quot;网络设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;路由器&apos;), (1, &apos;交换机&apos;), (2, &apos;负载均衡&apos;), (4, &apos;VPN设备&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;网络设备类型&quot;) vlan_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;VLanIP&quot;) intranet_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;内网IP&quot;) model = models.CharField(max_length=128, null=True, blank=True, verbose_name=&quot;网络设备型号&quot;) firmware = models.CharField(max_length=128, blank=True, null=True, verbose_name=&quot;设备固件版本&quot;) port_num = models.SmallIntegerField(null=True, blank=True, verbose_name=&quot;端口个数&quot;) device_detail = models.TextField(null=True, blank=True, verbose_name=&quot;详细配置&quot;) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; % (self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;网络设备&apos; verbose_name_plural = &quot;网络设备&quot;class Software(models.Model): &quot;&quot;&quot; 只保存付费购买的软件 &quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;操作系统&apos;), (1, &apos;办公\开发软件&apos;), (2, &apos;业务软件&apos;), ) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;软件类型&quot;) license_num = models.IntegerField(default=1, verbose_name=&quot;授权数量&quot;) version = models.CharField(max_length=64, unique=True, help_text=&apos;例如: CentOS release 6.7 (Final)&apos;, verbose_name=&apos;软件/系统版本&apos;) def __str__(self): return &apos;%s--%s&apos; % (self.get_sub_asset_type_display(), self.version) class Meta: verbose_name = &apos;软件/系统&apos; verbose_name_plural = &quot;软件/系统&quot; 说明： 每台安全、网络、存储设备都通过一对一的方式唯一关联这一个资产对象。 通过sub_asset_type又细分设备的子类型 对于软件，它没有物理形体，因此无须关联一个资产对象； 软件只管理那些大型的收费软件，关注点是授权数量和软件版本。对于那些开源的或者免费的软件，显然不算公司的资产。 4.机房、制造商、业务线、合同、资产标签等数据模型这一部分是CMDB中相关的内容，数据表建立后，可以通过手动添加。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class IDC(models.Model): &quot;&quot;&quot;机房&quot;&quot;&quot; name = models.CharField(max_length=64, unique=True, verbose_name=&quot;机房名称&quot;) memo = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;备注&apos;) def __str__(self): return self.name class Meta: verbose_name = &apos;机房&apos; verbose_name_plural = &quot;机房&quot;class Manufacturer(models.Model): &quot;&quot;&quot;厂商&quot;&quot;&quot; name = models.CharField(&apos;厂商名称&apos;, max_length=64, unique=True) telephone = models.CharField(&apos;支持电话&apos;, max_length=30, blank=True, null=True) memo = models.CharField(&apos;备注&apos;, max_length=128, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;厂商&apos; verbose_name_plural = &quot;厂商&quot;class BusinessUnit(models.Model): &quot;&quot;&quot;业务线&quot;&quot;&quot; parent_unit = models.ForeignKey(&apos;self&apos;, blank=True, null=True, related_name=&apos;parent_level&apos;) name = models.CharField(&apos;业务线&apos;, max_length=64, unique=True) memo = models.CharField(&apos;备注&apos;, max_length=64, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;业务线&apos; verbose_name_plural = &quot;业务线&quot;class Contract(models.Model): &quot;&quot;&quot;合同&quot;&quot;&quot; sn = models.CharField(&apos;合同号&apos;, max_length=128, unique=True) name = models.CharField(&apos;合同名称&apos;, max_length=64) memo = models.TextField(&apos;备注&apos;, blank=True, null=True) price = models.IntegerField(&apos;合同金额&apos;) detail = models.TextField(&apos;合同详细&apos;, blank=True, null=True) start_day = models.DateField(&apos;开始日期&apos;, blank=True, null=True) end_day = models.DateField(&apos;失效日期&apos;, blank=True, null=True) license_num = models.IntegerField(&apos;license数量&apos;, blank=True, null=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) m_day = models.DateField(&apos;修改日期&apos;, auto_now=True) def __str__(self): return self.name class Meta: verbose_name = &apos;合同&apos; verbose_name_plural = &quot;合同&quot;class Tag(models.Model): &quot;&quot;&quot;标签&quot;&quot;&quot; name = models.CharField(&apos;标签名&apos;, max_length=32, unique=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) def __str__(self): return self.name class Meta: verbose_name = &apos;标签&apos; verbose_name_plural = &quot;标签&quot; 说明： 机房可以有很多其它字段，比如城市、楼号、楼层和未知等等，如有需要可自行添加； 业务线可以有子业务线，因此使用一个外键关联自身模型； 合同模型主要存储财务部门关心的数据； 资产标签模型与资产是多对多的关系。 5.CPU模型通常一台服务器中只能有一种CPU型号，所以这里使用OneToOneField唯一关联一个资产对象，而不是外键关系。服务器上可以有多个物理CPU，它们的型号都是一样的。每个物理CPU又可能包含多核。1234567891011121314class CPU(models.Model): &quot;&quot;&quot;CPU组件&quot;&quot;&quot; asset = models.OneToOneField(&apos;Asset&apos;) # 设备上的cpu肯定都是一样的，所以不需要建立多个cpu数据，一条就可以，因此使用一对一。 cpu_model = models.CharField(&apos;CPU型号&apos;, max_length=128, blank=True, null=True) cpu_count = models.PositiveSmallIntegerField(&apos;物理CPU个数&apos;, default=1) cpu_core_count = models.PositiveSmallIntegerField(&apos;CPU核数&apos;, default=1) def __str__(self): return self.asset.name + &quot;: &quot; + self.cpu_model class Meta: verbose_name = &apos;CPU&apos; verbose_name_plural = &quot;CPU&quot; 6.RAM模型某个资产中可能有多条内存，所以这里必须是外键关系。其次，内存的sn号可能无法获得，就必须通过内存所在的插槽未知来唯一确定每条内存。因此，unique_together = (‘asset’, ‘slot’)这条设置非常关键，相当于内存的主键了，每条内存数据必须包含slot字段，否则就不合法。1234567891011121314151617class RAM(models.Model): &quot;&quot;&quot;内存组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 只能通过外键关联Asset。否则不能同时关联服务器、网络设备等等。 sn = models.CharField(&apos;SN号&apos;, max_length=128, blank=True, null=True) model = models.CharField(&apos;内存型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;内存制造商&apos;, max_length=128, blank=True, null=True) slot = models.CharField(&apos;插槽&apos;, max_length=64) capacity = models.IntegerField(&apos;内存大小(GB)&apos;, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s: %s&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;内存&apos; verbose_name_plural = &quot;内存&quot; unique_together = (&apos;asset&apos;, &apos;slot&apos;) #unique_together，也就是联合唯一，同一资产下的内存，根据插槽slot的不同，必须唯一 7. 硬盘模型与内存相同的是，硬盘也可能有很多块，所以也是外键关系。不同的是，硬盘通常都能获取到sn号，使用sn作为唯一值比较合适，也就是unique_together = (‘asset’, ‘sn’)。硬盘有不同的接口，这里设置了4种以及unknown，可自行添加其它类别。1234567891011121314151617181920212223242526class Disk(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; disk_interface_type_choice = ( (&apos;SATA&apos;, &apos;SATA&apos;), (&apos;SAS&apos;, &apos;SAS&apos;), (&apos;SCSI&apos;, &apos;SCSI&apos;), (&apos;SSD&apos;, &apos;SSD&apos;), (&apos;unknown&apos;, &apos;unknown&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;) sn = models.CharField(&apos;硬盘SN号&apos;, max_length=128) slot = models.CharField(&apos;所在插槽位&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;磁盘型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;磁盘制造商&apos;, max_length=128, blank=True, null=True) capacity = models.FloatField(&apos;磁盘容量(GB)&apos;, blank=True, null=True) interface_type = models.CharField(&apos;接口类型&apos;, max_length=16, choices=disk_interface_type_choice, default=&apos;unknown&apos;) def __str__(self): return &apos;%s: %s: %s: %sGB&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;硬盘&apos; verbose_name_plural = &quot;硬盘&quot; unique_together = (&apos;asset&apos;, &apos;sn&apos;) 8.网卡模型一台设备中可能有很多块网卡，所以网卡与资产也是外键的关系。另外，由于虚拟机的存在，网卡的mac地址可能会发生重复，无法唯一确定某块网卡，因此通过网卡型号加mac地址的方式来唯一确定网卡。123456789101112131415161718class NIC(models.Model): &quot;&quot;&quot;网卡组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 注意要用外键 name = models.CharField(&apos;网卡名称&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;网卡型号&apos;, max_length=128) mac = models.CharField(&apos;MAC地址&apos;, max_length=64) # 虚拟机有可能会出现同样的mac地址 ip_address = models.GenericIPAddressField(&apos;IP地址&apos;, blank=True, null=True) net_mask = models.CharField(&apos;掩码&apos;, max_length=64, blank=True, null=True) bonding = models.CharField(&apos;绑定地址&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s&apos; % (self.asset.name, self.model, self.mac) class Meta: verbose_name = &apos;网卡&apos; verbose_name_plural = &quot;网卡&quot; unique_together = (&apos;asset&apos;, &apos;model&apos;, &apos;mac&apos;) # 资产、型号和mac必须联合唯一。防止虚拟机中的特殊情况发生错误。 9.日志模型CMDB必须记录各种日志，这是毫无疑问的！我们通常要记录事件名称、类型、关联的资产、子事件、事件详情、谁导致的、发生时间。这些都很重要！ 尤其要注意的是，事件日志不能随着关联资产的删除被一并删除，也就是我们设置on_delete=models.SET_NULL的意义！1234567891011121314151617181920212223242526272829303132class EventLog(models.Model): &quot;&quot;&quot; 日志. 在关联对象被删除的时候，不能一并删除，需保留日志。 因此，on_delete=models.SET_NULL &quot;&quot;&quot; name = models.CharField(&apos;事件名称&apos;, max_length=128) event_type_choice = ( (0, &apos;其它&apos;), (1, &apos;硬件变更&apos;), (2, &apos;新增配件&apos;), (3, &apos;设备下线&apos;), (4, &apos;设备上线&apos;), (5, &apos;定期维护&apos;), (6, &apos;业务上线\更新\变更&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批成功时有这项数据 new_asset = models.ForeignKey(&apos;NewAssetApprovalZone&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批失败时有这项数据 event_type = models.SmallIntegerField(&apos;事件类型&apos;, choices=event_type_choice, default=4) component = models.CharField(&apos;事件子项&apos;, max_length=256, blank=True, null=True) detail = models.TextField(&apos;事件详情&apos;) date = models.DateTimeField(&apos;事件时间&apos;, auto_now_add=True) user = models.ForeignKey(User, blank=True, null=True, verbose_name=&apos;事件执行人&apos;, on_delete=models.SET_NULL) # 自动更新资产数据时没有执行人 memo = models.TextField(&apos;备注&apos;, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;事件纪录&apos; verbose_name_plural = &quot;事件纪录&quot; 10.新资产待审批区模型新资产的到来，并不能直接加入CMDB数据库中，而是要通过管理员审批后，才可以上线的。这就需要一个新资产的待审批区。在该区中，以资产的sn号作为唯一值，确定不同的资产。除了关键的包含资产所有信息的data字段，为了方便审批员查看信息，我们还设计了一些厂商、型号、内存大小、CPU类型等字段。同时，有可能出现资产还未审批，更新数据就已经发过来的情况，所以需要一个数据更新日期字段。1234567891011121314151617181920212223242526272829303132333435363738class NewAssetApprovalZone(models.Model): &quot;&quot;&quot;新资产待审批区&quot;&quot;&quot; sn = models.CharField(&apos;资产SN号&apos;, max_length=128, unique=True) # 此字段必填 asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;IDC&apos;, &apos;机房&apos;), (&apos;software&apos;, &apos;软件资产&apos;), ) asset_type = models.CharField(choices=asset_type_choice, default=&apos;server&apos;, max_length=64, blank=True, null=True, verbose_name=&apos;资产类型&apos;) manufacturer = models.CharField(max_length=64, blank=True, null=True, verbose_name=&apos;生产厂商&apos;) model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;型号&apos;) ram_size = models.PositiveIntegerField(blank=True, null=True, verbose_name=&apos;内存大小&apos;) cpu_model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;CPU型号&apos;) cpu_count = models.PositiveSmallIntegerField(blank=True, null=True) cpu_core_count = models.PositiveSmallIntegerField(blank=True, null=True) os_distribution = models.CharField(max_length=64, blank=True, null=True) os_type = models.CharField(max_length=64, blank=True, null=True) os_release = models.CharField(max_length=64, blank=True, null=True) data = models.TextField(&apos;资产数据&apos;) # 此字段必填 c_time = models.DateTimeField(&apos;汇报日期&apos;, auto_now_add=True) m_time = models.DateTimeField(&apos;数据更新日期&apos;, auto_now=True) approved = models.BooleanField(&apos;是否批准&apos;, default=False) def __str__(self): return self.sn class Meta: verbose_name = &apos;新上线待批准资产&apos; verbose_name_plural = &quot;新上线待批准资产&quot; ordering = [&apos;-c_time&apos;] 11.总结通过前面的内容，我们可以看出CMDB数据模型的设计非常复杂，我们这里还是省略了很多不太重要的部分，就这样总共都有400多行代码。其中每个模型需要保存什么字段、采用什么类型、什么关联关系、定义哪些参数、数据是否可以为空，这些都是踩过各种坑后总结出来的，不是随便就能定义的。所以，请务必详细阅读和揣摩这些模型的内容。 一切没有问题之后，注册app，然后makemigrations以及migrate! 注册app： cmdb/settings.py123456789INSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;assets&apos;, ##此处] 创建数据库表单：12python manage.py makemigrationspython manage.py migrate]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB项目需求分析（一）]]></title>
    <url>%2F2019%2F02%2F28%2F1%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、CMDB简介CMDB (Configuration Management Database)配置管理数据库: CMDB用于存储与管理企业IT架构中设备的各种配置信息，它与所有服务支持和服务交付流程都紧密相联，支持这些流程的运转、发挥配置信息的价值，同时依赖于相关流程保证数据的准确性。 CMDB是ITIL(Information Technology Infrastructure Library，信息技术基础架构库)的基础，常常被认为是构建其它ITIL流程的先决条件而优先考虑，ITIL项目的成败与是否成功建立CMDB有非常大的关系。 CMDB的核心是对整个公司的IT硬件/软件资源进行自动/手动收集、变更操作，说白了也就是对IT资产进行自动化管理，这也是本项目的重点。 二、项目需求分析本项目不是一个完整的的CMDB系统，重点针对服务器资产的自动数据收集、报告、接收、审批、更新和展示，搭建一个基础的面向运维的主机管理平台。 下面是项目需求的总结： 尽可能存储所有的IT资产数据，但不包括外设、优盘、显示器这种属于行政部门管理的设备； 硬件信息可自动收集、报告、分析、存储和展示； 具有后台管理人员的工作界面； 具有前端可视化展示的界面； 具有日志记录功能； 数据可手动添加、修改和删除。 当然，实际的CMDB项目需求绝对不止这些，还有诸如用户管理、权限管理、API安全认证、REST设计等等。 三、资产分类资产种类众多，不是所有的都需要CMDB管理，也不是什么都是CMDB能管理的。 下面是一个大致的分类，不一定准确、全面： 资产类型包括： 服务器 存储设备 安全设备 网络设备 软件资产 服务器又可分为： 刀片服务器 PC服务器 小型机 大型机 其它 存储设备包括： 磁盘阵列 网络存储器 磁带库 磁带机 其它 安全设备包括： 防火墙 入侵检测设备 互联网网关 漏洞扫描设备 数字签名设备 上网行为管理设备 运维审计设备 加密机 其它 网络设备包括： 路由器 交换器 负载均衡 VPN 流量分析 其它 软件资产包括： 操作系统授权 大型软件授权 数据库授权 其它 其中，服务器是运维部门最关心的，也是CMDB中最主要、最方便进行自动化管理的资产。 服务器又可以包含下面的部件： CPU 硬盘 内存 网卡 除此之外，我们还要考虑下面的一些内容： 机房 业务线 合同 管理员 审批员 资产标签 其它未尽事宜 大概对资产进行了分类之后，就要详细考虑各细分数据条目了。 共有数据条目： 有一些数据条目是所有资产都应该有的，比如： 资产名称 资产sn 所属业务线 设备状态 制造商 管理IP 所在机房 资产管理员 资产标签 合同 价格 购买日期 过保日期 批准人 批准日期 数据更新日期 备注 另外，不同类型的资产还有各自不同的数据条目，例如服务器： 服务器： 服务器类型 添加方式 宿主机 服务器型号 Raid类型 操作系统类型 发行版本 操作系统版本 其实，在开始正式编写CMDB项目代码之前，对项目的需求分析准确与否，数据条目的安排是否合理，是决定整个CMDB项目成败的关键。这一部分工作看似简单其实复杂，看似无用其实关键，做好了，项目基础就牢固，没做好，推到重来好几遍很正常！]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django基础学习Ⅰ]]></title>
    <url>%2F2019%2F02%2F11%2FDjango%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E2%85%A0%2F</url>
    <content type="text"><![CDATA[一、Django简介 Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的，即是CMS（内容管理系统）软件。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django是一个处理网络请求的webweb应用框架 Django是开源的 Django有四个核心组件： 1.数据模型和数据库之间的媒介ORM 2.基于正则表达式的URL分发器 3.视图处理系统 4.模板系统 MVC： 12345m modules 模型， 和数据库字段对应v views 视图 用来展示给用户的，就是我们所学到的前端c controll url控制， 一个url，对应一个方法或者类 二、Django特点1) 强大的数据库功能：用python的类继承，几行代码就可以拥有一个动态的数据库操作API，如果需要也能执行SQL语句。2) 自带的强大的后台功能：几行代码就让网站拥有一个强大的后台，轻松管理内容。3) 优雅的网址：用正则匹配网址，传递到对应函数。4) 模板系统：强大，易扩展的模板系统，设计简易，代码和样式分开设计，更易管理。5) 缓存系统：与memcached或其它缓存系统联用，表现更出色，加载速度更快。6) 国际化：完全支持多语言应用，允许你定义翻译的字符，轻松翻译成不同国家的语言 三、安装Django使用pip工具来安装Django，直接通过下面命令来安装就可以。1# pip install Django 用一下测试django是否安装成功：123456C:\Users\ZHDYA&gt;pythonPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import django&gt;&gt;&gt; print (django.VERSION)(2, 0, 6, &apos;final&apos;, 0) 四、创建项目首先，我们先通过django来创建一个项目，命令如下：1# django-admin startproject firstproject 然后在当前目录下就自动生成了一个firstproject的项目然后就可以启动这个项目了：1# python manage.py runserver 127.0.0.1:8080 默认不写ip绑定的是本机的所以ip地址，端口默认为8000 也可以通过在pycharm中直接创建一个Django项目，就自动创建好了文件，然后配置manage.py脚本的参数。 直接再次运行manage.py文件就好。 然后访问url：1http://127.0.0.1:8080 有一个欢迎的首页 4.1、项目目录结构12345678910111213第一层：DjangoTest 项目名称第二层： DjangoTest目录和__init__.py文件，声明是一个包，表示项目实际的python包，不要随意更改该目录，与配置有关联settings.py 项目的全局（所有项目）配置中心urls.py 项目的url配置中心wsgi.py 项目的wsgi配置中心templates 模板目录manage.py django命令管理脚本 setting.py1234567BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ## 类似于环境变量SECRET_KEY = ##密钥DEBUG = True ##当报错时显示的错误信息ALLOWED_HOSTS = [] ##允许哪些机器可以访问 五、创建app打个比方，jd网站：http://www.jd.com/，上面有各种各样的不同模块，我们划分为不同的app，那我们就需要在django里面创建不通的app啦。接下来，我们就来看看如何创建app如果是命令行模式： 12#python manage.py startapp linux#python manage.py startapp python 如果是pycharm，你就需要点击：1Tools-&gt;&gt;Run manage.py Task 然后出现的交互命令行上输入：1startapp newapp 这样就创建了newapp的app。 六、Django的解析顺序既然我们知道Django是使用的MVC的架构，那我们先来聊聊MVC是什么样的原理，首先，通过MVC中的C就是control，用来接收url请求，对应我们django中的url.py模块，M就代表Model，调用数据库的Model层，就是Django的model.py模块，然后经过业务逻辑层的处理，最后渲染到前端界面。前端就是MVC中的view层，对应django的view模块。 其实所有的参数定义都是以setting.py为准，++首先django先去setting.py中找到ROOT_URLCONF = ‘firstproject.urls’找到总url++。然后在firstproject下的urls.py文件中的urlpatterns列表变量，然后根据里面的URL的正则表达式进行解析，如果解析到，就调用第二个参数，第二个参数对应一个类或者一个函数，或者直接是一个前端页面，在经过类或者函数处理完以后，在展现在前端界面。而前端是单独的html文件，前端界面和后端处理分开，架构更加清晰。 在上面的目录结构中，每一个app都会有一个view.py， model.py，我们自己还要在创建一个url.py，通过include函数，在firstproject项目中的总url.py分出去，把属于各自的app的url分配到不通的APP的urls.py文件中，这样可以降低耦合度，增加代码的健壮性。。 6.1、创建urls.py文件urls作为程序的url入口，支持正则匹配，讲访问的url映射到view中的函数中。为了能调用每个app管理自己的url，我们首先需要在DjangoTest的urls.py文件中做如下修改： 12345678from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^newapp/&apos;, include(&apos;newapp.urls&apos;)),] 注意事项：为了避免和别的app取同样的名字，一般我们会在名字前加一个app名称作为前缀url匹配，主url不需要/反斜杠：==因为django已经给域名加了一个反斜杠，如：http://127.0.0.1/主url后面要加/， app的url前面就不需要加/了，主url后面一般不要加$符号， app的url后面要加$符号== 然后在创建newapp/urls.py文件，编辑如下： 1234567from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&apos;^$&apos;, views.index)] 配置 view.py文件而以上：http://127.0.0.1:8080/newapp的url对应的就是view模块中的index函数，在linux的view.py中定义index函数 1234from django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;) 然后在浏览器上访问：http://127.0.0.1:8080/newapp/，如下图所示： 6.2、如果再次增加内容：urls.py123456789from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index), url(r&apos;newapp/&apos;, views.index), url(r&apos;hello/&apos;, views.hello)] views.py1234567from django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;)def hello(request): return HttpResponse(&quot;&lt;h1 style=&apos;text-align:center&apos;&gt; hheello world!!!&lt;/h1&gt;&quot;) 当访问：http://127.0.0.1:8000/newapp/hello/ 这会出现一个一级标题且居中的字体。 6.3、urls捕获参数（匹配正则表达式）在urls.py中增加如下：1url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=hello), 在views.py中增加如下：12def hello(request, p1, p2): return HttpResponse(&quot;hello &#123;0&#125;, hello &#123;1&#125;&quot;.format(p1, p2)) url参数的捕获有两种方式：b 捕获关键字参数：在url函数中，正则表达是用（?P）进行捕获，然后在views.py中定义即可 在urls.py中增加如下：1url(r&apos;keyword/(?P&lt;ip&gt;\S+)/$&apos;, views.keyword, name=keyword), 在views.py中增加如下：12def keyword(request, ip): return HttpResponse(&quot;the ip is &#123;0&#125;&quot;.format(ip)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/keyword/1.1.1.1/ 七、urls重定向在学习url重定向之前，我们先来看看定义url的函数是怎样一个表达形式。 注意如下，有个参数 name= 这个是必须要写的，类似于起个别名。1def url(regex, view, kwargs=None, name=None): 1234567regex：url匹配的正则字符串view：一个可以调用的类型函数，或者使用include函数kwargs：关键字参数，必须是一个字典，可以通过这个传递更多参数给views.py，views通过kwargs.get(“key”)得到对应的valuename：给URL取得名字，以后可以通过reverse函数进行重定向 对于kwargs如何传递参数，我们来看一个例子： 在urls.py中增加如下：1url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), 在views.py中增加如下： 12def test(request, **kwargs): return HttpResponse(&quot;the name is : &#123;0&#125;&quot;.format(kwargs.get(&quot;name&quot;))) 在浏览器上访问url：http://127.0.0.1:8080/newapp/test/ 既然已经知道name属性的用法，现在我们就来说重定向，重定向常用name属性来进行重定向 修改urls.py12url(r&quot;^$&quot;, views.index, name=&quot;index&quot;),url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;), 修改views.py1234from django.http import HttpResponse, HttpResponseRedirectfrom django.urls import reversedef redirect(request): return HttpResponseRedirect(reverse(&quot;index&quot;)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/redirect，直接跳转到http://127.0.0.1:8080/newapp/， 当然也可以指定返回数据的具体类型，例如：Json格式返回 urls.py1234567891011from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index, name=&quot;index&quot;), # url(r&apos;hello/&apos;, views.hello) url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=&quot;hello&quot;), url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;)] 在views.py中修改主页为：1234567def index(request): test = dict() test[&apos;name&apos;] = &quot;zhdya&quot; test[&apos;sex&apos;] = &quot;man&quot; test[&apos;age&apos;] = 28 # return HttpResponse(&quot;This is a test Django index!!!&quot;) return HttpResponse(json.dumps(test))]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅲ]]></title>
    <url>%2F2019%2F02%2F10%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A2%2F</url>
    <content type="text"><![CDATA[测试集群123456789101112131415161718192021222324252627282930313233# 创建一个 nginx deplymentapiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ---apiVersion: v1 kind: Servicemetadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx 创建testnginx deployment123[root@master1 ~]# kubectl create -f testnginx.yamldeployment.extensions/nginx-dm createdservice/nginx-svc created 1234[root@master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-dm-fff68d674-j7dlk 1/1 Running 0 9m 10.254.108.115 node2 &lt;none&gt;nginx-dm-fff68d674-r5hb6 1/1 Running 0 9m 10.254.102.133 node1 &lt;none&gt; 在 安装了 calico 网络的node节点 里 curl1234567891011121314151617181920212223242526[root@node2 ~]# curl 10.254.102.133&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 查看 ipvs 规则12345678910[root@node2 ssl]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 1 0 -&gt; 192.168.161.162:6443 Masq 1 0 0TCP 10.254.18.37:80 rr -&gt; 10.254.75.1:80 Masq 1 0 0 -&gt; 10.254.102.133:80 Masq 1 0 0 配置 CoreDNS官方 地址 https://coredns.io 下载 yaml 文件123wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedmv coredns.yaml.sed coredns.yaml 修改配置文件中的部分配置：12345678910111213141516171819202122# vi coredns.yaml第一处：...data: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/18 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 &#125;... 第二处：搜索 /clusterIP 即可 clusterIP: 10.254.0.2 配置说明12345678910111213141516171）errors官方没有明确解释，后面研究2）health:健康检查，提供了指定端口（默认为8080）上的HTTP端点，如果实例是健康的，则返回“OK”。3）cluster.local：CoreDNS为kubernetes提供的域，10.254.0.0/18这告诉Kubernetes中间件它负责为反向区域提供PTR请求0.0.254.10.in-addr.arpa ..换句话说，这是允许反向DNS解析服务（我们经常使用到得DNS服务器里面有两个区域，即“正向查找区域”和“反向查找区域”，正向查找区域就是我们通常所说的域名解析，反向查找区域即是这里所说的IP反向解析，它的作用就是通过查询IP地址的PTR记录来得到该IP地址指向的域名，当然，要成功得到域名就必需要有该IP地址的PTR记录。PTR记录是邮件交换记录的一种，邮件交换记录中有A记录和PTR记录，A记录解析名字到地址，而PTR记录解析地址到名字。地址是指一个客户端的IP地址，名字是指一个客户的完全合格域名。通过对PTR记录的查询，达到反查的目的。）4）proxy:这可以配置多个upstream 域名服务器，也可以用于延迟查找 /etc/resolv.conf 中定义的域名服务器5）cache:这允许缓存两个响应结果，一个是肯定结果（即，查询返回一个结果）和否定结果（查询返回“没有这样的域”），具有单独的高速缓存大小和TTLs。# 这里 kubernetes cluster.local 为 创建 svc 的 IP 段kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IPclusterIP: 10.254.0.2 创建coreDNS1234567[root@master1 src]# kubectl apply -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.extensions/coredns createdservice/kube-dns created 查看创建：12345678910[root@master1 src]# kubectl get pod,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 23s 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 23s 10.254.75.21 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 23s k8s-app=kube-dns 检查日志123456[root@master1 src]# kubectl logs coredns-55f86bf584-hsrbp -n kube-system.:532018/09/22 02:03:06 [INFO] CoreDNS-1.2.22018/09/22 02:03:06 [INFO] linux/amd64, go1.11, eb51e8bCoreDNS-1.2.2linux/amd64, go1.11, eb51e8b 验证 dns 服务在验证 dns 之前，在 dns 未部署++之前创建的 pod 与 deployment 等，都必须删除++，重新部署，否则无法解析。 创建一个 pods 来测试一下 dns1234567891011apiVersion: v1kind: Podmetadata: name: alpinespec: containers: - name: alpine image: alpine command: - sleep - &quot;3600&quot; 查看 创建的服务123456789[root@master1 ~]# kubectl get po,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/alpine 1/1 Running 0 52s 10.254.102.141 node1 &lt;none&gt;pod/nginx-dm-fff68d674-fzhqk 1/1 Running 0 3m 10.254.102.140 node1 &lt;none&gt;pod/nginx-dm-fff68d674-h8n79 1/1 Running 0 3m 10.254.75.22 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 20d &lt;none&gt;service/nginx-svc ClusterIP 10.254.10.144 &lt;none&gt; 80/TCP 3m name=nginx 测试12345[root@master1 ~]# kubectl exec -it alpine nslookup nginx-svcnslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: nginx-svcAddress 1: 10.254.10.144 nginx-svc.default.svc.cluster.local 部署 DNS 自动伸缩按照 node 数量 自动伸缩 dns 数量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586vim dns-auto-scaling.yamlkind: ServiceAccountapiVersion: v1metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilerules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;list&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;replicationcontrollers/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;deployments/scale&quot;, &quot;replicasets/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] verbs: [&quot;get&quot;, &quot;create&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilesubjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-systemroleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: &quot;20m&quot; memory: &quot;10Mi&quot; command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params=&#123;&quot;linear&quot;:&#123;&quot;coresPerReplica&quot;:256,&quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true&#125;&#125; - --logtostderr=true - --v=2 tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot; serviceAccountName: kube-dns-autoscaler 导入文件12345[root@master1 ~]# kubectl apply -f dns-auto-scaling.yamlserviceaccount/kube-dns-autoscaler createdclusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler createddeployment.apps/kube-dns-autoscaler created ++如下是上面所用到的镜像，如果不可以下载使用如下的即可++：123registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:coredns-1.2.2registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cluster-proportional-autoscaler-amd64_1.1.2-r2 部署 Ingress 与 Dashboard部署 heapster官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster 下载 heapster 相关 yaml 文件1234567wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ==如上官方镜像一直在更新，修改的时候需要把如下的版本号也修改下↓== 下载 heapster 镜像下载123456789101112131415161718# 官方镜像k8s.gcr.io/heapster-grafana-amd64:v4.4.3k8s.gcr.io/heapster-amd64:v1.5.3k8s.gcr.io/heapster-influxdb-amd64:v1.3.3# 个人的镜像jicki/heapster-grafana-amd64:v4.4.3jicki/heapster-amd64:v1.5.3jicki/heapster-influxdb-amd64:v1.3.3# 备用阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-grafana-amd64-v4.4.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-amd64-v1.5.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-influxdb-amd64-v1.3.3# 替换所有yaml 镜像地址sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; *.yaml 修改 yaml 文件123456789# heapster.yaml 文件#### 修改如下部分 #####因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true 1234567891011121314151617181920212223242526272829303132# heapster-rbac.yaml 文件#### 修改为部分 #####将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapsterroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapstersubjects:- kind: ServiceAccount name: heapster namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapster-kubelet-apiroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-adminsubjects:- kind: ServiceAccount name: heapster namespace: kube-system 创建：12345678910[root@master1 dashboard180922]# kubectl apply -f .deployment.extensions/monitoring-grafana createdservice/monitoring-grafana createdclusterrolebinding.rbac.authorization.k8s.io/heapster createdclusterrolebinding.rbac.authorization.k8s.io/heapster-kubelet-api createdserviceaccount/heapster createddeployment.extensions/heapster createdservice/heapster createddeployment.extensions/monitoring-influxdb createdservice/monitoring-influxdb created 这儿可能需要等待一下，这个取决于自己server的网络情况：12345678910111213[root@node1 ~]# journalctl -u kubelet -f-- Logs begin at 六 2018-09-22 09:07:48 CST. --9月 22 10:34:55 node1 kubelet[2301]: I0922 10:34:55.701016 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=======&gt; ] 7.617MB/50.21MB&quot;9月 22 10:35:05 node1 kubelet[2301]: I0922 10:35:05.700868 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [========&gt; ] 8.633MB/50.21MB&quot;9月 22 10:35:15 node1 kubelet[2301]: I0922 10:35:15.701193 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==========&gt; ] 10.66MB/50.21MB&quot;9月 22 10:35:25 node1 kubelet[2301]: I0922 10:35:25.700980 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [============&gt; ] 12.69MB/50.21MB&quot;9月 22 10:35:35 node1 kubelet[2301]: I0922 10:35:35.700779 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [===============&gt; ] 15.74MB/50.21MB&quot;9月 22 10:35:45 node1 kubelet[2301]: I0922 10:35:45.701359 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================&gt; ] 18.28MB/50.21MB&quot;9月 22 10:35:55 node1 kubelet[2301]: I0922 10:35:55.701618 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [====================&gt; ] 20.82MB/50.21MB&quot;9月 22 10:36:05 node1 kubelet[2301]: I0922 10:36:05.701611 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=========================&gt; ] 25.39MB/50.21MB&quot;9月 22 10:36:15 node1 kubelet[2301]: I0922 10:36:15.700926 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==============================&gt; ] 30.99MB/50.21MB&quot;9月 22 10:36:25 node1 kubelet[2301]: I0922 10:36:25.700931 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot;9月 22 10:36:35 node1 kubelet[2301]: I0922 10:36:35.701950 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot; 查看部署情况1234567891011121314151617[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 44m 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 44m 10.254.75.21 node2 &lt;none&gt;pod/heapster-745d7bc8b7-zk65c 1/1 Running 0 13m 10.254.75.51 node2 &lt;none&gt;pod/kube-dns-autoscaler-66d448df8f-4zvw6 1/1 Running 0 32m 10.254.102.142 node1 &lt;none&gt;pod/monitoring-grafana-558c44f948-m2tzz 1/1 Running 0 1m 10.254.75.6 node2 &lt;none&gt;pod/monitoring-influxdb-f6bcc9795-496jd 1/1 Running 0 13m 10.254.102.147 node1 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/heapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 13m k8s-app=heapsterservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 44m k8s-app=kube-dnsservice/monitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 1m k8s-app=grafanaservice/monitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 13m k8s-app=influxdb 部署 dashboard下载 dashboard 镜像12345678# 官方镜像k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3# 个人的镜像jicki/kubernetes-dashboard-amd64:v1.8.3# 阿里的镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kubernetes-dashboard-amd64-v1.8.3 下载 yaml 文件1curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 导入 yaml 123# 替换所有的 images，注意修改镜像版本号为1.8.3sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; kubernetes-dashboard.yaml 创建dashboard1234567[root@master1 dashboard180922]# kubectl apply -f kubernetes-dashboard.yamlsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created 查看创建的dashboard1234[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wide | grep dashboardpod/kubernetes-dashboard-65666d4586-bb66s 1/1 Running 0 7m 10.254.102.151 node1 &lt;none&gt;service/kubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 7m k8s-app=kubernetes-dashboard 部署 Nginx Ingress++Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。++ 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ 配置 调度 node1234567891011121314151617181920212223242526# ingress 有多种方式 1. deployment 自由调度 replicas2. daemonset 全局调度 分配到所有node里# deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签# 默认如下:[root@master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONnode1 Ready &lt;none&gt; 20d v1.11.2node2 Ready &lt;none&gt; 8d v1.11.2# 对 node1 与 node2 打上 label[root@master1 ~]# kubectl label nodes node1 ingress=proxynode/node1 labeled[root@master1 ~]# kubectl label nodes node2 ingress=proxynode/node2 labeled# 打完标签以后[root@master1 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSnode1 Ready &lt;none&gt; 20d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node1node2 Ready &lt;none&gt; 9d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node2 下载镜像1234567891011# 官方镜像gcr.io/google_containers/defaultbackend:1.4quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2# 国内镜像jicki/defaultbackend:1.4jicki/nginx-ingress-controller:0.16.2# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:defaultbackend-1.4registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:nginx-ingress-controller-0.16.2 下载 yaml 文件部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml# 部署 Ingress RBAC 认证curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml# 部署 Ingress Controller 组件curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml# tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务# 为了更加方便理解，如下两个例子：# tcp 例子apiVersion: v1kind: ConfigMapmetadata: name: tcp-services namespace: ingress-nginxdata: 9000: &quot;default/tomcat:8080&quot; # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中# udp 例子apiVersion: v1kind: ConfigMapmetadata: name: udp-services namespace: ingress-nginxdata: 53: &quot;kube-system/kube-dns:53&quot;# 替换所有的 imagessed -i &apos;s/gcr\.io\/google_containers/jicki/g&apos; *sed -i &apos;s/quay\.io\/kubernetes-ingress-controller/jicki/g&apos; *# 上面 对 两个 node 打了 label 所以配置 replicas: 2# 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。vim with-rbac.yaml第一处：↓spec: replicas: 2 第二处：↓（搜索 /nginx-ingress-serviceaccount 即可，在其下添加） .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... 第三处：↓ # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 导入 yaml 文件123456789101112131415161718192021222324252627282930313233343536373839[root@master1 ingress-service]# kubectl apply -f namespace.yamlnamespace/ingress-nginx created[root@master1 ingress-service]# kubectl get nsNAME STATUS AGEdefault Active 20dingress-nginx Active 6skube-public Active 20dkube-system Active 20d[root@master1 ingress-service]# kubectl apply -f .configmap/nginx-configuration createddeployment.extensions/default-http-backend createdservice/default-http-backend creatednamespace/ingress-nginx configuredserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createdconfigmap/tcp-services createdconfigmap/udp-services createddeployment.extensions/nginx-ingress-controller created# 查看服务，可以看到这两个 pods 被分别调度到 77 与 78 中[root@master1 ingress-service]# kubectl get pods -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEdefault-http-backend-6b89c8bdcb-vvl9f 1/1 Running 0 9m 10.254.102.163 node1 &lt;none&gt;nginx-ingress-controller-cf8d4564d-5vz7h 1/1 Running 0 9m 10.254.75.16 node2 &lt;none&gt;nginx-ingress-controller-cf8d4564d-z7q4b 1/1 Running 0 9m 10.254.102.158 node1 &lt;none&gt;# 查看我们原有的 svc[root@master1 ingress-service]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEalpine 1/1 Running 3 6h 10.254.102.141 node1 &lt;none&gt;nginx-dm-fff68d674-fzhqk 1/1 Running 0 6h 10.254.102.140 node1 &lt;none&gt;nginx-dm-fff68d674-h8n79 1/1 Running 0 6h 10.254.75.22 node2 &lt;none&gt; 创建一个 基于 nginx-dm 的 ingress12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758vi nginx-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingressspec: rules: - host: nginx.zhdya.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 理解如下:- host指虚拟出来的域名，具体地址(我理解应该是Ingress-controller那台Pod所在的主机的地址)应该加入/etc/hosts中,这样所有去nginx.zhdya.cn的请求都会发到nginx- servicePort主要是定义服务的时候的端口，不是NodePort.# 查看服务[root@master1 ingress-service]# kubectl create -f nginx-ingress.yamlingress.extensions/nginx-ingress created[root@master1 ingress-service]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEnginx-ingress nginx.zhdya.cn 80 10s# 测试访问[root@node1 ~]# curl nginx.zhdya.cn&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 当然如果本地浏览器访问的话 我们也需要绑定hosts 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 创建一个基于 dashboard 的 https 的 ingress# 新版本的 dashboard 默认就是 ssl ,所以这里使用 tcp 代理到 443 端口# 查看 dashboard svc[root@master1 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 2dkube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 3dkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2dmonitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 2dmonitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 2d# 修改 tcp-services-configmap.yaml 文件[root@master1 src]# vim tcp-services-configmap.yamlkind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginxdata: 8888: &quot;kube-system/kubernetes-dashboard:443&quot;# 载入配置文件[root@master1 src]# kubectl apply -f tcp-services-configmap.yamlconfigmap/tcp-services configured# 查看服务[root@master1 src]# kubectl get configmap/tcp-services -n ingress-nginxNAME DATA AGEtcp-services 1 2d[root@master1 src]# kubectl describe configmap/tcp-services -n ingress-nginxName: tcp-servicesNamespace: ingress-nginxLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;8888&quot;:&quot;kube-system/kubernetes-dashboard:443&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;tcp-services&quot;,&quot;namesp...Data====8888:----kube-system/kubernetes-dashboard:443Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 20m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal UPDATE 1m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services# 测试访问[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888curl: (6) Could not resolve host: dashboard.zhdya.cn; 未知的名称或服务当然如上报错很正常，咱们需要绑定下hosts在master 上查询下：[root@master1 src]# kubectl get svc -n kube-system -o wide | grep dashboardkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2d k8s-app=kubernetes-dashboard然后再node端绑定hosts [root@node1 ~]# vim /etc/hosts10.254.3.42 dashboard.zhdya.cn[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888HTTP/1.1 200 OKAccept-Ranges: bytesCache-Control: no-storeContent-Length: 990Content-Type: text/html; charset=utf-8Last-Modified: Tue, 13 Feb 2018 11:17:03 GMTDate: Tue, 25 Sep 2018 02:51:18 GMT 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 配置一个基于域名的 https , ingress# 创建一个 基于 自身域名的 证书[root@master1 dashboard-keys]# openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.zhdya.cn-key.key -out dashboard.zhdya.cn.pem -subj &quot;/CN=dashboard.zhdya.cn&quot;Generating a 2048 bit RSA private key.......+++..............+++writing new private key to &apos;dashboard.zhdya.cn-key.key&apos;-----[root@master1 dashboard-keys]# kubectl create secret tls dashboard-secret --namespace=kube-system --cert dashboard.zhdya.cn.pem --key dashboard.zhdya.cn-key.keysecret/dashboard-secret created# 查看 secret[root@master1 dashboard-keys]# kubectl get secret -n kube-system | grep dashboarddashboard-secret kubernetes.io/tls 2 55skubernetes-dashboard-certs Opaque 0 2dkubernetes-dashboard-key-holder Opaque 2 2dkubernetes-dashboard-token-r98wk kubernetes.io/service-account-token 3 2d# 创建一个 ingressvi dashboard-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubernetes-dashboard namespace: kube-system annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot; nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;spec: tls: - hosts: - dashboard.zhdya.cn secretName: dashboard-secret rules: - host: dashboard.zhdya.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443# 创建配置文件[root@master1 src]# kubectl apply -f dashboard-ingress.yamlingress.extensions/kubernetes-dashboard created[root@master1 src]# kubectl get ingress -n kube-systemNAME HOSTS ADDRESS PORTS AGEkubernetes-dashboard dashboard.zhdya.cn 80, 443 37s 测试访问 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 登录认证# 首先创建一个 dashboard rbac 超级用户vi dashboard-admin-rbac.yaml---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system# 导入配置文件[root@master1 src]# kubectl apply -f dashboard-admin-rbac.yamlserviceaccount/kubernetes-dashboard-admin createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created# 查看超级用户的 token 名称[root@master1 src]# kubectl -n kube-system get secret | grep kubernetes-dashboard-adminkubernetes-dashboard-admin-token-kq27d kubernetes.io/service-account-token 3 38s# 查看 token 部分[root@master1 src]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-kq27d 然后我们登录 web ui 选择 令牌登录然后就发现了还是那熟悉的味道：]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅱ]]></title>
    <url>%2F2019%2F02%2F09%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A1%2F</url>
    <content type="text"><![CDATA[配置 kubelet 认证kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 123# RBAC 只需创建一次就可以kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes 创建 bootstrap kubeconfig 文件++注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token++ 创建 集群所有 kubelet 的 token==注意修改hostname==1234567891011121314[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master1 --kubeconfig ~/.kube/configof2phx.v39lq3ofeh0w6f3m[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master2 --kubeconfig ~/.kube/configb3stk9.edz2iylppqjo5qbc[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master3 --kubeconfig ~/.kube/configck2uqr.upeu75jzjj1ko901[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node1 --kubeconfig ~/.kube/config1ocjm9.7qa3rd5byuft9gwr[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node2 --kubeconfig ~/.kube/confightsqn3.z9z6579gxw5jdfzd 查看生成的 token1234567891011[root@master1 kubernetes]# kubeadm token list --kubeconfig ~/.kube/configTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS1ocjm9.7qa3rd5byuft9gwr 23h 2018-09-02T16:06:32+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node1b3stk9.edz2iylppqjo5qbc 23h 2018-09-02T16:03:46+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master2ck2uqr.upeu75jzjj1ko901 23h 2018-09-02T16:05:16+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master3htsqn3.z9z6579gxw5jdfzd 23h 2018-09-02T16:06:34+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node2of2phx.v39lq3ofeh0w6f3m 23h 2018-09-02T16:03:40+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master1 以下为了区分 会先生成 hostname 名称加 bootstrap.kubeconfig 生成 master1 的 bootstrap.kubeconfig12345678910111213141516171819202122232425262728# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=of2phx.v39lq3ofeh0w6f3m \ --kubeconfig=master1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master1-bootstrap.kubeconfig# 拷贝生成的 master1-bootstrap.kubeconfig 文件mv master1-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 master2 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=b3stk9.edz2iylppqjo5qbc \ --kubeconfig=master2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master2-bootstrap.kubeconfig# 拷贝生成的 master2-bootstrap.kubeconfig 文件scp master2-bootstrap.kubeconfig 192.168.161.162:/etc/kubernetes/bootstrap.kubeconfig 生成 master3 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master3-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=ck2uqr.upeu75jzjj1ko901 \ --kubeconfig=master3-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master3-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master3-bootstrap.kubeconfig# 拷贝生成的 master3-bootstrap.kubeconfig 文件scp master3-bootstrap.kubeconfig 192.168.161.163:/etc/kubernetes/bootstrap.kubeconfig 生成 node1 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=node1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=1ocjm9.7qa3rd5byuft9gwr \ --kubeconfig=node1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=node1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node1-bootstrap.kubeconfig# 拷贝生成的 node1-bootstrap.kubeconfig 文件scp node1-bootstrap.kubeconfig 192.168.161.77:/etc/kubernetes/bootstrap.kubeconfig 生成 node2 的 bootstrap.kubeconfig1234567891011121314151617181920212223242526272829# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=node2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=htsqn3.z9z6579gxw5jdfzd \ --kubeconfig=node2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=node2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node2-bootstrap.kubeconfig# 拷贝生成的 node2-bootstrap.kubeconfig 文件scp node2-bootstrap.kubeconfig 192.168.161.78:/etc/kubernetes/bootstrap.kubeconfig 配置 bootstrap RBAC 权限123456kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers# 否则报如下错误failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:1jezb7&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope 创建自动批准相关 CSR 请求的 ClusterRole12345678910111213141516171819202122232425vi /etc/kubernetes/tls-instructs-csr.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/selfnodeserver&quot;] verbs: [&quot;create&quot;]# 创建 yaml 文件[root@master1 kubernetes]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yamlclusterrole.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver created[root@master1 kubernetes]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserverName: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ClusterRole&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;system:certificates.k8s.io:certificatesigningreq...PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] 12345678910111213141516# 将 ClusterRole 绑定到适当的用户组# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes Node 端单 Node 部分 需要部署的组件有1docker， calico， kubelet， kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA 1234567# master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server;node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口;当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ++这种模式和我之前所接触的不太一样，之前所做的架构是基于KUBE-APISERVER 的负载均衡，所有的node节点都会去连接负载均衡的虚拟VIP。++ 创建Nginx 代理在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy 1234567891011121314151617181920212223242526272829303132# 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.161.161:6443; server 192.168.161.162:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf 123456789101112131415161718192021222324252627# 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\ -v /etc/nginx:/etc/nginx \\ --name nginx-proxy \\ --net=host \\ --restart=on-failure:5 \\ --memory=512M \\ nginx:1.13.7-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 Nginx12345678910111213141516171819202122232425262728293031323334systemctl daemon-reloadsystemctl start nginx-proxysystemctl enable nginx-proxysystemctl status nginx-proxyjournalctl -u nginx-proxy -f ##查看实时日志9月 01 17:34:55 node1 docker[4032]: 1.13.7-alpine: Pulling from library/nginx9月 01 17:34:57 node1 docker[4032]: 128191993b8a: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: 655cae3ea06e: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: dbc72c3fd216: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Waiting9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Verifying Checksum9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Download complete9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Verifying Checksum9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Download complete9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Verifying Checksum9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Download complete9月 01 17:35:17 node1 docker[4032]: 128191993b8a: Pull complete9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Verifying Checksum9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Download complete9月 01 17:35:51 node1 docker[4032]: 655cae3ea06e: Pull complete9月 01 17:35:51 node1 docker[4032]: dbc72c3fd216: Pull complete9月 01 17:35:51 node1 docker[4032]: f391a4589e37: Pull complete9月 01 17:35:51 node1 docker[4032]: Digest: sha256:34aa80bb22c79235d466ccbbfa3659ff815100ed21eddb1543c6847292010c4d9月 01 17:35:51 node1 docker[4032]: Status: Downloaded newer image for nginx:1.13.7-alpine9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: using the &quot;epoll&quot; event method9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: nginx/1.13.79月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: built by gcc 6.2.1 20160822 (Alpine 6.2.1)9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: OS: Linux 3.10.0-514.el7.x86_649月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:10485769月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker processes9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker process 5 创建 kubelet.service 文件==注意修改节点的hostname↓==123456789101112131415161718192021222324252627# 创建 kubelet 目录mkdir -p /var/lib/kubeletvi /etc/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \ --hostname-override=node1 \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 \ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --config=/etc/kubernetes/kubelet.config.json \ --cert-dir=/etc/kubernetes/ssl \ --logtostderr=true \ --v=2[Install]WantedBy=multi-user.target 创建 kubelet config 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445vi /etc/kubernetes/kubelet.config.json&#123; &quot;kind&quot;: &quot;KubeletConfiguration&quot;, &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;, &quot;authentication&quot;: &#123; &quot;x509&quot;: &#123; &quot;clientCAFile&quot;: &quot;/etc/kubernetes/ssl/ca.pem&quot; &#125;, &quot;webhook&quot;: &#123; &quot;enabled&quot;: true, &quot;cacheTTL&quot;: &quot;2m0s&quot; &#125;, &quot;anonymous&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;authorization&quot;: &#123; &quot;mode&quot;: &quot;Webhook&quot;, &quot;webhook&quot;: &#123; &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;, &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot; &#125; &#125;, &quot;address&quot;: &quot;192.168.161.77&quot;, &quot;port&quot;: 10250, &quot;readOnlyPort&quot;: 0, &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;, &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;, &quot;serializeImagePulls&quot;: false, &quot;RotateCertificates&quot;: true, &quot;featureGates&quot;: &#123; &quot;RotateKubeletClientCertificate&quot;: true, &quot;RotateKubeletServerCertificate&quot;: true &#125;, &quot;MaxPods&quot;: &quot;512&quot;, &quot;failSwapOn&quot;: false, &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;, &quot;containerLogMaxFiles&quot;: 5, &quot;clusterDomain&quot;: &quot;cluster.local.&quot;, &quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;]&#125;##其它node节点记得修改如上的IP地址 123456# 如上配置:node1 本机hostname10.254.0.2 预分配的 dns 地址cluster.local. 为 kubernetes 集群的 domainregistry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。&quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ++同理修改其它node节点++ 启动 kubelet123456systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubeletjournalctl -u kubelet -f 创建 kube-proxy 证书123456789101112131415161718192021222324# 证书方面由于我们node端没有装 cfssl# 我们回到 master 端 机器 去配置证书，然后拷贝过来cd /opt/sslvi kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 1234567891011121314151617生成 kube-proxy 证书和私钥/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成ls kube-proxy*kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem# 拷贝到目录cp kube-proxy* /etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.77:/etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.78:/etc/kubernetes/ssl/ 创建 kube-proxy kubeconfig 文件1234567891011121314151617181920212223242526272829303132333435# 配置集群kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=kube-proxy.kubeconfig# 配置客户端认证kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig # 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 配置默认关联kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig# 拷贝到需要的 node 端里scp kube-proxy.kubeconfig 192.168.161.77:/etc/kubernetes/scp kube-proxy.kubeconfig 192.168.161.78:/etc/kubernetes/ 创建 kube-proxy.service 文件1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 ==node== 中安装1yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go 123456789101112131415cd /etc/kubernetes/vi kube-proxy.config.yamlapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 192.168.161.77clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfigclusterCIDR: 10.254.64.0/18healthzBindAddress: 192.168.161.77:10256hostnameOverride: node1 ##注意修改此处的hostnamekind: KubeProxyConfigurationmetricsBindAddress: 192.168.161.77:10249mode: &quot;ipvs&quot; 1234567891011121314151617181920212223# 创建 kube-proxy 目录mkdir -p /var/lib/kube-proxyvi /etc/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/usr/local/bin/kube-proxy \ --config=/etc/kubernetes/kube-proxy.config.yaml \ --logtostderr=true \ --v=1Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 kube-proxy1234systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 检查 ipvs 情况1234567[root@node1 kubernetes]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 0 0 -&gt; 192.168.161.162:6443 Masq 1 0 0 配置 Calico 网络官方文档 https://docs.projectcalico.org/v3.1/introduction 下载 Calico yaml12345# 下载 yaml 文件wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yamlwget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml 下载镜像1234567891011121314151617181920# 下载 镜像# 国外镜像 有墙quay.io/calico/node:v3.1.3quay.io/calico/cni:v3.1.3quay.io/calico/kube-controllers:v3.1.3# 国内镜像jicki/node:v3.1.3jicki/cni:v3.1.3jicki/kube-controllers:v3.1.3# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:node_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cni_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kube-controllers_v3.1.3# 替换镜像sed -i &apos;s/quay\.io\/calico/jicki/g&apos; calico.yaml 修改配置12345678910111213141516171819202122232425262728293031vi calico.yaml# 注意修改如下选项:# etcd 地址 etcd_endpoints: &quot;https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379&quot; # etcd 证书路径 # If you&apos;re using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot; # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面)data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d &apos;\n&apos;) etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d &apos;\n&apos;) etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d &apos;\n&apos;) ## 如上需要去掉() 只需要填写生成的编码即可 # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: &quot;10.254.64.0/18&quot; 查看服务1234567891011[root@master1 kubernetes]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcalico-kube-controllers-79cfd7887-xbsd4 1/1 Running 5 11d 192.168.161.77 node1 &lt;none&gt;calico-node-2545t 2/2 Running 0 29m 192.168.161.78 node2 &lt;none&gt;calico-node-tbptz 2/2 Running 7 11d 192.168.161.77 node1 &lt;none&gt;[root@master1 kubernetes]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEnode1 Ready &lt;none&gt; 11d v1.11.2 192.168.161.77 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2node2 Ready &lt;none&gt; 29m v1.11.2 192.168.161.78 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2 修改 kubelet 配置==两台node节点都需要配置== 12345678910111213# kubelet 需要增加 cni 插件 --network-plugin=cnivim /etc/systemd/system/kubelet.service --network-plugin=cni \# 重新加载配置systemctl daemon-reloadsystemctl restart kubelet.servicesystemctl status kubelet.service 检查网络的互通性：12345678910111213141516171819202122232425[root@node1 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.102.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@node2 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.75.0 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 直接在node2上面ping：[root@node2 ~]# ping 10.254.102.128PING 10.254.102.128 (10.254.102.128) 56(84) bytes of data.64 bytes from 10.254.102.128: icmp_seq=1 ttl=64 time=72.3 ms64 bytes from 10.254.102.128: icmp_seq=2 ttl=64 time=0.272 ms 安装 calicoctl++calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 下载 二进制文件curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctlmv calicoctl /usr/local/bin/chmod +x /usr/local/bin/calicoctl# 创建 calicoctl.cfg 配置文件mkdir /etc/calicovim /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: datastoreType: &quot;kubernetes&quot; kubeconfig: &quot;/root/.kube/config&quot;# 查看 calico 状态[root@node1 src]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+-------------------+-------+----------+-------------+| 192.168.161.78 | node-to-node mesh | up | 06:54:19 | Established |+----------------+-------------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.[root@node1 src]# calicoctl get node ##当然我这边是在node节点操作的，node节点是没有/root/.kube/config 这个文件的，只需要从master节点copy过来即可！！NAMEnode1node2]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅰ]]></title>
    <url>%2F2019%2F02%2F08%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A0%2F</url>
    <content type="text"><![CDATA[1:服务器信息以及节点介绍 初次使用 ==CoreDNS==， ==Ingress==， ==Calico== 系统信息：centos7 主机名称 IP 备注 master1 192.168.161.161 master and etcd master2 192.168.161.162 master and etcd master3 192.168.161.163 etcd node1 192.168.161.77 node1 node2 192.168.161.78 node2 我这边将数据盘挂载了 /opt 目录下 一、环境初始化1：分别在4台主机设置主机名称12345hostnamectl set-hostname master1hostnamectl set-hostname master2hostnamectl set-hostname master3hostnamectl set-hostname node1hostnamectl set-hostname node2 2:配置主机映射 123456789cat &lt;&lt;EOF &gt; /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.161.161 master1192.168.161.162 master2192.168.161.163 master3192.168.161.77 node1192.168.161.78 node2EOF 3：node01上执行ssh免密码登陆配置 1ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.161.XXX 4：四台主机配置、停防火墙、关闭Swap、关闭Selinux、设置内核、K8S的yum源、安装依赖包、配置ntp（配置完后建议重启一次） 1234567891011121314151617181920212223242526272829303132333435systemctl stop firewalldsystemctl disable firewalldswapoff -a sed -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstabsetenforce 0 sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/selinux/config modprobe br_netfiltercat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.d/k8s.confls /proc/sys/net/bridgeyum install -y epel-releaseyum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl systemctl enable ntpdate.serviceecho &apos;*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com &gt;/dev/null 2&gt;&amp;1&apos; &gt; /tmp/crontab2.tmpcrontab /tmp/crontab2.tmpsystemctl start ntpdate.service echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft memlock unlimited&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard memlock unlimited&quot; &gt;&gt; /etc/security/limits.conf 二、环境说明123基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler这里配置2个Master 2个node, Master-161、Master-162 做 Master + etcd, master3 仅仅etcd， node-01 node-02 只做单纯 Node 创建 验证这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 安装 cfssl1234567891011121314mkdir -p /opt/local/cfsslcd /opt/local/cfsslwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64mv cfssl_linux-amd64 cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64mv cfssljson_linux-amd64 cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 cfssl-certinfochmod +x * 创建 CA 证书配置123mkdir /opt/sslcd /opt/ssl config.json 文件123456789101112131415161718192021vi config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; csr.json 文件123456789101112131415161718vi csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 CA 证书和私钥 1234567891011cd /opt/ssl//opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca[root@master1 ssl]# ls -lt总用量 20-rw-r--r-- 1 root root 1005 9月 1 13:36 ca.csr-rw------- 1 root root 1679 9月 1 13:36 ca-key.pem-rw-r--r-- 1 root root 1363 9月 1 13:36 ca.pem-rw-r--r-- 1 root root 210 9月 1 13:35 csr.json-rw-r--r-- 1 root root 292 9月 1 13:35 config.json 分发证书创建证书目录1mkdir -p /etc/kubernetes/ssl 拷贝所有文件到目录下12cp *.pem /etc/kubernetes/sslcp ca.csr /etc/kubernetes/ssl 这里要将文件拷贝到所有的k8s机器上1234scp *.pem *.csr 192.168.161.162:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.163:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.77:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.78:/etc/kubernetes/ssl/ 三、安装 docker所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 123456789101112131415161718192021222324252627# 导入 yum 源# 安装 yum-config-manageryum -y install yum-utils# 导入yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo# 更新 repoyum makecache# 查看yum 版本yum list docker-ce.x86_64 --showduplicates |sort -r# 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinuxwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmrpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmyum -y install docker-ce-17.03.2.cedocker version 更改docker 配置12345678910111213141516171819202122232425262728293031# 添加配置vi /etc/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.comAfter=network.target docker-storage-setup.serviceWants=docker-storage-setup.service[Service]Type=notifyEnvironment=GOTRACEBACK=crashExecReload=/bin/kill -s HUP $MAINPIDDelegate=yesKillMode=processExecStart=/usr/bin/dockerd \ $DOCKER_OPTS \ $DOCKER_STORAGE_OPTIONS \ $DOCKER_NETWORK_OPTIONS \ $DOCKER_DNS_OPTIONS \ $INSECURE_REGISTRYLimitNOFILE=1048576LimitNPROC=1048576LimitCORE=infinityTimeoutStartSec=1minRestart=on-abnormal[Install]WantedBy=multi-user.target 修改其他配置12345678910111213141516171819202122232425262728293031323334353637# 低版本内核， kernel 3.10.x 配置使用 overlay2vi /etc/docker/daemon.json&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;mkdir -p /etc/systemd/system/docker.service.d/vi /etc/systemd/system/docker.service.d/docker-options.conf# 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载)# docker 版本 17.03.2 之前配置为 --graph=/opt/docker# docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service]Environment=&quot;DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \ --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5&quot;vi /etc/systemd/system/docker.service.d/docker-dns.conf# 添加如下 : [Service]Environment=&quot;DOCKER_DNS_OPTIONS=\ --dns 10.254.0.2 --dns 114.114.114.114 \ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot; 重新读取配置，启动 docker123systemctl daemon-reloadsystemctl start dockersystemctl enable docker 如果报错 请使用1systemctl status docker -l 或 journalctl -u docker 来定位问题 etcd 集群etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.2 etcd 支持最新版本为 v3.2.18 安装 etcd官方地址 https://github.com/coreos/etcd/releases 123456789# 下载 二进制文件（3台master机器都需要）wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gztar zxvf etcd-v3.2.18-linux-amd64.tar.gzcd etcd-v3.2.18-linux-amd64mv etcd etcdctl /usr/bin/ 创建 etcd 证书etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发。 1234567891011121314151617181920212223242526cd /opt/ssl/vi etcd-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 etcd 密钥1234/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd 1234567891011121314151617181920212223242526# 查看生成[root@master1 ssl]# ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem# 检查证书[root@master1 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem# 拷贝到etcd服务器# etcd-1 cp etcd*.pem /etc/kubernetes/ssl/# etcd-2scp etcd*.pem 192.168.161.162:/etc/kubernetes/ssl/# etcd-3scp etcd*.pem 192.168.161.163:/etc/kubernetes/ssl/# 如果 etcd 非 root 用户，读取证书会提示没权限chmod 644 /etc/kubernetes/ssl/etcd-key.pem 修改 etcd 配置由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 创建 etcd data 目录， 并授权12345useradd etcdmkdir -p /opt/etcdchown -R etcd:etcd /opt/etcd etcd-1123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd1 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.161:2380 \ --listen-peer-urls=https://192.168.161.161:2380 \ --listen-client-urls=https://192.168.161.161:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.161:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-2123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd2 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.162:2380 \ --listen-peer-urls=https://192.168.161.162:2380 \ --listen-client-urls=https://192.168.161.162:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.162:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-3123456789101112131415161718192021222324252627282930313233343536vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd3 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.163:2380 \ --listen-peer-urls=https://192.168.161.163:2380 \ --listen-client-urls=https://192.168.161.163:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.163:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 etcd分别启动 所有节点的 etcd 服务 123456systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcdjournalctl -u etcd -f ##用此命令来动态查看具体日志 验证 etcd 集群状态12345678910etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ cluster-health member 60ce394098258c3 is healthy: got healthy result from https://192.168.161.163:2379member afe2d07db38fa5e2 is healthy: got healthy result from https://192.168.161.162:2379member ba8a716d98dac47b is healthy: got healthy result from https://192.168.161.161:2379cluster is healthy 查看 etcd 集群成员：123456789etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ member list60ce394098258c3: name=etcd3 peerURLs=https://192.168.161.163:2380 clientURLs=https://192.168.161.163:2379 isLeader=falseafe2d07db38fa5e2: name=etcd2 peerURLs=https://192.168.161.162:2380 clientURLs=https://192.168.161.162:2379 isLeader=falseba8a716d98dac47b: name=etcd1 peerURLs=https://192.168.161.161:2380 clientURLs=https://192.168.161.161:2379 isLeader=true 配置 Kubernetes 集群kubectl 安装在所有需要进行操作的机器上 Master and NodeMaster 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 安装组件1234567891011121314151617# 从github 上下载版本 (在两台master上节点执行)cd /usr/local/srcwget https://dl.k8s.io/v1.11.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetescp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm&#125; /usr/local/bin/scp server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm&#125; 192.168.161.162:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.77:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.78:/usr/local/bin/ 创建 admin 证书kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。12345678910111213141516171819202122cd /opt/ssl/vi admin-csr.json&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 1234567891011121314151617# 生成 admin 证书和私钥cd /opt/ssl//opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin# 查看生成[root@master1 ssl]# ls admin*admin.csr admin-csr.json admin-key.pem admin.pemcp admin*.pem /etc/kubernetes/ssl/scp admin*.pem 192.168.161.162:/etc/kubernetes/ssl/ 生成 kubernetes 配置文件生成证书相关的配置文件存储与 /root/.kube 目录中 1234567891011121314151617181920# 配置 kubernetes 集群kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443# 配置 客户端认证kubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/ssl/admin.pem \ --embed-certs=true \ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=adminkubectl config use-context kubernetes 创建 kubernetes 证书123456789101112131415161718192021222324252627282930313233343536cd /opt/sslvi kubernetes-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot;, &quot;192.168.161.77&quot;, &quot;192.168.161.78&quot;, &quot;10.254.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 192.168.161.161 和 172.16.161.162 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 生成 kubernetes 证书和私钥123456789101112131415161718/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes# 查看生成[root@master1 ssl]# ls -lt kubernetes*-rw-r--r-- 1 root root 1277 9月 1 15:31 kubernetes.csr-rw------- 1 root root 1679 9月 1 15:31 kubernetes-key.pem-rw-r--r-- 1 root root 1651 9月 1 15:31 kubernetes.pem-rw-r--r-- 1 root root 531 9月 1 15:31 kubernetes-csr.json# 拷贝到目录cp kubernetes*.pem /etc/kubernetes/ssl/scp kubernetes*.pem 192.168.161.162:/etc/kubernetes/ssl/ 配置 kube-apiserverkubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。12345678910111213141516171819202122232425262728# 生成 token[root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;97606de41d5ee3c3392aae432eb3143d# 创建 encryption-config.yaml 配置cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 97606de41d5ee3c3392aae432eb3143d - identity: &#123;&#125;EOF# 拷贝cp encryption-config.yaml /etc/kubernetes/scp encryption-config.yaml 192.168.161.162:/etc/kubernetes/ 123456789101112131415161718192021# 生成高级审核配置文件&gt; 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&gt;&gt; 如下为最低限度的日志审核cd /etc/kubernetescat &gt;&gt; audit-policy.yaml &lt;&lt;EOF# Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF# 拷贝scp audit-policy.yaml 192.168.161.162:/etc/kubernetes/ 创建 kube-apiserver.service 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下# 配置为 各自的本地 IPvi /etc/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]User=rootExecStart=/usr/local/bin/kube-apiserver \ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \ --anonymous-auth=false \ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \ --advertise-address=192.168.161.161 \ --allow-privileged=true \ --apiserver-count=3 \ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kubernetes/audit.log \ --authorization-mode=Node,RBAC \ --bind-address=0.0.0.0 \ --secure-port=6443 \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \ --enable-swagger-ui=true \ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \ --etcd-servers=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379 \ --event-ttl=1h \ --kubelet-https=true \ --insecure-bind-address=127.0.0.1 \ --insecure-port=8080 \ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \ --service-cluster-ip-range=10.254.0.0/18 \ --service-node-port-range=30000-32000 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --enable-bootstrap-token-auth \ --v=1Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target# --experimental-encryption-provider-config ，替代之前 token.csv 文件# 这里面要注意的是 --service-node-port-range=30000-32000# 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。记得在另外一台master上修改IP地址 启动 kube-apiserver1234systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 查看启动端口123456789101112[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-controller-manager两台master都需要配置：1234新增几个配置，用于自动 续期证书–feature-gates=RotateKubeletServerCertificate=true–experimental-cluster-signing-duration=86700h0m0s 12345678910111213141516171819202122232425262728293031323334# 创建 kube-controller-manager.service 文件vi /etc/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-controller-manager \ --address=0.0.0.0 \ --master=http://127.0.0.1:8080 \ --allocate-node-cidrs=true \ --service-cluster-ip-range=10.254.0.0/18 \ --cluster-cidr=10.254.64.0/18 \ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \ --feature-gates=RotateKubeletServerCertificate=true \ --controllers=*,tokencleaner,bootstrapsigner \ --experimental-cluster-signing-duration=86700h0m0s \ --cluster-name=kubernetes \ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s \ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-controller-manager1234systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 查看启动端口12345678910111213[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-scheduler1234567891011121314151617181920# 创建 kube-cheduler.service 文件vi /etc/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-scheduler \ --address=0.0.0.0 \ --master=http://127.0.0.1:8080 \ --leader-elect=true \ --v=1Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-scheduler1234systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 查看启动端口1234567891011121314[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::10251 :::* LISTEN 4023/kube-schedulertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 验证 Master 节点12345678910111213141516[root@master1 kubernetes]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;[root@master2 bin]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
