<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[K8S中部署SpringCloud微服务项目]]></title>
    <url>%2F2020%2F01%2F02%2FK8S%E4%B8%AD%E9%83%A8%E7%BD%B2SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[一、熟悉SpringCloud项目1.1、主机分配 主机IP 角色 192.168.171.11 k8s-master 192.168.171.12 k8s-node1 192.168.171.13 k8s-node2 192.168.171.10 harbor-mysql 1.2、代码分支详情： Dev1 交付代码； Dev2 编写Dockerfile构建镜像； Dev3 k8s资源编排文件； Dev4 微服务链路监控； Dev5 新功能测试； Master 最终上线； 二、代码编译构建（Maven）2.1、拉取项目代码代码分支：[root@k8s-master ~]# git clone -b dev1 https://github.com/xxxxx/simple-microservice 2.2、部署须知1、导入db目录下数据库文件到自己的MySQL服务器2、修改配置环境（xxx-service/src/main/resources/application.yml，active值决定启用环境配置文件）3、修改连接数据库配置（xxx-service/src/main/resources/application-fat.yml）4、修改前端页面连接网关地址（portal-service/src/main/resources/static/js/productList.js和orderList.js）5、服务启动顺序：eureka -&gt; mysql -&gt; product,stock,order -&gt; gateway -&gt; portal 2.3、架构介绍： 用户访问==》portal（前端）； 通过负载均衡调用后端api（通过ingress把网关的service暴露下）； 后端服务分别注册到Eureka，并由Eureka进行服务发现，注册，心跳等； 2.4、准备编译编译需要安装jdk , jdk版本要看开发那边使用什么，注意一下。我这里用的1.8.0# yum install java-1.8.0-openjdk maven -y# java -versionopenjdk version &quot;1.8.0_232&quot;OpenJDK Runtime Environment (build 1.8.0_232-b09)OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode) 进行编译： 进入源代码里，我们先进入交付的分支，开发工程师原封不动的把源代码交给我们，然后我们运维进行编译构建，把源代码打成jar包。 [root@k8s-master1 simple-microservice-dev1]# ll总用量 28drwxr-xr-x 4 root root 70 7月 28 11:56 basic-commondrwxr-xr-x 2 root root 59 7月 28 11:56 dbdrwxr-xr-x 3 root root 32 7月 28 11:56 eureka-servicedrwxr-xr-x 3 root root 32 7月 28 11:56 gateway-service-rw-r--r-- 1 root root 11357 7月 28 11:56 LICENSE-rw-r--r-- 1 root root 420 7月 28 11:56 lombok.configdrwxr-xr-x 4 root root 71 7月 28 11:56 order-service-rw-r--r-- 1 root root 5419 7月 28 11:56 pom.xmldrwxr-xr-x 3 root root 32 7月 28 11:56 portal-servicedrwxr-xr-x 4 root root 75 7月 28 11:56 product-service-rw-r--r-- 1 root root 24 7月 28 11:56 README.mddrwxr-xr-x 4 root root 71 7月 28 11:56 stock-service每个微服务的配置文件均在：XXX/src/main/resources/application-fat.yml需要修改的微服务为：stock-service，product-service， order-service中的mysql配置文件； Maven项目对象模型(POM),可以通过一小段描述信息来观念里项目的构建，报告和文档的项目管理工具软件mvn clean package -D maven.test.skip=true -P prodmvn clean package：清除目录中生成的结果，做一个清除，重新打新的包。-D maven.test.skip: 跳过单元测试，写的测试用例，如果写的有问题，是编译不过去的-P prod: 使用哪一套配置文件[root@k8s-master simple-microservice-dev1]# mvn clean package -D maven.test.skip=true构建完成会多出一个target且根据pom文件生成指定jar包：# cd product-service/product-service-api/target/classes/ maven-archiver/ maven-status/ product-service-api.jar 三、构建项目镜像并推送到镜像仓库构建镜像使用Docker和结合Dockerfile[root@k8s-master1 product-service-biz]# cat DockerfileFROM java:8-jdk-alpineRUN apk add -U tzdata &amp;&amp; \ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/product-service-biz.jar ./EXPOSE 8010CMD java -jar /product-service-biz.jar 打包： 如果不是https的Harbor需要在docker里面添加信任才能访问到镜像 [root@k8s-master ~]# vim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;http://f1361db2.m.daocloud.io&quot;], &quot;insecure-registries&quot;: [&quot;192.168.171.10&quot;]&#125; docker build -t product .[root@k8s-master1 product-service-biz]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEproduct latest f8d58232cd1b 5 seconds ago 191MB[root@k8s-master1 product-service-biz]# docker tag product 192.168.171.10/microservice/product:latest[root@k8s-master ~]# docker login 192.168.171.10Username: adminPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded推送镜像到Harbor仓库中：docker push 192.168.171.10/microservice/product:latest 四、推送镜像到harbor[root@k8s-master1 simple-microservice-dev3]# ll总用量 28drwxr-xr-x 4 root root 70 7月 28 11:50 basic-commondrwxr-xr-x 2 root root 59 7月 28 11:50 dbdrwxr-xr-x 3 root root 50 7月 28 11:50 eureka-servicedrwxr-xr-x 3 root root 50 7月 28 11:50 gateway-servicedrwxr-xr-x 2 root root 143 7月 28 11:50 k8s-rw-r--r-- 1 root root 11357 7月 28 11:50 LICENSE-rw-r--r-- 1 root root 420 7月 28 11:50 lombok.configdrwxr-xr-x 4 root root 71 7月 28 11:50 order-service-rw-r--r-- 1 root root 5419 7月 28 11:50 pom.xmldrwxr-xr-x 3 root root 50 7月 28 11:50 portal-servicedrwxr-xr-x 4 root root 75 7月 28 11:50 product-service-rw-r--r-- 1 root root 24 7月 28 11:50 README.mddrwxr-xr-x 4 root root 71 7月 28 11:50 stock-service 具体来看下批量将所有的微服务推送到镜像仓库并在K8S集群中发布：#!/bin/bash##创建拉取镜像的认证信息docker_registry=192.168.171.10kubectl create secret docker-registry registry-pull-secret --docker-server=$docker_registry --docker-username=admin --docker-password=XXXXXX --docker-email=zhdya@zhdya.cn -n ms##服务listservice_list=&quot;eureka-service gateway-service order-service product-service stock-service portal-service&quot;service_list=$&#123;1:-$&#123;service_list&#125;&#125;work_dir=$(dirname $PWD)current_dir=$PWD##编译cd $work_dirmvn clean package -Dmaven.test.skip=true##推送镜像并在K8S中发布（此dev3先取消发布）for service in $service_list; do cd $work_dir/$service if ls |grep biz &amp;&gt;/dev/null; then cd $&#123;service&#125;-biz fi service=$&#123;service%-*&#125; image_name=$docker_registry/microservice/$&#123;service&#125;:$(date +%F-%H-%M-%S) docker build -t $&#123;image_name&#125; . docker push $&#123;image_name&#125;# sed -i -r &quot;s#(image: )(.*)#\1$image_name#&quot; $&#123;current_dir&#125;/$&#123;service&#125;.yaml# kubectl apply -f $&#123;current_dir&#125;/$&#123;service&#125;.yamldone 其中一个service的dockerfile[root@k8s-master1 product-service-biz]# vim DockerfileFROM java:8-jdk-alpineRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/product-service-biz.jar ./EXPOSE 8010CMD java -jar /product-service-biz.jar 五、部署springCloud项目 1、服务编排 2、在k8s平台部署Erueka 3、导入数据库文件到Mysql 4、部署网关gateway 5、部署业务程序（product、stock、order） 6、部署前端（portal） 编译打成jar包，在dev3分支 5.1、创建命名空间kubectl create ns ms 部署环境要求： 跨主机网络：使用flannel或者Calico ，需要网络来打通主机之间资源的通信 CoreDNS: k8s内部的DNS。 ,用于对pod对service做记录的，好让其他的pod做访问。 Harbor镜像仓库：这个我们已经准备好了，并将项目镜像推送上去了。 Ingress Controller：同一暴露我们的应用，写yaml文件实现。 这里portal是门户网站-前端，用户访问www.XXX.com的页面，通过域名访问之后，进行的一个页面展示，我们我们通过pod来进行实现，ingress来定义我们的域名，域名定义哪个service，来定义到某个pod上，来影响静态页面，下订单请求交给网关api,采用异步调用，暴露网关，进行来用户访问，ingress也来调用，service来实现pod副本gateway网关，通过一些前端页面的页面功能，同gateway来调用实现，用户点击某个功能gateway拿到这个请求之后,通过路由转发规则，到后端的业务程序，比如商品信息（product）库存，订单，他会根据不同的业务需要来处理，库存服务会根据订单的使用来和内部的调用接口来实现，pod直接调用，需要跨主机网络，怎么找到这个服务，就需要这个注册中心，应用间的互相调用就需要这个注册中心，所有的服务都会放在这里，来进行消息通信，现在比较流行的就是erueka,订单服务都会放入到我们的mysql数据库中的，mysql是部署在外部的，有状态应用，这个部署在k8s中是比较麻烦大的，erueka是部署在k8s集群内的，只需要保证他的id是唯一性就可以了,不需要考虑他的存储。 先来看一个yaml（eureka，需要通过域名暴露服务）[root@k8s-master1 k8s]# cat eureka.yaml---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: eureka namespace: msspec: rules: - host: eureka.zhdya.cn http: paths: - path: / backend: serviceName: eureka servicePort: 8888---apiVersion: v1kind: Servicemetadata: name: eureka namespace: msspec: clusterIP: None ##无头服务 ports: - port: 8888 name: eureka selector: project: ms app: eureka---apiVersion: apps/v1kind: StatefulSetmetadata: name: eureka namespace: msspec: replicas: 3 selector: matchLabels: project: ms app: eureka serviceName: &quot;eureka&quot; template: metadata: labels: project: ms app: eureka spec: imagePullSecrets: - name: registry-pull-secret containers: - name: eureka image: 192.168.171.10/microservice/eureka:2020-01-12-13-14-40 ports: - protocol: TCP containerPort: 8888 env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8888 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8888 initialDelaySeconds: 60 periodSeconds: 10 注释：如上名为eureka的service，其clusterIP为None，所以它是无头服务。下边为名eureka的StatefulSet，其.spec.serviceName的值为eureka，指明通过上边的eureka无头服务提供DNS解析功能功。 Dockerfile：[root@k8s-master1 k8s]# cat ../eureka-service/DockerfileFROM java:8-jdk-alpineRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/eureka-service.jar ./EXPOSE 8888CMD java -jar -Deureka.instance.hostname=$&#123;MY_POD_NAME&#125;.eureka.ms /eureka-service.jar 再来看一个微服务（不需要暴露域名）：[root@k8s-master1 k8s]# cat product.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: product namespace: msspec: replicas: 2 selector: matchLabels: project: ms app: product template: metadata: labels: project: ms app: product spec: imagePullSecrets: - name: registry-pull-secret containers: - name: product image: 192.168.171.10/microservice/product:2020-01-12-13-14-56 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8010 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8010 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8010 initialDelaySeconds: 60 periodSeconds: 10 创建k8s登录harbor信息认证：[root@k8s-master1 k8s]# kubectl create secret docker-registry registry-pull-secret --docker-server=192.168.171.10 --docker-username=admin --docker-password=Zhang --docker-email=zhdya@zhdya.cn -n mssecret/registry-pull-secret created 查看其中一个微服务的配置信息：[root@k8s-master1 k8s]# cat ../stock-service/stock-service-biz/src/main/resources/application-fat.ymlspring: datasource: url: jdbc:mysql://192.168.171.10:3306/tb_stock?characterEncoding=utf-8 username: root password: XXXXXX driver-class-name: com.mysql.jdbc.Drivereureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka 部署eureka： 补充说明：StatefulSet的应用特点: 稳定且有唯一的网络标识符 当节点挂掉，既pod重新调度后其PodName和HostName不变，基于Headless Service来实现。 我们知道kubernetes中的service是定义pod暴露外部访问的一种机制，例如：3个pod，我们可以定义一个service通过标签选择器选到这三个pod，然后让问这个service就可以访问这个pod。 Headless service是Service通过DNS访问的其中一种方式，只要我们访问”mypod.stsname.namespace.svc.cluster.local”，我们就会访问到stsname下的mypod。而Service DNS的方式下有两种处理方法： Normal Service 这里访问”mypod.stsname.namespace.svc.cluster.local”的时候会得到mypod的service的IP，既VIP。 Headless Service 这里访问”mypod.stsname.namespace.svc.cluster.local”的时候会得到mypod的IP，这里我们可以看到区别是，Headless Service 不需要分配一个VIP，而是通过DNS访问的方式可以解析出带代理的Pod的IP[root@k8s-master1 k8s]# kubectl apply -f eureka.yamlingress.extensions/eureka createdservice/eureka createdstatefulset.apps/eureka created##因为是有状态应用，所以是一个一个的启动[root@k8s-master1 k8s]# kubectl get po -n msNAME READY STATUS RESTARTS AGEeureka-0 1/1 Running 0 5m37s[root@k8s-master1 k8s]# kubectl get po -n msNAME READY STATUS RESTARTS AGEeureka-0 1/1 Running 0 8m19seureka-1 1/1 Running 0 3m7seureka-2 1/1 Running 0 108s[root@k8s-master1 k8s]# kubectl exec -it eureka-1 sh -n ms/ # nslookup eurekanslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: eurekaAddress 1: 10.244.36.123 eureka-1.eureka.ms.svc.cluster.local ##通过headless的stateful部署的应用 就算IP变了之后，eureka-1.eureka.ms.svc.cluster.local这个也会自动的绑定新的IP上，且不会变！Address 2: 10.244.169.176 eureka-2.eureka.ms.svc.cluster.localAddress 3: 10.244.159.171 eureka-0.eureka.ms.svc.cluster.localStatefulSet+Headless DNS名称格式：&lt;statefulsetName-index&gt;.&lt;service-name&gt; .&lt;namespacename&gt;.svc.cluster.localEureka集群节点Pod名称：http://eureka-0.eureka.ms.svc.cluster.localhttp://eureka-1.eureka.ms.svc.cluster.localhttp://eureka-2.eureka.ms.svc.cluster.local[root@k8s-master1 k8s]# kubectl get ing -n msNAME HOSTS ADDRESS PORTS AGEeureka eureka.zhdya.cn 80 23m 部署gateway（线上环境务必部署至少要2个副本，测试）：# cat gateway.yaml---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: gateway namespace: msspec: rules: - host: gateway.zhdya.cn http: paths: - path: / backend: serviceName: gateway servicePort: 9999---apiVersion: v1kind: Servicemetadata: name: gateway namespace: msspec: ports: - port: 9999 name: gateway selector: project: ms app: gateway---apiVersion: apps/v1kind: Deploymentmetadata: name: gateway namespace: msspec: replicas: 1 selector: matchLabels: project: ms app: gateway template: metadata: labels: project: ms app: gateway spec: imagePullSecrets: - name: registry-pull-secret containers: - name: gateway image: 192.168.171.10/microservice/gateway:2020-01-12-13-14-44 imagePullPolicy: Always ports: - protocol: TCP containerPort: 9999 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10[root@k8s-master1 k8s]# kubectl apply -f gateway.yamlingress.extensions/gateway createdservice/gateway createddeployment.apps/gateway created 部署portal:# cat portal.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: portal namespace: msspec: rules: - host: portal.zhdya.cn http: paths: - path: / backend: serviceName: portal servicePort: 8080---apiVersion: v1kind: Servicemetadata: name: portal namespace: msspec: ports: - port: 8080 name: portal selector: project: ms app: portal---apiVersion: apps/v1kind: Deploymentmetadata: name: portal namespace: msspec: replicas: 1 selector: matchLabels: project: ms app: portal template: metadata: labels: project: ms app: portal spec: imagePullSecrets: - name: registry-pull-secret containers: - name: portal image: 192.168.171.10/microservice/portal:2020-01-12-13-15-06 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8080 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 60 periodSeconds: 10# kubectl apply -f portal.yamlingress.extensions/portal createdservice/portal createddeployment.apps/portal created 部署其它微服务：[root@k8s-master1 k8s]# kubectl get po -n msNAME READY STATUS RESTARTS AGEeureka-0 1/1 Running 0 39meureka-1 1/1 Running 0 34meureka-2 1/1 Running 0 33mgateway-5b6b78b54c-vjq2l 1/1 Running 0 10morder-58cc95cf96-4t9pb 1/1 Running 0 2m35sportal-5574cbd9d6-56tzc 1/1 Running 0 8m53sproduct-74bb9d98d-8hz95 1/1 Running 0 2m42sstock-845f745db5-q868l 1/1 Running 0 2m39s 查看部署情况： 六、总结： 第一步：熟悉Spring Cloud微服务项目 第二步：源代码编译构建 第三步：构建项目镜像并推送到镜像仓库 第四步：在K8S中部署Spring Cloud微服务项目的逻辑架构 第五步： K8S服务编排 第六步：在K8S中部署Eureka集群（注册中心） 第七步：部署微服务网关服务 第八步：部署微服务业务程序 第九步：部署微服务前端 第十步：微服务扩容与发布]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从运维角度看微服务架构]]></title>
    <url>%2F2020%2F01%2F01%2F6.1%E3%80%81%E4%BB%8E%E8%BF%90%E7%BB%B4%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[一、引言 微服务架构是一项在云中部署应用和服务的新技术。 1.1、微服务是什么？微服务是一种软件设计风格，开发人员在开发项目时，是一种微服务这种标准去设计的，这种的设计风格是一种将单体的应用，拆分为多个小的组件去开发，那每个组件是独立的部署，独立的测试，服务之间采用轻量级的通信。 1.2、微服务的特点 服务的组件化 每个服务独立开发、部署、有效避免一个服务的修改引起整个系统重新部署。 技术栈灵活 约定通信方式，使得服务本身功能实现对技术要求不再那么敏感，可以根据不停语言进行开发。 独立部署 每个微服务独立部署，加快部署速度，方便扩展，比起单体应用来讲要小，轻量级的，方便快速部署，扩展。 扩展性强 每个微服务可以部署多个，没有多少依赖，并且有负载均衡能力，比如一个服务部署一个副本或5个副本，通过k8s可以更好的去扩展我们的应用。 独立数据 每个微服务有独立的基本组件，例如数据库、缓存等，可能有不同的开放人员，不依赖。 1.3、微服务不足 沟通成本：由于组件都是分开来开发的，不同的项目组，沟通起来不方便，单体应用就是集中起来开发的。 数据一致性：保证这个数据，独立的组件数据是一致性。 运维成本：虚拟机部署，需要考虑组件性，调用关系，监控，配置。 内部架构复杂性：分布式的，需要轻量级的通信，rbac,MQ,还有很多的数据库。 1.4、单体应用 vs 微服务 单体架构优势： 单体架构不足： 易于部署 代码膨胀，难以维护 易于测试 构建、部署成本大、新人上手难 单体应用：适合于轻量级的应用，不提供复杂的应用。 微服务适合：比较大的应用，复杂一些的。 1.5、java微服务框架spring Boot 是独立的。 spring cloud ,基于spring boot的。 Dubbo 阿里巴巴的开源微服务框架，通过rbc实现组件之间的通信。 微服务架构图： 为什么要用注册中心？（主流注册中心： Eureka， Nacos） 1、需要记录一个或者多个微服务多个副本接口地址； 2、需要实现一个或者多个微服务多个副本负载均衡； 3、需要判断一个或者多个微服务的副本是否可用； 不同环境如何区分配置文件？ 第一种：java-jar –spring.profile.active=dev xxx.jar 第二种：统一的配置中心，例如携程的Apollo，百度的Disconf,动态根据不同的环境进行配置，页面进行管理，需要二次开发。 1.6、项目迁移到k8s平台是怎样的流程举个例子了解一下大概的一个怎样的流程： 1制作镜像 –&gt; 2控制管理pod –&gt; 3暴露应用 –&gt; 4对外发布应用 –&gt; 5日志/监控 控制器管理Pod： deployment :无状态部署statefulset ：有状态部署Daemonset :守护进程部署job &amp; cronjob：批处理 暴露应用Service service定义pod的逻辑集合，提供服务发现及负载均衡 支持cluster ip， nodeport， loadbalancer三种类型 底层实现iptables/ipvs两种网络模式 通过label关联pod 使用Coredns解析service名称 ingress 通过service关联pod 基于域名访问 通过ingress controller实现pod的负载均衡 支持tcp、udp 4层和http7层 pod数据持久化 容器部署过程中一般有以下三种数据： 启动时需要的初始数据，可以是配置文件； 启动过程中产生的临时数据，该数据需要多个容器间共享； 启动过程中产生的持久化数据；]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七、Ceph日常运维管理]]></title>
    <url>%2F2019%2F12%2F29%2F%E4%B8%83%E3%80%81Ceph%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、集群监控管理集群整体运行状态[root@cephnode01 ~]# ceph -s cluster: id: 8230a918-a0de-4784-9ab8-cd2a2b8671d0 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 27h) mgr: cephnode01(active, since 53m), standbys: cephnode03, cephnode02 osd: 4 osds: 4 up (since 27h), 4 in (since 19h) rgw: 1 daemon active (cephnode01) data: pools: 6 pools, 96 pgs objects: 235 objects, 3.6 KiB usage: 4.0 GiB used, 56 GiB / 60 GiB avail pgs: 96 active+clean id：集群ID health：集群运行状态，这里有一个警告，说明是有问题，意思是pg数大于pgp数，通常此数值相等。 mon：Monitors运行状态。 osd：OSDs运行状态。 mgr：Managers运行状态。 mds：Metadatas运行状态。 pools：存储池与PGs的数量。 objects：存储对象的数量。 usage：存储的理论用量。 pgs：PGs的运行状态 ~]$ ceph -w~]$ ceph health detail PG状态查看pg状态查看通常使用下面两个命令即可，dump可以查看更详细信息，如。~]$ ceph pg dump~]$ ceph pg stat Pool状态~]$ ceph osd pool stats~]$ ceph osd pool stats OSD状态~]$ ceph osd stat~]$ ceph osd dump~]$ ceph osd tree~]$ ceph osd df Monitor状态和查看仲裁状态~]$ ceph mon stat~]$ ceph mon dump~]$ ceph quorum_status 集群空间用量~]$ ceph df~]$ ceph df detail 二、集群配置管理(临时和全局，服务平滑重启)有时候需要更改服务的配置，但不想重启服务，或者是临时修改。这时候就可以使用tell和daemon子命令来完成此需求。 1、查看运行配置命令格式：# ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config show 命令举例：# ceph daemon osd.0 config show 2、tell子命令格式使用 tell 的方式适合对整个集群进行设置，使用 * 号进行匹配，就可以对整个集群的角色进行设置。而出现节点异常无法设置时候，只会在命令行当中进行报错，不太便于查找。命令格式：# ceph tell &#123;daemon-type&#125;.&#123;daemon id or *&#125; injectargs --&#123;name&#125;=&#123;value&#125; [--&#123;name&#125;=&#123;value&#125;]命令举例：# ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1 daemon-type：为要操作的对象类型如osd、mon、mds等。 daemon id：该对象的名称，osd通常为0、1等，mon为ceph -s显示的名称，这里可以输入*表示全部。 injectargs：表示参数注入，后面必须跟一个参数，也可以跟多个 3、daemon子命令使用 daemon 进行设置的方式就是一个个的去设置，这样可以比较好的反馈，此方法是需要在设置的角色所在的主机上进行设置。命令格式：# ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config set &#123;name&#125;=&#123;value&#125;命令举例：# ceph daemon mon.ceph-monitor-1 config set mon_allow_pool_delete false 三、集群操作命令包含start、restart、status1、启动所有守护进程# systemctl start ceph.target2、按类型启动守护进程# systemctl start ceph-mgr.target# systemctl start ceph-osd@id# systemctl start ceph-mon.target# systemctl start ceph-mds.target# systemctl start ceph-radosgw.target 四、添加和删除OSD1、添加OSD1、格式化磁盘ceph-volume lvm zap /dev/sd&lt;id&gt;2、进入到ceph-deploy执行目录/my-cluster，添加OSD# ceph-deploy osd create --data /dev/sd&lt;id&gt; $hostname 2、删除OSD1、调整osd的crush weight为 0ceph osd crush reweight osd.&lt;ID&gt; 0.02、将osd进程stopsystemctl stop ceph-osd@&lt;ID&gt;3、将osd设置outceph osd out &lt;ID&gt;4、立即执行删除OSD中数据ceph osd purge osd.&lt;ID&gt; --yes-i-really-mean-it5、卸载磁盘umount /var/lib/ceph/osd/ceph-？ 五、扩容PGceph osd pool set &#123;pool-name&#125; pg_num 128ceph osd pool set &#123;pool-name&#125; pgp_num 128 注：1、扩容大小取跟它接近的2的N次方2、在更改pool的PG数量时，需同时更改PGP的数量。PGP是为了管理placement而存在的专门的PG，它和PG的数量应该保持一致。如果你增加pool的pg_num，就需要同时增加pgp_num，保持它们大小一致，这样集群才能正常rebalancing。 六、Pool操作列出存储池ceph osd lspools 创建存储池命令格式：# ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; [&#123;pgp-num&#125;]命令举例：# ceph osd pool create rbd 32 32 设置存储池配额命令格式：# ceph osd pool set-quota &#123;pool-name&#125; [max_objects &#123;obj-count&#125;] [max_bytes &#123;bytes&#125;]命令举例：# ceph osd pool set-quota rbd max_objects 10000 删除存储池ceph osd pool delete &#123;pool-name&#125; [&#123;pool-name&#125; --yes-i-really-really-mean-it] 重命名存储池ceph osd pool rename &#123;current-pool-name&#125; &#123;new-pool-name&#125; 查看存储池统计信息rados df 给存储池做快照ceph osd pool mksnap &#123;pool-name&#125; &#123;snap-name&#125; 删除存储池的快照ceph osd pool rmsnap &#123;pool-name&#125; &#123;snap-name&#125; 获取存储池选项值ceph osd pool get &#123;pool-name&#125; &#123;key&#125; 调整存储池选项值ceph osd pool set &#123;pool-name&#125; &#123;key&#125; &#123;value&#125;size：设置存储池中的对象副本数，详情参见设置对象副本数。仅适用于副本存储池。min_size：设置 I/O 需要的最小副本数，详情参见设置对象副本数。仅适用于副本存储池。pg_num：计算数据分布时的有效 PG 数。只能大于当前 PG 数。pgp_num：计算数据分布时使用的有效 PGP 数量。小于等于存储池的 PG 数。hashpspool：给指定存储池设置/取消 HASHPSPOOL 标志。target_max_bytes：达到 max_bytes 阀值时会触发 Ceph 冲洗或驱逐对象。target_max_objects：达到 max_objects 阀值时会触发 Ceph 冲洗或驱逐对象。scrub_min_interval：在负载低时，洗刷存储池的最小间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_min_interval 。scrub_max_interval：不管集群负载如何，都要洗刷存储池的最大间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_max_interval 。deep_scrub_interval：“深度”洗刷存储池的间隔秒数。如果是 0 ，就按照配置文件里的 osd_deep_scrub_interval 。 获取对象副本数ceph osd dump | grep &apos;replicated size&apos; 七、用户管理Ceph 把数据以对象的形式存于各存储池中。Ceph 用户必须具有访问存储池的权限才能够读写数据。另外，Ceph 用户必须具有执行权限才能够使用 Ceph 的管理命令。 1、查看用户信息查看所有用户信息# ceph auth list获取所有用户的key与权限相关信息# ceph auth get client.admin如果只需要某个用户的key信息，可以使用pring-key子命令# ceph auth print-key client.admin 2、添加用户# ceph auth add client.john mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;# ceph auth get-or-create client.paul mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;# ceph auth get-or-create client.george mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos; -o george.keyring# ceph auth get-or-create-key client.ringo mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos; -o ringo.key 3、修改用户权限# ceph auth caps client.john mon &apos;allow r&apos; osd &apos;allow rw pool=liverpool&apos;# ceph auth caps client.paul mon &apos;allow rw&apos; osd &apos;allow rwx pool=liverpool&apos;# ceph auth caps client.brian-manager mon &apos;allow *&apos; osd &apos;allow *&apos;# ceph auth caps client.ringo mon &apos; &apos; osd &apos; &apos; 4、删除用户# ceph auth del &#123;TYPE&#125;.&#123;ID&#125;其中， &#123;TYPE&#125; 是 client，osd，mon 或 mds 的其中一种。&#123;ID&#125; 是用户的名字或守护进程的 ID 。 八、增加和删除Monitor一个集群可以只有一个 monitor，推荐生产环境至少部署 3 个。 Ceph 使用 Paxos 算法的一个变种对各种 map 、以及其它对集群来说至关重要的信息达成共识。建议（但不是强制）部署奇数个 monitor 。Ceph 需要 mon 中的大多数在运行并能够互相通信，比如单个 mon，或 2 个中的 2 个，3 个中的 2 个，4 个中的 3 个等。初始部署时，建议部署 3 个 monitor。后续如果要增加，请一次增加 2 个。 1、新增一个monitor# ceph-deploy mon create $hostname注意：执行ceph-deploy之前要进入之前安装时候配置的目录。/my-cluster 2、删除Monitor# ceph-deploy mon destroy $hostname注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个。 nearfull osd(s) or pool(s) nearfull此时说明部分osd的存储已经超过阈值，mon会监控ceph集群中OSD空间使用情况。如果要消除WARN,可以修改这两个参数，提高阈值，但是通过实践发现并不能解决问题，可以通过观察osd的数据分布情况来分析原因。 配置文件设置阈值 &quot;mon_osd_full_ratio&quot;: &quot;0.95&quot;, &quot;mon_osd_nearfull_ratio&quot;: &quot;0.85&quot;， 自动处理ceph osd reweight-by-utilizationceph osd reweight-by-pg 105 cephfs_data(pool_name) 手动处理：ceph osd reweight osd.2 0.8 全局处理ceph mgr module lsceph mgr module enable balancerceph balancer onceph balancer mode crush-compatceph config-key set &quot;mgr/balancer/max_misplaced&quot;: &quot;0.01&quot; PG 故障状态PG状态概述一个PG在它的生命周期的不同时刻可能会处于以下几种状态中: Creating(创建中)在创建POOL时,需要指定PG的数量,此时PG的状态便处于creating,意思是Ceph正在创建PG。 Peering(互联中)peering的作用主要是在PG及其副本所在的OSD之间建立互联,并使得OSD之间就这些PG中的object及其元数据达成一致。 Active(活跃的)处于该状态意味着数据已经完好的保存到了主PG及副本PG中,并且Ceph已经完成了peering工作。 Clean(整洁的)当某个PG处于clean状态时,则说明对应的主OSD及副本OSD已经成功互联,并且没有偏离的PG。也意味着Ceph已经将该PG中的对象按照规定的副本数进行了复制操作。 Degraded(降级的)当某个PG的副本数未达到规定个数时,该PG便处于degraded状态,例如: 在客户端向主OSD写入object的过程,object的副本是由主OSD负责向副本OSD写入的,直到副本OSD在创建object副本完成,并向主OSD发出完成信息前,该PG的状态都会一直处于degraded状态。又或者是某个OSD的状态变成了down,那么该OSD上的所有PG都会被标记为degraded。当Ceph因为某些原因无法找到某个PG内的一个或多个object时,该PG也会被标记为degraded状态。此时客户端不能读写找不到的对象,但是仍然能访问位于该PG内的其他object。 Recovering(恢复中)当某个OSD因为某些原因down了,该OSD内PG的object会落后于它所对应的PG副本。而在该OSD重新up之后,该OSD中的内容必须更新到当前状态,处于此过程中的PG状态便是recovering。 Backfilling(回填)当有新的OSD加入集群时,CRUSH会把现有集群内的部分PG分配给它。这些被重新分配到新OSD的PG状态便处于backfilling。 Remapped(重映射)当负责维护某个PG的acting set变更时,PG需要从原来的acting set迁移至新的acting set。这个过程需要一段时间,所以在此期间,相关PG的状态便会标记为remapped。 Stale(陈旧的)默认情况下,OSD守护进程每半秒钟便会向Monitor报告其PG等相关状态,如果某个PG的主OSD所在acting set没能向Monitor发送报告,或者其他的Monitor已经报告该OSD为down时,该PG便会被标记为stale。 OSD状态单个OSD有两组状态需要关注,其中一组使用in/out标记该OSD是否在集群内,另一组使用up/down标记该OSD是否处于运行中状态。两组状态之间并不互斥,换句话说,当一个OSD处于“in”状态时,它仍然可以处于up或down的状态。 OSD状态为in且up这是一个OSD正常的状态,说明该OSD处于集群内,并且运行正常。 OSD状态为in且down此时该OSD尚处于集群中,但是守护进程状态已经不正常,默认在300秒后会被踢出集群,状态进而变为out且down,之后处于该OSD上的PG会迁移至其它OSD。 OSD状态为out且up这种状态一般会出现在新增OSD时,意味着该OSD守护进程正常,但是尚未加入集群。 OSD状态为out且down在该状态下的OSD不在集群内,并且守护进程运行不正常,CRUSH不会再分配PG到该OSD上。]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六、K8S 使用Ceph存储]]></title>
    <url>%2F2019%2F12%2F28%2F%E5%85%AD%E3%80%81K8S%20%E4%BD%BF%E7%94%A8Ceph%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[一、PV、PVC概述管理存储是管理计算的一个明显问题。PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。于是引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象包含存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 管理员需要能够提供多种不同于PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。 StorageClass为集群提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件” 二、POD动态供给 动态供给主要是能够自动帮你创建pv，需要多大的空间就创建多大的pv。k8s帮助创建pv，创建pvc就直接api调用存储类来寻找pv。 如果是存储静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态。而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后帮你去连接，再帮你去自动创建pv。 三、POD使用RBD做为持久数据卷3.1、安装与配置RBD支持ReadWriteOnce，ReadOnlyMany两种模式 3.1.1、配置rbd-provisioner# 如果使用kubeadm部署的集群需要这些额外的步骤 # 由于使用动态存储时 controller-manager 需要使用 rbd 命令创建 image # 所以 controller-manager 需要使用 rbd 命令 # 由于官方controller-manager镜像里没有rbd命令 # 如果没使用如下方式会报错无法成功创建pvc # 相关 issue https://github.com/kubernetes/kubernetes/issues/38923cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: rbd-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionerrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;kube-dns&quot;] verbs: [&quot;list&quot;, &quot;get&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionersubjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: rbd-provisioner namespace: kube-systemrules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbd-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisionersubjects:- kind: ServiceAccount name: rbd-provisioner namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: selector: matchLabels: app: rbd-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: &quot;quay.io/external_storage/rbd-provisioner:v2.0.0-k8s1.11&quot; env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisionerEOF# kubectl apply -f external-storage-rbd-provisioner.yaml# 查看状态 等待running之后 再进行后续的操作# kubectl get pod -n kube-system 3.1.2、配置storageclass1、创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common。将ceph的ceph.client.admin.keyring和ceph.conf文件拷贝到master的/etc/ceph目录下yum -y install ceph-common如果K8S中是没有ceph客户端和配置文件，需要从ceph集群中copy下：scp /etc/ceph/ceph.c* root@192.168.171.11:/etc/ceph/scp /etc/ceph/ceph.c* root@192.168.171.12:/etc/ceph/scp /etc/ceph/ceph.c* root@192.168.171.13:/etc/ceph/这样就可以在K8S集群中查看集群的状态了！2、创建 osd pool 在ceph的mon或者admin节点ceph osd pool create kube 128 128 ceph osd pool ls3、创建k8s访问ceph的用户 在ceph的mon或者admin节点ceph auth get-or-create client.kube mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=kube&apos; -o ceph.client.kube.keyring4、查看key 在ceph的mon或者admin节点ceph auth get-key client.adminceph auth get-key client.kube5、# 创建 admin secret # CEPH_ADMIN_SECRET 替换为 client.admin 获取到的keykubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=AQCeEwpeo+I8HRAAnBphr8lyGc6+JBT7jU7rgA== \--namespace=kube-system6、# 在 default 命名空间创建pvc用于访问ceph的 secret # CEPH_KUBE_SECRET 替换为 client.kube 获取到的keykubectl create secret generic ceph-user-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=AQC3OhNeYrGQLRAA8Xd/e1NUto/fXnGEk6hVMg== \--namespace=default# 查看 secretkubectl get secret ceph-user-secret -o yamlkubectl get secret ceph-secret -n kube-system -o yaml 3.1.3、配置StorageClasscat &gt;storageclass-ceph-rdb.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-ceph-rdbprovisioner: ceph.com/rbdparameters: monitors: 192.168.171.135:6789,192.168.171.136:6789,192.168.171.137:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-user-secret fsType: ext4 imageFormat: &quot;2&quot; imageFeatures: &quot;layering&quot;EOF 3.1.4、创建yamlkubectl apply -f storageclass-ceph-rdb.yaml 3.1.5、查看sckubectl get sc 四、测试使用1、创建pvc测试cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: ceph-rdb-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-ceph-rdb resources: requests: storage: 2GiEOFkubectl apply -f ceph-rdb-pvc-test.yaml 2、查看[root@k8s-master1 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEceph-rdb-claim Bound pvc-e5f13194-67db-4d98-b69c-5a4272c2498d 2Gi RWO dynamic-ceph-rdb 7m10s[root@k8s-master1 ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-e5f13194-67db-4d98-b69c-5a4272c2498d 2Gi RWO Delete Bound default/ceph-rdb-claim dynamic-ceph-rdb 48s 3、创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: ceph-rdb mountPath: /usr/share/nginx/html volumes: - name: ceph-rdb persistentVolumeClaim: claimName: ceph-rdb-claimEOFkubectl apply -f nginx-pod.yaml 4、查看kubectl get pods -o wide 5、修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c &apos;echo Hello World from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html&apos; # 访问测试 6、访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk &apos;&#123;print $6&#125;&apos;)curl http://$POD_ID #测试 7、清理kubectl delete -f nginx-pod.yamlkubectl delete -f ceph-rdb-pvc-test.yaml 五、POD使用CephFS做为持久数据卷CephFS方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany 5.1、Ceph端创建CephFS pool1、如下操作在ceph的mon或者admin节点CephFS需要使用两个Pool来分别存储数据和元数据ceph osd pool create fs_data 128ceph osd pool create fs_metadata 128ceph osd lspools 2、创建一个CephFSceph fs new cephfs fs_metadata fs_data 3、查看ceph fs ls 5.2、部署 cephfs-provisioner1、使用社区提供的cephfs-provisionercat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: Namespacemetadata: name: cephfs labels: name: cephfs ---apiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: cephfs ---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;create&quot;, &quot;get&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] ---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;kube-dns&quot;,&quot;coredns&quot;] verbs: [&quot;list&quot;, &quot;get&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;policy&quot;] resourceNames: [&quot;cephfs-provisioner&quot;] resources: [&quot;podsecuritypolicies&quot;] verbs: [&quot;use&quot;] ---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner ---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ---apiVersion: apps/v1kind: Deploymentmetadata: name: cephfs-provisioner namespace: cephfsspec: selector: matchLabels: app: cephfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: &quot;quay.io/external_storage/cephfs-provisioner:latest&quot; env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - &quot;/usr/local/bin/cephfs-provisioner&quot; args: - &quot;-id=cephfs-provisioner-1&quot; - &quot;-disable-ceph-namespace-isolation=true&quot; serviceAccount: cephfs-provisionerEOFkubectl apply -f external-storage-cephfs-provisioner.yaml 2、查看状态 等待running之后 再进行后续的操作kubectl get pod -n cephfs 5.3、配置 storageclass1、查看key 在ceph的mon或者admin节点ceph auth get-key client.admin 2、# 创建 admin secret # CEPH_ADMIN_SECRET 替换为 client.admin 获取到的key # 如果在测试 ceph rbd 方式已经添加 可以略过此步骤kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot; \--from-literal=key=AQCeEwpeo+I8HRAAnBphr8lyGc6+JBT7jU7rgA== \--namespace=kube-system 3、查看 secretkubectl get secret ceph-secret -n kube-system -o yaml 4、配置 StorageClasscat &gt;storageclass-cephfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-cephfsprovisioner: ceph.com/cephfsparameters: monitors: 192.168.171.135:6789,192.168.171.136:6789,192.168.171.137:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: &quot;kube-system&quot; claimRoot: /volumes/kubernetesEOF 5、创建kubectl apply -f storageclass-cephfs.yaml 6、查看kubectl get sc 5.4、测试使用1、创建pvc测试cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-cephfs resources: requests: storage: 2GiEOFkubectl apply -f cephfs-pvc-test.yaml 2、查看[root@k8s-master1 ceph-all]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEcephfs-claim Bound pvc-50ebdaab-c6ad-47ad-86cb-149327481a67 2Gi RWO dynamic-cephfs 4s[root@k8s-master1 ceph-all]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-50ebdaab-c6ad-47ad-86cb-149327481a67 2Gi RWO Delete Bound default/cephfs-claim dynamic-cephfs 6s 3、创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod2 labels: name: nginx-pod2spec: containers: - name: nginx-pod2 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: cephfs mountPath: /usr/share/nginx/html volumes: - name: cephfs persistentVolumeClaim: claimName: cephfs-claimEOFkubectl apply -f nginx-pod.yaml 4、查看 kubectl get pods -o wide 5、修改文件内容 kubectl exec -ti nginx-pod2 -- /bin/sh -c &apos;echo Hello World from CephFS!!! &gt; /usr/share/nginx/html/index.html&apos; # 访问测试 6、访问pod测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod2 | awk &apos;&#123;print $6&#125;&apos;)curl http://$POD_ID 7、清理kubectl delete -f nginx-pod.yamlkubectl delete -f cephfs-pvc-test.yaml]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五、Promethus+Grafana监控Ceph（2）]]></title>
    <url>%2F2019%2F12%2F27%2F%E4%BA%94%E3%80%81Promethus%2BGrafana%E7%9B%91%E6%8E%A7Ceph%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、安装grafana1、配置yum源文件# vim /etc/yum.repos.d/grafana.repo[grafana]name=grafanabaseurl=https://mirrors.tuna.tsinghua.edu.cn/grafana/yum/rpmrepo_gpgcheck=0enabled=1gpgcheck=02.通过yum命令安装grafana# yum -y install grafana3.启动grafana并设为开机自启# systemctl start grafana-server.service # systemctl enable grafana-server.service 二、安装promethus1、下载安装包，下载地址https://prometheus.io/download/2、解压压缩包# tar fvxz prometheus-2.14.0.linux-amd64.tar.gz3、将解压后的目录改名# mv prometheus-2.13.1.linux-amd64 /opt/prometheus4、查看promethus版本# ./prometheus --version5、配置系统服务启动# vim /etc/systemd/system/prometheus.service[Unit]Description=Prometheus Monitoring SystemDocumentation=Prometheus Monitoring System[Service]ExecStart=/opt/prometheus/prometheus \ --config.file /opt/prometheus/prometheus.yml \ --web.listen-address=:9090[Install]WantedBy=multi-user.target6、加载系统服务# systemctl daemon-reload7、启动服务和添加开机自启动# systemctl start prometheus# systemctl enable prometheus 三、ceph mgr prometheus插件配置（回到cephnode01设置）# ceph mgr module enable prometheus ##开启后就会吐ceph相关数据，然后prometheus去收集，grafana展示；# ceph mgr module ls | more 查看是否开启模块# netstat -nltp | grep mgr 检查端口# curl 127.0.0.1:9283/metrics 测试返回值 四、配置promethus1、在 scrape_configs: 配置项下添加vim prometheus.yml- job_name: &apos;ceph_cluster&apos; honor_labels: true scrape_interval: 5s static_configs: - targets: [&apos;192.168.171.135:9283&apos;] labels: instance: ceph 2、重启promethus服务# systemctl restart prometheus 3、检查prometheus服务器中是否添加成功# 浏览器-》 http://x.x.x.x:9090 -》status -》Targets 五、配置grafana1、浏览器登录 grafana 管理界面2、添加data sources，点击configuration–》data sources3、添加dashboard，点击HOME–》find dashboard on grafana.com4、搜索ceph的dashboard (编号:2842)5、点击HOME–》Import dashboard, 选择合适的dashboard，记录编号]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五、Ceph Dashboard（1）]]></title>
    <url>%2F2019%2F12%2F26%2F%E4%BA%94%E3%80%81Ceph%20Dashboard%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、Ceph Dashboard介绍Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。mimic版 (nautilus版) dashboard 安装。如果是 (nautilus版) 需要安装 ceph-mgr-dashboard 二、配置Ceph Dashboard配置yum源：# cat &gt; /etc/yum.repos.d/cephbak.repo &lt;&lt; EOF[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/gpgcheck=0priority=1 [ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/gpgcheck=0priority=1 [ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMSgpgcheck=0priority=1EOF 1、在每个mgr节点安装# yum install ceph-mgr-dashboard -y2、开启mgr功能# ceph mgr module enable dashboard3、生成并安装自签名的证书# ceph dashboard create-self-signed-cert 4、创建一个dashboard登录用户名密码# ceph dashboard ac-user-create admin 1q2w3e4r administrator ##创建用户admin 密码1q2w3e4r 权限是administrator5、查看服务访问方式# ceph mgr services6、查看已安装模块# ceph mgr module ls | more&#123; &quot;enabled_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;restful&quot;, &quot;status&quot; ], 三、修改默认配置命令指定集群dashboard的访问端口# ceph config-key set mgr/dashboard/server_port 7000指定集群 dashboard的访问IP# ceph config-key set mgr/dashboard/server_addr $IP 四、开启Object Gateway管理功能1、创建rgw用户# radosgw-admin user create --uid=user01 --display-name=user011.1、查看rgw用户# radosgw-admin user info --uid=user01 --display-name=user012、提供Dashboard证书# ceph dashboard set-rgw-api-access-key $access_key# ceph dashboard set-rgw-api-secret-key $secret_key3、配置rgw主机名和端口# ceph dashboard set-rgw-api-host 10.151.30.1254、刷新web页面]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三、Ceph RBD介绍与使用]]></title>
    <url>%2F2019%2F12%2F25%2F%E4%B8%89%E3%80%81Ceph%20RBD%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、RBD介绍 RBD即RADOS Block Device的简称，RBD块存储是最稳定且最常用的存储类型。RBD块设备类似磁盘可以被挂载。RBD块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。如下是对Ceph RBD的理解。 RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用； resizable：这个块可大可小； data striped：这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下； thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间 ； 块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。 ceph可以通过内核模块和librbd库提供块设备支持。客户端可以通过内核模块挂在rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。 使用场景： 云平台（OpenStack做为云的存储后端提供镜像存储） K8s容器 map成块设备直接使用 ISCIS，安装Ceph客户端 二、RBD常用命令 命令 功能 rbd create 创建块设备映像 rbd ls 列出 rbd 存储池中的块设备 rbd info 查看块设备信息 rbd diff 可以统计 rbd 使用量 rbd map 映射块设备 rbd showmapped 查看已映射块设备 rbd remove 删除块设备 rbd resize 更改块设备的大小 三、RBD配置操作3.1、RBD挂载到本地操作系统1、创建rbd使用的pool# ceph osd pool create rbd 32 32pool &apos;rbd&apos; created# ceph osd pool ls detail ##查看创建的详细信息# ceph osd pool application enable rbd rbdenabled application &apos;rbd&apos; on pool &apos;rbd&apos; 2、创建一个块设备# rbd create --size 10240 image01 3、查看块设备# rbd lsimage01# rbd info image01rbd image &apos;image01&apos;: size 10GiB in 2560 objects order 22 (4MiB objects) block_name_prefix: rbd_data.10836b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Tue Dec 31 13:40:22 2019# rados -p rbd ls --all ##查看底层存储格式 rbd_object_map.10836b8b4567 rbd_id.image01 rbd_directory rbd_info rbd_header.10836b8b4567 4、将块设备映射到系统内核# rbd map image01 5、禁用当前系统内核不支持的feature# rbd feature disable image01 exclusive-lock, object-map, fast-diff, deep-flatten 6、再次映射# rbd map image01/dev/rbd0# rbd info image01rbd image &apos;image01&apos;: size 10GiB in 2560 objects order 22 (4MiB objects) block_name_prefix: rbd_data.10836b8b4567 format: 2 features: layering flags: create_timestamp: Tue Dec 31 13:40:22 2019# fdisk -l...省略磁盘 /dev/rbd0：10.7 GB, 10737418240 字节，20971520 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：4194304 字节 / 4194304 字节 7、格式化块设备镜像# mkfs.xfs /dev/rbd0 8、mount到本地# mount /dev/rbd0 /mnt# df -h文件系统 容量 已用 可用 已用% 挂载点...省略.../dev/rbd0 10G 33M 10G 1% /mnt# umount /mnt 9、取消块设备和内核映射# rbd unmap image01 10、删除RBD块设备# rbd rm image01Removing image: 100% complete...done. 11、扩容# rbd --image image02 resize --size=15240Resizing image: 100% complete...done.# rbd info image02rbd image &apos;image02&apos;: size 14.9GiB in 3810 objects order 22 (4MiB objects) block_name_prefix: rbd_data.10c26b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Tue Dec 31 14:43:23 2019 如上就是本地如何挂载rbd块设备的步骤； 四、快照配置1、创建快照（占用存储比较大）# rbd create --size 10240 image02# rbd info image02# rbd snap create image02@image02_snap01 ## 本地快照名@快照名字 2、列出创建的快照# rbd snap list image02或# rbd ls -lNAME SIZE PARENT FMT PROT LOCKimage02 10GiB 2image02@image02_snap01 10GiB 2 3、查看快照详细信息# rbd info image02@image02_snap01 4、克隆快照（快照必须处于被保护状态&lt;没有写入&gt;才能被克隆）# rbd snap protect image02@image02_snap01# ceph osd pool create kube 16 16# rbd clone rbd/image02@image02_snap01 kube/image02_clone01 ##将刚刚克隆的image02镜像克隆到 kube 资源池；## rbd ls -p kube 5、查看快照的children# rbd children image02 6、去掉快照的parent# rbd flatten kube/image02_clone01 7、恢复快照# rbd snap rollback image02@image02_snap01 8、删除快照# rbd snap unprotect image02@image02_snap01# rbd snap remove image02@image02_snap01# rbd snap ls image02 五、导出导入RBD镜像1、导出RBD镜像# rbd export image02 /tmp/image02# ll /tmp/image02-rw-r--r-- 1 root root 10737418240 12月 31 14:41 /tmp/image02 2、导入RBD镜像# rbd remove image02 ##删除本地rbd设备Removing image: 100% complete...done.# rbd ls# rbd import /tmp/image02 rbd/image02 --image-format 2 ##导入Importing image: 100% complete...done.# rbd info image02rbd image &apos;image02&apos;: size 10GiB in 2560 objects order 22 (4MiB objects) block_name_prefix: rbd_data.10c26b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Tue Dec 31 14:43:23 2019]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二、部署Ceph集群]]></title>
    <url>%2F2019%2F12%2F24%2F%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2Ceph%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[一、Ceph版本选择Ceph版本来源介绍Ceph 社区最新版本是 14，而 Ceph 12 是市面用的最广的稳定版本。第一个 Ceph 版本是 0.1 ，要回溯到 2008 年 1 月。多年来，版本号方案一直没变，直到 2015 年 4 月 0.94.1 （ Hammer 的第一个修正版）发布后，为了避免 0.99 （以及 0.100 或 1.00 ？），制定了新策略。 x.0.z - 开发版（给早期测试者和勇士们） x.1.z - 候选版（用于测试集群、高手们） x.2.z - 稳定、修正版（给用户们） x 将从 9 算起，它代表 Infernalis （ I 是第九个字母），这样第九个发布周期的第一个开发版就是 9.0.0 ；后续的开发版依次是 9.0.1 、 9.0.2 等等。| 版本名称 | 版本号 | 发布时间 || —— | —— | —— || Argonaut | 0.48版本(LTS) | 2012年6月3日 || Bobtail | 0.56版本(LTS) | 2013年5月7日 || Cuttlefish | 0.61版本 | 2013年1月1日 || Dumpling | 0.67版本(LTS) | 2013年8月14日 || Emperor | 0.72版本 | 2013年11月9 || Firefly | 0.80版本(LTS) | 2014年5月 || Giant | Giant | October 2014 - April 2015 || Hammer | Hammer | April 2015 - November 2016|| Infernalis | Infernalis | November 2015 - June 2016 || Jewel | 10.2.9 | 2016年4月 || Kraken | 11.2.1 | 2017年10月 || Luminous |12.2.12 | 2017年10月 || mimic | 13.2.7 | 2018年5月 || nautilus | 14.2.5 | 2019年2月 | Luminous新版本特性 Bluestore ceph-osd的新后端存储BlueStore已经稳定，是新创建的OSD的默认设置。BlueStore通过直接管理物理HDD或SSD而不使用诸如XFS的中间文件系统，来管理每个OSD存储的数据，这提供了更大的性能和功能。 BlueStore支持Ceph存储的所有的完整的数据和元数据校验。 BlueStore内嵌支持使用zlib，snappy或LZ4进行压缩。（Ceph还支持zstd进行RGW压缩，但由于性能原因，不为BlueStore推荐使用zstd） 集群的总体可扩展性有所提高。我们已经成功测试了多达10,000个OSD的集群。 ceph-mgr ceph-mgr是一个新的后台进程，这是任何Ceph部署的必须部分。虽然当ceph-mgr停止时，IO可以继续，但是度量不会刷新，并且某些与度量相关的请求（例如，ceph df）可能会被阻止。我们建议您多部署ceph-mgr的几个实例来实现可靠性。 ceph-mgr守护进程daemon包括基于REST的API管理。注：API仍然是实验性质的，目前有一些限制，但未来会成为API管理的基础。 ceph-mgr还包括一个Prometheus插件。 ceph-mgr现在有一个Zabbix插件。使用zabbix_sender，它可以将集群故障事件发送到Zabbix Server主机。这样可以方便地监视Ceph群集的状态，并在发生故障时发送通知。 二、安装前准备 安装要求： 最少三台Centos7系统虚拟机用于部署Ceph集群。硬件配置：2C4G，另外每台机器最少挂载三块硬盘(每块盘5G) 主机名 IP cephnode01 192.168.171.135 cephnode02 192.168.171.136 cephnode03 192.168.171.137 cephyumresource01 192.168.171.10（内网yum源服务器） 环境准备（在Ceph三台机器上操作）（1）关闭防火墙：systemctl stop firewalldsystemctl disable firewalld（2）关闭selinux：sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/configsetenforce 0（3）关闭NetworkManagersystemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager（4）添加主机名与IP对应关系：vim /etc/hosts192.168.171.135 cephnode01192.168.171.136 cephnode02192.168.171.137 cephnode03（5）设置主机名：hostnamectl set-hostname cephnode01hostnamectl set-hostname cephnode02hostnamectl set-hostname cephnode03（6）同步网络时间和修改时区systemctl restart chronyd.service &amp;&amp; systemctl enable chronyd.servicecp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime（7）设置文件描述符echo &quot;ulimit -SHn 102400&quot; &gt;&gt; /etc/rc.localcat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 65535* hard nofile 65535EOF（8）内核参数优化cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFkernel.pid_max = 4194303echo &quot;vm.swappiness = 0&quot; /etc/sysctl.conf EOFsysctl -p（9）在cephnode01上配置免密登录到cephnode02、cephnode03ssh-copy-id root@cephnode02ssh-copy-id root@cephnode03(10)read_ahead,通过数据预读并且记载到随机访问内存方式提高磁盘读操作echo &quot;8192&quot; &gt; /sys/block/sda/queue/read_ahead_kb(11) I/O Scheduler，SSD要用noop，SATA/SAS使用deadlineecho &quot;deadline&quot; &gt;/sys/block/sd[x]/queue/schedulerecho &quot;noop&quot; &gt;/sys/block/sd[x]/queue/scheduler 三、安装内网yum源仅在192.168.171.10操作 1、安装httpd、createrepo和epel源yum install httpd createrepo epel-release -y 2、编辑yum源文件[root@cephyumresource01 ~]# more /etc/yum.repos.d/ceph.repo[Ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.asc或者阿里云yum源：（推荐！！）[Ceph-SRPMS]name=Ceph SRPMS packagesnobaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS/enabled=1gpgcheck=0type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[Ceph-noarch]name=Ceph noarch packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/enabled=1gpgcheck=0type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc [Ceph-x86_64]name=Ceph x86_64 packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/x86_64/enabled=1gpgcheck=0gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc 3、下载Ceph安装包yum --downloadonly --downloaddir=/var/www/html/ceph/rpm-nautilus/el7/x86_64/ install ceph ceph-radosgw 4、下载Ceph依赖文件wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-14.2.4-0.el7.src.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-deploy-2.0.1-0.src.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-cloud-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-local-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-rook-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-ssh-14.2.4-0.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-release-1-1.el7.src.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-medic-1.0.4-16.g60cf7e9.el7.src.rpm wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/repomd.xml wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/repomd.xml wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite.bz2 wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/183278bb826f5b8853656a306258643384a1547c497dd8b601ed6af73907bb22-other.sqlite.bz2 wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/52bf459e39c76b2ea2cff2c5340ac1d7b5e17a105270f5f01b454d5a058adbd2-filelists.sqlite.bz2 wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/4f3141aec1132a9187ff5d1b4a017685e2f83a761880884d451a288fcedb154e-primary.sqlite.bz2 wget mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/0c554884aa5600b1311cd8f616aa40d036c1dfc0922e36bcce7fd84e297c5357-other.sqlite.bz2 wget mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/597468b64cddfc386937869f88c2930c8e5fda3dd54977c052bab068d7438fcb-primary.sqlite.bz2 5、更新yum源createrepo --update /var/www/html/ceph/rpm-nautilus 四、安装Ceph集群1、编辑内网yum源,将yum源同步到其它节点并提前做好yum makecache 当然如果外网没有任何限制，也建议直接配置如上阿里云镜像源即可！# vim /etc/yum.repos.d/ceph.repo [Ceph]name=Ceph packages for $basearchbaseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/$basearchgpgcheck=0priority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/noarchgpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://192.168.171.10/ceph/rpm-nautilus/el7/srpmsgpgcheck=0priority=1 只在cephnode01上执行即可（如下标注：每个节点执行，需要在所有节点执行）2、安装ceph-deploy(确认ceph-deploy版本是否为2.0.1)# yum list | grep ceph# yum install -y ceph-deploy# ceph-deploy --version然后测试一下，发现报错：Traceback (most recent call last): File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt; from ceph_deploy.cli import main File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt; import pkg_resourcesImportError: No module named pkg_resources原因是缺python-setuptools，安装它即可：# yum install -y python-setuptools# ceph-deploy --version2.0.1 3、创建一个my-cluster目录，所有命令都需要在此目录下进行（文件位置和名字可以随意）# mkdir /my-cluster# cd /my-cluster 4、创建一个Ceph集群# ceph-deploy new cephnode01 cephnode02 cephnode03...省略[ceph_deploy.new][DEBUG ] Resolving host cephnode03[ceph_deploy.new][DEBUG ] Monitor cephnode03 at 192.168.171.137[ceph_deploy.new][DEBUG ] Monitor initial members are [&apos;cephnode01&apos;, &apos;cephnode02&apos;, &apos;cephnode03&apos;][ceph_deploy.new][DEBUG ] Monitor addrs are [&apos;192.168.171.135&apos;, &apos;192.168.171.136&apos;, &apos;192.168.171.137&apos;][ceph_deploy.new][DEBUG ] Creating a random mon key...[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... 如果安装异常可以查看：# ls ceph-deploy-ceph.log 5、安装Ceph软件（每个节点执行）# yum -y install epel-release# yum install -y ceph# ceph -vceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable) 6、生成monitor检测集群所使用的的秘钥# ceph-deploy mon create-initial# lsceph.bootstrap-mds.keyring ceph.bootstrap-osd.keyring ceph.client.admin.keyring ceph-deploy-ceph.logceph.bootstrap-mgr.keyring ceph.bootstrap-rgw.keyring ceph.conf ceph.mon.keyring 7、安装Ceph CLI，方便执行一些管理命令# ceph-deploy admin cephnode01 cephnode02 cephnode03 8、配置mgr，用于管理集群# ceph-deploy mgr create cephnode01 cephnode02 cephnode03# more ceph.conf[global]fsid = b1f800c7-a4bc-4fc7-87e2-239291f2e4c7mon_initial_members = cephnode01, cephnode02, cephnode03mon_host = 192.168.171.135,192.168.171.136,192.168.171.137auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 9、部署rgw（生产一般都是多台，然后nginx做负载均衡）# yum install -y ceph-radosgw# ceph-deploy rgw create cephnode01 10、部署MDS（CephFS）# ceph-deploy mds create cephnode01 cephnode02 cephnode03 11、添加osd# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 30G 0 disk├─sda1 8:1 0 4G 0 part└─sda2 8:2 0 26G 0 part /sdb 8:16 0 8G 0 disk ##磁盘仅仅为一块裸盘，没有做任何的初始化和处理；sr0 11:0 1 4.3G 0 romceph-deploy osd create --data /dev/sdb cephnode01 ##会自动的格式化成ceph可以读取的格式；...省略[ceph_deploy.osd][DEBUG ] Host cephnode01 is now ready for osd use.# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF-1 0.00780 root default-3 0.00780 host cephnode01 0 hdd 0.00780 osd.0 up 1.00000 1.00000##继续添加其它盘及其它node的盘ceph-deploy osd create --data /dev/sdX cephnode01 如有，修改sdX即可；ceph-deploy osd create --data /dev/sdX cephnode01ceph-deploy osd create --data /dev/sdb cephnode02ceph-deploy osd create --data /dev/sdX cephnode02ceph-deploy osd create --data /dev/sdX cephnode02ceph-deploy osd create --data /dev/sdb cephnode03ceph-deploy osd create --data /dev/sdX cephnode03ceph-deploy osd create --data /dev/sdX cephnode03[root@cephnode01 my-cluster]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF-1 0.02339 root default-3 0.00780 host cephnode01 0 hdd 0.00780 osd.0 up 1.00000 1.00000-5 0.00780 host cephnode02 1 hdd 0.00780 osd.1 up 1.00000 1.00000-7 0.00780 host cephnode03 2 hdd 0.00780 osd.2 up 1.00000 1.00000 查看集群状态：[root@cephnode01 my-cluster]# ceph -s cluster: id: b1f800c7-a4bc-4fc7-87e2-239291f2e4c7 health: HEALTH_OK services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 mgr: cephnode01(active), standbys: cephnode02, cephnode03 osd: 3 osds: 3 up, 3 in rgw: 1 daemon active data: pools: 4 pools, 32 pgs objects: 187 objects, 1.09KiB usage: 3.01GiB used, 21.0GiB / 24.0GiB avail pgs: 32 active+clean[root@cephnode01 my-cluster]# ceph health detailHEALTH_OK[root@cephnode01 my-cluster]# ceph osd df ##每块盘的使用量ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00780 1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00 32 1 hdd 0.00780 1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00 32 2 hdd 0.00780 1.00000 8.00GiB 1.00GiB 6.99GiB 12.55 1.00 32 TOTAL 24.0GiB 3.01GiB 21.0GiB 12.55MIN/MAX VAR: 1.00/1.00 STDDEV: 0 五、ceph.conf1、该配置文件采用init文件语法，###和;为注释，ceph集群在启动的时候会按照顺序加载所有的conf配置文件。 配置文件分为以下几大块配置。 global：全局配置。 osd：osd专用配置，可以使用osd.N，来表示某一个OSD专用配置，N为osd的编号，如0、2、1等。 mon：mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、ceph-monitor-1等。使用命令 ceph mon dump可以获取节点的名称。 client：客户端专用配置。 2、配置文件可以从多个地方进行顺序加载，如果冲突将使用最新加载的配置，其加载顺序为。 $CEPH_CONF环境变量 -c 指定的位置 /etc/ceph/ceph.conf ~/.ceph/ceph.conf ./ceph.conf 3、配置文件还可以使用一些元变量应用到配置文件，如。 $cluster：当前集群名。 $type：当前服务类型。 $id：进程的标识符。 $host：守护进程所在的主机名。 $name：值为$type.$id。 4、ceph.conf详细参数[global]#全局设置fsid = xxxxxxxxxxxxxxx #集群标识ID mon host = 10.0.1.1,10.0.1.2,10.0.1.3 #monitor IP 地址auth cluster required = cephx #集群认证auth service required = cephx #服务认证auth client required = cephx #客户端认证osd pool default size = 3 #最小副本数 默认是3osd pool default min size = 1 #PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数public network = 10.0.1.0/24 #公共网络(monitorIP段) cluster network = 10.0.2.0/24 #集群网络max open files = 131072 #默认0###如果设置了该选项，Ceph会设置系统的max open fdsmon initial members = node1, node2, node3 #初始monitor (由创建monitor命令而定)###########################################################################################################################[mon]mon data = /var/lib/ceph/mon/ceph-$idmon clock drift allowed = 1 #默认值0.05###monitor间的clock driftmon osd min down reporters = 13 #默认值1###向monitor报告down的最小OSD数mon osd down out interval = 600 #默认值300 #标记一个OSD状态为down和out之前ceph等待的秒数###########################################################################################################################[osd]osd data = /var/lib/ceph/osd/ceph-$idosd mkfs type = xfs #格式化系统类型osd max write size = 512 #默认值90 #OSD一次可写入的最大值(MB)osd client message size cap = 2147483648 #默认值100 #客户端允许在内存中的最大数据(bytes)osd deep scrub stride = 131072 #默认值524288 #在Deep Scrub时候允许读取的字节数(bytes)osd op threads = 16 #默认值2 #并发文件系统操作数osd disk threads = 4 #默认值1 #OSD密集型操作例如恢复和Scrubbing时的线程osd map cache size = 1024 #默认值500 #保留OSD Map的缓存(MB)osd map cache bl size = 128 #默认值50 #OSD进程在内存中的OSD Map缓存(MB)osd mount options xfs = &quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&quot; ###默认值rw,noatime,inode64 ###Ceph OSD xfs Mount选项osd recovery op priority = 2 #默认值10 #恢复操作优先级，取值1-63，值越高占用资源越高osd recovery max active = 10 #默认值15 #同一时间内活跃的恢复请求数 osd max backfills = 4 #默认值10 #一个OSD允许的最大backfills数osd min pg log entries = 30000 #默认值3000 #修建PGLog是保留的最大PGLog数osd max pg log entries = 100000 #默认值10000 #修建PGLog是保留的最大PGLog数osd mon heartbeat interval = 40 #默认值30 #OSD ping一个monitor的时间间隔（默认30s）ms dispatch throttle bytes = 1048576000 #默认值 104857600 #等待派遣的最大消息数objecter inflight ops = 819200 #默认值1024 #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限osd op log threshold = 50 #默认值5 #一次显示多少操作的logosd crush chooseleaf type = 0 #默认值为1 #CRUSH规则用到chooseleaf时的bucket的类型###########################################################################################################################[client]rbd cache = true #默认值 true #RBD缓存rbd cache size = 335544320 #默认值33554432 #RBD缓存大小(bytes)rbd cache max dirty = 134217728 #默认值25165824 #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-throughrbd cache max dirty age = 30 #默认值1 #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)rbd cache writethrough until flush = false #默认值true #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写 #设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。rbd cache max dirty object = 2 #默认值0 #最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分 #每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能rbd cache target dirty = 235544320 #默认值16777216 #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一、Ceph介绍]]></title>
    <url>%2F2019%2F12%2F23%2F%E4%B8%80%E3%80%81Ceph%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[为什么要用Ceph Ceph是当前非常流行的开源分布式存储系统，具有高扩展性、高性能、高可靠性等优点，同时提供块存储服务(rbd)、对象存储服务(rgw)以及文件系统存储服务(cephfs)，Ceph在存储的时候充分利用存储节点的计算能力，在存储每一个数据时都会通过计算得出该数据的位置，尽量的分布均衡。。目前也是OpenStack的主流后端存储，随着OpenStack在云计算领域的广泛使用，ceph也变得更加炙手可热。国内目前使用ceph搭建分布式存储系统较为成功的企业有x-sky,深圳元核云，上海UCloud等三家企业。 Ceph架构介绍 Ceph使用RADOS提供对象存储，通过librados封装库提供多种存储方式的文件和对象转换。外层通过RGW（Object，有原生的API，而且也兼容Swift和S3的API，适合单客户端使用）、RBD（Block，支持精简配置、快照、克隆，适合多客户端有目录结构）、CephFS（File，Posix接口，支持快照，社会和更新变动少的数据，没有目录结构不能直接打开）将数据写入存储。 高性能a. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高b.考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等c. 能够支持上千个存储节点的规模，支持TB到PB级的数据 高可扩展性a. 去中心化b. 扩展灵活c. 随着节点增加而线性增长 特性丰富a. 支持三种存储接口：块存储、文件存储、对象存储b. 支持自定义接口，支持多种语言驱动 Ceph核心概念ceph架构介绍：目前多数公司选择的是RGW模式； RADOS 全称Reliable Autonomic Distributed Object Store，即可靠的、自动化的、分布式对象存储系统。RADOS是Ceph集群的精华，用户实现数据分配、Failover等集群操作。《场景：坏盘的数据迁移；新盘的数据一致性》 Librados Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。 Crush Crush算法是Ceph的两大创新之一，通过Crush算法的寻址操作，Ceph得以摒弃了传统的集中式存储元数据寻址方案。而Crush算法在一致性哈希基础上很好的考虑了容灾域的隔离，使得Ceph能够实现各类负载的副本放置规则，例如跨机房、机架感知等。同时，Crush算法有相当强大的扩展性，理论上可以支持数千个存储节点，这为Ceph在大规模云环境中的应用提供了先天的便利。 Pool Pool是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略（默认一份数据需要存三份，为的就是保证数据的强一致性，一旦md5不一致就会报错！！），支持两种类型：副本（replicated）和 纠删码（ Erasure Code）； PG PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略，简单点说就是相同PG内的对象都会放到相同的硬盘上，PG是 ceph的逻辑概念，服务端数据均衡和恢复的最小粒度就是PG，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据； Object 简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。能否弄一个读写快，利 于共享的出来呢。于是就有了对象存储。最底层的存储单元，包含元数据和原始数据。 ceph资源划分： ceph各层级架构： Ceph核心组件OSD OSD是负责物理存储的进程，一般配置成和磁盘一一对应，一块磁盘启动一个OSD进程。主要功能是：==存储数据、复制数据、平衡数据、恢复数据，以及与其它OSD间进行心跳检查，负责响应客户端请求返回具体数据的进程等==； Pool、PG和OSD的关系： 一个Pool里有很多PG； 一个PG里包含一堆对象，一个对象只能属于一个PG； PG有主从之分，一个PG分布在不同的OSD上（针对三副本类型）; Monitor（生产：至少要用3个monitor，使用奇数的monitor组成一个分布式高可用的monitor集群） 一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。负责==监视整个Ceph集群运行的Map视图（如OSD Map、Monitor Map、PG Map和CRUSH Map），维护集群的健康状态，维护展示集群状态的各种图表，管理集群客户端认证与授权==； MDS MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。负责==保存文件系统的元数据，管理目录结构。对象存储和块设备存储不需要元数据服务==； Mgr ceph 官方开发了 ceph-mgr，主要目标==实现 ceph 集群的管理，为外界提供统一的入口==。例如cephmetrics、zabbix、calamari、promethus RGW RGW全称RADOS gateway，是==Ceph对外提供的对象存储服务==，接口与S3和Swift兼容。 Admin Ceph常用管理接口通常都是命令行工具，如rados、ceph、rbd等命令，另外Ceph还有可以有一个专用的管理节点，在此节点上面部署专用的管理工具来实现近乎集群的一些管理工作，如集群部署，集群组件管理等。 Ceph三种存储类型1、 块存储（RBD） 优点： 通过Raid与LVM等手段，对数据提供了保护； 多块廉价的硬盘组合起来，提高容量； 多块磁盘组合出来的逻辑盘，提升读写效率； 缺点： 采用SAN架构组网时，光纤交换机，造价成本高； 主机之间无法共享数据； 使用场景 docker容器、虚拟机磁盘存储分配； 日志存储； 文件存储； 2、文件存储（CephFS） 优点： 造价低，随便一台机器就可以了； 方便文件共享； 缺点： 读写速率低； 传输速率慢； 使用场景 日志存储； FTP、NFS； 其它有目录结构的文件存储3、对象存储（Object）(适合更新变动较少的数据) 优点： 具备块存储的读写高速； 具备文件存储的共享等特性； 使用场景 图片存储； 视频存储； 文末补充： 强一致性： 当我们保存一份数据的时候，主副本保存完毕是不可以读取数据的，必须要所有的副本全部同步完成后，且数据一致才可以读取这部分数据！]]></content>
      <categories>
        <category>K8s, ceph</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S集群网络生产级探究]]></title>
    <url>%2F2019%2F12%2F22%2FK8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E7%94%9F%E4%BA%A7%E7%BA%A7%E6%8E%A2%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[K8S集群网络网络基础知识1、公司网络架构 路由器：网络出口 核心层：主要完成数据高效转发、链路备份等 汇聚层：网络策略、安全、工作站交换机的接入、VLAN之间通信等功能 接入层：工作站的接入 1、一个局域网内主机A（10）和主机B（20）之间通讯流程：网段：192.168.31.0/24源IP和目的IP均在同一子网下。四元组：源IP 源MAC 目的IP 目的MAC- 在本机查找ARP缓存表，ARP广播包询问20的MAC地址是多少。 2、主机A（10）和主机B（20）不在同一个局域网内之间通讯流程：VLAN1:192.168.31.0/24VLAN2:192.168.32.0/24- 主机A自顶向下将数据封装成链路层帧，缓存在主机A的适配器缓存中，主机A通过查看适配器中ARP表对应的MAC地址，将链路层帧发送到与其连接的第一跳路由器上，当链路层帧到达该路由器后，路由器根据自身的路由器转发表转发到指定的出口IP再一层层转向目的VLAN。 2、交换技术有想过局域网内主机怎么通信的？主机访问外网又是怎么通信的？ 想要搞懂这些问题得从交换机、路由器讲起。 交换机工作在OSI参考模型的第二次，即数据链路层。交换机拥有一条高带宽的背部总线交换矩阵，在同一时间可进行多个端口对之间的数据传输。 交换技术分为2层和3层： 2层：主要用于小型局域网，仅支持在数据链路层转发数据，对工作站接入。 3层：三层交换技术诞生，最初是为了解决广播域的问题，多年发展，三层交换机书已经成为构建中大型网络的主要力量。 广播域 交换机在转发数据时会先进行广播，这个广播可以发送的区域就是一个广播域。交换机之间对广播帧是透明的，所以交换机之间组成的网络是一个广播域。 路由器的一个接口下的网络是一个广播域，所以路由器可以隔离广播域。 ARP（地址解析协议，在IPV6中用NDP替代） 发送这个广播帧是由ARP协议实现，ARP是通过IP地址获取物理地址的一个TCP/IP协议。 三层交换机 前面讲的二层交换机只工作在数据链路层，路由器则工作在网络层。而功能强大的三层交换机可同时工作在数据链路层和网络层，并根据 MAC地址或IP地址转发数据包。 VLAN（Virtual Local Area Network）：虚拟局域网 VLAN是一种将局域网设备从逻辑上划分成一个个网段。 一个VLAN就是一个广播域，VLAN之间的通信是通过第3层的路由器来完成的。VLAN应用非常广泛，基本上大部分网络项目都会划分vlan。 VLAN的主要好处： 分割广播域，减少广播风暴影响范围。 提高网络安全性，根据不同的部门、用途、应用划分不同网段 3、路由技术 路由器主要分为两个端口类型：LAN口和WAN口 WAN口：配置公网IP，接入到互联网，转发来自LAN口的IP数据包。 LAN口：配置内网IP（网关），连接内部交换机。 路由器是连接两个或多个网络的硬件设备，将从端口上接收的数据包，根据数据包的目的地址智能转发出去。 路由器的功能： 路由 转发 隔离子网 隔离广播域 路由器是互联网的枢纽，是连接互联网中各个局域网、广域网的设备，相比交换机来说，路由器的数据转发很复杂，它会根据目的地址给出一条最优的路径。那么路径信息的来源有两种：动态路由和静态路由。 静态路由：指人工手动指定到目标主机的地址然后记录在路由表中，如果其中某个节点不可用则需要重新指定。 动态路由：则是路由器根据动态路由协议自动计算出路径永久可用，能实时地适应网络结构的变化。 常用的动态路由协议： RIP（ Routing Information Protocol ，路由信息协议） OSPF（Open Shortest Path First，开放式最短路径优先） BGP（Border Gateway Protocol，边界网关协议） 4、OSI七层模型OSI（Open System Interconnection）是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系，一般称为OSI参考模型或七层模型。 层次 名称 功能 协议数据单元（PDU） 常见协议 7 应用层 为用户的应用程序提供网络服务，提供一个接口。 数据 HTTP、FTP、Telnet 6 表示层 数据格式转换、数据加密/解密 数据单元 ASCII 5 会话层 建立、管理和维护会话 数据单元 SSH、RPC 4 传输层 建立、管理和维护端到端的连接 段/报文 TCP、UDP 3 网络层 IP选址及路由选择 分组/包 IP、ICMP、RIP、OSPF 2 数据链路层 硬件地址寻址，差错效验等。 帧 ARP、WIFI 1 物理层 利用物理传输介质提供物理连接，传送比特流。 比特流 RJ45、RJ11 5、TCP/UDP协议TCP（Transmission Control Protocol，传输控制协议），面向连接协议，双方先建立可靠的连接，再发送数据。适用于传输数据量大，可靠性要求高的应用场景。 UDP（User Data Protocol，用户数据报协议），面向非连接协议，不与对方建立连接，直接将数据包发送给对方。适用于一次只传输少量的数据，可靠性要求低的应用场景。相对TCP传输速度快。 4.2 Kubernetes网络模型Kubernetes 要求所有的网络插件实现必须满足如下要求： 一个Pod一个IP 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射 Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个。 1、Docker容器网络模型先看下Linux网络名词： 网络的命名空间：Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；Docker利用这一特性，实现不同容器间的网络隔离。 Veth设备对：Veth设备对的引入是为了实现在不同网络命名空间的通信。 Iptables/Netfilter：Docker使用Netfilter实现容器网络转发。 网桥：网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来，并实现类似交换机那样的多对多的通信。 路由：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里。 Docker容器网络示意图如下： 2、Pod 网络问题：Pod是K8S最小调度单元，一个Pod由一个容器或多个容器组成，当多个容器时，怎么都用这一个Pod IP？ 实现：k8s会在每个Pod里先启动一个infra container小容器，然后让其他的容器连接进来这个网络命名空间，然后其他容器看到的网络试图就完全一样了。即网络设备、IP地址、Mac地址等。这就是解决网络共享的一种解法。在Pod的IP地址就是infra container的IP地址。 在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。 Pod之间通信会有两种情况： 两个Pod在同一个Node上 两个Pod在不同Node上 先看下第一种情况：两个Pod在同一个Node上 同节点Pod之间通信道理与Docker网络一样的，如下图： 对 Pod1 来说，eth0 通过虚拟以太网设备（veth0）连接到 root namespace； 网桥 cbr0 中为 veth0 配置了一个网段。一旦数据包到达网桥，网桥使用ARP 协议解析出其正确的目标网段 veth1； 网桥 cbr0 将数据包发送到 veth1； 数据包到达 veth1 时，被直接转发到 Pod2 的 network namespace 中的 eth0 网络设备。 再看下第二种情况：两个Pod在不同Node上 K8S网络模型要求Pod IP在整个网络中都可访问，这种需求是由第三方网络组件实现。 3、CNI（容器网络接口）CNI（Container Network Interface，容器网络接口)：是一个容器网络规范，Kubernetes网络采用的就是这个CNI规范，CNI实现依赖两种插件，一种CNI Plugin是负责容器连接到主机，另一种是IPAM负责配置容器网络命名空间的网络。 CNI插件默认路径： # ls /opt/cni/bin/ 地址：https://github.com/containernetworking/cni 当你在宿主机上部署Flanneld后，flanneld 启动后会在每台宿主机上生成它对应的CNI 配置文件（它其实是一个 ConfigMap），从而告诉Kubernetes，这个集群要使用 Flannel 作为容器网络方案。 CNI配置文件路径： /etc/cni/net.d/10-flannel.conflist 当 kubelet 组件需要创建 Pod 的时候，先调用dockershim它先创建一个 Infra 容器。然后调用 CNI 插件为 Infra 容器配置网络。 这两个路径在kubelet启动参数中定义： --network-plugin=cni \--cni-conf-dir=/etc/cni/net.d \--cni-bin-dir=/opt/cni/bin]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flannel网络组件实践(vxlan、host-gw)]]></title>
    <url>%2F2019%2F12%2F22%2FK8S%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9CFlannel%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(vxlan%E3%80%81host-gw)%2F</url>
    <content type="text"><![CDATA[Kubernetes网络组件之 FlannelFlannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。 1、Flannel 部署https://github.com/coreos/flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2、 Flannel工作模式及原理Flannel支持多种数据转发方式： UDP：最早支持的一种方式，由于性能最差，目前已经弃用。 VXLAN：Overlay Network方案，源数据包封装在另一种网络包里面进行路由转发和通信。==（100台左右node适用）== Host-GW：Flannel通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机都有整个容器网络的路由数据了。==（node数量超过130台,性能瓶颈）== 查看flannel 分配子网信息：[root@k8s-master1 ~]# cat /var/run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true 查看flannel配置文件：[root@k8s-master1 ~]# more /etc/cni/net.d/10-flannel.conflist VXLAN# kubeadm部署指定Pod网段kubeadm init --pod-network-cidr=10.244.0.0/16# 二进制部署指定（启用cni）cat /opt/kubernetes/cfg/kube-controller-manager.conf--allocate-node-cidrs=true \--cluster-cidr=10.244.0.0/16 \ # kube-flannel.ymlnet-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125; 为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。下图flannel.1的设备就是VXLAN所需的VTEP设备。示意图如下： 如果Pod 1访问Pod 2，源地址10.244.1.10，目的地址10.244.2.10 ，数据包传输流程如下： 容器路由：容器根据路由表从eth0发出 / # ip routedefault via 10.244.0.1 dev eth0 10.244.0.0/24 dev eth0 scope link src 10.244.0.45 10.244.0.0/16 via 10.244.0.1 dev eth0 主机路由：数据包进入到宿主机虚拟网卡cni0，根据路由表转发到flannel.1虚拟网卡，也就是，来到了隧道的入口。 # ip routedefault via 192.168.31.1 dev ens33 proto static metric 100 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink 10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink VXLAN封装：而这些VTEP设备（二层）之间组成二层网络必须要知道目的MAC地址。这个MAC地址从哪获取到呢？其实在flanneld进程启动后，就会自动添加其他节点ARP记录，可以通过ip命令查看，如下所示： # ip neigh show dev flannel.110.244.1.0 lladdr ca:2a:a4:59:b6:55 PERMANENT10.244.2.0 lladdr d2:d0:1b:a7:a9:cd PERMANENT 二次封包：知道了目的MAC地址，封装二层数据帧（容器源IP和目的IP）后，对于宿主机网络来说这个帧并没有什么实际意义。接下来，Linux内核还要把这个数据帧进一步封装成为宿主机网络的一个普通数据帧，好让它载着内部数据帧，通过宿主机的eth0网卡进行传输。 封装到UDP包发出去：现在能直接发UDP包嘛？到目前为止，我们只知道另一端的flannel.1设备的MAC地址，却不知道对应的宿主机地址是什么。 flanneld进程也维护着一个叫做FDB的转发数据库，可以通过bridge fdb命令查看： # bridge fdb show dev flannel.1 d2:d0:1b:a7:a9:cd dst 192.168.31.61 self permanent ca:2a:a4:59:b6:55 dst 192.168.31.63 self permanent 可以看到，上面用的对方flannel.1的MAC地址对应宿主机IP，也就是UDP要发往的目的地。使用这个目的IP进行封装。 数据包到达目的宿主机：Node1的eth0网卡发出去，发现是VXLAN数据包，把它交给flannel.1设备。flannel.1设备则会进一步拆包，取出原始二层数据帧包，发送ARP请求，经由cni0网桥转发给container。 Host-GWhost-gw模式相比vxlan简单了许多， 直接添加路由，将目的主机当做网关，直接路由原始封包。 下面是示意图： ==线上更改网路模式一定要放在夜深人静去变更！！==# more /tmp/k8s/kube-flannel.yamlnet-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;host-gw&quot; &#125; &#125;修改如上为host-gw然后应用：[root@k8s-master1 ~]# kubectl apply -f kube-flannel.ymlpodsecuritypolicy.policy/psp.flannel.unprivileged configuredclusterrole.rbac.authorization.k8s.io/flannel unchangedclusterrolebinding.rbac.authorization.k8s.io/flannel unchangedserviceaccount/flannel unchangedconfigmap/kube-flannel-cfg configureddaemonset.apps/kube-flannel-ds-amd64 configureddaemonset.apps/kube-flannel-ds-arm64 createddaemonset.apps/kube-flannel-ds-arm createddaemonset.apps/kube-flannel-ds-ppc64le createddaemonset.apps/kube-flannel-ds-s390x created###如上如果未生效，评估下风险，可删除后再次重建[root@k8s-master1 ~]# kubectl delete -f kube-flannel.yml[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yml 当你设置flannel使用host-gw模式,flanneld会在宿主机上创建节点的路由表： # ip route ##这样我们就看到了如上host-gw的特色，将目的主机当做网关。default via 192.168.31.1 dev ens33 proto static metric 100 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 10.244.1.0/24 via 192.168.31.63 dev ens33 ##集群另外两台node的路由10.244.2.0/24 via 192.168.31.61 dev ens33 ##集群另外两台node的路由192.168.31.0/24 dev ens33 proto kernel scope link src 192.168.31.62 metric 100 目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址是 192.168.31.63（即：via 192.168.31.63）。 一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。 而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.20，即 container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 container-2 当中。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Calico网络组件实践(BGP、RR、IPIP)]]></title>
    <url>%2F2019%2F12%2F22%2FK8S%E9%9B%86%E7%BE%A4%E7%BD%91%E8%B7%AFCalico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(BGP%E3%80%81RR%E3%80%81IPIP)%2F</url>
    <content type="text"><![CDATA[Kubernetes网络方案之 CalicoCalico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。 Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。 此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。 1、BGP概述实际上，Calico项目提供的网络解决方案，与Flannel的host-gw模式几乎一样。也就是说，Calico也是基于路由表实现容器数据包转发，但不同于Flannel使用flanneld进程来维护路由信息的做法，而Calico项目使用BGP协议来自动维护整个集群的路由信息。 BGP英文全称是Border Gateway Protocol，即边界网关协议，它是一种自治系统间的动态路由发现协议，与其他 BGP 系统交换网络可达信息。 为了能让你更清楚理解BGP，举个例子： 在这个图中，有两个自治系统（autonomous system，简称为AS）：AS 1 和 AS 2。 在互联网中，一个自治系统(AS)是一个有权自主地决定在本系统中应采用何种路由协议的小型单位。这个网络单位可以是一个简单的网络也可以是一个由一个或多个普通的网络管理员来控制的网络群体，它是一个单独的可管理的网络单元（例如一所大学，一个企业或者一个公司个体）。一个自治系统有时也被称为是一个路由选择域（routing domain）。一个自治系统将会分配一个全局的唯一的16位号码，有时我们把这个号码叫做自治系统号（ASN）。 在正常情况下，自治系统之间不会有任何来往。如果两个自治系统里的主机，要通过 IP 地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。BGP协议就是让他们互联的一种方式。 2、Calico BGP实现 在了解了 BGP 之后，Calico 项目的架构就非常容易理解了，Calico主要由三个部分组成： Felix：以DaemonSet方式部署，运行在每一个Node节点上，主要负责维护宿主机上路由规则以及ACL规则。 BGP Client（BIRD）：主要负责把 Felix 写入 Kernel 的路由信息分发到集群 Calico 网络。 Etcd：分布式键值存储，保存Calico的策略和网络配置状态。 calicoctl：允许您从简单的命令行界面实现高级策略和网络。 3、Calico 部署curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico.yaml 下载完后还需要修改里面配置项： 具体步骤如下： 配置连接etcd地址，如果使用https，还需要配置证书。（ConfigMap，Secret） 根据实际网络规划修改Pod CIDR（CALICO_IPV4POOL_CIDR） 选择工作模式（CALICO_IPV4POOL_IPIP），支持BGP，IPIP 查看证书所在位置：[root@k8s-master1 ~]# ls /opt/etcd/ssl/ca.pem server-key.pem server.pem拼接ca密钥, ca的key以及数字证书内容为1条字符串：[root@k8s-master1 ~]# cat /opt/etcd/ssl/ca.pem |base64 -w 0[root@k8s-master1 ~]# cat /opt/etcd/ssl/server-key.pem |base64 -w 0[root@k8s-master1 ~]# cat /opt/etcd/ssl/server.pem |base64 -w 0[root@k8s-master1 ~]# vim calico.yaml###etcd证书：···###第一处：etcd-key: 对应如上server-key.pem生成的字符串etcd-cert: 对应如上server.pem生成的字符串etcd-ca: 对应如上ca.pem生成的字符串###第二处：（直接删除注释即可） etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot;###第三处：可查看此文件：[root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-apiserver.conf中etcd的配置：etcd_endpoints: &quot;https://192.168.171.11:2379,https://192.168.171.12:2379,https://192.168.171.13:2379&quot;···###修改Pod CIDR（查看：cat /opt/kubernetes/cfg/kube-controller-manager.conf 的 --cluster-cidr=10.244.0.0/16 \）···- name: CALICO_IPV4POOL_CIDR value: &quot;10.244.0.0/16&quot;···###选择工作模式calico默认工作模式是IPIP，我们需要注释掉，然后默认就会改为BGP，Always改为Never即可# Enable IPIP- name: CALICO_IPV4POOL_IPIP value: &quot;Nerver&quot; ★★★修改完成如上配置文件后，因为flannel升级calico，flannel默认的一些路由策略还会依旧保存，我们需要先删除，不然路由策略依然还会走之前的，导致问题！！★★★[root@k8s-master1 ~]# ip link delete cni0[root@k8s-master1 ~]# ip link delete flannel.1查看路由表再次确认（每个node都需要检查一下）：[root@k8s-node2 ~]# ip routedefault via 192.168.171.2 dev ens33 proto static metric 100172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1192.168.171.0/24 dev ens33 proto kernel scope link src 192.168.171.13 metric 100或者手动删除一些静态路由：[root@k8s-master1 ~]# ip route del 10.244.1.0/24 via 192.168.171.12 dev ens33[root@k8s-master1 ~]# ip route del 10.244.2.0/24 via 192.168.171.13 dev ens33[root@k8s-master1 ~]# ip route del 10.244.3.0/24 via 192.168.171.14 dev ens33 修改完后应用清单：[root@k8s-master1 ~]# kubectl apply -f calico.yaml[root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-f68c55884-tx2km 1/1 Running 0 4m26scalico-node-5kqcl 1/1 Running 0 4m27scalico-node-ck4gf 1/1 Running 0 4m27scalico-node-jp9kj 1/1 Running 0 4m26scalico-node-mnslt 1/1 Running 0 4m27s ★★★ 最后一点：当我们升级完毕，查看路由表时候，神奇的发现竟然没有任何路由！！ 最终查找资料确认了其中的问题：现有容器需要重建！！ ★★★ 再次查看路由：[root@k8s-master1 ~]# ip routedefault via 192.168.171.2 dev ens33 proto static metric 10010.244.36.64/26 via 192.168.171.12 dev ens33 proto bird10.244.107.192/26 via 192.168.171.14 dev ens33 proto birdblackhole 10.244.159.128/26 proto bird10.244.159.132 dev calicc2e4908bc6 scope link10.244.159.133 dev calid85356b2213 scope link10.244.169.128/26 via 192.168.171.13 dev ens33 proto bird172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1192.168.171.0/24 dev ens33 proto kernel scope link src 192.168.171.11 metric 100 4、Calico 管理工具下载工具：https://github.com/projectcalico/calicoctl/releases # wget -O /usr/local/bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v3.9.1/calicoctl# chmod +x /usr/local/bin/calicoctl # mkdir /etc/calico# vim /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: datastoreType: &quot;etcdv3&quot; etcdEndpoints: &quot;https://192.168.31.61:2379,https://192.168.31.62:2379,https://192.168.31.63:2379&quot; etcdKeyFile: &quot;/opt/etcd/ssl/server-key.pem&quot; etcdCertFile: &quot;/opt/etcd/ssl/server.pem&quot; etcdCACertFile: &quot;/opt/etcd/ssl/ca.pem&quot; 使用calicoctl查看服务状态： [root@k8s-master1 ~]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+-------------------+-------+----------+-------------+| 192.168.171.12 | node-to-node mesh | up | 14:33:24 | Established || 192.168.171.13 | node-to-node mesh | up | 14:33:29 | Established || 192.168.171.14 | node-to-node mesh | up | 14:33:26 | Established |+----------------+-------------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found. [root@k8s-master1 ~]# calicoctl get nodesNAMEk8s-master1k8s-node1k8s-node2k8s-node3 查看 IPAM的IP地址池： [root@k8s-master1 ~]# calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTORdefault-ipv4-ippool 10.244.0.0/16 true Never Never false all() 5、Calico BGP 原理剖析 Pod 1 访问 Pod 2大致流程如下： 数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）； 宿主机根据路由规则，将数据包转发给下一跳（网关）； 到达Node2，根据路由规则将数据包转发给cali设备，从而到达容器2。 路由表： # node110.244.36.65 dev cali4f18ce2c9a1 scope link 10.244.169.128/26 via 192.168.31.63 dev ens33 proto bird 10.244.235.192/26 via 192.168.31.61 dev ens33 proto bird # node210.244.169.129 dev calia4d5b2258bb scope link 10.244.36.64/26 via 192.168.31.62 dev ens33 proto bird10.244.235.192/26 via 192.168.31.61 dev ens33 proto bird 其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。 不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。 6、Route Reflector 模式（RR）https://docs.projectcalico.org/master/networking/bgp Calico 维护的网络在默认是（Node-to-Node Mesh）全互联模式，Calico集群中的节点之间都会相互建立连接，用于路由交换。但是随着集群规模的扩大，mesh模式将形成一个巨大服务网格，连接数成倍增加。 例如：node1 node2 node3node1 ==&gt; node2 ==&gt; node3node2 ==&gt; node1 ==&gt; node3node3 ==&gt; node1 ==&gt; node2noden ...如上路由模式是node-node,这种模式下整个集群的node需要控制在100台左右！！也就是说node1上需要建立2条路由 node2上也需要建立node1和node3这2条路由，如集群很大，一旦增加1台node，就需要成倍的路由需要建立！！ 这时就需要使用 Route Reflector（路由器反射）模式解决这个问题。 这就类似一个nginx 后端的node作为负载，一旦有新的node只需要和路有反射的机器建立路由关系即可！！ 确定一个或多个Calico节点充当路由反射器，让其他节点从这个RR节点获取路由信息。 具体步骤如下： 1、关闭 node-to-node BGP网格 ★★★一旦更改，整个集群网路就会断掉！！！线上一定要先评估！！★★★ 添加 default BGP配置，调整 nodeToNodeMeshEnabled和asNumber： cat &lt;&lt; EOF | calicoctl create -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata: name: defaultspec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 63400EOF ASN号可以通过获取： # calicoctl get nodes --output=wide 2、配置指定节点充当路由反射器 为方便让BGPPeer轻松选择节点，通过标签选择器匹配。 给路由器反射器节点打标签： [root@k8s-master1 ~]# kubectl label node k8s-node2 route-reflector=truenode/k8s-node2 labeled 然后编辑刚打了tag的node节点：[root@k8s-master1 ~]# calicoctl get node k8s-node2 -o yaml &gt; node2.yaml 然后配置路由器反射器节点routeReflectorClusterID： [root@k8s-master1 ~]# vim node2.yamlapiVersion: projectcalico.org/v3kind: Nodemetadata: annotations: projectcalico.org/kube-labels: &apos;&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;kubernetes.io/hostname&quot;:&quot;k8s-node2&quot;,&quot;kubernetes.io/os&quot;:&quot;linux&quot;,&quot;route-reflector&quot;:&quot;true&quot;&#125;&apos; creationTimestamp: 2019-12-25T14:02:11Z labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: k8s-node2 kubernetes.io/os: linux route-reflector: &quot;true&quot; name: k8s-node2 resourceVersion: &quot;349138&quot; uid: 9387fc14-f6f4-4809-a8f2-418ed81ba6caspec: bgp: ipv4Address: 192.168.171.13/24 routeReflectorClusterID: 244.0.0.1 # 集群ID，保证唯一性 orchRefs: - nodeName: k8s-node2 orchestrator: k8s 应用：[root@k8s-master1 ~]# calicoctl apply -f node2.yamlSuccessfully applied 1 &apos;Node&apos; resource(s) 现在，很容易使用标签选择器将路由反射器节点与其他非路由反射器节点配置为对等： [root@k8s-master1 ~]# vim setbgprr.yamlapiVersion: projectcalico.org/v3kind: BGPPeermetadata: name: peer-with-route-reflectorsspec: nodeSelector: all() peerSelector: route-reflector == &apos;true&apos; ##创建规则：[root@k8s-master1 ~]# calicoctl apply -f setbgprr.yamlSuccessfully applied 1 &apos;BGPPeer&apos; resource(s) 查看节点的BGP连接状态：（只与171.13建立了连接，再次ping网路就通了） [root@k8s-master1 ~]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+---------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+---------------+-------+----------+-------------+| 192.168.171.13 | node specific | up | 15:19:42 | Established |+----------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found. 查看规则：[root@k8s-master1 ~]# calicoctl get bgppeerNAME PEERIP NODE ASNpeer-with-route-reflectors all() 0 ==当然rr和nginx也是一致的，也需要高可用，所以我们尽量配置2台以上的节点！== 条件允许的话，最好找2台单独的机器！ [root@k8s-master1 ~]# kubectl label node k8s-node1 route-reflector=truenode/k8s-node1 labeled[root@k8s-master1 ~]# calicoctl get node k8s-node1 -o yaml &gt; node1.yaml[root@k8s-master1 ~]# vim node1.yaml（增加和node2一致即可）···routeReflectorClusterID: 244.0.0.1 # 集群ID，保证唯一性···[root@k8s-master1 ~]# calicoctl apply -f node1.yamlSuccessfully applied 1 &apos;Node&apos; resource(s) 再次查看：[root@k8s-master1 ~]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+---------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+---------------+-------+----------+-------------+| 192.168.171.13 | node specific | up | 15:19:42 | Established || 192.168.171.12 | node specific | up | 15:26:13 | Established |+----------------+---------------+-------+----------+-------------+ 7、IPIP模式和flannel的vxlan差不多模式，也是基于二层数据的封装和解封；也会创建一个虚拟网卡 tunl0 在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。 修改为IPIP模式： # calicoctl get ipPool -o yaml &gt; ipip.yaml# vi ipip.yamlapiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: default-ipv4-ippoolspec: blockSize: 26 cidr: 10.244.0.0/16 ipipMode: Always natOutgoing: true# calicoctl apply -f ipip.yaml# calicoctl get ippool -o wide IPIP示意图： Pod 1 访问 Pod 2大致流程如下： 数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）； 进入IP隧道设备（tunl0），由Linux内核IPIP驱动封装在宿主机网络的IP包中（新的IP包目的地之是原IP包的下一跳地址，即192.168.31.63），这样，就成了Node1 到Node2的数据包； 数据包经过路由器三层转发到Node2； Node2收到数据包后，网络协议栈会使用IPIP驱动进行解包，从中拿到原始IP包； 然后根据路由规则，根据路由规则将数据包转发给cali设备，从而到达容器2。 路由表： # node110.244.36.65 dev cali4f18ce2c9a1 scope link 10.244.169.128/26 via 192.168.31.63 dev tunl0 proto bird onlink # node210.244.169.129 dev calia4d5b2258bb scope link 10.244.36.64/26 via 192.168.31.62 dev tunl0 proto bird onlink ==不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。所以建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。== 8、CNI 网络方案优缺点及最终选择先考虑几个问题： 需要细粒度网络访问控制？ ==》 flannel不支持，calico支持（ACL）； 追求网络性能？ ==》 flannel（host-gw），calico（BGP）； 服务器之前是否可以跑BGP协议？ ==》 公有云有些不支持； 集群规模多大？ ==》 100台node左右推荐（flannel&lt;host-gw性能好些&gt;）维护方便； 是否有维护能力？ ==》 calico维护复杂，路由表！ 小话题：办公网络与K8S网络如何互通4.5 网络策略1、为什么需要网络隔离？CNI插件插件解决了不同Node节点Pod互通问题，从而形成一个扁平化网络，默认情况下，Kubernetes 网络允许所有 Pod 到 Pod 的流量，在一些场景中，我们不希望Pod之间默认相互访问，例如： 应用程序间的访问控制。例如微服务A允许访问微服务B，微服务C不能访问微服务A 开发环境命名空间不能访问测试环境命名空间Pod 当Pod暴露到外部时，需要做Pod白名单 多租户网络环境隔离 所以，我们需要使用network policy对Pod网络进行隔离。支持对Pod级别和Namespace级别网络访问控制。 路由器层面解决：1、K8S集群测试环境在办公网路子网：# ip route add 10.244.0.0/16 via &lt;K8S-NODE1&gt; dev A2、K8S集群与办公网路不在同VLAN, 不同机房：前提：三层可达1）路由器添加路由表：10.244.0.0/16 &lt;K8S-NODE1&gt;2) 路由器BGP与路由反射器建立连接； Pod网络入口方向隔离 基于Pod级网络隔离：只允许特定对象访问Pod（使用标签定义），允许白名单上的IP地址或者IP段访问Pod 基于Namespace级网络隔离：多个命名空间，A和B命名空间Pod完全隔离。 Pod网络出口方向隔离 拒绝某个Namespace上所有Pod访问外部 基于目的IP的网络隔离：只允许Pod访问白名单上的IP地址或者IP段 基于目标端口的网络隔离：只允许Pod访问白名单上的端口 2、网络策略概述一个NetworkPolicy例子： apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db ##db的这个应用的容器 policyTypes: - Ingress - Egress ingress: ##访问pod的流量（入方向） - from: - ipBlock: cidr: 172.17.0.0/16 ##此ip段可以访问 except: ##除了如下这个IP段不可以访问 - 172.17.1.0/24 - namespaceSelector: ##namespace matchLabels: project: myproject - podSelector: ##pod matchLabels: role: frontend ports: ##ports端口 - protocol: TCP port: 6379 egress: ##出去的流量（出方向） - to: - ipBlock: ##只能访问如下此网段 cidr: 10.0.0.0/24 ports: - protocol: TCP ##且端口为5978 port: 5978 配置解析： podSelector：用于选择策略应用到的Pod组。 policyTypes：其可以包括任一Ingress，Egress或两者。该policyTypes字段指示给定的策略用于Pod的入站流量、还是出站流量，或者两者都应用。如果未指定任何值，则默认值为Ingress，如果网络策略有出口规则，则设置egress。 Ingress：from是可以访问的白名单，可以来自于IP段、命名空间、Pod标签等，ports是可以访问的端口。 Egress：这个Pod组可以访问外部的IP段和端口。 3、入站、出站网络流量访问控制案例Pod访问限制 准备测试环境，一个web pod，两个client pod # kubectl create deployment nginxweb --image=nginx# kubectl scale deployment nginxweb --replicas=3# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginxweb-c5d5747f8-4wff9 1/1 Running 1 24h app=nginxweb,pod-template-hash=c5d5747f8nginxweb-c5d5747f8-6zbk6 1/1 Running 0 3m2s app=nginxweb,pod-template-hash=c5d5747f8nginxweb-c5d5747f8-xxfc9 1/1 Running 1 24h app=nginxweb,pod-template-hash=c5d5747f8 需求：将default命名空间携带run=nginxweb标签的Pod隔离，只允许default命名空间携带run=client1标签的Pod访问80端口。 apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: app: nginxweb policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: project: default - podSelector: matchLabels: run: client1 ports: - protocol: TCP port: 80 ##创建如上规则# kubectl apply -f networkpolicy.yamlnetworkpolicy.networking.k8s.io/test-network-policy created# kubectl run client1 --generator=run-pod/v1 --image=busybox --command -- sleep 36000 ##快速的启动一个测试容器# kubectl exec -it client1 sh/ # ping 10.244.169.143 ##从容器中测试访问另外的podPING 10.244.169.143 (10.244.169.143): 56 data bytes64 bytes from 10.244.169.143: seq=0 ttl=62 time=0.639 ms完全可以访问；# kubectl run client2 --generator=run-pod/v1 --image=busybox --command -- sleep 36000 ##再次启动一个client2pod，按照规则client2是不允许访问80端口的[root@k8s-master1 ~]# kubectl exec -it client2 sh/ # wget 10.244.36.73Connecting to 10.244.36.73 (10.244.36.73:80)^C ##看来是不可以访问的##再来测试下client1[root@k8s-master1 ~]# kubectl exec -it client1 sh/ # wget 10.244.36.73Connecting to 10.244.36.73 (10.244.36.73:80)saving to &apos;index.html&apos;index.html 100% |***********************************************************************************************************************| 612 0:00:00 ETA&apos;index.html&apos; saved / # ping 10.244.36.73PING 10.244.36.73 (10.244.36.73): 56 data bytes^C--- 10.244.36.73 ping statistics ---3 packets transmitted, 0 packets received, 100% packet loss如上可以看得出：##可以访问！！但是不可以ping；看来如上策略就生效了，达到了预期的效果，只可以访问80, 其余均不可以！ 隔离策略配置： Pod对象：default命名空间携带run=web标签的Pod 允许访问端口：80 允许访问对象：default命名空间携带run=client1标签的Pod 拒绝访问对象：除允许访问对象外的所有对象 命名空间隔离 需求：default命名空间下所有pod可以互相访问，但不能访问其他命名空间Pod，其他命名空间也不能访问default命名空间Pod。 apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: deny-from-other-namespaces namespace: defaultspec: podSelector: &#123;&#125; policyTypes: - Ingress ingress: - from: - podSelector: &#123;&#125; podSelector: {}：default命名空间下所有Pod from.podSelector: {} : 如果未配置具体的规则，默认不允许 解除如上的规则：[root@k8s-master1 ~]# kubectl delete -f networkpolicy.yamlnetworkpolicy.networking.k8s.io &quot;test-network-policy&quot; deleted[root@k8s-master1 ~]# kubectl exec -it client2 sh/ # ping 10.244.36.73 ##现在已经可以ping了PING 10.244.36.73 (10.244.36.73): 56 data bytes64 bytes from 10.244.36.73: seq=0 ttl=63 time=0.313 ms64 bytes from 10.244.36.73: seq=1 ttl=63 time=0.083 ms如上我们的新需求是：default命名空间下所有pod可以互相访问，但不能访问其他命名空间Pod，其他命名空间也不能访问default命名空间Pod。# kubectl run client3 --generator=run-pod/v1 -n kube-system --image=busybox --command -- sleep 36000pod/client3 created ##在kube-system环境中创建一个pod[root@k8s-master1 ~]# kubectl exec -it client3 sh -n kube-system/ # ping 10.244.36.73 ##也是可以访问default空间的podPING 10.244.36.73 (10.244.36.73): 56 data bytes64 bytes from 10.244.36.73: seq=0 ttl=62 time=1.394 ms64 bytes from 10.244.36.73: seq=1 ttl=62 time=0.425 ms^C--- 10.244.36.73 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.425/0.909/1.394 ms##应用如上规则：[root@k8s-master1 ~]# kubectl apply -f ns22.yamlnetworkpolicy.networking.k8s.io/deny-from-other-namespaces created[root@k8s-master1 ~]# kubectl exec -it client3 sh -n kube-system/ # ping 10.244.36.73 ##再次访问就不可以了！！PING 10.244.36.73 (10.244.36.73): 56 data bytes^C--- 10.244.36.73 ping statistics ---3 packets transmitted, 0 packets received, 100% packet loss 文末彩蛋：来讲下我们目前公司的网路架构吧： calico（BGP）一个集群支持上千个节点问题不大； 将所有机房的网路打通； 3台路由反射器，对接每个节点的网关路由器进行BGP模式下的路由交换，这样就会将整个的pod_ip全部暴露在网路中； LVS对pod做负载均衡；]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s Ingress Nginx使用]]></title>
    <url>%2F2019%2F12%2F18%2FK8s%20Ingress%20Nginx%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[案例1（基本转发，https配置与annotations基础使用）apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress namespace: test annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;spec: tls: - hosts: - nginx-a.gogen.cn secretName: gogen.cn rules: - host: nginx-a.gogen.cn http: paths: - path: / backend: serviceName: nginx-a servicePort: 80 - path: /.*.(txt|css|doc) backend: serviceName: nginx-b servicePort: 80 - path: /(api|app)/ backend: serviceName: nginx-c servicePort: 80 - path: /api backend: serviceName: nginx-d servicePort: 80 上面我们定义了一个ingress，并指定运行在test名称空间（此名名称空间需要自行创建）。后端我们定义了四组服务，分别为：nginx-a，nginx-b，nginx-c和nginx-d，并指定服务的port为80（这四组服务也需要自行定义）。 然后我们ingress的主要配置里面我们定义了tls证书，并指定可使用的host和需要使用的secret。我们是将证书先导入进secret，然后直接引用secret，导入方法如下：kubectl create secret tls gogen.cn --cert=1592339__gogen.cn.pem --key=1592339__gogen.cn.key -n test tls详解：tls:- hosts: #此为固定项，是一个列表，我们可以有另外的证书对应其它域名 - nginx-a.gogen.cn #此为列表，必须为一个域名，一个secret可以对多个域名 secretName: gogen.cn #创建secret时指定的名称 annotations配置：作用于server# 指定了我们使用后端ingress controller的类别，如果后端有多个ingress controller的时候很重要kubernetes.io/ingress.class: &quot;nginx&quot; # 指定我们的rules的path可以使用正则表达式，如果我们没有使用正则表达式，此项则可不使用nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; rules配置：作用于locationrules:- host: nginx-a.gogen.cn #相当于定义了nginx的一个server_name http: paths: - path: / #一个path就相当于一个location，path的值必须为“/”。这里为匹配的规则，根表示默认请求转发规则 backend: serviceName: nginx-a #定义后端的service servicePort: 80 #定义后端service的访问端口，也就是service port指定的端口 - path: /.*.(txt|css|doc) #这里使用的正则（低版本不支持）,默认情况下都是不区分大小写，可以进入到ingress controller查看nginx的配置,这里相当于把结尾为txt,css,doc的url请求转发到nginx-b service backend: serviceName: nginx-b servicePort: 80 - path: /(api|app)/ #这里相当于将api和app开头的目录语法转发至nginx-c service backend: serviceName: nginx-c servicePort: 80 - path: /api #这里相当于将api开头的url（可以是一个文件，也可以是一个目录）的请求，转发到nginx-d backend: serviceName: nginx-d servicePort: 80 说明：上面定义的所有path到ingress controller都将会转换成nginx location规则，那么关于location的优先级与nginx一样。path转换到nginx后，会将path规则最长的排在最前面，最短的排在最后面。 案例2（通过annotations对nginx做个性化配置）apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress namespace: test annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; nginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;600&quot; nginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot; nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot; nginx.ingress.kubernetes.io/proxy-body-size: &quot;10m&quot;spec: tls: - hosts: - nginx-a.gogen.cn secretName: gogen.cn rules: - host: nginx-a.gogen.cn http: paths: - path: / backend: serviceName: nginx-a servicePort: 80 - path: /.*.(txt|css|doc) backend: serviceName: nginx-b servicePort: 80 - path: /(api|app)/ backend: serviceName: nginx-c servicePort: 80 - path: /api backend: serviceName: nginx-d servicePort: 80 在案例1的基础上面我们增加了annotations的一些配置kubernetes.io/ingress.class: &quot;nginx&quot;nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; #连接超时时间，默认为5snginx.ingress.kubernetes.io/proxy-connect-timeout: &quot;600&quot; #后端服务器回转数据超时时间，默认为60snginx.ingress.kubernetes.io/proxy-send-timeout: &quot;600&quot; #后端服务器响应超时时间，默认为60snginx.ingress.kubernetes.io/proxy-read-timeout: &quot;600&quot; #客户端上传文件，最大大小，默认为20mnginx.ingress.kubernetes.io/proxy-body-size: &quot;10m&quot; 上面我们自定义了四项基本配置，我们还可以定义更多的基本配置，可参考nginx-configuration annotations相关文档 案例3（通过annotations做rewrite基本配置）apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-rewrite-tfs namespace: test annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: https://gogen-test.oss-cn-hangzhou.aliyuncs.comspec: tls: - hosts: - nginx-a.gogen.cn secretName: gogen.cn rules: - host: nginx-a.gogen.cn http: paths: - path: /v1/tfs backend: serviceName: nginx-a servicePort: 80 上面的方法也是官方的方法： 使用“nginx.ingress.kubernetes.io/rewrite-target”来定义。 通过上面的的定义代表如果访问https://nginx-a.gogen.cn/v1/tfs/(.*)，则会rewrite为https://gogen-test.oss-cn-hangzhou.aliyuncs.com/$1，如果有多个path，每个都会被rewrite，所以如果只需要替换单个path（也就是location）的时候单独使用个manifests写。 案例4（介绍 nginx.ingress.kubernetes.io/server-snippet 使用方法）apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress namespace: test annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; nginx.ingress.kubernetes.io/server-snippet: | if ($uri ~* &quot;/v1/tfs/.*&quot;) &#123; rewrite ^/v1/tfs/(.*)$ https://gogen-test.oss-cn-hangzhou.aliyuncs.com/$1 permanent; &#125;spec: tls: - hosts: - nginx-a.gogen.cn secretName: gogen.cn rules: - host: nginx-a.gogen.cn http: paths: - path: / backend: serviceName: nginx-a servicePort: 80 - path: /.*.(txt|css|doc) backend: serviceName: nginx-b servicePort: 80 - path: /(api|app)/ backend: serviceName: nginx-c servicePort: 80 - path: /api backend: serviceName: nginx-d servicePort: 80 这里直接使用了“nginx.ingress.kubernetes.io/server-snippet”来指定配置，这里可以直接写nginx的配置，通过这里可以不止是实现rewrite重写，还可以实现更多的功能需求，只要是作用于server的都可以。 案例5（使用configMap做更多个性化配置）有些时候我们使用annotations并不能全完实现我们nginx灵活个性化配置，这时候就需要使用借助configMap配置。官方configMap使用文档，annotations与configMap对照关系表。 首先创建一个configMap文件，如下所示：apiVersion: v1kind: ConfigMapmetadata: name: nginx-configuration namespace: kube-systemdata: use-http2: &quot;false&quot; ssl-protocols: &quot;TLSv1 TLSv1.1 TLSv1.2&quot; ssl-ciphers: &quot;HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM&quot; data里面内容参考上面给出的【官方configMap使用文档】，metadata内的 name和namespace不可随意写，需要参考nginx-ingress-controller YAML配置文件的配置containers:- args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --annotations-prefix=nginx.ingress.kubernetes.io - --publish-service=$(POD_NAMESPACE)/nginx-ingress-lb - --v=2 参考”- –configmap=$(POD_NAMESPACE)/nginx-configuration)“，其中configmap的名称空间需要与nginx-ingress-controller在同一个名称空间，名称为”/“后面的名称 完成配置后apply配置清单，通过configMap配置的配置apply无法直接生效，需要重启pods，最简单的方法使用edit编辑nginx-ingress-controller的controller，改个不影响pods运行的参数，来触发pods升级，从而让我们的配置生效，如：livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 可以将”initialDelaySeconds“改为11或者其它的，该值的含义为pod启动多少时候后开始执行健康检查，单位为秒。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s二进制安装环境下证书过期问题]]></title>
    <url>%2F2019%2F12%2F17%2Fk8s%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[k8s配置信息的工作目录一般为/etc/kubernetes，证书目录一般为/etc/kubernetes/ssl 一、重新生成证书当你的kubernetes报错：certificate has expired or is not yet valid，可以通过命令：openssl x509 -in [证书全路径] -noout -text查看证书详情。 1、备份[root@zhdya]# cd /etc/kubernetes;[root@zhdya]# cp kubelet.kubeconfig kubelet.kubeconfig.bak;[root@zhdya]# mkdir sslbak &amp;&amp; cp ssl/ sslbak; 2、清理[root@zhdya]# rm -f kubelet.kubeconfig;[root@zhdya]# rm ssl/kubelet.*; 3. 重启kubelete[root@zhdya]# systemctl restart kubelet &amp;&amp; systemctl status kubelet 4. 使用CSR重新生产证书[root@zhdya]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-NsKZSz-myrv 11s kubelet-bootstrap Pending[root@zhdya]# kubectl certificate approve node-csr-NsKZSz-myrvcertificatesigningrequest.certificates.k8s.io/node-csr-NsKZSz-myrv approved 备注：CSR授权完成后，kubelete会自动发起CSR请求更新证书。 5、查看集群状态大概等待10秒，运行如下命令[root@zhdya]# kubectl get node]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Helm应用包管理器（上）]]></title>
    <url>%2F2019%2F12%2F16%2FHelm%E5%BA%94%E7%94%A8%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[三、Helm应用包管理器3.1 为什么需要Helm？K8S上的应用对象，都是由特定的资源描述组成，包括deployment、service等。都保存各自文件中或者集中写到一个配置文件。然后kubectl apply –f 部署。 如果应用只由一个或几个这样的服务组成，上面部署方式足够了。 而对于一个复杂的应用，会有很多类似上面的资源描述文件，例如微服务架构应用，组成应用的服务可能多达十个，几十个。如果有更新或回滚应用的需求，可能要修改和维护所涉及的大量资源文件，而这种组织和管理应用的方式就显得力不从心了。 且由于缺少对发布过的应用版本管理和控制，使Kubernetes上的应用维护和更新等面临诸多的挑战，主要面临以下问题： 如何将这些服务作为一个整体管理 这些资源文件如何高效复用 不支持应用级别的版本管理 3.2 Helm 介绍Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。 Helm有两个重要概念： helm：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。 Chart：应用描述，一系列用于描述 k8s 资源相关文件的集合。 Release：基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在k8s中创建出真实运行的资源对象。 3.3 Helm v3 变化2019年11月13日， Helm团队发布 Helm v3的第一个稳定版本。 该版本主要变化如下： 1、 架构变化最明显的变化是 Tiller的删除 2、Release名称可以在不同命名空间重用3、支持将 Chart 推送至 Docker 镜像仓库中4、使用JSONSchema验证chart values5、其他1）为了更好地协调其他包管理者的措辞 Helm CLI个别更名 helm delete` 更名为 `helm uninstallhelm inspect` 更名为 `helm showhelm fetch` 更名为 `helm pull 但以上旧的命令当前仍能使用。 2）移除了用于本地临时搭建 Chart Repository的 helm serve 命令。 3）自动创建名称空间 在不存在的命名空间中创建发行版时，Helm 2创建了命名空间。Helm 3遵循其他Kubernetes对象的行为，如果命名空间不存在则返回错误。 4） 不再需要requirements.yaml, 依赖关系是直接在chart.yaml中定义。 3.4 Helm客户端1、部署Helm客户端Helm客户端下载地址：https://github.com/helm/helm/releases 解压移动到/usr/bin/目录即可。 wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ 2、Helm常用命令 命令 描述 create 创建一个chart并指定名字 dependency 管理chart依赖 get 下载一个release。可用子命令：all、hooks、manifest、notes、values history 获取release历史 install 安装一个chart list 列出release package 将chart目录打包到chart存档文件中 pull 从远程仓库中下载chart并解压到本地 # helm pull stable/mysql –untar repo 添加，列出，移除，更新和索引chart仓库。可用子命令：add、index、list、remove、update rollback 从之前版本回滚 search 根据关键字搜索chart。可用子命令：hub、repo show 查看chart详细信息。可用子命令：all、chart、readme、values status 显示已命名版本的状态 template 本地呈现模板 uninstall 卸载一个release upgrade 更新一个release version 查看helm客户端版本 3、配置国内Chart仓库 微软仓库（http://mirror.azure.cn/kubernetes/charts/）这个仓库强烈推荐，基本上官网有的chart这里都有。 阿里云仓库（https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ） 官方仓库（https://hub.kubeapps.com/charts/incubator）官方chart仓库，国内有点不好使。 添加存储库： helm repo add stable http://mirror.azure.cn/kubernetes/chartshelm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo update 查看配置的存储库： helm repo listhelm search repo stable 一直在stable存储库中安装charts，你可以配置其他存储库。 删除存储库： helm repo remove aliyun 3.5 Helm基本使用主要介绍三个命令： chart install chart update chart rollback 1、使用chart部署一个应用查找chart： # helm search repo# helm search repo mysql 为什么mariadb也在列表中？因为他和mysql有关。 查看chart信息： # helm show chart stable/mysql 安装包： # helm install mysql(自定义name) stable/mysql 查看发布状态： # helm status mysql 查看安装的应用 [root@k8s-master1 ~]# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONmysql default 1 2019-12-14 13:25:51.506486337 +0800 CST deployed mysql-1.6.1 5.7.27 2、安装前自定义chart配置选项上面部署的mysql并没有成功，这是因为并不是所有的chart都能按照默认配置运行成功，且helm仅仅只是一个包管理器，并不能判断可能会需要一些环境依赖，例如有状态应用的PV。 所以我们需要自定义chart配置选项，安装过程中有两种方法可以传递配置数据： –values（或-f）：指定带有覆盖的YAML文件。这可以多次指定，最右边的文件优先 –set：在命令行上指定替代。如果两者都用，–set优先级高 –values使用，先将修改的变量写到一个文件中 # helm show values stable/mysql[root@k8s-master1 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmysql Pending 6m34s通过如上命令我们可以看到mysql已经自动创建了一个pvc，pod没有正常启动就是因为没有pv与之绑定！[root@k8s-master1 ~]# cat pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 8Gi accessModes: - ReadWriteOnce nfs: path: /opt/sharedata/mysql/ server: 192.168.171.12[root@k8s-master1 ~]# kubectl apply -f pv.yamlpersistentvolume/pv0003 created[root@k8s-master1 ~]# kubectl get pv,pvc ##已经绑定了mysqlNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/pv0003 8Gi RWO Retain Bound default/mysql 14sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql Bound pv0003 8Gi RWO 18m再次查看下是否已经启动：[root@k8s-master1 ~]# kubectl get podNAME READY STATUS RESTARTS AGEmetrics-app-7674cfb699-5l72f 1/1 Running 1 38hmetrics-app-7674cfb699-btch5 1/1 Running 1 38hmysql-8558fcfbb4-98wzl 0/1 Running 0 18mnfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 2 37h然后就可以登录到容器的mysql中；## 但是有时候我们不需要mysql默认的一些配置，我们需要自定义的配置一些简单的参数随着容器一起启动：# cat config.yaml persistence: enabled: true storageClass: &quot;managed-nfs-storage&quot; ##pv的自动供给 通过kubectl get sc 获取 accessMode: ReadWriteOnce size: 8Gi ##容量mysqlUser: &quot;k8s&quot; ##创建个k8s的用户mysqlPassword: &quot;123456&quot; ##k8s的密码mysqlDatabase: &quot;k8s&quot; ##创建个k8s的database[root@k8s-master1 ~]# helm install mysql2 -f mysql2conf.yaml stable/mysql[root@k8s-master1 ~]# kubectl get poNAME READY STATUS RESTARTS AGEmysql-8558fcfbb4-98wzl 1/1 Running 0 3h32mmysql2-7899bb48c9-w85gf 1/1 Running 0 47s[root@k8s-master1 ~]# kubectl exec -it mysql2-7899bb48c9-w85gf bashroot@mysql2-7899bb48c9-w85gf:/# mysql -uk8s -p123456Enter password:mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || k8s || mysql || performance_schema || sys |+--------------------+5 rows in set (0.02 sec)mysql&gt; select user from mysql.user;+------+| user |+------+| k8s || root |+------+2 rows in set (0.02 sec) 以上将创建具有名称的默认MySQL用户k8s，并授予此用户访问新创建的k8s数据库的权限，但将接受该图表的所有其余默认值。 命令行替代变量：# helm install mysql3 --set persistence.storageClass=&quot;managed-nfs-storage&quot; stable/mysql 也可以把chart包下载下来查看详情： # helm pull stable/mysql --untar[root@k8s-master1 ~]# cd mysql[root@k8s-master1 mysql]# lsChart.yaml README.md templates values.yaml[root@k8s-master1 mysql]# cd templates/[root@k8s-master1 templates]# lsconfigurationFiles-configmap.yaml _helpers.tpl NOTES.txt secrets.yaml servicemonitor.yaml testsdeployment.yaml initializationFiles-configmap.yaml pvc.yaml serviceaccount.yaml svc.yaml values yaml与set使用： 该helm install命令可以从多个来源安装： chart存储库 本地chart存档（helm install foo-0.1.1.tgz） chart目录（helm install path/to/foo） 完整的URL（helm install https://example.com/charts/foo-1.2.3.tgz） 3、构建一个Helm Chart创建目录和各个文件。 # helm create mychartCreating mychart# tree mychart/mychart/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ └── service.yaml└── values.yaml Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。 values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。 Templates： 目录里面存放所有yaml模板文件。 charts：目录里存放这个chart依赖的所有子chart。 NOTES.txt ：用于介绍Chart帮助信息， helm install 部署后展示给用户。例如：如何使用这个 Chart、列出缺省的设置等。 _helpers.tpl：放置模板助手的地方，可以在整个 chart 中重复使用 接下来我来手动创建一个简单的helm应用包：[root@k8s-master1 ~]# helm create mytestchartCreating mytestchart##修改后剩余：[root@k8s-master1 ~]# tree mytestchart/mytestchart/├── Chart.yaml├── README.md├── templates└── values.yaml[root@k8s-master1 templates]# kubectl create deployment zhdyaweb --image=nginx:1.17 --dry-run -o yamlapiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: zhdyaweb name: zhdyawebspec: replicas: 1 selector: matchLabels: app: zhdyaweb strategy: &#123;&#125; template: metadata: creationTimestamp: null labels: app: zhdyaweb spec: containers: - image: nginx:1.17 name: nginx resources: &#123;&#125;status: &#123;&#125;[root@k8s-master1 templates]# kubectl create deployment helloWeb --image=nginx:1.17 --dry-run -o yaml &gt; deployment.yaml[root@k8s-master1 templates]# cat deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; .Values.name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicas &#125;&#125; selector: matchLabels: app: &#123;&#123; .Values.replicas &#125;&#125; template: metadata: labels: app: &#123;&#123; .Values.replicas &#125;&#125; spec: containers: - image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag &#125;&#125; name: nginx[root@k8s-master1 mytestchart]# cat values.yamlname: zhdyawebreplicas: 3image: nginximageTag: 1.17##如下成功安装[root@k8s-master1 ~]# helm install zhdyaweb zhdyacc/NAME: zhdyawebLAST DEPLOYED: Sat Dec 14 18:53:32 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 ~]# kubectl get poNAME READY STATUS RESTARTS AGEzhdyacc-6bd459478b-5lvzd 1/1 Running 0 2m3szhdyacc-6bd459478b-cpv8g 1/1 Running 0 2m3szhdyacc-6bd459478b-qbqwt 1/1 Running 0 2m3s[root@k8s-master1 ~]# helm list ##查看安装的服务NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONmysql2 default 1 2019-12-14 16:57:16.387584786 +0800 CST deployed mysql-1.6.1 5.7.27zhdyaweb default 1 2019-12-14 18:53:32.545438231 +0800 CST deployed zhdyacc-0.1.0 1.16.0[root@k8s-master1 ~]# helm get manifest zhdyaweb ##查看渲染后的yaml---# Source: zhdyacc/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: zhdyaccspec: replicas: 3 selector: matchLabels: app: zhdyacc template: metadata: labels: app: zhdyacc spec: containers: - image: nginx:1.17 name: nginx 也可以打包推送的charts仓库共享别人使用。 # helm package mychart/mychart-0.1.0.tgz 4、升级、回滚和删除发布新版本的chart时，或者当您要更改发布的配置时，可以使用该helm upgrade 命令。 ##升级nginx版本为1.16[root@k8s-master1 ~]# helm upgrade --set imageTag=1.16 zhdyaweb zhdyacc/Release &quot;zhdyaweb&quot; has been upgraded. Happy Helming!NAME: zhdyawebLAST DEPLOYED: Sat Dec 14 19:04:22 2019NAMESPACE: defaultSTATUS: deployedREVISION: 2TEST SUITE: None[root@k8s-master1 ~]# helm get manifest zhdyaweb---# Source: zhdyacc/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: zhdyaccspec: replicas: 3 selector: matchLabels: app: zhdyacc template: metadata: labels: app: zhdyacc spec: containers: - image: nginx:1.16 ##版本已经更改 name: nginx[root@k8s-master1 ~]# curl 10.244.2.26 -IHTTP/1.1 200 OKServer: nginx/1.16.1##或者另外一种方法：# helm upgrade -f values.yaml zhdyaweb zhdyacc/ 回滚：[root@k8s-master1 ~]# helm rollback zhdyawebRollback was a success! Happy Helming![root@k8s-master1 ~]# kubectl get poNAME READY STATUS RESTARTS AGEmetrics-app-7674cfb699-5l72f 1/1 Running 1 44hmetrics-app-7674cfb699-btch5 1/1 Running 1 44hmysql2-7899bb48c9-w85gf 1/1 Running 0 130mnfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 2 43hzhdyacc-6bd459478b-9mcpt 1/1 Running 0 9szhdyacc-6bd459478b-9ntcg 1/1 Running 0 6szhdyacc-6bd459478b-jmjlh 1/1 Running 0 7s[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 1 44h 10.244.1.19 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 1/1 Running 1 44h 10.244.2.21 k8s-node2 &lt;none&gt; &lt;none&gt;mysql2-7899bb48c9-w85gf 1/1 Running 0 130m 10.244.0.22 k8s-master1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 2 43h 10.244.2.22 k8s-node2 &lt;none&gt; &lt;none&gt;zhdyacc-6bd459478b-9mcpt 1/1 Running 0 11s 10.244.0.25 k8s-master1 &lt;none&gt; &lt;none&gt;zhdyacc-6bd459478b-9ntcg 1/1 Running 0 8s 10.244.3.19 k8s-node3 &lt;none&gt; &lt;none&gt;zhdyacc-6bd459478b-jmjlh 1/1 Running 0 9s 10.244.2.27 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# curl 10.244.2.27 -IHTTP/1.1 200 OKServer: nginx/1.17.6Date: Sat, 14 Dec 2019 11:08:34 GMT如上我们看到再次回滚到了1.17的版本！！ 如果在发布后没有达到预期的效果，则可以使用helm rollback回滚到之前的版本。 例如将应用回滚到第一个版本： # helm rollback web 2 卸载发行版，请使用以下helm uninstall命令： # helm uninstall web 查看历史版本配置信息 # helm get --revision 1 web]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次java_MetaspaceSize设置不当导致的问题]]></title>
    <url>%2F2019%2F12%2F15%2FJava%E5%8F%82%E6%95%B0%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[先来了解下： 内存的分配及回收–堆中的新生代和老年代：java堆被分为两部分， 一部分被称为新生代， 一部分称为老年代， 他们的比例通常为 1：2 一、堆（新生代、老年代）与元空间。1.1、新生代：主要是用来存放新生的对象。一般占据堆空间的1/3，由于频繁创建对象，所以新生代会频繁触发MinorGC进行垃圾回收。新生代分为Eden区、ServivorFrom、ServivorTo三个区，==比例为 8：1：1==。 Eden区：Java新对象的出生地(如果新创建的对象占用内存很大则直接分配给老年代)。 1.1.1、MinorGC 触发机制： 当Eden区内存不够的时候就会触发一次MinorGc，对新生代区进行一次垃圾回收。 ServivorTo：保留了一次MinorGc过程中的幸存者。 ServivorFrom: 上一次GC的幸存者，作为这一次GC的被扫描者。当JVM无法为新建对象分配内存空间的时候(Eden区满的时候)，JVM触发MinorGc。因此新生代空间占用越低，MinorGc越频繁。 每次对象会存在于Eden区和一个Survivor区， 当内存不够时，触发GC，然后把依然存活的对象复制到另一个空白的Survivor区， 然后直接清空其余新生代内存，然后如此循环。总有一个Survivor区是空白的。每进行一次GC，存活对象的年龄+1， 默认情况下，对象年龄达到15时，就会移动到老年代中。 默认的，Edem : from : to = 8 :1 : 1 ( 可以通过参数–XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。 1.2、老年代：老年代的对象比较稳定，所以MajorGC不会频繁执行。 1.2.1、触发MinorGC的条件： 在进行MajorGC之前，一般都先进行了一次MinorGC，使得有新生代的对象进入老年代，当老年代空间不足时就会触发MajorGC。 当无法找到足够大的连续空间分配给新创建的较大对象时，也会触发MajorGC进行垃圾回收腾出空间。 当老年代也满了装不下的时候，就会抛出OOM。 1.3、元空间：JVM 使用本地内存来存储存放Class和Meta（元数据）的信息并称之为：元空间（Metaspace） Metaspace 垃圾回收对于僵死的类及类加载器的垃圾回收将在元数据使用达到MaxMetaspaceSize参数的设定值时进行。 适时地监控和调整元空间对于减小垃圾回收频率和减少延时是很有必要的。持续的元空间垃圾回收说明，可能存在类、类加载器导致的内存泄漏或是大小设置不合适。 二、GC 堆Java 中的堆是 GC 收集垃圾的主要区域。堆内GC 分为：Minor GC、FullGC ( 或称为 Major GC )。 2.1、Minor GCMinor GC 是发生在新生代中的垃圾收集动作，所采用的是复制算法。 简要概括如下： 当对象在 Eden ( 包括一个 Survivor 区域，这里假设是 from 区域 ) 出生后，在经过一次 Minor GC 后，如 果对象还存活，并且能够被另外一块 Survivor 区域所容纳( 上面已经假设为 from 区域，这里应为 to 区域， 即 to 区域有足够的内存空间来存储 Eden 和 from 区域中存活的对象 )，则使用复制算法将这些仍然还存活的对 象复制到另外一块 Survivor 区域 ( 即 to 区域 ) 中，然后清理所使用过的 Eden 以及 Survivor 区域 ( 即 from 区域 )，并且将这些对象的年龄设置为1，以后对象在 Survivor 区每熬过一次 Minor GC，就将对象的年龄 + 1，当对象的年龄达到某个值时 ( 默认是 15 岁，可以通过参数 -XX:MaxTenuringThreshold 来设定)，这些对象就会成为老年代。 2.2、Full GCFull GC 是发生在老年代的垃圾收集动作，所采用的是标记-清除-整理算法。 现实的生活中，老年代的人通常会比新生代的人”早死”。堆内存中的老年代(Old)不同于这个，老年代里面的对象 几乎个个都是在 Survivor 区域中熬过来的，它们是不会那么容易就 “死掉” 了的。因此，Full GC 发生的次数不 会有 Minor GC 那么频繁，并且做一次 Full GC 要比进行一次 Minor GC 的时间更长。 另外，标记-清除算法收集垃圾的时候会产生许多的内存碎片 ( 即不连续的内存空间 )，此后需要为较大的对象 分配内存空间时，若无法找到足够的连续的内存空间，就会提前触发一次 GC 的收集动作。 三、故障重现某开发负责的其中一个业务其中1台容器无故gg了，已知不是核心应用，且目前线上1台虚机也可以扛住目前的量，赶紧一起同开发分析问题：- 先看虚机和宿主机的负载，正常；- 进入容器看下进程还在；- 业务日志已经停止写入；- 查看是否有gclog； 再次看下大盘： gc问题，有异常了，节点被下掉了； 先问下开发此业务当然有没有什么其它动作？经查看业务日志发现，刚刚那一阵在跑job，为什么metadata区会爆掉呢？ 开发给的答案： 类加载器创建过多，带来的一个问题是，在类加载器第一次加载类的时候，会在Metaspace里会给它分配内存块，为了分配高效，每个类加载器用来存放类信息的内存块都是独立的，所以哪怕你这个类加载器只加载一个类，也会为之分配一块空的内存给这个类加载器，其实是至少两个内存块，于是你有可能会发现Metaspace的内存使用率非常低，但是committed的内存已经达到了阈值，从而触发了Full GC，如果这种只加载很少类的类加载器非常多，那造成的后果就是很多碎片化的内存！ ##先来看下目前的JVM参数：[root@xhy-18-175-61 dsf]# ps aux | grep javaroot 136 13.7 0.7 18846112 1968564 ? Sl+ 18:28 4:08 java -Duser.timezone=GMT+08 -server -Xms3840m -Xmx3840m -XX:NewSize=1024m -XX:MaxNewSize=1024m -XX:MaxDirectMemorySize=256m -XX:MetaspaceSize=128-XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_heapDump.hprof -XX:+UseParNewGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1024M -XX:+ExplicitGCInvokesConcurrent -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:CMSFullGCsBeforeCompaction=2 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/data/logs/skynet-cxgc.java.dsf.zkt.service/cxgc.java.dsf.zkt.service_gc.log -javaagent:/usr/local/apm_agent/apm.agent.bootstrap.jar -Dapm.applicationName=cxgc.java.dsf.zkt.service -Dapm.agentId=172.18.175.61-17851 -Dapm.env=product -classpath -DDSF_HOME=/usr/local/dsf -DDio.netty.leakDetectionLevel=PARANOID -jar /usr/local/dsf/dsfapps/dsf.zkt.service.jar##分析下：通过jstat -gcutil pid查看M的值为97.37，即Meta区使用率也达到了97.37%：[root@xhy-18-175-61 dsf]# jstat -gcutil 136 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 89.03 35.00 3.79 97.37 97.18 19 4.312 0 0.000 4.312那么-XX:MetaspaceSize=256m的含义到底是什么呢？其实，这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。先来解决下：- MetaspaceSize改成256m；增加 -XX:MinMetaspaceFreeRatio=40； 这里有几个要点需要明确： 无论-XX:MetaspaceSize配置什么值，Metaspace的初始容量一定是21807104（约20.8m）； Metaspace由于使用不断扩容到-XX:MetaspaceSize参数指定的量，就会发生FGC；且之后每次Metaspace扩容都会发生FGC； 如果Old区配置CMS垃圾回收，那么第2点的FGC也会使用CMS算法进行回收； Meta区容量范围为(20.8m, MaxMetaspaceSize)； 如果MaxMetaspaceSize设置太小，可能会导致频繁FGC，甚至OOM； 最后建议： MetaspaceSize和MaxMetaspaceSize设置一样大； 具体设置多大，建议稳定运行一段时间后通过jstat -gc pid确认且这个值大一些，对于大部分项目256m即可。 写这篇日志已经是第二天了 暂时没有发现异常：]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S弹性伸缩之基于Metrics Server CPU指标的HPA]]></title>
    <url>%2F2019%2F12%2F13%2FK8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A93%2F</url>
    <content type="text"><![CDATA[基于CPU指标缩放1、 Kubernetes API Aggregation什么是k8sapi聚合？ 它是允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。 当然看如下图↓：你也可以把agg当作一个nginx的反代，它就是代理层。 在 Kubernetes 1.7 版本引入了聚合层，允许第三方应用程序通过将自己注册到kube-apiserver上，仍然通过 API Server 的 HTTP URL 对新的 API 进行访问和操作。为了实现这个机制，Kubernetes 在 kube-apiserver 服务中引入了一个 API 聚合层（API Aggregation Layer），用于将扩展 API 的访问请求转发到用户服务的功能。 当你访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端 。通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。 如果你使用kubeadm部署的，默认已开启。如果你使用==二进制方式部署==的话，需要在kube-APIServer中添加启动参数，增加以下配置： # vi /opt/kubernetes/cfg/kube-apiserver.conf...--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \--requestheader-allowed-names=kubernetes \--requestheader-extra-headers-prefix=X-Remote-Extra- \--requestheader-group-headers=X-Remote-Group \--requestheader-username-headers=X-Remote-User \--enable-aggregator-routing=true \... 在设置完成重启 kube-apiserver 服务，就启用 API 聚合功能了。 2、部署 Metrics ServerMetrics Server是一个集群范围的资源使用情况的数据聚合器。作为一个应用部署在集群中。 Metric server从每个节点上Kubelet公开的摘要API收集指标。 Metrics server通过Kubernetes聚合器注册在Master APIServer中。 # git clone https://github.com/kubernetes-incubator/metrics-server# cd metrics-server/deploy/1.8+/# vi metrics-server-deployment.yaml # 添加2条启动参数 ... containers: - name: metrics-server image: zhdya/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-insecure-tls ##忽略证书的验证 - --kubelet-preferred-address-types=InternalIP ##一般node都是用主机名注册的，但是metricserver是通过pod启动的，解析不到hostname，所以需要采集节点的IP...# kubectl create -f . 可通过Metrics API在Kubernetes中获得资源使用率指标，例如容器CPU和内存使用率。这些度量标准既可以由用户直接访问（例如，通过使用kubectl top命令），也可以由集群中的控制器（例如，Horizontal Pod Autoscaler）用于进行决策。 [root@k8s-master1 metrics-server]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-pbbbc 1/1 Running 0 131m 10.244.2.2 k8s-node2 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-4gj8p 1/1 Running 1 142m 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-k5lfh 1/1 Running 1 75m 192.168.171.14 k8s-node3 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-kddhv 1/1 Running 1 142m 192.168.171.12 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-q8g25 1/1 Running 1 141m 192.168.171.13 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-server-7dbbcf4c7-v5zpm 1/1 Running 0 49s 10.244.2.4 k8s-node2 &lt;none&gt; &lt;none&gt; 查看是否注册到apiservice[root@k8s-master1 metrics-server]# kubectl get apiservicev1beta1.metrics.k8s.io kube-system/metrics-server True 3m22s 测试： [root@k8s-master1 metrics-server]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes&#123;&quot;kind&quot;:&quot;NodeMetricsList&quot;,&quot;apiVersion&quot;:&quot;metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;&#125;,&quot;items&quot;:[&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-master1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-master1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:34Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;104718141n&quot;,&quot;memory&quot;:&quot;1158508Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node1&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:26Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;61460193n&quot;,&quot;memory&quot;:&quot;556328Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node2&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node2&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:32Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;57896643n&quot;,&quot;memory&quot;:&quot;570056Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;k8s-node3&quot;,&quot;selfLink&quot;:&quot;/apis/metrics.k8s.io/v1beta1/nodes/k8s-node3&quot;,&quot;creationTimestamp&quot;:&quot;2019-12-10T15:41:01Z&quot;&#125;,&quot;timestamp&quot;:&quot;2019-12-10T15:40:30Z&quot;,&quot;window&quot;:&quot;30s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;30890872n&quot;,&quot;memory&quot;:&quot;403264Ki&quot;&#125;&#125;]&#125;[root@k8s-master1 metrics-server]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%k8s-master1 105m 5% 1131Mi 65%k8s-node1 62m 3% 543Mi 31%k8s-node2 58m 2% 556Mi 32%k8s-node3 31m 1% 393Mi 22%[root@k8s-master1 metrics-server]# kubectl top podNAME CPU(cores) MEMORY(bytes)web-944cddf48-4x6qr 0m 3Miweb-944cddf48-64d7r 0m 2Miweb-944cddf48-hjwng 0m 3Miweb-944cddf48-skh82 0m 3Mi 3、autoscaling/v1（CPU指标实践）autoscaling/v1版本只支持CPU一个指标。 首先部署一个应用： [root@k8s-master1 hpa]# kubectl run web --image=nginx --replicas=8 --requests=&quot;cpu=100m,memory=100Mi&quot; --expose 80 --port 80 --dry-run -o yaml &gt;app.yaml 创建HPA策略： [root@k8s-master1 hpa]# kubectl autoscale deployment web --min=2 --max=8 -o yaml --dry-run &gt;hpav1.yaml[root@k8s-master1 hpa]# kubectl apply -f hpav1.yamlhorizontalpodautoscaler.autoscaling/web created###cat hpav1.yamlapiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: webspec: maxReplicas: 8 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web targetCPUUtilizationPercentage: 60 scaleTargetRef：表示当前要伸缩对象是谁 targetCPUUtilizationPercentage：当整体的资源利用率超过50%的时候，会进行扩容。 查看当前状态：[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 0%/60% 2 8 3 24s 开启压测： # yum install httpd-tools# ab -n 100000 -c 100 http://10.1.206.176/status.php 10.0.0.147 为ClusterIP。 检查扩容状态： # kubectl get hpa# kubectl top pods# kubectl get pods[root@k8s-master1 hpa]# ab -n 1000000 -c 1000 http://10.0.0.201/index.html ##压测关闭压测，过一会检查缩容状态。[root@k8s-master1 ~]# kubectl top poNAME CPU(cores) MEMORY(bytes)web-77cfdb7c6c-dpm5t 116m 5Miweb-77cfdb7c6c-lwkbj 343m 5Mi[root@k8s-master1 ~]# kubectl get poNAME READY STATUS RESTARTS AGEweb-77cfdb7c6c-94p6x 1/1 Running 0 7sweb-77cfdb7c6c-9xwbj 1/1 Running 0 23sweb-77cfdb7c6c-dpm5t 1/1 Running 0 33mweb-77cfdb7c6c-gpk6d 1/1 Running 0 7sweb-77cfdb7c6c-l7b4r 1/1 Running 0 23sweb-77cfdb7c6c-lwkbj 1/1 Running 0 33mweb-77cfdb7c6c-w6lz6 1/1 Running 0 7sweb-77cfdb7c6c-wpzb5 1/1 Running 0 7s ==工作流程==：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor) 4、autoscaling/v2beta2（多指标）为满足更多的需求， HPA 还有 autoscaling/v2beta1和 autoscaling/v2beta2两个版本。 这两个版本的区别是 autoscaling/v1beta1支持了 Resource Metrics（CPU）和 Custom Metrics（应用程序指标），而在 autoscaling/v2beta2的版本中额外增加了 External Metrics的支持。 # kubectl get hpa.v2beta2.autoscaling -o yaml &gt; /tmp/hpa-v2.yaml apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: web namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web minReplicas: 1 maxReplicas: 10 metrics: - resource: type: Resource name: cpu target: averageUtilization: 60 type: Utilization 与上面v1版本效果一样，只不过这里格式有所变化。 v2还支持其他另种类型的度量指标，：Pods和Object。 type: Podspods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k type: Objectobject: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 2k metrics中的type字段有四种类型的值：Object、Pods、Resource、External。 Resource：指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。 Object：指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。 Pods：指的是伸缩对象Pods的指标，数据需要第三方的adapter提供，只允许AverageValue类型的目标值。 External：指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。 # hpa-v2.yamlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: web namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 - type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 10k ==工作流程==：hpa -&gt; apiserver -&gt; kube aggregation -&gt; prometheus-adapter -&gt; prometheus -&gt; pods]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S弹性伸缩之Nodes 资源扩容（CA，Ansible）]]></title>
    <url>%2F2019%2F12%2F13%2FK8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[弹性伸缩1 传统弹性伸缩的困境从传统意义上，弹性伸缩主要解决的问题是容量规划与实际负载的矛盾。 蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 1、Kubernetes中弹性伸缩存在的问题常规的做法是给集群资源预留保障集群可用，通常20%左右。这种方式看似没什么问题，但放到Kubernetes中，就会发现如下2个问题。 机器规格不统一造成机器利用率百分比碎片化 在一个Kubernetes集群中，通常不只包含一种规格的机器，假设集群中存在4C8G与16C32G两种规格的机器，对于10%的资源预留，这两种规格代表的意义是完全不同的。 特别是在缩容的场景下，为了保证缩容后集群稳定性，我们一般会一个节点一个节点从集群中摘除，那么如何判断节点是否可以摘除其利用率百分比就是重要的指标。此时如果大规格机器有较低的利用率被判断缩容，那么很有可能会造成节点缩容后，容器重新调度后的争抢。如果优先缩容小规格机器，则可能造成缩容后资源的大量冗余。 机器利用率不单纯依靠宿主机计算 在大部分生产环境中，资源利用率都不会保持一个高的水位，但从调度来讲，调度应该保持一个比较高的水位，这样才能保障集群稳定性，又不过多浪费资源。 2、弹性伸缩概念的延伸不是所有的业务都存在峰值流量，越来越细分的业务形态带来更多成本节省和可用性之间的跳转。 在线负载型：微服务、网站、API 离线任务型：离线计算、机器学习 定时任务型：定时批量计算 不同类型的负载对于弹性伸缩的要求有所不同，在线负载对弹出时间敏感，离线任务对价格敏感，定时任务对调度敏感。 2.2 kubernetes 弹性伸缩布局在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。 有三种弹性伸缩： CA（Cluster Autoscaler）：Node级别自动扩/缩容 cluster-autoscaler组件 HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容 VPA（Vertical Pod Autoscaler）：Pod配置自动扩/缩容，主要是CPU、内存 addon-resizer组件 如果在云上建议 HPA 结合 cluster-autoscaler 的方式进行集群的弹性伸缩管理。 2.3 Node 自动扩容/缩容1、Cluster AutoScaler扩容：Cluster AutoScaler 定期检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。 缩容：Cluster AutoScaler 也会定期监测 Node 的资源使用情况，当一个 Node 长时间资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除。此时，原来的 Pod 会自动调度到其他 Node 上面。 支持的云提供商： 阿里云：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md AWS： https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md Azure： https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md 2、Ansible扩容Node自动化流程： 触发新增Node 调用Ansible脚本部署组件 检查服务是否可用 调用API将新Node加入集群或者启用Node自动加入 观察新Node状态 完成Node扩容，接收新Pod 扩容 # cat hosts ...[newnode]192.168.31.71 node_name=k8s-node3# ansible-playbook -i hosts add-node.yml -k 缩容 如果你想从Kubernetes集群中删除节点，正确流程如下： 1、获取节点列表 kubectl get node 2、设置不可调度 kubectl cordon $node_name 3、驱逐节点上的Pod kubectl drain $node_name --ignore-daemonsets 4、移除节点 该节点上已经没有任何资源了，可以直接移除节点： kubectl delete node $node_name 这样，我们平滑移除了一个 k8s 节点。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S弹性伸缩之Pods 资源扩容方案]]></title>
    <url>%2F2019%2F12%2F13%2FK8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A92%2F</url>
    <content type="text"><![CDATA[Pod自动扩容/缩容（HPA）Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据资源利用率或者自定义指标自动调整replication controller, deployment 或 replica set，实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适于无法缩放的对象，例如DaemonSet。 1、HPA基本原理Kubernetes 中的 Metrics Server 持续采集所有 Pod 副本的指标数据。HPA 控制器通过 Metrics Server 的 API（Heapster 的 API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod 副本数量。当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。如图所示。 在弹性伸缩中，冷却周期是不能逃避的一个话题， 由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸， 所以在每次做出扩容缩容后，冷却时间是多少。 在 HPA 中，默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。 可以通过调整kube-controller-manager组件启动参数设置冷却时间： –horizontal-pod-autoscaler-downscale-delay ：扩容冷却 –horizontal-pod-autoscaler-upscale-delay ：缩容冷却 2、HPA的演进历程目前 HPA 已经支持了 autoscaling/v1、autoscaling/v2beta1和autoscaling/v2beta2 三个大版本 。 目前大多数人比较熟悉是autoscaling/v1，这个版本只支持CPU一个指标的弹性伸缩。 而autoscaling/v2beta1增加了支持自定义指标，autoscaling/v2beta2又额外增加了外部指标支持。 而产生这些变化不得不提的是Kubernetes社区对监控与监控指标的认识与转变。从早期Heapster到Metrics Server再到将指标边界进行划分，一直在丰富监控生态。 示例： v1版本： apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache ##指定对象 minReplicas: 1 ##指定副本范围 maxReplicas: 10 targetCPUUtilizationPercentage: 50 ##指定阈值 v2beta2版本：（可以基于resource，pod，object，external等） apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache ##选择对象 minReplicas: 1 ##范围 maxReplicas: 10 metrics: - type: Resource resource: name: cpu ##基于cpu target: type: Utilization averageUtilization: 50 - type: Pods ##基于pod pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object ##基于qps object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route target: type: Value value: 10k - type: External external: metric: name: queue_messages_ready selector: &quot;queue=worker_tasks&quot; target: type: AverageValue averageValue: 30]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S弹性伸缩之基于Prometheus QPS指标的HPA]]></title>
    <url>%2F2019%2F12%2F13%2FK8S%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A94%2F</url>
    <content type="text"><![CDATA[基于Prometheus自定义指标缩放资源指标只包含CPU、内存，一般来说也够了。但如果想根据自定义指标:如请求qps/5xx错误数来实现HPA，就需要使用自定义指标了，目前比较成熟的实现是 Prometheus Custom Metrics。自定义指标由Prometheus来提供，再利用k8s-prometheus-adpater聚合到apiserver，实现和核心指标（metric-server)同样的效果。 1、部署PrometheusPrometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。 Prometheus 特点： 自动采集，服务发现； 多维数据模型：由度量名称和键值对标识的时间序列数据； PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询； 不依赖分布式存储，单个服务器节点可直接工作； 基于HTTP的pull方式采集时间序列数据； 推送时间序列数据通过PushGateway组件支持； 通过服务发现或静态配置发现目标； 多种图形模式及仪表盘支持（grafana）； Prometheus组成及架构： Prometheus Server：收集指标和存储时间序列数据，并提供查询接口 ClientLibrary：客户端库 Push Gateway：短期存储指标数据。主要用于临时性的任务 Exporters：采集已有的第三方服务监控指标并暴露metrics Alertmanager：告警 Web UI：简单的Web控制台 部署： 现在node上安装：[root@k8s-node1 ~]# yum install -y nfs-utilsNFS 配置及使用我们在服务端创建一个共享目录 /data/share ，作为客户端挂载的远端入口，然后设置权限。$ mkdir -p /opt/sharedata/$ chmod 666 /opt/sharedata/然后，修改 NFS 配置文件 /etc/exports[root@k8s-node1 ~]# cat /etc/exports/opt/sharedata 192.168.171.0/24(rw,sync,insecure,no_subtree_check,no_root_squash)说明一下，这里配置后边有很多参数，每个参数有不同的含义，具体可以参考下边。此处，我配置了将 /data/share 文件目录设置为允许 IP 为该 192.168.171.0/24 区间的客户端挂载，当然，如果客户端 IP 不在该区间也想要挂载的话，可以设置 IP 区间更大或者设置为 * 即允许所有客户端挂载，例如：/home *(ro,sync,insecure,no_root_squash) 设置 /home 目录允许所有客户端只读挂载。# 启动 NFS 服务$ service nfs start# 或者使用如下命令亦可/bin/systemctl start nfs.service[root@k8s-node1 ~]# showmount -e localhostExport list for localhost:/opt/sharedata 192.168.171.0/24示例：挂载远端目录到本地 /share 目录。$ mount 192.168.171.11:/opt/sharedata /share$ df -h | grep 192.168.171.11Filesystem Size Used Avail Use% Mounted on192.168.171.11:/opt/sharedata 27G 11G 17G 40% /share客户端要卸载 NFS 挂载的话，使用如下命令即可。$ umount /share 现在master上安装：链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg 提取码：7l3z从分享包中导入nfs-client.zip# cd nfs-client# [root@k8s-master1 nfs-client]# cat deployment.yaml...省略serviceAccountName: nfs-client-provisionercontainers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.171.12 ##nfs的server地址 - name: NFS_PATH value: /opt/sharedata ##暴露的目录volumes: - name: nfs-client-root nfs: server: 192.168.171.12 path: /opt/sharedata...省略[root@k8s-master1 nfs-client]# kubectl apply -f .storageclass.storage.k8s.io/managed-nfs-storage createdserviceaccount/nfs-client-provisioner createddeployment.apps/nfs-client-provisioner createdserviceaccount/nfs-client-provisioner unchangedclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner createdrole.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner createdrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created[root@k8s-master1 nfs-client]# kubectl get poNAME READY STATUS RESTARTS AGEnfs-client-provisioner-9c784f97-cqzhb 1/1 running 0 2m16s 链接：https://pan.baidu.com/s/1b4Fu8j4Flf2Lzd0naT_iRg 提取码：7l3z# cd prometheus# kubectl apply -f .[root@k8s-master1 nfs-client]# kubectl get po -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-pbbbc 1/1 Running 2 2d1h 10.244.2.15 k8s-node2 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-q8g25 1/1 Running 3 2d1h 192.168.171.13 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-server-7dbbcf4c7-v5zpm 1/1 Running 3 47h 10.244.2.14 k8s-node2 &lt;none&gt; &lt;none&gt;prometheus-0 2/2 Running 0 6m48s 10.244.3.15 k8s-node3 &lt;none&gt; &lt;none&gt;[root@k8s-master1 nfs-client]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2d1hmetrics-server ClusterIP 10.0.0.5 &lt;none&gt; 443/TCP 47hprometheus NodePort 10.0.0.147 &lt;none&gt; 9090:30090/TCP 7m46s 访问Prometheus UI：http://NdeIP:30090 2、 部署 Custom Metrics Adapter但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，还需要另外一个组件(k8s-prometheus-adpater)，将prometheus的metrics 数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主APIServer中注册，以便直接通过/apis/来访问。 https://github.com/DirectXMan12/k8s-prometheus-adapter 该 PrometheusAdapter 有一个稳定的Helm Charts，我们直接使用。 先准备下helm环境： wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/helm repo add stable http://mirror.azure.cn/kubernetes/chartshelm repo updatehelm repo list 部署prometheus-adapter，指定prometheus地址： # helm install prometheus-adapter stable/prometheus-adapter --namespace kube-system --set prometheus.url=http://prometheus.kube-system,prometheus.port=9090# helm list -n kube-system # kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEprometheus-adapter-77b7b4dd8b-ktsvx 1/1 Running 0 9m 确保适配器注册到APIServer： [root@k8s-master1 ~]# kubectl get apiservices |grep customv1beta1.custom.metrics.k8s.io kube-system/prometheus-adapter True 87s[root@k8s-master1 ~]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot;&#123;&quot;kind&quot;:&quot;APIResourceList&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;groupVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;resources&quot;:[&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_capacity_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_inodes_free&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;persistentvolumeclaims/kubelet_volume_stats_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;pods/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_container_log_filesystem_used_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;namespaces/kubelet_volume_stats_available_bytes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:false,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;,&#123;&quot;name&quot;:&quot;jobs.batch/kubelet_volume_stats_inodes_used&quot;,&quot;singularName&quot;:&quot;&quot;,&quot;namespaced&quot;:true,&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;verbs&quot;:[&quot;get&quot;]&#125;]&#125; 基于QPS指标实践部署一个应用： apiVersion: apps/v1kind: Deploymentmetadata: labels: app: metrics-app name: metrics-appspec: replicas: 3 selector: matchLabels: app: metrics-app template: metadata: labels: app: metrics-app annotations: prometheus.io/scrape: &quot;true&quot; ##是否可以被采集数据 prometheus.io/port: &quot;80&quot; ##采集访问的端口 prometheus.io/path: &quot;/metrics&quot; ##采集访问的URL spec: containers: - image: zhdya/metrics-app name: metrics-app ports: - name: web containerPort: 80 resources: requests: cpu: 200m memory: 256Mi readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5---apiVersion: v1kind: Servicemetadata: name: metrics-app labels: app: metrics-appspec: ports: - name: web port: 80 targetPort: 80 selector: app: metrics-app [root@k8s-master1 hpa]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 19s 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 0/1 Running 0 19s 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kksjr 0/1 Running 0 19s 10.244.0.15 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 hpa]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 2d1hmetrics-app ClusterIP 10.0.0.163 &lt;none&gt; 80/TCP 39s 该metrics-app暴露了一个Prometheus指标接口，可以通过访问service看到： [root@k8s-master1 hpa]# curl 10.0.0.163/metrics# HELP http_requests_total The amount of requests in total# TYPE http_requests_total counterhttp_requests_total 20# HELP http_requests_per_second The amount of requests per second the latest ten seconds# TYPE http_requests_per_second gaugehttp_requests_per_second 0.5##顺带测试下负载均衡：[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-btch5. The last 10 seconds, the average QPS has been 0.5. Total requests served: 35[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-5l72f. The last 10 seconds, the average QPS has been 0.5. Total requests served: 38[root@k8s-master1 hpa]# curl 10.0.0.163Hello! My name is metrics-app-7674cfb699-kksjr. The last 10 seconds, the average QPS has been 0.5. Total requests served: 37 收集到的每个容器被访问的次数： 创建HPA策略： # vi app-hpa-v2.ymlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: metrics-app-hpa namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: metrics-app minReplicas: 1 maxReplicas: 8 metrics: - type: Pods pods: metric: name: http_requests_per_second target: type: AverageValue averageValue: 800m # 800m 即0.8个/秒 [root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app &lt;unknown&gt;/800m 1 8 3 36s 这里使用Prometheus提供的指标测试来测试自定义指标（QPS）的自动缩放。 4、配置适配器收集特定的指标当创建好HPA还没结束，因为适配器还不知道你要什么指标（http_requests_per_second），HPA也就获取不到Pod提供指标。 ConfigMap在default名称空间中编辑prometheus-adapter ，并seriesQuery在该rules: 部分的顶部添加一个新的： # kubectl edit cm prometheus-adapter -n kube-systemapiVersion: v1kind: ConfigMapmetadata: labels: app: prometheus-adapter chart: prometheus-adapter-v0.1.2 heritage: Tiller release: prometheus-adapter name: prometheus-adapterdata: config.yaml: | rules: ##增加如下一段： - seriesQuery: &apos;http_requests_total&#123;kubernetes_namespace!=&quot;&quot;,kubernetes_pod_name!=&quot;&quot;&#125;&apos; ##在prometheus中就可以直接查询到这部分数据 resources: overrides: kubernetes_namespace: &#123;resource: &quot;namespace&quot;&#125; kubernetes_pod_name: &#123;resource: &quot;pod&quot;&#125; name: matches: &quot;^(.*)_total&quot; as: &quot;$&#123;1&#125;_per_second&quot; metricsQuery: &apos;sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&apos;... 该规则将http_requests在2分钟的间隔内收集该服务的所有Pod的平均速率。 测试API： [root@k8s-master1 hpa]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot;&#123;&quot;kind&quot;:&quot;MetricValueList&quot;,&quot;apiVersion&quot;:&quot;custom.metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&quot;selfLink&quot;:&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;&#125;,&quot;items&quot;:[&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-5l72f&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-btch5&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;,&#123;&quot;describedObject&quot;:&#123;&quot;kind&quot;:&quot;Pod&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;metrics-app-7674cfb699-kksjr&quot;,&quot;apiVersion&quot;:&quot;/v1&quot;&#125;,&quot;metricName&quot;:&quot;http_requests_per_second&quot;,&quot;timestamp&quot;:&quot;2019-12-12T15:52:47Z&quot;,&quot;value&quot;:&quot;416m&quot;&#125;]&#125; [root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 416m/800m 1 8 2 20m 压测： ab -n 100000 -c 100 http://10.0.0.163/metrics 查看容器扩容的情况：[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 48m 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-6rht6 1/1 Running 0 16s 10.244.0.16 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-9ltvr 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 1/1 Running 0 48m 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kft7p 1/1 Running 0 16s 10.244.3.16 k8s-node3 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-plhrp 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-sgvln 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-wr56r 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 0 8m7s 10.244.2.17 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmetrics-app-7674cfb699-5l72f 1/1 Running 0 48m 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-6rht6 1/1 Running 0 18s 10.244.0.16 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-9ltvr 0/1 Running 0 3s 10.244.0.17 k8s-master1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-btch5 1/1 Running 0 48m 10.244.2.16 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-kft7p 1/1 Running 0 18s 10.244.3.16 k8s-node3 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-plhrp 0/1 Running 0 3s 10.244.2.18 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-sgvln 0/1 Running 0 3s 10.244.1.16 k8s-node1 &lt;none&gt; &lt;none&gt;metrics-app-7674cfb699-wr56r 0/1 Running 0 3s 10.244.1.17 k8s-node1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-f9fdd5cc9-ffzbd 1/1 Running 0 8m9s 10.244.2.17 k8s-node2 &lt;none&gt; &lt;none&gt; 查看HPA状态： [root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 414345m/800m 1 8 8 21m[root@k8s-master1 ~]# kubectl describe hpa metrics-app-hpa...省略Metrics: ( current / target ) &quot;http_requests_per_second&quot; on pods: 818994m / 800mMin replicas: 1Max replicas: 8Deployment pods: 8 current / 8 desiredConditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale recommended size matches current size ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second ScalingLimited True TooManyReplicas the desired replica count is more than the maximum replica countEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedComputeMetricsReplicas 19m (x12 over 22m) horizontal-pod-autoscaler invalid metrics (1 invalid out of 1), first error is: failed to get pods metric value: unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods Warning FailedGetPodsMetric 7m18s (x61 over 22m) horizontal-pod-autoscaler unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods Normal SuccessfulRescale 88s horizontal-pod-autoscaler New size: 4; reason: pods metric http_requests_per_second above target 小结 1、应用程序暴露/metrics监控指标并且是prometheus数据格式；2、通过/metrics收集每个Pod的http_request_total指标；3、prometheus将收集到的信息汇总；4、APIServer定时从Prometheus查询，获取request_per_second的数据；5、HPA定期向APIServer查询以判断是否符合配置的autoscaler规则；6、如果符合autoscaler规则，则修改Deployment的ReplicaSet副本数量进行伸缩。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S之Ingress-Nginx实现高可用]]></title>
    <url>%2F2019%2F12%2F10%2FK8S%E4%B9%8BIngress-Nginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[承接上文，我们部署好了ingress，为了达到生产级的阈值，我们必须要要配置ingress的高可用： 假定我们在Kubernetes 指定两个worker节点中部署了ingress nginx来为后端的pod做proxy，这时候我们就需要通过keepalived实现高可用，提供对外的VIP，也就是externalLB的upstream只需要绑定此VIP即可。 首先我们要先确保有两个worker节点部署了ingress nginx 在本实验中，环境如下： IP地址 主机名 描述 10.0.0.31 k8s-master01 10.0.0.34 k8s-node02 ingress nginx、keepalived 10.0.0.35 k8s-node03 ingress nginx、keepalived 1、查看ingress nginx状态[root@k8s-master01 Ingress]# kubectl get pod -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-85bd8789cd-8c4xh 1/1 Running 0 62s 10.0.0.34 k8s-node02 &lt;none&gt; &lt;none&gt;nginx-ingress-controller-85ff8dfd88-vqkhx 1/1 Running 0 3m56s 10.0.0.35 k8s-node03 &lt;none&gt; &lt;none&gt; 创建一个用于测试环境的namespace kubectl create namespace test 2、部署一个Deployment（用于测试）apiVersion: apps/v1kind: Deploymentmetadata: name: myweb-deploy # 部署在测试环境 namespace: testspec: replicas: 3 selector: matchLabels: name: myweb type: test template: metadata: labels: name: myweb type: test spec: containers: - name: nginx image: nginx:1.13 imagePullPolicy: IfNotPresent ports: - containerPort: 80---# serviceapiVersion: v1kind: Servicemetadata: name: myweb-svcspec: selector: name: myweb type: test ports: - port: 80 targetPort: 80 protocol: TCP---# ingress 执行kubectl create 创建deployment kubectl create -f myweb-demo.yaml 查看deployment是否部署成功[root@k8s-master01 Project]# kubectl get pods -n test -o wide | grep &quot;myweb&quot;myweb-deploy-6d586d7db4-2g5ll 1/1 Running 0 23s 10.244.3.240 k8s-node02 &lt;none&gt; &lt;none&gt;myweb-deploy-6d586d7db4-cf7w7 1/1 Running 0 4m2s 10.244.1.132 k8s-node01 &lt;none&gt; &lt;none&gt;myweb-deploy-6d586d7db4-rp5zc 1/1 Running 0 3m59s 10.244.2.5 k8s-node03 &lt;none&gt; 3、在两个worker节点部署keepalivedVIP：10.0.0.130，接口：eth0 3.1、安装keepalivedyum -y install keepalived 3.1.1、k8s-node03节点作为MASTER配置keepalived[root@k8s-node03 ~]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email_from Alexandre.Cassen@firewall.loc router_id k8s-node03 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 51 priority 110 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.130/24 dev eth0 label eth0:1 &#125;&#125; 3.1.2、k8s-node02节点作为BACKUP配置keeplived[root@k8s-node02 ~]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id k8s-node02 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.130/24 dev eth0 label eth0:1 &#125;&#125; 两个节点启动keepalived并加入开机启动 systemctl start keepalived.servicesystemctl enable keepalived.service 启动完成后检查k8s-node03的IP地址是否已有VIP [root@k8s-node03 ~]# ip add | grep &quot;130&quot; inet 10.0.0.130/24 scope global secondary eth0:1 然后我们在externalLB上的后端配置此VIP，即可实现预期的效果！！ 效果不展示了！自己实验便是最好的展示！！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之存储卷及pvc]]></title>
    <url>%2F2019%2F12%2F09%2Fk8s%E4%B9%8B%E5%AD%98%E5%82%A8%E5%8D%B7%E5%8F%8Apvc%2F</url>
    <content type="text"><![CDATA[一、概述因为pod是有生命周期的,pod一重启,里面的数据就没了,所以我们需要数据持久化存储,在k8s中,存储卷不属于容器,而是属于pod,也就是说同一个pod中的容器可以共享一个存储卷,存储卷可以是宿主机上的目录,也可以是挂载在宿主机上的外部设备。 1.1、存储卷类型： emptyDIR存储卷：pod一重启,存储卷也删除,这叫emptyDir存储卷,一般用于当做临时空间或缓存关系; hostPath存储卷：宿主机上目录作为存储卷,这种也不是真正意义实现了数据持久性; SAN(iscsi)或NAS(nfs、cifs)：网络存储设备; 分布式存储：ceph,glusterfs,cephfs,rbd； 云存储：亚马逊的EBS,Azure Disk,阿里云,关键数据一定要有异地备份； emptyDIR存储卷：vim podtest/pod-vol-demo.yamlapiVersion: v1kind: Podmetadata: name: pod-demo namespace: default labels: app: myapp tier: frontendspec: containers: - name: myapp image: ikubernetes/myapp:v2 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent volumeMounts: - name: html mountPath: /data/ command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;,&quot;while true;do echo $(date) &gt;&gt; /data/index.html; sleep 10;done&quot;] volumes: - name: html emptyDir: &#123;&#125;volumeMounts:把哪个存储卷挂到pod中的哪个目录下；emptyDir:不设置意味着对这个参数下的两个选项不做限制； hostPath:使用宿主机上目录作为存储卷kubectl explain pods.spec.volumes.hostPath.typeDirectoryOrCreate:要挂载的路径是一个目录,不存在就创建目录;Directory:宿主机上必须实现存在目录,如果不存在就报错;FileOrCreate:表示挂载的是文件,如果不存在就创建;File:表示要挂载的文件必须事先存在,否则就报错. cat pod-hostpath-vol.yamlapiVersion: v1kind: Podmetadata: name: pod-vol-hostpath namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v2 volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html hostPath: path: /data/pod/volume1 type: DirectoryOrCreate hostPath:宿主机上的目录；volumes的名字可以随便取,这是存储卷的名字,但是上面的volumeMounts指定时,name必须和存储卷的名字一致,这样两者才建立了联系； nfs做共享存储这里为了方便,把master节点当做nfs存储,三个节点均执行：yum -y install nfs-utils # 然后在master上启动nfsmkdir /data/volumescat /etc/exports/data/volumes 10.0.0.0/16(rw,no_root_squash)systemctl start nfs在node1和node2上试挂载mount -t nfs k8s-master:/data/volumes /mntcat pod-vol-nfs.yamlapiVersion: v1kind: Podmetadata: name: pod-vol-nfs namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v2 volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html nfs: path: /data/volumes server: k8s-master kubectl apply -f pod-vol-nfs.yaml此时不管pod被建立在哪个节点上,对应节点上是不存放数据的,数据都在nfs主机上 pvc和pv用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。pvc和pv的关系与pod和node关系类似，前者消耗后者的资源，pvc可以向pv申请指定大小的存储资源并设置访问模式。 在定义pod时,我们只需要说明我们要一个多大的存储卷就行了,pvc存储卷必须与当前namespace的pvc建立直接绑定关系,pvc必须与pv建立绑定关系,而pv是真正的某个存储设备上的空间. 一个pvc和pv是一一对应关系,一旦一个pv被一个pvc绑定了,那么这个pv就不能被其他pvc绑定了,一个pvc是可以被多个pod所访问的,pvc在名称空间中,pv是集群级别的。 将master作为存储节点,创建挂载目录cd /data/volumes &amp;&amp; mkdir v&#123;1,2,3,4,5&#125;cat /etc/exports/data/volumes/v1 10.0.0.0/16(rw,no_root_squash)/data/volumes/v2 10.0.0.0/16(rw,no_root_squash)/data/volumes/v3 10.0.0.0/16(rw,no_root_squash)exportfs -arvshowmount -ekubectl explain pv.spec.nfsaccessModes模式有:ReadWriteOnce:单路读写,可以简写为RWO;ReadOnlyMany:多路只读,可以简写为ROX;ReadWriteMany:多路读写,可以简写为RWX # 先将存储设备定义为pvcat pv-demo.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv001 # 定义pv时不用加名称空间,因为pv是集群级别 labels: name: pv001spec: nfs: path: /data/volumes/v1 server: k8s-master accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;] capacity: # 分配磁盘空间大小 storage: 3Gi---apiVersion: v1kind: PersistentVolumemetadata: name: pv002 labels: name: pv002spec: nfs: path: /data/volumes/v2 server: k8s-master accessModes: [&quot;ReadWriteOnce&quot;] capacity: storage: 5Gi---apiVersion: v1kind: PersistentVolumemetadata: name: pv003 labels: name: pv003spec: nfs: path: /data/volumes/v3 server: k8s-master accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;] capacity: storage: 8Gi kubectl apply -f pv-demo.yamlkubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS pv001 3Gi RWO,RWX Retain Availablepv002 5Gi RWO Retain Availablepv003 8Gi RWO,RWX Retain Available 回收策略：如果某个pvc在pv里面存数据了,后来pvc删了,那么pv里面的数据怎么处理？ reclaim_policy：即pvc删了,但pv里面的数据不删除,还保留着； recycle：即pvc删了,那么就把pv里面的数据也删了； delete：即pvc删了,那么就把pv也删了； # 创建pvc的清单文件kubectl explain pods.spec.volumes.persistentVolumeClaimcat pod-vol-pvc.yamlapiVersion: v1kind: PersistentVolumeClaim # 简称pvcmetadata: name: mypvc namespace: default # pvc和pod在同一个名称空间spec: accessModes: [&quot;ReadWriteMany&quot;] # 一定是pv策略的子集 resources: requests: storage: 7Gi # 申请一个大小至少为7G的pv---apiVersion: v1kind: Podmetadata: name: pod-vol-pvc namespace: defaultspec: containers: - name: myapp image: ikubernetes/myapp:v1 volumeMounts: - name: html # 使用的存储卷的名字 mountPath: /usr/share/nginx/html/ #挂载路径 volumes: - name: html persistentVolumeClaim: claimName: mypvc # 表示要使用哪个pvc 所以pod的存储卷类型如果是pvc,则:pod指定的pvc需要先匹配一个pv,才能被pod所挂载,在k8s 1.10之后,不能手工从底层删除pv。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S负载均衡之nginx-ingress]]></title>
    <url>%2F2019%2F12%2F07%2FK8S%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E4%B9%8Bnginx-ingress%2F</url>
    <content type="text"><![CDATA[++本篇引自多篇大佬文档整合而来，加上自己所理解整理如下：++ 一、目前状况：k8s有了coreDNS解决了k8s集群内部通过dns域名的方式相互访问容器服务，但是集群内部的域名无法在外部被访问，也没有解决域名7层负载均衡的问题，而nginx-ingress就是为了解决基于k8s的7层负载均衡，nginx-ingress也是已addon方式加入k8s集群，以pod的方式运行，多个副本，高可用。 二、Nginx Ingress 一般有三个组件组成： 1）ingress是kubernetes的一个资源对象，用于编写定义规则。 2）反向代理负载均衡器，通常以Service的Port方式运行，接收并按照ingress定义的规则进行转发，通常为nginx，haproxy，traefik等，本文使用nginx。 3）ingress-controller，监听apiserver，获取服务新增，删除等变化，并结合ingress规则动态更新到反向代理负载均衡器上，并重载配置使其生效。 以上三者有机的协调配合起来，就可以完成 Kubernetes 集群服务的暴露。 来看个图例： Nginx 对后端运行的服务（Service1、Service2）提供反向代理，在配置文件中配置了域名与后端服务 Endpoints 的对应关系。客户端通过使用 DNS 服务或者直接配置本地的 hosts 文件，将域名都映射到 Nginx 代理服务器。当客户端访问 service1.com 时，浏览器会把包含域名的请求发送给 nginx 服务器，nginx 服务器根据传来的域名，选择对应的 Service，这里就是选择 Service 1 后端服务，然后根据一定的负载均衡策略，选择 Service1 中的某个容器接收来自客户端的请求并作出响应。过程很简单，nginx 在整个过程中仿佛是一台根据域名进行请求转发的“路由器”，这也就是7层代理的整体工作流程了！ 对于 Nginx 反向代理做了什么，我们已经大概了解了。在 k8s 系统中，后端服务的变化是十分频繁的，单纯依靠人工来更新nginx 的配置文件几乎不可能，nginx-ingress 由此应运而生。Nginx-ingress 通过监视 k8s 的资源状态变化实现对 nginx 配置文件的自动更新，下面本文就来分析下其工作原理。 2.1、nginx-ingress 工作流程分析首先，上一张整体工作模式架构图（只关注配置同步更新）： 不考虑 nginx 状态收集等附件功能，nginx-ingress 模块在运行时主要包括三个主体：NginxController、Store、SyncQueue。 Store 主要负责从 kubernetes APIServer 收集运行时信息，感知各类资源（如 ingress、service等）的变化，并及时将更新事件消息（event）写入一个环形管道； SyncQueue 协程定期扫描 syncQueue 队列，发现有任务就执行更新操作，即借助 Store 完成最新运行数据的拉取，然后根据一定的规则产生新的 nginx 配置，（有些更新必须reload，就本地写入新配置，执行 reload），然后执行动态更新操作，即构造 POST 数据，向本地 Nginx Lua 服务模块发送 post 请求，实现配置更新； NginxController 作为中间的联系者，监听 updateChannel，一旦收到配置更新事件，就向同步队列 syncQueue 里写入一个更新请求。 大白话描述下： 1、ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置； 2、再写到nginx-ingress-control的pod里，这个Ingress； controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中； 3、然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。 2.2、Ingress 可以解决什么问题1、动态配置服务 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作。 2、减少不必要的端口暴露 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式。 2.3、Pod与Ingress的关系 通过Ingress Controller实现Pod的负载均衡, 支持TCP/UDP 4层和HTTP 7层Ingress 只是定义规则，具体的负载均衡服务是由Ingress controller控制器完成。 访问流程：用户—&gt; Ingress Controller(Node) —&gt;service —&gt; Pod 三、部署nginx-ingress-controller以及定义ingress策略20191205最新版目前： 获取配置文件位置: https://github.com/kubernetes/ingress-nginx/tree/nginx-0.26.1/deploy 3.1、下载部署文件提供了两种方式 ： 默认下载最新的yaml： wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 指定版本号下载对应的yaml； 修改镜像路径image [root@localhost src]# grep image mandatory.yaml image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 上面的镜像我没办法pull下来，改成使用阿里的google库 image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 修改后的yaml：apiVersion: v1kind: Namespacemetadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx --- kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; - &quot;networking.k8s.io&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; - &quot;networking.k8s.io&quot; resources: - ingresses/status verbs: - update ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps resourceNames: # Defaults to &quot;&lt;election-id&gt;-&lt;ingress-class&gt;&quot; # Here: &quot;&lt;ingress-controller-leader&gt;-&lt;nginx&gt;&quot; # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - &quot;ingress-controller-leader-nginx&quot; verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx ---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx --- apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: # wait up to five minutes for the drain of connections terminationGracePeriodSeconds: 300 serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: allowPrivilegeEscalation: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 lifecycle: preStop: exec: command: - /wait-shutdown--- mandatory.yaml这一个yaml中包含了很多资源的创建，包括namespace、ConfigMap、role，ServiceAccount等等所有部署ingress-controller需要的资源，配置太多就不粘出来了，我们重点看下如上deployment部分↑ 可以看到主要使用了“registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1”这个镜像 指定了一些启动参数。同时开放了80与443两个端口，并在10254端口做了健康检查。 然后修改上面mandatory.yaml的deployment部分配置为：# 修改api版本及kind# apiVersion: apps/v1# kind: DeploymentapiVersion: apps/v1kind: DaemonSetmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec:# 删除Replicas# replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: &quot;10254&quot; prometheus.io/scrape: &quot;true&quot; spec: serviceAccountName: nginx-ingress-serviceaccount # 选择对应标签的node nodeSelector: isIngress: &quot;true&quot; # 使用hostNetwork暴露服务 hostNetwork: true containers: - name: nginx-ingress-controller image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.26.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: allowPrivilegeEscalation: true capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10--- kind: DaemonSet：官方原始文件使用的是deployment，replicate 为 1，这样将会在某一台节点上启动对应的nginx-ingress-controller pod。外部流量访问至该节点，由该节点负载分担至内部的service。测试环境考虑防止单点故障，改为DaemonSet然后删掉replicate ，配合亲和性部署在制定节点上启动nginx-ingress-controller pod，确保有多个节点启动nginx-ingress-controller pod，后续将这些节点加入到外部硬件负载均衡组实现高可用性。 hostNetwork: true：添加该字段，暴露nginx-ingress-controller pod的服务端口（80） nodeSelector: 增加亲和性部署，有isIngress=”true” 标签的节点才会部署该DaemonSet 为需要部署nginx-ingress-controller的节点设置lable，这里测试部署在”k8s-node1，k8s-node2，k8s-node3”这个节点。 $ kubectl label node k8s-node1 isIngress=&quot;true&quot;$ kubectl label node k8s-node2 isIngress=&quot;true&quot;$ kubectl label node k8s-node3 isIngress=&quot;true&quot; 执行yaml文件部署 [root@k8s-master1 src]# kubectl apply -f mandatory.yaml # 执行结果 namespace/ingress-nginx createdconfigmap/nginx-configuration createdconfigmap/tcp-services createdconfigmap/udp-services createdserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createddaemonset.apps/nginx-ingress-controller created 3.2、检查部署情况（此处个人电脑资源有限我就打了一个node的tag如下：）[root@k8s-master1 src]# kubectl get daemonset -n ingress-nginxNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEnginx-ingress-controller 1 1 1 1 1 isIngress=true 3m24s[root@k8s-master1 src]# kubectl get po -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-ql4x5 1/1 Running 0 3m39s 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt; 可以看到，nginx-controller的pod已经部署在在k8s-node1上了。 到node-1上看下本地端口：[root@k8s-node1 ~]# netstat -lntp | grep nginxtcp 0 0 127.0.0.1:10247 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:8181 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 34132/nginx: mastertcp 0 0 127.0.0.1:10245 0.0.0.0:* LISTEN 34078/nginx-ingresstcp 0 0 127.0.0.1:10246 0.0.0.0:* LISTEN 34132/nginx: mastertcp6 0 0 :::10254 :::* LISTEN 34078/nginx-ingresstcp6 0 0 :::80 :::* LISTEN 34132/nginx: mastertcp6 0 0 :::8181 :::* LISTEN 34132/nginx: mastertcp6 0 0 :::443 :::* LISTEN 34132/nginx: master 由于配置了hostnetwork，nginx已经在node主机本地监听80/443/8181端口。其中8181是nginx-controller默认配置的一个default backend。这样，只要访问node主机有公网IP，就可以直接映射域名来对外网暴露服务了。如果要nginx高可用的话，可以在多个node上部署，并在前面再搭建一套LVS+keepalive做负载均衡。用hostnetwork的另一个好处是，如果lvs用DR模式的话，是不支持端口映射的，这时候如果用nodeport，暴露非标准的端口，管理起来会很麻烦。 划重点：生产须知将keepalived与ingress关联现状：因为pod可以分配在很多node上，若域名与一个node节点绑定，这一个node服务器出现问题，则这个域名就挂了，不能实现高可用解决：将每个node上装上keepalived服务，设置vip，主master，备用的backup，然后域名 绑定到 vip上就实现高可用（假如10台node，其中1台设置为master，其余9台设置为backup，一旦master挂了，其余9台马上顶替，到时候域名直接绑定虚拟vip即可） 3.3、部署service用于对外提供服务wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml修改service文件，指定一下nodePort，使用30080端口和30443端口作为nodePort 修改后的配置文件如下 apiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP nodePort: 30080 # http请求对外映射30080端口 - name: https port: 443 targetPort: 443 protocol: TCP nodePort: 30443 # https请求对外映射30443端口 selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx--- 3.4、部署一个tomcat用于测试ingress转发功能vim k8s-tomcat-test.yamlapiVersion: v1kind: Servicemetadata: name: tomcat namespace: defaultspec: selector: app: tomcat release: canary ports: - name: http targetPort: 8080 port: 8080 - name: ajp targetPort: 8009 port: 8009 --- apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deploy namespace: defaultspec: replicas: 1 selector: matchLabels: app: tomcat release: canary template: metadata: labels: app: tomcat release: canary spec: containers: - name: tomcat image: tomcat ports: - name: http containerPort: 8080 3.5、定义ingress策略vim k8s-tomcat-test-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-tomcat annotations: kubernets.io/ingress.class: &quot;nginx&quot;spec: rules: - host: myapp.zhdya.com http: paths: - path: backend: serviceName: tomcat servicePort: 8080 手动绑定hosts测试：192.168.171.188 myapp.zhdya.com 把tomcat service通过ingress发布出去： 在浏览器输入：http://myapp.zhdya.com:30080/ 3.6、下面我们对tomcat服务添加https服务[root@k8s-master ingress-nginx]# openssl genrsa -out tls.key 2048Generating RSA private key, 2048 bit long modulus.......+++..............................+++e is 65537 (0x10001)[root@k8s-master ingress-nginx]# openssl req -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=myapp.zhdya.com #注意域名要和服务的域名一致 [root@k8s-master ingress-nginx]# kubectl create secret tls tomcat-ingress-secret --cert=tls.crt --key=tls.key #创建secretsecret &quot;tomcat-ingress-secret&quot; created[root@k8s-master ingress-nginx]# kubectl get secretNAME TYPE DATA AGEdefault-token-bf52l kubernetes.io/service-account-token 3 9dtomcat-ingress-secret kubernetes.io/tls 2 7s[root@k8s-master ingress-nginx]# kubectl describe secret tomcat-ingress-secretName: tomcat-ingress-secretNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: kubernetes.io/tlsData====tls.crt: 1294 bytes #base64加密tls.key: 1679 bytes 将证书应用至tomcat服务中[root@k8s-master01 ingress]# vim k8s-tomcat-test-ingress-tls.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-tomcat-tls annotations: kubernets.io/ingress.class: &quot;nginx&quot;spec: tls: - hosts: - myapp.zhdya.com #与secret证书的域名需要保持一致 secretName: tomcat-ingress-secret #secret证书的名称 rules: - host: myapp.zhdya.com http: paths: - path: backend: serviceName: tomcat servicePort: 8080[root@k8s-master01 ingress]# kubectl apply -f k8s-tomcat-test-ingress-tls.yaml 再次访问服务： https://myapp.zhdya.com:30443/ 文末彩蛋：从3.3-3.6大家有没有发现，我在测试tomcat的容器的时候手动创建了一个service-nodeport.yaml 这个文件，这个文件刚刚也讲到了，就是为了把容器内部的服务暴露出来，当然我们自己也测试了手动绑定了tomcat pod容器所在node节点的IP192.168.171.188 myapp.zhdya.com 之前我是为了给大家证明，内部pod是如何把service从pod中暴露出来，但是细心的人肯定发现了，在3.1章节，我们明明已经创建了一个nginx-ingress-controller 这个就完全可以帮我们完成服务暴露啊。对的，非常对！！！ nginx-ingress-controller这个重要的组件具体实现了什么功能看文首！！ [root@k8s-master1 ~]# kubectl get po,svc,ep --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdefault pod/busybox 1/1 Running 7 8d 10.244.0.16 k8s-node1 &lt;none&gt; &lt;none&gt;default pod/tomcat-deploy-758b795dcc-69gjz 1/1 Running 1 2d6h 10.244.0.17 k8s-node1 &lt;none&gt; &lt;none&gt;default pod/tomcat-deploy-758b795dcc-llcp5 1/1 Running 2 2d6h 10.244.1.17 k8s-node2 &lt;none&gt; &lt;none&gt;default pod/web-d86c95cc9-k9vnf 1/1 Running 4 9d 10.244.1.20 k8s-node2 &lt;none&gt; &lt;none&gt;default pod/web-d86c95cc9-x2wn6 1/1 Running 4 8d 10.244.0.18 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-ql4x5 1/1 Running 0 47m 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/coredns-6d8cfdd59d-gbd2m 1/1 Running 5 8d 10.244.0.19 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-d2gzx 1/1 Running 3 9d 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-lwsnd 1/1 Running 4 9d 192.168.171.137 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-wrkfl 1/1 Running 3 9d 10.244.1.19 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/kubernetes-dashboard-7b5bf5d559-csfwm 1/1 Running 4 9d 10.244.1.18 k8s-node2 &lt;none&gt; &lt;none&gt; [root@k8s-master1 src]# kubectl exec -it nginx-ingress-controller-ql4x5 -n ingress-nginx -- cat nginx.conf...start省略不重要的配置...## start server myapp.zhdya.com server &#123; server_name myapp.zhdya.com ; listen 80 ; listen [::]:80 ; listen 443 ssl http2 ; listen [::]:443 ssl http2 ; set $proxy_upstream_name &quot;-&quot;; ssl_certificate_by_lua_block &#123; certificate.call() &#125; location / &#123; set $namespace &quot;default&quot;; set $ingress_name &quot;ingress-tomcat&quot;; set $service_name &quot;tomcat&quot;; set $service_port &quot;8080&quot;; set $location_path &quot;/&quot;; rewrite_by_lua_block &#123; lua_ingress.rewrite(&#123; force_ssl_redirect = false, ssl_redirect = true, force_no_ssl_redirect = false, use_port_in_redirects = false, &#125;) balancer.rewrite() plugins.run() &#125; header_filter_by_lua_block &#123; plugins.run() &#125; body_filter_by_lua_block &#123; &#125; log_by_lua_block &#123; balancer.log() monitor.call() plugins.run() &#125; port_in_redirect off; set $balancer_ewma_score -1; set $proxy_upstream_name &quot;default-tomcat-8080&quot;; set $proxy_host $proxy_upstream_name; set $pass_access_scheme $scheme; set $pass_server_port $server_port; set $best_http_host $http_host; set $pass_port $pass_server_port; set $proxy_alternative_upstream_name &quot;&quot;; client_max_body_size 1m; proxy_set_header Host $best_http_host; # Pass the extracted client certificate to the backend # Allow websocket connections proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_set_header X-Request-ID $req_id; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Host $best_http_host; proxy_set_header X-Forwarded-Port $pass_port; proxy_set_header X-Forwarded-Proto $pass_access_scheme; proxy_set_header X-Scheme $pass_access_scheme; # Pass the original X-Forwarded-For proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for; # mitigate HTTPoxy Vulnerability # https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/ proxy_set_header Proxy &quot;&quot;; # Custom headers to proxied server proxy_connect_timeout 5s; proxy_send_timeout 60s; proxy_read_timeout 60s; proxy_buffering off; proxy_buffer_size 4k; proxy_buffers 4 4k; proxy_max_temp_file_size 1024m; proxy_request_buffering on; proxy_http_version 1.1; proxy_cookie_domain off; proxy_cookie_path off; # In case of errors try the next upstream server before returning an error proxy_next_upstream error timeout; proxy_next_upstream_timeout 0; proxy_next_upstream_tries 3; proxy_pass http://upstream_balancer; proxy_redirect off; &#125; &#125; ## end server myapp.zhdya.com...end省略不重要的配置... 建议大家一定要把这个nginx.conf文件细细的看下你就会证实nginx-ingress-controller 这个组件的功劳是多么的强大！！ 然后我们换掉之前手动绑定的hosts，变更为 ingress-nginx pod所在的节点：192.168.171.136 myapp.zhdya.com 再次访问 是不是就不需要所谓的 30443端口了呢？？？ 再然后，小伙伴们也知道了外层的externalLB改如何操作和绑定了吧？当然还有我文中的重点！！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8S集群优化之路由转发：使用IPVS替代iptables]]></title>
    <url>%2F2019%2F12%2F06%2FK8S%E9%9B%86%E7%BE%A4%E4%BC%98%E5%8C%96%E4%B9%8B%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91%EF%BC%9A%E4%BD%BF%E7%94%A8IPVS%E6%9B%BF%E4%BB%A3iptables%2F</url>
    <content type="text"><![CDATA[一、为什么要使用IPVS从k8s的1.8版本开始，kube-proxy引入了IPVS模式，IPVS模式与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 二、具体步骤2.1、开启内核参数cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1EOFsysctl -p 2.2、开启ipvs支持yum -y install ipvsadm ipset# 临时生效modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4# 永久生效cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOFmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF 2.3、配置kube-proxy# 添加下面两行 --proxy-mode=ipvs \ --masquerade-all=true \# 修改服务文件vim /usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/data/k8s/kube-proxyExecStart=/data/k8s/bin/kube-proxy \ --bind-address=192.168.1.145 \ --hostname-override=192.168.1.145 \ --cluster-cidr=10.254.0.0/16 \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --logtostderr=true \ --proxy-mode=ipvs \ --masquerade-all=true \ --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 2.4、重启kube-proxysystemctl daemon-reloadsystemctl restart kube-proxysystemctl status kube-proxy 三、测试测试是否生效[root@k8sNode01 docker]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.1.142:6443 Masq 1 0 0 -&gt; 192.168.1.143:6443 Masq 1 1 0 -&gt; 192.168.1.144:6443 Masq 1 1 0 TCP 10.254.27.38:80 rr -&gt; 172.30.36.4:9090 Masq 1 0 0 TCP 10.254.72.60:80 rr -&gt; 172.30.90.4:8080 Masq 1 0 0 TCP 10.254.72.247:80 rr -&gt; 172.30.36.5:3000 Masq 1 0 0 TCP 127.0.0.1:27841 rr -&gt; 172.30.36.2:80 Masq 1 0 0 -&gt; 172.30.90.2:80 Masq 1 0 0 TCP 127.0.0.1:28453 rr -&gt; 172.30.36.5:3000 Masq 1 0 0 TCP 127.0.0.1:36018 rr -&gt; 172.30.36.4:9090 Masq 1 0 0 TCP 172.30.90.0:27841 rr -&gt; 172.30.36.2:80 Masq 1 0 0 -&gt; 172.30.90.2:80 Masq 1 0 0]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible自动化部署kubernetes-1.16]]></title>
    <url>%2F2019%2F12%2F05%2Fansible%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2kubernetes-1.16%2F</url>
    <content type="text"><![CDATA[概述：集群包含coreDNS、cni、nginx-ingress、HA、flanneld 百度网盘链接：https://pan.baidu.com/s/1KYbpshhpTu62DnQwF1LUnQ 提取码：vi5e 一、单master部署[root@k8s-ansible1 ~]# tree ansible-install-k8s-masteransible-install-k8s-master├── add-node.yml├── ansible.cfg├── group_vars│ └── all.yml├── hosts├── multi-master-deploy.yml├── multi-master.jpg├── README.md├── roles│ ├── addons│ │ ├── files│ │ │ ├── coredns.yaml│ │ │ ├── ingress-controller.yaml│ │ │ ├── kube-flannel.yaml│ │ │ └── kubernetes-dashboard.yaml│ │ └── tasks│ │ └── main.yml│ ├── common│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ └── hosts.j2│ ├── docker│ │ ├── files│ │ │ ├── daemon.json│ │ │ └── docker.service│ │ └── tasks│ │ └── main.yml│ ├── etcd│ │ ├── files│ │ │ └── etcd_cert│ │ │ ├── ca-key.pem│ │ │ ├── ca.pem│ │ │ ├── server-key.pem│ │ │ └── server.pem│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── etcd.conf.j2│ │ ├── etcd.service.j2│ │ └── etcd.sh.j2│ ├── ha│ │ ├── files│ │ │ └── check_nginx.sh│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── keepalived.conf.j2│ │ └── nginx.conf.j2│ ├── master│ │ ├── files│ │ │ ├── apiserver-to-kubelet-rbac.yaml│ │ │ ├── etcd_cert│ │ │ │ ├── ca.pem│ │ │ │ ├── server-key.pem│ │ │ │ └── server.pem│ │ │ ├── k8s_cert│ │ │ │ ├── admin-key.pem│ │ │ │ ├── admin.pem│ │ │ │ ├── ca-key.pem│ │ │ │ ├── ca.pem│ │ │ │ ├── kube-proxy-key.pem│ │ │ │ ├── kube-proxy.pem│ │ │ │ ├── server-key.pem│ │ │ │ └── server.pem│ │ │ ├── kubelet-bootstrap-rbac.yaml│ │ │ └── token.csv│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── kube-apiserver.conf.j2│ │ ├── kube-apiserver.service.j2│ │ ├── kube-controller-manager.conf.j2│ │ ├── kube-controller-manager.service.j2│ │ ├── kube-scheduler.conf.j2│ │ └── kube-scheduler.service.j2│ ├── node│ │ ├── files│ │ │ └── k8s_cert│ │ │ ├── ca.pem│ │ │ ├── kube-proxy-key.pem│ │ │ └── kube-proxy.pem│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ ├── bootstrap.kubeconfig.j2│ │ ├── kubelet-config.yml.j2│ │ ├── kubelet.conf.j2│ │ ├── kubelet.service.j2│ │ ├── kube-proxy-config.yml.j2│ │ ├── kube-proxy.conf.j2│ │ ├── kube-proxy.kubeconfig.j2│ │ └── kube-proxy.service.j2│ └── tls│ ├── files│ │ ├── generate_etcd_cert.sh│ │ └── generate_k8s_cert.sh│ ├── tasks│ │ └── main.yml│ └── templates│ ├── etcd│ │ ├── ca-config.json.j2│ │ ├── ca-csr.json.j2│ │ └── server-csr.json.j2│ └── k8s│ ├── admin-csr.json.j2│ ├── ca-config.json.j2│ ├── ca-csr.json.j2│ ├── kube-proxy-csr.json.j2│ └── server-csr.json.j2├── single-master-deploy.yml└── single-master.jpg 1.1、解压缩binary_pkg.tar.gz[root@k8s-ansible1 ~]# lsanaconda-ks.cfg ansible-install-k8s-master ansible-install-k8s-master.zip binary_pkg.tar.gz[root@k8s-ansible1 ~]# tar zxvf binary_pkg.tar.gz 1.2、修改所以节点hosts 及ansible内的hosts[root@k8s-ansible1 ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.171.11 k8s-ansible1 ##ansible机器192.168.171.12 k8s-ansible2 ##master192.168.171.13 k8s-ansible3 ##node1192.168.171.14 k8s-ansible4 ##node2[root@k8s-ansible1 ~]# cd ansible-install-k8s-master[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts[master]# 如果部署单Master，只保留一个Master节点192.168.171.12 node_name=k8s-ansible2#192.168.171.111 node_name=k8s-master2[node]192.168.171.13 node_name=k8s-ansible3192.168.171.14 node_name=k8s-ansible4[etcd]192.168.171.12 etcd_name=k8s-ansible2192.168.171.13 etcd_name=k8s-ansible3192.168.171.14 etcd_name=k8s-ansible4[lb]# 如果部署单Master，该项忽略192.168.31.63 lb_name=lb-master192.168.31.71 lb_name=lb-backup[k8s:children]masternode[newnode]#192.168.31.91 node_name=k8s-node3 1.3、更改全局环境配置[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml# 安装目录software_dir: &apos;/root/binary_pkg&apos;k8s_work_dir: &apos;/opt/kubernetes&apos;etcd_work_dir: &apos;/opt/etcd&apos;tmp_dir: &apos;/tmp/k8s&apos;# 集群网络service_cidr: &apos;10.0.0.0/24&apos;cluster_dns: &apos;10.0.0.2&apos; # 与roles/addons/files/coredns.yaml中IP一致pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: &apos;30000-32767&apos;cluster_domain: &apos;cluster.local&apos;# 高可用，如果部署单Master，该项忽略vip: &apos;192.168.31.88&apos;nic: &apos;ens33&apos;# 自签证书可信任IP列表，为方便扩展，可添加多个预留IPcert_hosts: # 包含所有LB、VIP、Master（多多益善，可以多余出来几个后期扩展用） IP和service_cidr的第一个IP k8s: - 10.0.0.1 - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 - 192.168.171.15 - 192.168.171.16 - 192.168.171.17 - 192.168.171.18 - 192.168.171.19 - 192.168.171.111 # 包含所有etcd节点IP etcd: - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 二、准备部署2.1、单Master版：ansible-playbook -i hosts single-master-deploy.yml -uroot -k 2.2、历史记录[root@k8s-ansible1 ansible-install-k8s-master]# ansible-playbook -i hosts single-master-deploy.yml -uroot -kSSH password:PLAY [0.系统初始化] ********************************************************************************************************************************************************TASK [common : 关闭firewalld] *******************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.12]ok: [192.168.171.13]TASK [common : 关闭selinux] *********************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.13]ok: [192.168.171.12]TASK [common : 关闭swap] ************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [common : 即时生效] **************************************************************************************************************************************************changed: [192.168.171.13]changed: [192.168.171.12]changed: [192.168.171.14]TASK [common : 拷贝时区] **************************************************************************************************************************************************ok: [192.168.171.14]ok: [192.168.171.12]ok: [192.168.171.13]TASK [common : 添加hosts] ***********************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.14]ok: [192.168.171.13]PLAY [1.自签证书] *********************************************************************************************************************************************************TASK [tls : 获取Ansible工作目录] ********************************************************************************************************************************************changed: [localhost]TASK [tls : 创建工作目录] ***************************************************************************************************************************************************ok: [localhost] =&gt; (item=etcd)ok: [localhost] =&gt; (item=k8s)TASK [tls : 准备cfssl工具] ************************************************************************************************************************************************ok: [localhost]TASK [tls : 准备etcd证书请求文件] *********************************************************************************************************************************************ok: [localhost] =&gt; (item=ca-config.json.j2)ok: [localhost] =&gt; (item=ca-csr.json.j2)ok: [localhost] =&gt; (item=server-csr.json.j2)TASK [tls : 准备生成etcd证书脚本] *********************************************************************************************************************************************ok: [localhost]TASK [tls : 生成etcd证书] *************************************************************************************************************************************************changed: [localhost]TASK [tls : 准备k8s证书请求文件] **********************************************************************************************************************************************ok: [localhost] =&gt; (item=ca-config.json.j2)ok: [localhost] =&gt; (item=ca-csr.json.j2)ok: [localhost] =&gt; (item=server-csr.json.j2)ok: [localhost] =&gt; (item=admin-csr.json.j2)ok: [localhost] =&gt; (item=kube-proxy-csr.json.j2)TASK [tls : 准备生成k8s证书脚本] **********************************************************************************************************************************************ok: [localhost]TASK [tls : 生成k8s证书] **************************************************************************************************************************************************changed: [localhost]PLAY [2.部署Docker] *****************************************************************************************************************************************************TASK [docker : 创建临时目录] ************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 分发并解压docker二进制包] ***************************************************************************************************************************************ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/docker-18.09.6.tgz)TASK [docker : 移动docker二进制文件] *****************************************************************************************************************************************changed: [192.168.171.13]changed: [192.168.171.12]changed: [192.168.171.14]TASK [docker : 分发service文件] *******************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 创建目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 配置docker] **********************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [docker : 启动docker] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [docker : 查看状态] **************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [docker : debug] *************************************************************************************************************************************************ok: [192.168.171.13] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible3&quot;, &quot;ID: O3LF:KDZ3:CXD6:MU6T:3DKL:PS42:6ATX:R4QE:GMI7:QHNO:CVQO:7ZW6&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;ok: [192.168.171.12] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible2&quot;, &quot;ID: DFQT:2YYV:YWY5:IXSS:U6VS:BW7R:6MPH:WCLC:QKOW:Y63I:5TV6:C3HT&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;ok: [192.168.171.14] =&gt; &#123; &quot;docker.stdout_lines&quot;: [ &quot;Containers: 0&quot;, &quot; Running: 0&quot;, &quot; Paused: 0&quot;, &quot; Stopped: 0&quot;, &quot;Images: 0&quot;, &quot;Server Version: 18.09.6&quot;, &quot;Storage Driver: overlay2&quot;, &quot; Backing Filesystem: xfs&quot;, &quot; Supports d_type: true&quot;, &quot; Native Overlay Diff: true&quot;, &quot;Logging Driver: json-file&quot;, &quot;Cgroup Driver: cgroupfs&quot;, &quot;Plugins:&quot;, &quot; Volume: local&quot;, &quot; Network: bridge host macvlan null overlay&quot;, &quot; Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog&quot;, &quot;Swarm: inactive&quot;, &quot;Runtimes: runc&quot;, &quot;Default Runtime: runc&quot;, &quot;Init Binary: docker-init&quot;, &quot;containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84&quot;, &quot;runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30&quot;, &quot;init version: fec3683&quot;, &quot;Security Options:&quot;, &quot; seccomp&quot;, &quot; Profile: default&quot;, &quot;Kernel Version: 3.10.0-957.el7.x86_64&quot;, &quot;Operating System: CentOS Linux 7 (Core)&quot;, &quot;OSType: linux&quot;, &quot;Architecture: x86_64&quot;, &quot;CPUs: 2&quot;, &quot;Total Memory: 1.777GiB&quot;, &quot;Name: k8s-ansible4&quot;, &quot;ID: M3EP:OAKQ:6AMI:RDC6:QX2H:U34M:5GTT:Q2E7:AFP7:C4M3:FUO2:UHS3&quot;, &quot;Docker Root Dir: /var/lib/docker&quot;, &quot;Debug Mode (client): false&quot;, &quot;Debug Mode (server): false&quot;, &quot;Registry: https://index.docker.io/v1/&quot;, &quot;Labels:&quot;, &quot;Experimental: false&quot;, &quot;Insecure Registries:&quot;, &quot; 192.168.31.70&quot;, &quot; 127.0.0.0/8&quot;, &quot;Registry Mirrors:&quot;, &quot; http://bc437cce.m.daocloud.io/&quot;, &quot;Live Restore Enabled: false&quot;, &quot;Product License: Community Engine&quot; ]&#125;PLAY [3.部署ETCD集群] *****************************************************************************************************************************************************TASK [etcd : 创建工作目录] **************************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=bin)ok: [192.168.171.13] =&gt; (item=bin)ok: [192.168.171.14] =&gt; (item=bin)ok: [192.168.171.12] =&gt; (item=cfg)ok: [192.168.171.13] =&gt; (item=cfg)ok: [192.168.171.14] =&gt; (item=cfg)ok: [192.168.171.12] =&gt; (item=ssl)ok: [192.168.171.13] =&gt; (item=ssl)ok: [192.168.171.14] =&gt; (item=ssl)TASK [etcd : 创建临时目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [etcd : 分发并解压etcd二进制包] *******************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)ok: [192.168.171.14] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)ok: [192.168.171.13] =&gt; (item=/root/binary_pkg/etcd-v3.3.13-linux-amd64.tar.gz)TASK [etcd : 移动etcd二进制文件] *********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]TASK [etcd : 分发证书] ****************************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.13] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.13] =&gt; (item=server.pem)changed: [192.168.171.14] =&gt; (item=server.pem)changed: [192.168.171.14] =&gt; (item=server-key.pem)changed: [192.168.171.13] =&gt; (item=server-key.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [etcd : 分发etcd配置文件] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : 分发service文件] *********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : 启动etcd] **************************************************************************************************************************************************changed: [192.168.171.14]changed: [192.168.171.12]changed: [192.168.171.13]TASK [etcd : 分发etcd脚本] ************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]TASK [etcd : 获取etcd集群状态] **********************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [etcd : debug] ***************************************************************************************************************************************************ok: [192.168.171.13] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;ok: [192.168.171.12] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;ok: [192.168.171.14] =&gt; &#123; &quot;status.stdout_lines&quot;: [ &quot;member 5da6acb8be0f9647 is healthy: got healthy result from https://192.168.171.14:2379&quot;, &quot;member a747b20fc3712bbc is healthy: got healthy result from https://192.168.171.12:2379&quot;, &quot;member f351513a2b4642ac is healthy: got healthy result from https://192.168.171.13:2379&quot;, &quot;cluster is healthy&quot; ]&#125;PLAY [4.部署K8S Master] *************************************************************************************************************************************************TASK [master : 创建工作目录] ************************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=bin)changed: [192.168.171.12] =&gt; (item=cfg)changed: [192.168.171.12] =&gt; (item=ssl)changed: [192.168.171.12] =&gt; (item=logs)TASK [master : 创建临时目录] ************************************************************************************************************************************************ok: [192.168.171.12]TASK [master : 分发并解压k8s二进制包] ******************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)TASK [master : 移动k8s master二进制文件] *************************************************************************************************************************************changed: [192.168.171.12]TASK [master : 分发k8s证书] ***********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=ca-key.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [master : 分发etcd证书] **********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=server.pem)changed: [192.168.171.12] =&gt; (item=server-key.pem)TASK [master : 分发token文件] *********************************************************************************************************************************************changed: [192.168.171.12]TASK [master : 分发k8s配置文件] *********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-controller-manager.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-scheduler.conf.j2)TASK [master : 分发service文件] *******************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver.service.j2)changed: [192.168.171.12] =&gt; (item=kube-controller-manager.service.j2)changed: [192.168.171.12] =&gt; (item=kube-scheduler.service.j2)TASK [master : 启动k8s master组件] ****************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kube-apiserver)changed: [192.168.171.12] =&gt; (item=kube-controller-manager)changed: [192.168.171.12] =&gt; (item=kube-scheduler)TASK [master : 查看集群状态] ************************************************************************************************************************************************changed: [192.168.171.12]TASK [master : debug] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;cs.stdout_lines&quot;: [ &quot;NAME AGE&quot;, &quot;scheduler &lt;unknown&gt;&quot;, &quot;controller-manager &lt;unknown&gt;&quot;, &quot;etcd-0 &lt;unknown&gt;&quot;, &quot;etcd-1 &lt;unknown&gt;&quot;, &quot;etcd-2 &lt;unknown&gt;&quot; ]&#125;TASK [master : 拷贝RBAC文件] **********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kubelet-bootstrap-rbac.yaml)changed: [192.168.171.12] =&gt; (item=apiserver-to-kubelet-rbac.yaml)TASK [master : 授权APIServer访问Kubelet与授权kubelet bootstrap] **************************************************************************************************************changed: [192.168.171.12]PLAY [5.部署K8S Node] ***************************************************************************************************************************************************TASK [node : 创建工作目录] **************************************************************************************************************************************************changed: [192.168.171.13] =&gt; (item=bin)changed: [192.168.171.14] =&gt; (item=bin)ok: [192.168.171.12] =&gt; (item=bin)changed: [192.168.171.13] =&gt; (item=cfg)changed: [192.168.171.14] =&gt; (item=cfg)ok: [192.168.171.12] =&gt; (item=cfg)changed: [192.168.171.14] =&gt; (item=ssl)changed: [192.168.171.13] =&gt; (item=ssl)ok: [192.168.171.12] =&gt; (item=ssl)changed: [192.168.171.14] =&gt; (item=logs)ok: [192.168.171.12] =&gt; (item=logs)changed: [192.168.171.13] =&gt; (item=logs)TASK [node : 创建cni插件目录] ***********************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=/opt/cni/bin)changed: [192.168.171.13] =&gt; (item=/opt/cni/bin)changed: [192.168.171.12] =&gt; (item=/opt/cni/bin)changed: [192.168.171.13] =&gt; (item=/etc/cni/net.d)changed: [192.168.171.14] =&gt; (item=/etc/cni/net.d)changed: [192.168.171.12] =&gt; (item=/etc/cni/net.d)TASK [node : 创建临时目录] **************************************************************************************************************************************************ok: [192.168.171.12]ok: [192.168.171.13]ok: [192.168.171.14]TASK [node : 分发并解压k8s二进制包] ********************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/kubernetes-server-linux-amd64-1.16.tar.gz)TASK [node : 分发并解压cni插件二进制包] ******************************************************************************************************************************************changed: [192.168.171.14] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)changed: [192.168.171.12] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)changed: [192.168.171.13] =&gt; (item=/root/binary_pkg/cni-plugins-linux-amd64-v0.8.2.tgz)TASK [node : 移动k8s node二进制文件] *****************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.13]changed: [192.168.171.14]TASK [node : 分发k8s证书] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; (item=ca.pem)changed: [192.168.171.13] =&gt; (item=ca.pem)changed: [192.168.171.14] =&gt; (item=ca.pem)changed: [192.168.171.12] =&gt; (item=kube-proxy.pem)changed: [192.168.171.14] =&gt; (item=kube-proxy.pem)changed: [192.168.171.13] =&gt; (item=kube-proxy.pem)changed: [192.168.171.12] =&gt; (item=kube-proxy-key.pem)changed: [192.168.171.13] =&gt; (item=kube-proxy-key.pem)changed: [192.168.171.14] =&gt; (item=kube-proxy-key.pem)TASK [node : 分发k8s配置文件] ***********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.13] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=bootstrap.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.13] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.12] =&gt; (item=kubelet.conf.j2)changed: [192.168.171.12] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.13] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kubelet-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.kubeconfig.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.conf.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy-config.yml.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy-config.yml.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy-config.yml.j2)TASK [node : 分发service文件] *********************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=kubelet.service.j2)changed: [192.168.171.13] =&gt; (item=kubelet.service.j2)changed: [192.168.171.14] =&gt; (item=kubelet.service.j2)changed: [192.168.171.14] =&gt; (item=kube-proxy.service.j2)changed: [192.168.171.12] =&gt; (item=kube-proxy.service.j2)changed: [192.168.171.13] =&gt; (item=kube-proxy.service.j2)TASK [node : 启动k8s node组件] ********************************************************************************************************************************************changed: [192.168.171.13] =&gt; (item=kubelet)changed: [192.168.171.14] =&gt; (item=kubelet)changed: [192.168.171.12] =&gt; (item=kubelet)changed: [192.168.171.14] =&gt; (item=kube-proxy)changed: [192.168.171.13] =&gt; (item=kube-proxy)changed: [192.168.171.12] =&gt; (item=kube-proxy)TASK [node : 分发预准备镜像] *************************************************************************************************************************************************changed: [192.168.171.14]changed: [192.168.171.13]changed: [192.168.171.12]TASK [node : 导入镜像] ****************************************************************************************************************************************************changed: [192.168.171.12]changed: [192.168.171.14]changed: [192.168.171.13]PLAY [6.部署插件] *********************************************************************************************************************************************************TASK [addons : 允许Node加入集群] ********************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 拷贝YAML文件到Master] ***************************************************************************************************************************************changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/coredns.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/ingress-controller.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kube-flannel.yaml)changed: [192.168.171.12] =&gt; (item=/root/ansible-install-k8s-master/roles/addons/files/kubernetes-dashboard.yaml)TASK [addons : 部署Flannel,Dashboard,CoreDNS,Ingress] *******************************************************************************************************************changed: [192.168.171.12]TASK [addons : 替换Dashboard证书] *****************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 查看Pod状态] ***********************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : debug] *************************************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;getall.stdout_lines&quot;: [ &quot;NAMESPACE NAME READY STATUS RESTARTS AGE&quot;, &quot;kube-system pod/coredns-6d8cfdd59d-hcfw5 0/1 Pending 0 2s&quot;, &quot;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-nk7t8 0/1 Pending 0 1s&quot;, &quot;kubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-cxgb6 0/1 Pending 0 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE&quot;, &quot;default service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 2m19s&quot;, &quot;ingress-nginx service/ingress-nginx ClusterIP 10.0.0.158 &lt;none&gt; 80/TCP,443/TCP 2s&quot;, &quot;kube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2s&quot;, &quot;kubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.38 &lt;none&gt; 8000/TCP 1s&quot;, &quot;kubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.180 &lt;none&gt; 443:30001/TCP 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE&quot;, &quot;ingress-nginx daemonset.apps/nginx-ingress-controller 0 0 0 0 0 &lt;none&gt; 2s&quot;, &quot;kube-system daemonset.apps/kube-flannel-ds-amd64 0 0 0 0 0 &lt;none&gt; 2s&quot;, &quot;&quot;, &quot;NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE&quot;, &quot;kube-system deployment.apps/coredns 0/1 1 0 2s&quot;, &quot;kubernetes-dashboard deployment.apps/dashboard-metrics-scraper 0/1 1 0 1s&quot;, &quot;kubernetes-dashboard deployment.apps/kubernetes-dashboard 0/1 1 0 1s&quot;, &quot;&quot;, &quot;NAMESPACE NAME DESIRED CURRENT READY AGE&quot;, &quot;kube-system replicaset.apps/coredns-6d8cfdd59d 1 1 0 2s&quot;, &quot;kubernetes-dashboard replicaset.apps/dashboard-metrics-scraper-566cddb686 1 1 0 1s&quot;, &quot;kubernetes-dashboard replicaset.apps/kubernetes-dashboard-c4bc5bd44 1 1 0 1s&quot; ]&#125;TASK [addons : 创建Dashboard管理员令牌] **************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : 获取Dashboard管理员令牌] **************************************************************************************************************************************changed: [192.168.171.12]TASK [addons : Kubernetes Dashboard登录信息] ******************************************************************************************************************************ok: [192.168.171.12] =&gt; &#123; &quot;ui.stdout_lines&quot;: [ &quot;访问地址---&gt;https://NodeIP:30001&quot;, &quot;令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IlhOV0FZU1ZXRU80MU5oRUlYeGsxbExFcVB1R1k0bEEzMDhQQWdWVE5oZG8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tenQ2Y3ciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTU3YjEyYmYtNjNjNC00NzU1LWI4YTAtN2IyY2ZkZmRmNmE3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fKCHvmsmZmIErB9YLHtQrqWQBL_b89W0i_gDa4rwgV9x4UfzVAXskUiZiQs_yAHNmyUaIqPpBdUI64pvAoXilr-6wIk8-R8hpp4BJXLL4OsTtPXxrhIQF4_NP0D-4flg9sHba-I9X9A_2RWskcY53PAPTOjlyOQuldUyTdIT9tXi6jeSgj8CrDBc9O_A3xYWZ1f7RvrdEdU4Kkotc1rsBeGg-OzabU1nNLxWAaDHZJFciYeABtbPoY2fTkdz0JGoIxLpAqcQKoFp9ztGPcoOboCOqeb_hc-caBAmyvVIfbPvBiywdtuidjvb1IazETt_GQlzg7FMBoUpHhJYOTvnAA&quot; ]&#125;PLAY RECAP ************************************************************************************************************************************************************192.168.171.12 : ok=61 changed=40 unreachable=0 failed=0192.168.171.13 : ok=38 changed=23 unreachable=0 failed=0192.168.171.14 : ok=38 changed=23 unreachable=0 failed=0localhost : ok=9 changed=3 unreachable=0 failed=0 2.3、测试[root@k8s-ansible2 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-ansible2 Ready &lt;none&gt; 117s v1.16.0k8s-ansible3 Ready &lt;none&gt; 117s v1.16.0k8s-ansible4 Ready &lt;none&gt; 117s v1.16.0[root@k8s-ansible2 ~]# kubectl get po --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESingress-nginx nginx-ingress-controller-7g9fh 1/1 Running 0 2m41s 192.168.171.13 k8s-ansible3 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-hc492 1/1 Running 0 2m41s 192.168.171.14 k8s-ansible4 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-s4slm 1/1 Running 0 2m41s 192.168.171.12 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system coredns-6d8cfdd59d-hcfw5 1/1 Running 0 3m8s 10.244.1.2 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-6nbt4 1/1 Running 0 2m51s 192.168.171.12 k8s-ansible2 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-mfksz 1/1 Running 0 2m51s 192.168.171.14 k8s-ansible4 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-vclgg 1/1 Running 0 2m51s 192.168.171.13 k8s-ansible3 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-566cddb686-nk7t8 1/1 Running 0 3m7s 10.244.2.2 k8s-ansible4 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-c4bc5bd44-cxgb6 1/1 Running 0 3m7s 10.244.0.2 k8s-ansible3 &lt;none&gt; &lt;none&gt;[root@k8s-ansible2 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-96KwqYYgTQ0ajISzq2pUc5Dzu07UzjaKRZqRWs31yUk 5m5s kubelet-bootstrap Approved,Issuednode-csr-9PduNyHHpXtDmuNFj3fCkpoNkGcDkO2NPEk3uGQ3kIk 5m6s kubelet-bootstrap Approved,Issuednode-csr-WLFKgflHlDK2f0RFvuTYKlkHcr8hz0iOrzYcp2V50JE 5m6s kubelet-bootstrap Approved,Issued 三、多master部署3.1、hosts[root@k8s-ansible1 ansible-install-k8s-master]# cat hosts[master]# 如果部署单Master，只保留一个Master节点192.168.171.11 node_name=k8s-master1192.168.171.12 node_name=k8s-master2[node]192.168.171.13 node_name=k8s-node1192.168.171.14 node_name=k8s-node2[etcd]192.168.171.11 etcd_name=etcd-1192.168.171.12 etcd_name=etcd-2192.168.171.13 etcd_name=etcd-3[lb]# 如果部署单Master，该项忽略192.168.171.15 lb_name=lb-master192.168.171.16 lb_name=lb-backup[k8s:children]masternode[newnode]#192.168.31.91 node_name=k8s-node3 3.2、全局参数配置[root@k8s-ansible1 ansible-install-k8s-master]# cat group_vars/all.yml# 安装目录software_dir: &apos;/root/binary_pkg&apos;k8s_work_dir: &apos;/opt/kubernetes&apos;etcd_work_dir: &apos;/opt/etcd&apos;tmp_dir: &apos;/tmp/k8s&apos;# 集群网络service_cidr: &apos;10.0.0.0/24&apos;cluster_dns: &apos;10.0.0.2&apos; # 与roles/addons/files/coredns.yaml中IP一致pod_cidr: &apos;10.244.0.0/16&apos; # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: &apos;30000-32767&apos;cluster_domain: &apos;cluster.local&apos;# 高可用，如果部署单Master，该项忽略vip: &apos;192.168.171.88&apos;nic: &apos;ens33&apos;# 自签证书可信任IP列表，为方便扩展，可添加多个预留IPcert_hosts: # 包含所有LB、VIP、Master IP和service_cidr的第一个IP（多多益善，可以多余出来几个后期扩展用） k8s: - 10.0.0.1 - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 - 192.168.171.14 - 192.168.171.15 - 192.168.171.16 - 192.168.171.17 - 192.168.171.18 - 192.168.171.19 - 192.168.171.10 - 192.168.171.21 - 192.168.171.88 # 包含所有etcd节点IP etcd: - 192.168.171.11 - 192.168.171.12 - 192.168.171.13 3.3、准备部署多Master版：ansible-playbook -i hosts multi-master-deploy.yml -uroot -k 3.4、输出历史（重点是结果）省略... 3.5、测试[root@k8s-ansible1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 85s v1.16.0k8s-master2 Ready &lt;none&gt; 85s v1.16.0k8s-node1 Ready &lt;none&gt; 85s v1.16.0k8s-node2 Ready &lt;none&gt; 85s v1.16.0[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEingress-nginx pod/nginx-ingress-controller-92b8v 1/1 Running 0 90singress-nginx pod/nginx-ingress-controller-dfkp5 1/1 Running 0 90singress-nginx pod/nginx-ingress-controller-hckvr 1/1 Running 0 91singress-nginx pod/nginx-ingress-controller-qckdd 1/1 Running 0 90skube-system pod/coredns-6d8cfdd59d-lsdps 1/1 Running 0 117skube-system pod/kube-flannel-ds-amd64-2mc74 1/1 Running 1 100skube-system pod/kube-flannel-ds-amd64-4hqq7 1/1 Running 0 101skube-system pod/kube-flannel-ds-amd64-dgzrb 1/1 Running 0 100skube-system pod/kube-flannel-ds-amd64-zjtpq 1/1 Running 0 100skubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-9xh7b 1/1 Running 0 116skubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-4f45q 1/1 Running 0 116sNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5m40singress-nginx service/ingress-nginx ClusterIP 10.0.0.170 &lt;none&gt; 80/TCP,443/TCP 117skube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 117skubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.172 &lt;none&gt; 8000/TCP 116skubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.57 &lt;none&gt; 443:30001/TCP 116s[root@k8s-ansible1 ~]# kubectl get po,svc --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESingress-nginx pod/nginx-ingress-controller-92b8v 1/1 Running 0 96s 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-dfkp5 1/1 Running 0 96s 192.168.171.12 k8s-master2 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-hckvr 1/1 Running 0 97s 192.168.171.13 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx pod/nginx-ingress-controller-qckdd 1/1 Running 0 96s 192.168.171.14 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system pod/coredns-6d8cfdd59d-lsdps 1/1 Running 0 2m3s 10.244.1.2 k8s-master2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-2mc74 1/1 Running 1 106s 192.168.171.12 k8s-master2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-4hqq7 1/1 Running 0 107s 192.168.171.13 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-dgzrb 1/1 Running 0 106s 192.168.171.14 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system pod/kube-flannel-ds-amd64-zjtpq 1/1 Running 0 106s 192.168.171.11 k8s-master1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/dashboard-metrics-scraper-566cddb686-9xh7b 1/1 Running 0 2m2s 10.244.2.2 k8s-master1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard pod/kubernetes-dashboard-c4bc5bd44-4f45q 1/1 Running 0 2m2s 10.244.3.2 k8s-node2 &lt;none&gt; &lt;none&gt;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORdefault service/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5m46s &lt;none&gt;ingress-nginx service/ingress-nginx ClusterIP 10.0.0.170 &lt;none&gt; 80/TCP,443/TCP 2m3s app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginxkube-system service/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 2m3s k8s-app=kube-dnskubernetes-dashboard service/dashboard-metrics-scraper ClusterIP 10.0.0.172 &lt;none&gt; 8000/TCP 2m2s k8s-app=dashboard-metrics-scraperkubernetes-dashboard service/kubernetes-dashboard NodePort 10.0.0.57 &lt;none&gt; 443:30001/TCP 2m2s k8s-app=kubernetes-dashboard### 检查高可用的两台机器：[root@k8s-ansible5 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:85:37:e0 brd ff:ff:ff:ff:ff:ff inet 192.168.171.15/24 brd 192.168.171.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.171.88/24 scope global secondary ens33 ##虚拟VIP valid_lft forever preferred_lft forever inet6 fe80::d20b:b903:7edd:b18b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::5d9e:cf1f:ea7f:801f/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever inet6 fe80::5c0:8885:2874:a77b/64 scope link tentative noprefixroute dadfailed valid_lft forever preferred_lft forever[root@k8s-ansible5 ~]# ps aux | grep nginxroot 7895 0.0 0.0 46356 1168 ? Ss 22:57 0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 7896 0.1 0.1 47184 2308 ? S 22:57 0:00 nginx: worker processnginx 7897 0.0 0.1 46780 1980 ? S 22:57 0:00 nginx: worker processnginx 7898 0.0 0.1 46924 2216 ? S 22:57 0:00 nginx: worker processnginx 7899 0.0 0.1 46876 2220 ? S 22:57 0:00 nginx: worker processroot 11331 0.0 0.0 112728 988 pts/0 S+ 23:07 0:00 grep --color=auto nginx[root@k8s-ansible5 ~]# ps aux | grep keepalivedroot 7975 0.0 0.0 122884 1404 ? Ss 22:57 0:00 /usr/sbin/keepalived -Droot 7976 0.0 0.1 133844 3336 ? S 22:57 0:00 /usr/sbin/keepalived -Droot 7977 0.0 0.1 133784 2892 ? S 22:57 0:00 /usr/sbin/keepalived -Droot 11369 0.0 0.0 112724 992 pts/0 R+ 23:07 0:00 grep --color=auto keepalived]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个生产级K8S高可用集群（2）]]></title>
    <url>%2F2019%2F12%2F01%2F%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[写在前面：此次为了贴合线上的真实情况，此次K8S搭建将不会和咱们网路上的一气呵成相媲美，更多的表现在： 最新版K8S_1.16； 完全基于离线模式的二进制HA搭建（政企）《链接：https://pan.baidu.com/s/1aCmiYdfn5gujnyMutVdaVw提取码：m39k》； 全部组件均采用二进制部署（包含Docker）； 逐一摸索每个组件的配置文件，做到线上有故障能清楚的定位到问题； 既然是分布式，本次安装完全基于： 先单Master到双Master高可用； 新Node如何加到集群； 服务器硬件配置推荐： 生产环境K8S平台规划 – 单Master集群 生产环境K8S平台规划 – 多Master集群（HA） 一、服务器规划 角色 IP 组件 k8s-master1 192.168.171.134 kube-apiserver，kube-controller-manager，kube-scheduler，etcd k8s-master2 192.168.171.135 kube-apiserver，kube-controller-manager，kube-scheduler，etcd k8s-node1 192.168.171.136 kubelet，kube-proxy，docker，etcd k8s-node2 192.168.171.137 kubelet，kube-proxy，docker Load Balancer（Master） 192.168.171.138，192.168.171.188 (VIP) Nginx L4，Keepalived Load Balancer（Backup） 192.168.171.139 Nginx L4，Keepalived 1.1、系统初始化关闭防火墙：# systemctl stop firewalld# systemctl disable firewalld关闭selinux：# setenforce 0 # 临时# sed -i &apos;s/enforcing/disabled/&apos; /etc/selinux/config # 永久关闭swap：# swapoff -a # 临时# vim /etc/fstab # 永久同步系统时间：# ntpdate time.windows.com添加hosts：# vim /etc/hosts192.168.171.134 k8s-master1192.168.171.135 k8s-master2192.168.171.136 k8s-node1192.168.171.137 k8s-node2修改主机名：hostnamectl set-hostname k8s-master1##开启转发cat /etc/sysctl.d/kubernetes.confnet.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1vm.swappiness=0vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100sysctl -p /etc/sysctl.d/kubernetes.conf 二、ETCD集群整个集群中所有的组件均是走的https协议进行交互，所以我们需要配置自签证书到各个服务中； 2.1、将下载好的证书文件上传到K8s-master1中，并解压[root@k8s-master1 ~]# lsanaconda-ks.cfg TLS.tar.gz[root@k8s-master1 ~]# tar zxvf TLS.tar.gzTLS/TLS/cfsslTLS/cfssl-certinfoTLS/cfssljsonTLS/etcd/TLS/etcd/ca-config.jsonTLS/etcd/ca-csr.jsonTLS/etcd/generate_etcd_cert.shTLS/etcd/server-csr.jsonTLS/k8s/TLS/k8s/ca-config.jsonTLS/k8s/ca-csr.jsonTLS/k8s/kube-proxy-csr.jsonTLS/k8s/server-csr.jsonTLS/k8s/generate_k8s_cert.shTLS/cfssl.sh[root@k8s-master1 ~]# cd TLS[root@k8s-master1 TLS]# lscfssl cfssl-certinfo cfssljson cfssl.sh etcd k8s 将超cfssl移动到可执行目录中：运行脚本：（cfssl.sh）《注意脚本中curl原始是被注释掉了》[root@k8s-master1 TLS]# cat cfssl.shcurl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfsslcurl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljsoncurl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfocp -rf cfssl cfssl-certinfo cfssljson /usr/local/binchmod +x /usr/local/bin/cfssl* 执行完成脚本后：[root@k8s-master1 TLS]# ls /usr/local/bin/cfssl cfssl-certinfo cfssljson [root@k8s-master1 TLS]# lscfssl cfssl-certinfo cfssljson cfssl.sh etcd k8s[root@k8s-master1 TLS]# cd etcd/[root@k8s-master1 etcd]# lsca-config.json ca-csr.json generate_etcd_cert.sh server-csr.json[root@k8s-master1 etcd]# vim server-csr.json[root@k8s-master1 etcd]# cat server-csr.json ###修改如下hosts中的host&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.171.134&quot;, &quot;192.168.171.135&quot;, &quot;192.168.171.136&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125; 执行脚本：[root@k8s-master1 etcd]# sh generate_etcd_cert.sh2019/11/29 20:15:53 [INFO] generating a new CA key and certificate from CSR2019/11/29 20:15:53 [INFO] generate received request2019/11/29 20:15:53 [INFO] received CSR2019/11/29 20:15:53 [INFO] generating key: rsa-20482019/11/29 20:15:53 [INFO] encoded CSR2019/11/29 20:15:53 [INFO] signed certificate with serial number 241029724755122032470009319168181161854241472802019/11/29 20:15:53 [INFO] generate received request2019/11/29 20:15:53 [INFO] received CSR2019/11/29 20:15:53 [INFO] generating key: rsa-20482019/11/29 20:15:53 [INFO] encoded CSR2019/11/29 20:15:53 [INFO] signed certificate with serial number 129361955165654850485179523415464104941810882902019/11/29 20:15:53 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@k8s-master1 etcd]# ls server*server.csr server-csr.json server-key.pem server.pem 至此etcd密钥和证书生成完毕！！ 上传etcd.tar.gz 并解压到k8s-master1中：[root@k8s-master1 ~]# tar zxvf etcd.tar.gzetcd/etcd/bin/etcd/bin/etcdetcd/bin/etcdctletcd/cfg/etcd/cfg/etcd.confetcd/ssl/etcd/ssl/ca.pemetcd/ssl/server.pemetcd/ssl/server-key.pemetcd.service 先来了解下etcd.service[root@k8s-master1 ~]# cat etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.conf ##etcd配置文件目录ExecStart=/opt/etcd/bin/etcd \ ##etcd执行文件所在的目录 --name=$&#123;ETCD_NAME&#125; \ --data-dir=$&#123;ETCD_DATA_DIR&#125; \ --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \ --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster-state=new \ --cert-file=/opt/etcd/ssl/server.pem \ --key-file=/opt/etcd/ssl/server-key.pem \ --peer-cert-file=/opt/etcd/ssl/server.pem \ --peer-key-file=/opt/etcd/ssl/server-key.pem \ --trusted-ca-file=/opt/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target [root@k8s-master1 etcd]# lsbin cfg ssl[root@k8s-master1 etcd]# cd bin/ ##此目录为etcd的执行文件目录（后期升级可直接下载二进制的可执行文件覆盖升级即可）[root@k8s-master1 bin]# lsetcd etcdctl 再来看下etcd的配置文件目录：[root@k8s-master1 cfg]# cat etcd.conf#[Member]ETCD_NAME=&quot;etcd-1&quot; ##集群节点的nameETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot; ##数据存放位置ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.134:2380&quot; ##etcd集群内部通讯urlETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot; ##etcd客户端通讯url#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.134:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.134:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot; ##集群节点的配置信息ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot; ##集群简单认证的TOKENETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; ##集群的状态（新增的节点要改为existing） copy刚刚生成的etcd证书文件到指定的目录（/root/etcd/ssl）[root@k8s-master1 etcd]# cp /root/TLS/etcd/&#123;ca,server,server-key&#125;.pem ssl/[root@k8s-master1 etcd]# ls ssl/ca.pem server-key.pem server.pem 然后下发配置etcd和etcd.service到三台集群机器：[root@k8s-master1 ~]# lsanaconda-ks.cfg etcd etcd.service etcd.tar.gz TLS TLS.tar.gz[root@k8s-master1 ~]# scp -r etcd root@192.168.171.134:/opt/etcd 100% 16MB 51.2MB/s 00:00etcdctl 100% 13MB 58.8MB/s 00:00.etcd.conf.swp 100% 12KB 11.8MB/s 00:00etcd.conf 100% 523 634.0KB/s 00:00ca.pem 100% 1265 788.8KB/s 00:00server.pem 100% 1338 1.8MB/s 00:00server-key.pem 100% 1675 1.5MB/s 00:00[root@k8s-master1 ~]# scp -r etcd root@192.168.171.135:/opt/root@192.168.171.135&apos;s password:etcd 100% 16MB 82.4MB/s 00:00etcdctl 100% 13MB 92.3MB/s 00:00.etcd.conf.swp 100% 12KB 7.7MB/s 00:00etcd.conf 100% 523 169.7KB/s 00:00ca.pem 100% 1265 1.3MB/s 00:00server.pem 100% 1338 1.4MB/s 00:00server-key.pem 100% 1675 1.5MB/s 00:00[root@k8s-master1 ~]# scp -r etcd root@192.168.171.136:/opt/etcd 100% 16MB 68.7MB/s 00:00etcdctl 100% 13MB 80.8MB/s 00:00.etcd.conf.swp 100% 12KB 12.5MB/s 00:00etcd.conf 100% 523 385.2KB/s 00:00ca.pem 100% 1265 1.5MB/s 00:00server.pem 100% 1338 2.0MB/s 00:00server-key.pem 100% 1675 2.2MB/s 00 同理copyetcd.service文件：[root@k8s-master1 ~]# scp etcd.service root@192.168.171.134:/usr/lib/systemd/system/etcd.service 100% 1078 577.1KB/s 00:00[root@k8s-master1 ~]# scp etcd.service root@192.168.171.135:/usr/lib/systemd/system/etcd.service 100% 1078 780.0KB/s 00:00[root@k8s-master1 ~]# scp etcd.service root@192.168.171.136:/usr/lib/systemd/system/etcd.service 修改另外2台etcd的配置文件： 192.168.171.135中[root@k8s-master2 ~]# cat /opt/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd-2&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.135:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.135:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.135:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; 192.168.171.136中[root@k8s-node1 ~]# cat /opt/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd-3&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.171.136:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.171.136:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.171.136:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.171.134:2380,etcd-2=https://192.168.171.135:2380,etcd-3=https://192.168.171.136:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; 启动etcd（第一台启动的时候有些慢是因为在侦听其它节点）[root@k8s-master1 ~]# systemctl daemon-reload[root@k8s-master1 ~]# systemctl start etcd[root@k8s-master1 ~]# systemctl enable etcdCreated symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service. 查看etcd集群的日志：[root@k8s-master1 ~]# tail /var/log/messages -fNov 29 21:06:20 localhost etcd: set the initial cluster version to 3.0Nov 29 21:06:20 localhost etcd: enabled capabilities for version 3.0Nov 29 21:06:24 localhost etcd: peer 92fcf2aa055d676f became activeNov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message reader)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 reader)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream Message writer)Nov 29 21:06:24 localhost etcd: established a TCP streaming connection with peer 92fcf2aa055d676f (stream MsgApp v2 writer)Nov 29 21:06:24 localhost etcd: updating the cluster version from 3.0 to 3.3Nov 29 21:06:24 localhost etcd: updated the cluster version from 3.0 to 3.3Nov 29 21:06:24 localhost etcd: enabled capabilities for version 3.3 查看etcd集群的状态：# /opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints=&quot;https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379&quot; \cluster-healthmember 3530acf25e9921b5 is healthy: got healthy result from https://192.168.171.134:2379member 833528c821fcdcd2 is healthy: got healthy result from https://192.168.171.135:2379member 92fcf2aa055d676f is healthy: got healthy result from https://192.168.171.136:2379cluster is healthy 二、部署Master节点2.1、自签证书[root@k8s-master1 ~]# cd TLS/k8s/[root@k8s-master1 k8s]# pwd/root/TLS/k8s[root@k8s-master1 k8s]# lsca-config.json ca-csr.json generate_k8s_cert.sh kube-proxy-csr.json server-csr.jsonkube-proxy-csr.json：为kube-proxy服务自签的证书ca-config.json，ca-csr.json，server-csr.json：为Api-server服务自签的证书 2.2、划重点（K8S集群内部是用证书进行校验通信） 一定要把和API-SERVER 通信服务的IP写到如下hosts中（master节点，LB，etcd，keepalived，VIP）； 当然这个也是我之前的疑问，如果后期扩展了master 如何加入到当前集群？ 目前得到的验证是先提前多增加IP； [root@k8s-master1 k8s]# cat server-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;10.0.0.1&quot;, &quot;127.0.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot;, &quot;192.168.171.134&quot;, &quot;192.168.171.135&quot;, &quot;192.168.171.136&quot;, &quot;192.168.171.137&quot;, &quot;192.168.171.138&quot;, &quot;192.168.171.139&quot;, &quot;192.168.171.188&quot;, &quot;192.168.171.140&quot;, &quot;192.168.171.141&quot;, &quot;192.168.171.142&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 执行脚本生成证书：[root@k8s-master1 k8s]# sh generate_k8s_cert.sh2019/11/30 16:08:18 [INFO] generating a new CA key and certificate from CSR2019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:18 [INFO] encoded CSR2019/11/30 16:08:18 [INFO] signed certificate with serial number 3418263221184942457507420707234268862304733819592019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:18 [INFO] encoded CSR2019/11/30 16:08:18 [INFO] signed certificate with serial number 2989165026649416994797859334541381614109130609662019/11/30 16:08:18 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).2019/11/30 16:08:18 [INFO] generate received request2019/11/30 16:08:18 [INFO] received CSR2019/11/30 16:08:18 [INFO] generating key: rsa-20482019/11/30 16:08:19 [INFO] encoded CSR2019/11/30 16:08:19 [INFO] signed certificate with serial number 114546326222977492622969866107478344620111189522019/11/30 16:08:19 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;). 查看生成的证书：[root@k8s-master1 k8s]# ls *.pemca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem 准备部署master组件：二进制包下载地址：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#v1161 上传包中的k8s-master.tar.gz到/目录 [root@k8s-master1 ~]# tar zxvf k8s-master.tar.gzkubernetes/kubernetes/bin/kubernetes/bin/kubectlkubernetes/bin/kube-apiserverkubernetes/bin/kube-controller-managerkubernetes/bin/kube-schedulerkubernetes/cfg/kubernetes/cfg/token.csvkubernetes/cfg/kube-apiserver.confkubernetes/cfg/kube-controller-manager.confkubernetes/cfg/kube-scheduler.confkubernetes/ssl/kubernetes/logs/kube-apiserver.servicekube-controller-manager.servicekube-scheduler.service copy刚刚生成的证书文件放到当前ssl中：[root@k8s-master1 kubernetes]# cp /root/TLS/k8s/*pem ssl/[root@k8s-master1 kubernetes]# lsbin cfg logs ssl[root@k8s-master1 kubernetes]# ls ssl/ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem kube-apiserver[root@k8s-master1 cfg]# cat kube-apiserver.confKUBE_APISERVER_OPTS=&quot;--logtostderr=false \ ##输出日志--v=2 \ ##日志级别--log-dir=/opt/kubernetes/logs \ ##日志存放目录--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \ ##etcd集群IP--bind-address=192.168.171.134 \ ##绑定IP（可以为外网IP）--secure-port=6443 \ ##安全端口--advertise-address=192.168.171.134 \ ##集群内部通讯地址--allow-privileged=true \ ##允许pod有超级权限--service-cluster-ip-range=10.0.0.0/24 \ ##service的IP范围--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \ ##启动准入控制插件--authorization-mode=RBAC,Node \ ##授权模式--enable-bootstrap-token-auth=true \ ##bootstrap-token认证，自动颁发证书--token-auth-file=/opt/kubernetes/cfg/token.csv \ ##token文件--service-node-port-range=30000-32767 \ ##service的ip范围--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \--audit-log-maxage=30 \ ##如下均为日志的一些策略--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log&quot; kube-controller-manager[root@k8s-master1 cfg]# cat kube-controller-manager.confKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \ ##日志存放路径--leader-elect=true \ ##选举模式--master=127.0.0.1:8080 \ ##连接本地api-server--address=127.0.0.1 \ ##监听地址--allocate-node-cidrs=true \ ##cni组件--cluster-cidr=10.244.0.0/16 \ ##cni组件IP段--service-cluster-ip-range=10.0.0.0/24 \ ##service范围和api-server中一致--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \--root-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \--experimental-cluster-signing-duration=87600h0m0s&quot; kube-scheduler[root@k8s-master1 cfg]# cat kube-scheduler.confKUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--leader-elect \--master=127.0.0.1:8080 \--address=127.0.0.1&quot; 启动apiserver[root@k8s-master1 cfg]# cd[root@k8s-master1 ~]# mv kubernetes/ /opt/[root@k8s-master1 ~]# mv *.service /usr/lib/systemd/system/[root@k8s-master1 ~]# systemctl daemon-reload[root@k8s-master1 ~]# systemctl start kube-apiserver[root@k8s-master1 ~]# less /opt/kubernetes/logs/kube-apiserver.INFO ##查看启动日志[root@k8s-master1 ~]# ps aux | grep kuberoot 17717 24.2 18.0 549604 336048 ? Ssl 16:39 0:06 /opt/kubernetes/bin/kube-apiserver --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 --bind-address=192.168.171.134 --secure-port=6443 --advertise-address=192.168.171.134 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth=true --token-auth-file=/opt/kubernetes/cfg/token.csv --service-node-port-range=30000-32767--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem --kubelet-client-key=/opt/kubernetes/ssl/server-key.pem --tls-cert-file=/opt/kubernetes/ssl/server.pem --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem --client-ca-file=/opt/kubernetes/ssl/ca.pem --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem --etcd-cafile=/opt/etcd/ssl/ca.pem --etcd-certfile=/opt/etcd/ssl/server.pem --etcd-keyfile=/opt/etcd/ssl/server-key.pem --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/opt/kubernetes/logs/k8s-audit.logroot 17731 0.0 0.0 112724 988 pts/1 S+ 16:39 0:00 grep --color=auto kube 再次启动kube-controller-manager 及 kube-scheduler[root@k8s-master1 ~]# systemctl start kube-controller-manager[root@k8s-master1 ~]# systemctl start kube-scheduler[root@k8s-master1 ~]# systemctl enable kube-apiserver[root@k8s-master1 ~]# systemctl enable kube-controller-manager[root@k8s-master1 ~]# systemctl enable kube-scheduler 移动kubectl到可执行目录[root@k8s-master1 ~]# mv /opt/kubernetes/bin/kubectl /usr/local/bin/[root@k8s-master1 ~]# kubectl get nodeNo resources found in default namespace.[root@k8s-master1 ~]# kubectl get cs ##经过查看发现了此版本的bugNAME AGEcontroller-manager &lt;unknown&gt;scheduler &lt;unknown&gt;etcd-2 &lt;unknown&gt;etcd-0 &lt;unknown&gt;etcd-1 &lt;unknown&gt; 如上bug：https://segmentfault.com/a/1190000020912684 启用TLS Bootstrapping为kubelet TLS Bootstrapping 授权：# cat /opt/kubernetes/cfg/token.csv c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,&quot;system:node-bootstrapper&quot;格式：token,用户,uid,用户组 给kubelet-bootstrap授权： 自动的给kubelet创建证书 kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap token也可自行生成替换： head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos; ==但apiserver配置的token必须要与node节点bootstrap.kubeconfig配置里一致。== 三、部署Worker Node二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ 上传k8s-node.tar.gz到node节点[root@k8s-node1 ~]# tar zxvf k8s-node.tar.gzcni-plugins-linux-amd64-v0.8.2.tgzdaemon.jsondocker-18.09.6.tgzdocker.servicekubelet.servicekube-proxy.servicekubernetes/kubernetes/bin/kubernetes/bin/kubeletkubernetes/bin/kube-proxykubernetes/cfg/kubernetes/cfg/kubelet-config.ymlkubernetes/cfg/bootstrap.kubeconfigkubernetes/cfg/kube-proxy.kubeconfigkubernetes/cfg/kube-proxy.confkubernetes/cfg/kubelet.confkubernetes/cfg/kube-proxy-config.ymlkubernetes/ssl/kubernetes/logs/ 3.1、配置并启动Docker# tar zxvf docker-18.09.6.tgz# mv docker/* /usr/bin[root@k8s-node1 ~]# ls /usr/bin/docker dockerd docker-init docker-proxy domainname# mkdir /etc/docker[root@k8s-node1 ~]# cat daemon.json ##配置镜像加速器&#123; &quot;registry-mirrors&quot;: [&quot;http://bc437cce.m.daocloud.io&quot;], &quot;insecure-registries&quot;: [&quot;192.168.171.170&quot;]&#125;# mv daemon.json /etc/docker# mv docker.service /usr/lib/systemd/system# systemctl start docker# systemctl enable docker[root@k8s-node1 ~]# ps aux | grep dockerroot 17326 2.1 1.5 405704 28404 ? Ssl 17:05 0:00 /usr/bin/dockerdroot 17333 1.2 0.8 316224 15048 ? Ssl 17:05 0:00 containerd --config /var/run/docker/containerd/containerd.toml --log-level inforoot 17534 0.0 0.0 112724 988 pts/2 R+ 17:05 0:00 grep --color=auto docker 在查看docker info的时候发现了：WARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled 解决方法：vim /etc/sysctl.conf添加以下内容net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1最后再执行sysctl -p 3.2、部署kubelet和kube-proxy在master上拷贝证书到Node（有多少node节点就需要scp到多少节点）：[root@k8s-master1 ~]# cd TLS/k8s/[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.136:/opt/kubernetes/ssl/[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@192.168.171.137:/opt/kubernetes/ssl/ node节点目录[root@k8s-node1 ~]# cd kubernetes/[root@k8s-node1 kubernetes]# tree ..├── bin│ ├── kubelet│ └── kube-proxy├── cfg│ ├── bootstrap.kubeconfig│ ├── kubelet.conf│ ├── kubelet-config.yml│ ├── kube-proxy.conf│ ├── kube-proxy-config.yml│ └── kube-proxy.kubeconfig├── logs└── ssl 先来看下几个主要的配置文件[root@k8s-node1 cfg]# lsbootstrap.kubeconfig kubelet.conf kubelet-config.yml kube-proxy.conf kube-proxy-config.yml kube-proxy.kubeconfigconf：基本配置文件kubeconfig：连接apiserver的配置文件yml：主要配置文件 kubelet.conf[root@k8s-node1 cfg]# cat kubelet.confKUBELET_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--hostname-override=k8s-node1 \ ##每个node的name（必须要唯一）--network-plugin=cni \ ##指定网路组件--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \ ##配置文件--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \--config=/opt/kubernetes/cfg/kubelet-config.yml \--cert-dir=/opt/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; ##镜像 bootstrap.kubeconfig（自动为即将要加入集群的node颁发证书）[root@k8s-node1 cfg]# cat bootstrap.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem ##拿着master的ca证书 server: https://192.168.171.134:6443 ##master的地址 name: kubernetescontexts:- context: cluster: kubernetes user: kubelet-bootstrap name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kubelet-bootstrap user: token: c47ffb939f5ca36231d9e3121a252940 ## 这个token一定要和如上master上token一致 我们也来了解下启动kubelet后如何和apiserver通信的： kubelet 启动带着bootstrap.kubeconfig请求apiserver，apiserver首先会校验所携带的token是否正确，正确则会颁发证书，不正确则会启动失败。 kubelet-config.yml[root@k8s-node1 cfg]# cat kubelet-config.ymlkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 0.0.0.0port: 10250readOnlyPort: 10255cgroupDriver: cgroupfs ##底层驱动（和docker一致）clusterDNS: ##dns- 10.0.0.2clusterDomain: cluster.local ##域failSwapOn: false ##swap关闭authentication: ##认证信息 anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30sevictionHard: ##资源配置 imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%maxOpenFiles: 1000000maxPods: 110 kube-proxy.kubeconfig[root@k8s-node1 cfg]# cat kube-proxy.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem server: https://192.168.171.134:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: client-certificate: /opt/kubernetes/ssl/kube-proxy.pem client-key: /opt/kubernetes/ssl/kube-proxy-key.pem kube-proxy-config.yml[root@k8s-node1 cfg]# vim kube-proxy-config.ymlkind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1address: 0.0.0.0metricsBindAddress: 0.0.0.0:10249clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfighostnameOverride: k8s-node1 ##全node唯一clusterCIDR: 10.0.0.0/24mode: ipvs ##模式ipvs: scheduler: &quot;rr&quot;iptables: masqueradeAll: true 启动kubelet、kube-proxy服务# mv kubernetes /opt# cp kubelet.service kube-proxy.service /usr/lib/systemd/system修改以下三个文件中IP地址：# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.134:6443kubelet.kubeconfig: server: https://192.168.171.134:6443kube-proxy.kubeconfig: server: https://192.168.171.134:6443修改以下两个文件中主机名：# grep hostname *kubelet.conf:--hostname-override=k8s-node1 \kube-proxy-config.yml:hostnameOverride: k8s-node1 [root@k8s-node1 ~]# systemctl start kubelet[root@k8s-node1 ~]# systemctl status kubelet● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled) Active: active (running) since 六 2019-11-30 19:10:01 CST; 11s ago Main PID: 17702 (kubelet) Tasks: 9 Memory: 17.2M CGroup: /system.slice/kubelet.service └─17702 /opt/kubernetes/bin/kubelet --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --hostname-override=k8s-node1 --network-plugin=cni --kubeco...11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a11月 30 19:10:01 k8s-node1 systemd[1]: Stopped Kubernetes Kubelet.11月 30 19:10:01 k8s-node1 systemd[1]: Unit kubelet.service entered failed state.11月 30 19:10:01 k8s-node1 systemd[1]: kubelet.service failed.11月 30 19:10:01 k8s-node1 systemd[1]: Started Kubernetes Kubelet.[root@k8s-node1 ~]# systemctl enable kubelet 查看kubelet日志：less /opt/kubernetes/logs/kubelet.INFO其中我们会看到：W1130 19:27:08.379468 17702 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.dE1130 19:27:08.929388 17702 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin isnot ready: cni config uninitialized如上是因为cni的组件没有安装，稍后安装后即可恢复； 然后我们再次回到master节点 查看是否有node节点：[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo 2m1s kubelet-bootstrap Pending[root@k8s-master1 k8s]# kubectl certificate approve node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNocertificatesigningrequest.certificates.k8s.io/node-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo approved[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-TuI-Gg5yTowa_3OkCafMXBVynLwUJB2ZKrwtYG-EdNo 14m kubelet-bootstrap Approved,Issued[root@k8s-master1 k8s]# kubectl get node ##等待配置完毕cni则会readyNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 25s v1.16.0 启动kube-proxy服务：[root@k8s-node1 ~]# systemctl start kube-proxy[root@k8s-node1 ~]# systemctl status kube-proxy 查看kube-proxy日志：[root@k8s-node1 ~]# tailf /opt/kubernetes/logs/kube-proxy.INFOI1130 19:32:23.692156 18623 proxier.go:1729] Not using `--random-fully` in the MASQUERADE rule for iptables because the local version of iptables does not support it解决方案：https://blog.51cto.com/juestnow/2440260 3.3、部署CNI网络二进制包下载地址：https://github.com/containernetworking/plugins/releases # mkdir /opt/cni/bin /etc/cni/net.d# tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz –C /opt/cni/bin确保kubelet启用CNI：# cat /opt/kubernetes/cfg/kubelet.conf --network-plugin=cni 3.4、同理增加另外一个node节点57 tar zxvf k8s-node.tar.gz58 mv *.service /usr/lib/systemd/system/59 tar zxvf docker-18.09.6.tgz60 mv docker/* /usr/bin/61 mkdir /etc/docker62 vim daemon.json63 mv daemon.json /etc/docker/64 systemctl start docker65 systemctl enable docker66 systemctl status docker67 mv kubernetes/ /opt/68 cd /opt/kubernetes/69 ls70 cd cfg/71 ls72 vim bootstrap.kubeconfig73 vim kubelet.conf74 vim kubelet-config.yml75 vim kube-proxy.conf76 vim kube-proxy-config.yml77 vim kube-proxy.kubeconfig78 grep 192 *79 grep hostname *80 systemctl start kubelet81 systemctl start kube-proxy82 systemctl enable kubelet83 systemctl enable kube-proxy84 systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy85 mkdir /opt/cni/bin /etc/cni/net.d -p86 cd87 tar zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/ 虽然如上我只是把node2节点上的操作历史copy了一下，但是足以证明正确的操作步骤就是如上这些步骤，唯一需要注意的地方就是 如上的 kubelet和kube-proxy的配置文件。 3.5、部署flannel组件如要实现cni网路覆盖，我们就必须部署实现这个组件的flannel服务。 在master上操作： 上传kube-flannel.yaml到/目录[root@k8s-master1 ~]# cat kube-flannel.yaml ##来看几个主要的信息：1、 net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125;如上的网路信息要和：[root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-controller-manager.conf--cluster-cidr=10.244.0.0/16 \ 一致2、（DaemonSet模式：每个node节点都会自动部署这个服务）apiVersion: apps/v1kind: DaemonSet 在Master执行：[root@k8s-master1 ~]# kubectl apply -f kube-flannel.yaml[root@k8s-master1 ~]# kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-d2gzx 1/1 Running 0 51skube-flannel-ds-amd64-lwsnd 1/1 Running 0 51s[root@k8s-master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 67m v1.16.0k8s-node2 Ready &lt;none&gt; 30m v1.16.0 授权apiserver访问kubelet为提供安全性，kubelet禁止匿名访问，必须授权才可以。 上传apiserver-to-kubelet-rbac.yaml到/目录[root@k8s-master1 ~]# cat apiserver-to-kubelet-rbac.yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - &quot;&quot; resources: ##允许直接在master上操作如下的权限 - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics - pods/log verbs: - &quot;*&quot;---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: &quot;&quot;roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes 测试：[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system ##没有权限查看Error from server (Forbidden): Forbidden (user=kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-flannel-ds-amd64-d2gzx)[root@k8s-master1 ~]# kubectl apply -f apiserver-to-kubelet-rbac.yamlclusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created[root@k8s-master1 ~]# kubectl logs kube-flannel-ds-amd64-d2gzx -n kube-system ##现在可以查看了I1130 12:30:26.695707 1 main.go:514] Determining IP address of default interfaceI1130 12:30:26.698072 1 main.go:527] Using interface with name ens33 and address 192.168.171.136I1130 12:30:26.698106 1 main.go:544] Defaulting external address to interface address (192.168.171.136)[root@k8s-master1 ~]# kubectl create deployment web --image=nginx ##创建测试deploymentdeployment.apps/web created[root@k8s-master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-d86c95cc9-ztx9n 1/1 Running 0 2m49s 10.244.0.2 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl expose deployment web --port=80 --type=NodePort ##创建一个port测试下nginx是否OKservice/web exposed[root@k8s-master1 ~]# kubectl get po,svcNAME READY STATUS RESTARTS AGEpod/web-d86c95cc9-k9vnf 1/1 Running 0 2m34sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4h49mservice/web NodePort 10.0.0.34 &lt;none&gt; 80:32762/TCP 17m 至此单master节点的K8S集群搭建完毕！ 四、部署Web UI和DNS上传yaml/dashboard.yaml# vi dashboard.yaml…kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard…[root@k8s-master1 ~]# kubectl apply -f dashboard.yamlnamespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 创建token登录：##在创建token之前我们需要先创建service account[root@k8s-master1 ~]# cat dashboard-adminuser.yamlapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 创建service account并绑定默认cluster-admin管理员集群角色：[root@k8s-master1 ~]# kubectl apply -f dashboard-adminuser.yamlserviceaccount/admin-user createdclusterrolebinding.rbac.authorization.k8s.io/admin-user created 获取token[root@k8s-master1 ~]# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)Name: admin-user-token-bccwwNamespace: kubernetes-dashboardLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 6e6e1b2d-a0a3-4150-a611-98ce1653b79cType: kubernetes.io/service-account-tokenData====ca.crt: 1359 bytesnamespace: 20 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IktJYmdRdDdkbW1US0dnOHRKemdPMjJ6eUEzTXEtMGQyS0h6cWRpRUVLRE0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWJjY3d3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2ZTZlMWIyZC1hMGEzLTQxNTAtYTYxMS05OGNlMTY1M2I3OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YusDtTl_glNewEO0kMaiZDqOcbSMkRNY6sRT9BQYbzTjdmediGHcEB49wHepo_mXsW0isBnu4Mgpb4KL5y27OkE2hFICwQwQBX5gvHQI2CxuoHaVVi7G8eZn85fR7aKmKi7Uxppv6qOL5icZyl_74_-iQVIm3U59B-x2zoyoUa3tsFgQEpUWvkmbCajD-4sANU-UMyisR3uMdXvnyvz2oCUQBjuqJ5ZqqAupqrvtoJ1L27vHK1t7i_sLgVR_2X8MARrwgynHatEYAODVEsVRMJCBzR4ZW09xcCSbeQ1CopNyGbyPi7o9re_9FyGK18y3q7EmjaEOr2NJ3Yk0MesIyw 部署coreDNS[root@k8s-master1 ~]# kubectl apply -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created 测试是否dns正常[root@k8s-master1 k8s]# kubectl apply -f bs.yamlpod/busybox created[root@k8s-master1 ~]# kubectl exec -it busybox sh/ # ping 10.0.0.34 ##测试内网IP是否通过PING 10.0.0.34 (10.0.0.34): 56 data bytes64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.086 ms64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.068 ms^C--- 10.0.0.34 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.068/0.077/0.086 ms/ # ping web ##测试dns是否可以解析PING web (10.0.0.34): 56 data bytes64 bytes from 10.0.0.34: seq=0 ttl=64 time=0.049 ms64 bytes from 10.0.0.34: seq=1 ttl=64 time=0.065 ms/ # nslookup kubernetes （均可以解析）Server: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local 五、Master高可用5.1、部署Master组件（与Master1一致）拷贝master1/opt/kubernetes和service文件：# scp –r /opt/kubernetes root@192.168.171.135:/opt# scp -r /opt/etcd/ssl/ root@192.168.171.135:/opt/etcd/# scp /usr/lib/systemd/system/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125;.service root@192.168.171.135:/usr/lib/systemd/system# scp /usr/local/bin/kubectl root@192.168.171.135:/usr/local/bin/ 修改apiserver配置文件为本地IP：# cat /opt/kubernetes/cfg/kube-apiserver.conf KUBE_APISERVER_OPTS=&quot;--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://192.168.171.134:2379,https://192.168.171.135:2379,https://192.168.171.136:2379 \--bind-address=192.168.171.135 \--secure-port=6443 \--advertise-address=192.168.171.135 \…… 启动kube-apiserver，kube-controller-manager，kube-scheduler[root@k8s-master2 cfg]# systemctl start kube-apiserver[root@k8s-master2 cfg]# systemctl start kube-controller-manager[root@k8s-master2 cfg]# systemctl start kube-scheduler[root@k8s-master2 cfg]# systemctl enable kube-apiserverCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /usr/lib/systemd/system/kube-apiserver.service.[root@k8s-master2 cfg]# systemctl enable kube-controller-managerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.[root@k8s-master2 cfg]# systemctl enable kube-schedulerCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service. 在master2上面查看node节点的po[root@k8s-master2 cfg]# kubectl get node -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-node1 Ready &lt;none&gt; 25h v1.16.0 192.168.171.136 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://18.9.6k8s-node2 Ready &lt;none&gt; 25h v1.16.0 192.168.171.137 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://18.9.6[root@k8s-master2 cfg]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-gbd2m 1/1 Running 2 21h 10.244.0.9 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-d2gzx 1/1 Running 1 24h 192.168.171.136 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-lwsnd 1/1 Running 2 24h 192.168.171.137 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master2 cfg]# kubectl get po -n kubernetes-dashboard -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdashboard-metrics-scraper-566cddb686-wrkfl 1/1 Running 1 23h 10.244.1.8 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard-7b5bf5d559-csfwm 1/1 Running 1 23h 10.244.1.6 k8s-node2 &lt;none&gt; &lt;none&gt; 5.2、部署Nginx负载均衡nginx rpm包：http://nginx.org/packages/rhel/7/x86_64/RPMS/# rpm -vih http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.0-1.el7.ngx.x86_64.rpm[root@localhost ~]# cat /etc/nginx/nginx.confuser nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;####此处↓stream &#123; log_format main &apos;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent&apos;; access_log /var/log/nginx/k8s-access.log main; upstream k8s-apiserver &#123; server 192.168.171.134:6443; server 192.168.171.135:6443; &#125; server &#123; listen 6443; proxy_pass k8s-apiserver; &#125;&#125;####此处↑http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125;# systemctl start nginx# systemctl enable nginx[root@localhost ~]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:6443 0.0.0.0:* LISTEN 7142/nginx: master 5.3、Nginx+KeepAlived高可用主节点（192.168.171.138）：# yum install keepalived# vim /etc/keepalived/keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_MASTER&#125; vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot;&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 100 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.171.188/24 &#125; track_script &#123; check_nginx &#125; &#125;# cat /etc/keepalived/check_nginx.sh #!/bin/bashcount=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)if [ &quot;$count&quot; -eq 0 ];then exit 1else exit 0fi# chmod +x /etc/keepalived/check_nginx.sh# systemctl start keepalived# systemctl enable keepalived 备节点（192.168.171.139）：# cat /etc/keepalived/keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_BACKUP&#125; vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot;&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 90 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.171.188/24 &#125; track_script &#123; check_nginx &#125; &#125;# cat /etc/keepalived/check_nginx.sh #!/bin/bashcount=$(ps -ef |grep nginx |egrep -cv &quot;grep|$$&quot;)if [ &quot;$count&quot; -eq 0 ];then exit 1else exit 0fi# chmod +x /etc/keepalived/check_nginx.sh# systemctl start keepalived# systemctl enable keepalived 查看虚拟VIP[root@localhost ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9b:85:86 brd ff:ff:ff:ff:ff:ff inet 192.168.171.138/24 brd 192.168.171.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.171.188/24 scope global secondary ens33 valid_lft forever preferred_lft forever inet6 fe80::d3c5:e3e2:26f6:f6b5/64 scope link noprefixroute valid_lft forever preferred_lft forever 5.4、修改Node连接VIP[root@k8s-node1 ~]# cd /opt/kubernetes/cfg/[root@k8s-node1 cfg]# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.134:6443kubelet.kubeconfig: server: https://192.168.171.134:6443kube-proxy.kubeconfig: server: https://192.168.171.134:6443[root@k8s-node1 cfg]# sed -i &apos;s#192.168.171.134#192.168.171.188#&apos; *[root@k8s-node1 cfg]# grep 192 *bootstrap.kubeconfig: server: https://192.168.171.188:6443kubelet.kubeconfig: server: https://192.168.171.188:6443kube-proxy.kubeconfig: server: https://192.168.171.188:6443[root@k8s-node1 cfg]# systemctl restart kubelet &amp;&amp; systemctl restart kube-proxy同理操作其它node节点 测试VIP是否正常工作：[root@k8s-node2 cfg]# curl -k --header &quot;Authorization: Bearer c47ffb939f5ca36231d9e3121a252940&quot; https://192.168.171.188:6443/version&#123; &quot;major&quot;: &quot;1&quot;, &quot;minor&quot;: &quot;16&quot;, &quot;gitVersion&quot;: &quot;v1.16.0&quot;, &quot;gitCommit&quot;: &quot;2bd9643cee5b3b3a5ecbd3af49d09018f0773c77&quot;, &quot;gitTreeState&quot;: &quot;clean&quot;, &quot;buildDate&quot;: &quot;2019-09-18T14:27:17Z&quot;, &quot;goVersion&quot;: &quot;go1.12.9&quot;, &quot;compiler&quot;: &quot;gc&quot;, &quot;platform&quot;: &quot;linux/amd64&quot;&#125;分别在node1和node2上测试，你会发现nginx会以轮训的方式分别请求apiserver； 至此生产级K8S高可用集群搭建完毕！]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个生产级K8S高可用集群（1）]]></title>
    <url>%2F2019%2F11%2F26%2F%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%94%9F%E4%BA%A7%E7%BA%A7K8S%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[不多说了，2019年底再开始~~ 一、K8S特性①自我修复在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端的请求，确保服务不中断。 ②弹性伸缩使用命令、UI管控或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行业务。 ③自动部署和回滚K8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不影响业务。 ④服务发现和负载均衡K8S为多个容器提供一个统一的访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。 ⑤机密和配置管理管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。 ⑥存储编排挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网路存储（如NFS，ClusterFS，Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 ⑦批处理提供一次性任务，定时任务；满足批量数据处理和分析的场景。 二、K8S集群架构和组件 Master Master主要负责资源调度，控制副本，和提供统一访问集群的入口。 Node Node由Master管理，并汇报容器状态给Master，同时根据Master要求管理容器生命周期。 Pod Docker最小部署单元是容器，而Kubernetes最小部署单元是Pod，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，一个Pod在同一台Node上运行。 Service Service一个应用服务抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。Service代理Pod集合对外表现是为一个访问入口，分配一个集群IP地址，来自这个IP的请求将负载均衡转发到后端Pod中的容器。用过负载均衡器的朋友可能很好理解，其实Service就是一个抽象的负载均衡器。Service通过Lable Selector选择一组Pod提供服务。 Lable 标签是一个key=value的键值对，附加在某个资源上，每个对象可以有多个标签，然后根据这个lable关联、查询和筛选。就像Service与Pod，当多个Service、多个Pod情况下，访问某个Service怎么就知道转发到指定Pod呢？ Volume 数据卷，挂载宿主机文件、目录或者外部存储到Pod中，为应用服务提供存储，也可以Pod中容器之间共享数据。 Namespace 命名空间将资源对象逻辑上分配到不同Namespace，可以是不同的项目、用户等区分管理，并设定控制策略，从而实现多租户。命名空间也称为虚拟集群。 2.1、Master组件 kube-apiserver Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。 kube-controller-manager 处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。 kube-scheduler 根据调度算法为新创建的Pod选择一个Node节点，可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上。 etcd 分布式键值存储系统。用于保存集群状态数据，比如Pod、Service等对象信息。 2.2、Node组件 kubelet kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。cadvise:监控容器和节点资源 kube-proxy service 在逻辑上代表了后端的多个 Pod，外界通过 service 访问 Pod。service 接收到的请求是如何转发到 Pod 的呢？这就是 kube-proxy 要完成的工作。每个 Node 都会运行 kube-proxy 服务，它负责将访问 service 的 TCP/UPD 数据流转发到后端的容器。如果有多个副本，kube-proxy 会实现负载均衡。 三、下面是更高层次抽象对象： ReplicaSet（确保预期的Pod副本数量） 确保任何给定时间指定的Pod副本数量，并提供声明式更新等功能。 Deployment（无状态应用部署） Deployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。官方建议使用Deployment管理ReplicaSets，而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象，因此Deployment将会是使用最频繁的资源对象。 StatefulSet（有状态应用部署） StatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。典型场景：++Zookeper集群++ DaemonSet（确保所有Node运行同一个Pod） DaemonSet确保所有节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。典型场景：++在每个节点部署日志收集程序（如filebeat），监控程序（agent）++ Job（一次性任务） 一次性任务，运行完成后Pod销毁，不再重新启动新容器。还可以任务定时运行。 Cron Job（定时任务） 定时任务，一个CronJob对象就像一个crontab文件的一行。给定时间定期运行，以Cron格式编写。典型场景：数据库备份，发送邮件]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产级k8s二进制v14.1高可用部署]]></title>
    <url>%2F2019%2F06%2F18%2F%E7%94%9F%E4%BA%A7%E7%BA%A7k8s%E4%BA%8C%E8%BF%9B%E5%88%B6v14.1%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、环境准备1.1、角色划分10.8.13.80 vip 10.8.13.81 master01 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.82 master02 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.83 master03 haproxy、keepalived、etcd、kube-apiserver、kube-controller-manager、kube-scheduler10.8.13.84 node01 kubelet、docker、kube_proxy、flanneld10.8.13.85 node02 kubelet、docker、kube_proxy、flanneld 1.2、各主机ssh互通#ssh-keygen#ssh-copy-id 10.8.13.82(83-85) 1.3、环境初始化1.3.1、停止iptablessystemctl stop firewalld.service systemctl disable firewalld.service 1.3.2、关闭selinux# cat /etc/selinux/config SELINUX=disabled# setenforce 0 1.3.3、设置sysctl，开启路由转发# cat /etc/sysctl.conf fs.file-max=1000000 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 net.ipv4.ip_forward = 1 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_sack = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_rmem = 4096 87380 4194304 net.ipv4.tcp_wmem = 4096 16384 4194304 net.ipv4.tcp_max_syn_backlog = 16384 net.core.netdev_max_backlog = 32768 net.core.somaxconn = 32768 net.core.wmem_default = 8388608 net.core.rmem_default = 8388608 net.core.rmem_max = 16777216 net.core.wmem_max = 16777216 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_fin_timeout = 20 net.ipv4.tcp_synack_retries = 2 net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_mem = 94500000 915000000 927000000 net.ipv4.tcp_max_orphans = 3276800 net.ipv4.ip_local_port_range = 1024 65000 net.nf_conntrack_max = 6553500 net.netfilter.nf_conntrack_max = 6553500 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60 net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120 net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120 net.netfilter.nf_conntrack_tcp_timeout_established = 3600 1.3.4、加载ipvscat &lt;&lt; EOF | tee /etc/sysconfig/modules/ipvs.modules#!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 二、集群各功能模块描述 Master节点：Master节点上面主要由四个模块组成，etcd，APIServer，schedule,controller-manager（haproxy、keepalived高可用后面单独说） etcd：etcd是一个高可用的键值存储系统，kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。 APIServer:APIServer负责对外提供Restful的kubernetes API的服务，它是系统管理指令的统一接口，任何对资源的增删该查都要交给APIServer处理后再交给etcd。kubectl(kubernetes提供的客户端工具，该工具内部是对kubernetes API的调用）是直接和APIServer交互的。 schedule:schedule负责调度Pod到合适的Node上，如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定。 kubernetes目前提供了调度算法，同样也保留了接口。用户根据自己的需求定义自己的调度算法。 controller manager:如果APIServer做的是前台的工作的话，那么controller manager就是负责后台的。每一个资源都对应一个控制器。而control manager就是负责管理这些控制器的，比如我们通过APIServer创建了一个Pod，当这个Pod创建成功后，APIServer的任务就算完成了。 Node节点：每个Node节点主要由四个模板组成：kublet， kube-proxy，docker，flanneld kube-proxy:该模块实现了kubernetes中的服务发现和反向代理功能。kube-proxy支持TCP和UDP连接转发，默认基Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响，另外，kube-proxy还支持session affinity。 kublet：kublet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上的所有容器，但是如果容器不是通过kubernetes创建的，它并不会管理。本质上，它负责使Pod的运行状态与期望的状态一致。 flanneld：源主机的flanneld服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一下的有docker0路由到达目标容器。 三、下载链接Client Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gzServer Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gzNode Binarieshttps://dl.k8s.io/v1.14.1/kubernetes-node-linux-amd64.tar.gzetcdhttps://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gzflannelhttps://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 四、Master部署以下操作都在master01上执行，生成证书之后拷贝到master02和master03 4.1、下载软件wget https://dl.k8s.io/v1.14.1/kubernetes-server-linux-amd64.tar.gzwget https://dl.k8s.io/v1.14.1/kubernetes-client-linux-amd64.tar.gzwget https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gzwget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 4.2、ssl安装wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 4.3、创建etcd证书在所有节点（master01-03、node01-02）创建此路径mkdir /k8s/etcd/&#123;bin,cfg,ssl&#125; -pmkdir /k8s/kubernetes/&#123;bin,cfg,ssl&#125; -p 1)、etcd ca配置cd /k8s/etcd/ssl/cat &lt;&lt; EOF | tee ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;etcd&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF 2)、etcd ca证书cat &lt;&lt; EOF | tee ca-csr.json&#123; &quot;CN&quot;: &quot;etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOF 3)、etcd server证书cat &lt;&lt; EOF | tee server-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;10.8.13.81&quot;, &quot;10.8.13.82&quot;, &quot;10.8.13.83&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOF 4)、生成etcd ca证书和私钥 初始化cacfssl gencert -initca ca-csr.json | cfssljson -bare ca [root@master01 ssl]# lsca-config.json ca-csr.json server-csr.json[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca 2019/05/01 16:13:54 [INFO] generating a new CA key and certificate from CSR2019/05/01 16:13:54 [INFO] generate received request2019/05/01 16:13:54 [INFO] received CSR2019/05/01 16:13:54 [INFO] generating key: rsa-20482019/05/01 16:13:54 [INFO] encoded CSR2019/05/01 16:13:54 [INFO] signed certificate with serial number 144752911121073185391033754516204538929473929443[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server-csr.json 生成server证书cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server2019/05/01 16:18:53 [INFO] generate received request2019/05/01 16:18:53 [INFO] received CSR2019/05/01 16:18:53 [INFO] generating key: rsa-20482019/05/01 16:18:54 [INFO] encoded CSR2019/05/01 16:18:54 [INFO] signed certificate with serial number 3881225870405999866391591631675576849701590300572019/05/01 16:18:54 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server.csr server-csr.json server-key.pem server.pem 4.4、etcd安装1）解压缩tar -zxf etcd-v3.3.11-linux-amd64.tar.gzcd etcd-v3.3.11-linux-amd64/cp etcd etcdctl /k8s/etcd/bin/mkdir /data1/etcd 2）配置etcd主文件vim /k8s/etcd/cfg/etcd.conf #[Member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.81:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot; #[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.81:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.81:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; 3）配置etcd启动文件vim /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/data1/etcd/EnvironmentFile=-/k8s/etcd/cfg/etcd.conf# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /k8s/etcd/bin/etcd --name=\&quot;$&#123;ETCD_NAME&#125;\&quot; --data-dir=\&quot;$&#123;ETCD_DATA_DIR&#125;\&quot; --listen-client-urls=\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\&quot; --listen-peer-urls=\&quot;$&#123;ETCD_LISTEN_PEER_URLS&#125;\&quot; --advertise-client-urls=\&quot;$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\&quot; --initial-cluster-token=\&quot;$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\&quot; --initial-cluster=\&quot;$&#123;ETCD_INITIAL_CLUSTER&#125;\&quot; --initial-cluster-state=\&quot;$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\&quot; --cert-file=\&quot;$&#123;ETCD_CERT_FILE&#125;\&quot; --key-file=\&quot;$&#123;ETCD_KEY_FILE&#125;\&quot; --trusted-ca-file=\&quot;$&#123;ETCD_TRUSTED_CA_FILE&#125;\&quot; --client-cert-auth=\&quot;$&#123;ETCD_CLIENT_CERT_AUTH&#125;\&quot; --peer-cert-file=\&quot;$&#123;ETCD_PEER_CERT_FILE&#125;\&quot; --peer-key-file=\&quot;$&#123;ETCD_PEER_KEY_FILE&#125;\&quot; --peer-trusted-ca-file=\&quot;$&#123;ETCD_PEER_TRUSTED_CA_FILE&#125;\&quot; --peer-client-cert-auth=\&quot;$&#123;ETCD_PEER_CLIENT_CERT_AUTH&#125;\&quot;&quot;Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 4)、拷贝master01etcd的证书、配置文件、启动文件到master02和master03对应路径下scp /k8s/etcd/ssl/* 10.8.13.82:/k8s/etcd/ssl/scp /k8s/etcd/ssl/* 10.8.13.83:/k8s/etcd/ssl/scp /k8s/etcd/cfg/* 10.8.13.82:/k8s/etcd/cfg/scp /k8s/etcd/cfg/* 10.8.13.83:/k8s/etcd/cfg/scp /k8s/etcd/bin/* 10.8.13.82:/k8s/etcd/bin/scp /k8s/etcd/bin/* 10.8.13.83:/k8s/etcd/bin/scp /usr/lib/systemd/system/etcd.service 10.8.13.82:/usr/lib/systemd/system/etcd.servicescp /usr/lib/systemd/system/etcd.service 10.8.13.83:/usr/lib/systemd/system/etcd.service 5)、修改master02、master03 etcd的conf配置文件 matser02 etcd.conf配置如下：ssh 10.8.13.82vim /k8s/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd02&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.82:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.82:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.82:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; matser03 etcd.conf配置如下：ssh 10.8.13.83vim /k8s/etcd/cfg/etcd.conf#[Member]ETCD_NAME=&quot;etcd03&quot;ETCD_DATA_DIR=&quot;/data1/etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.8.13.83:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.8.13.83:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.8.13.83:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.8.13.81:2380,etcd02=https://10.8.13.82:2380,etcd03=https://10.8.13.83:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[Security]ETCD_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/k8s/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/k8s/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/k8s/etcd/ssl/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot; 6)、启动etcd服务，并加入开机自启动(master三个节点都执行)systemctl daemon-reloadsystemctl enable etcdsystemctl start etcd 7)、etcd服务检查/k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379&quot; cluster-health以下为输出：member 262d942ab474feaa is healthy: got healthy result from https://10.8.13.82:2379member 3e95c59733e7d54f is healthy: got healthy result from https://10.8.13.83:2379member fe03446cb13e0221 is healthy: got healthy result from https://10.8.13.81:2379cluster is healthy 至此etcd安装完成。。。 4.5、haproxy安装配置1)、master01配置(需要注意的是端口自定义为16443) yum -y install haproxy master01、master02、master03都安装haproxy vim /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# kubernetes apiserver frontend which proxys to the backends#---------------------------------------------------------------------frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------backend kubernetes-apiserver mode tcp balance roundrobin server k8s01 10.8.13.81:6443 check server k8s02 10.8.13.82:6443 check server k8s03 10.8.13.83:6443 check#---------------------------------------------------------------------# collection haproxy statistics message#---------------------------------------------------------------------listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\ Statistics stats uri /admin?stats 2）拷贝master01的haproxy到master02和master03对应路径下scp /etc/haproxy/haproxy.cfg 10.8.13.82:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg 10.8.13.83:/etc/haproxy/haproxy.cfg 3)启动haproxy服务，并加入开机自启动(master三个节点都执行)systemctl daemon-reloadsystemctl enable haproxysystemctl start haproxy 4.6、keepalived安装配置1）master01配置 yum -y install keepalived master01、master02、master03都安装keepalivedvim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens160 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 2）master02配置vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens160 virtual_router_id 51 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 3）master03配置vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_DEVEL&#125;vrrp_script check_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens160 virtual_router_id 51 priority 98 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.8.13.80 &#125; track_script &#123; check_haproxy &#125;&#125; 4）启动keepalived服务（vip在master01上）systemctl daemon-reloadsystemctl enable keepalivedsystemctl start keepalived[root@master01 ~]# systemctl status keepalived● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:33 CST; 3 days ago Process: 992 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 1115 (keepalived) CGroup: /system.slice/keepalived.service ├─1115 /usr/sbin/keepalived -D ├─1116 /usr/sbin/keepalived -D └─1117 /usr/sbin/keepalived -DWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.[root@hwzx-test-cmpmaster01 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:50:56:90:22:79 brd ff:ff:ff:ff:ff:ff inet 10.8.13.81/24 brd 10.8.13.255 scope global ens160 valid_lft forever preferred_lft forever inet 10.8.13.80/32 scope global ens160 valid_lft forever preferred_lft forever inet6 fe80::6772:8bb6:b50c:57fe/64 scope link valid_lft forever preferred_lft forever 5)keepalived配置注意事项&gt;1.killall -0 根据进程名称检测进程是否存活，如果服务器没有该命令，请使用yum install psmisc -y安装&gt;2.第一个master节点的state为MASTER，其他master节点的state为BACKUP&gt;3.priority表示各个节点的优先级，范围：0～250（非强制要求） 4.7、生成kubernets证书与私钥1）制作kubernetes ca证书cd /k8s/kubernetes/sslcat &lt;&lt; EOF | tee ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOF cat &lt;&lt; EOF | tee ca-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@master01 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2019/05/01 09:47:08 [INFO] generating a new CA key and certificate from CSR2019/05/01 09:47:08 [INFO] generate received request2019/05/01 09:47:08 [INFO] received CSR2019/05/01 09:47:08 [INFO] generating key: rsa-20482019/05/01 09:47:08 [INFO] encoded CSR2019/05/01 09:47:08 [INFO] signed certificate with serial number 156611735285008649323551446985295933852737436614[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem 2）制作apiserver证书 ==注意hosts处，所有IP都写进去，包括vip==cat &lt;&lt; EOF | tee server-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;10.254.0.1&quot;, &quot;127.0.0.1&quot;, &quot;10.8.13.81&quot;, &quot;10.8.13.82&quot;, &quot;10.8.13.83&quot;, &quot;10.8.13.84&quot;, &quot;10.8.13.85&quot;, &quot;10.8.13.80&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server2019/05/01 09:51:56 [INFO] generate received request2019/05/01 09:51:56 [INFO] received CSR2019/05/01 09:51:56 [INFO] generating key: rsa-20482019/05/01 09:51:56 [INFO] encoded CSR2019/05/01 09:51:56 [INFO] signed certificate with serial number 3993762167311946548683871990816488873345085010052019/05/01 09:51:56 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca.csr ca-csr.json ca-key.pem ca.pem server.csr server-csr.json server-key.pem server.pem 3）制作kube-proxy证书cat &lt;&lt; EOF | tee kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy[root@master01 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy2019/05/01 09:52:40 [INFO] generate received request2019/05/01 09:52:40 [INFO] received CSR2019/05/01 09:52:40 [INFO] generating key: rsa-20482019/05/01 09:52:40 [INFO] encoded CSR2019/05/01 09:52:40 [INFO] signed certificate with serial number 6339327317875053655115067555587944693891651234172019/05/01 09:52:40 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).[root@master01 ssl]# lsca-config.json ca-csr.json ca.pem kube-proxy-csr.json kube-proxy.pem server-csr.json server.pemca.csr ca-key.pem kube-proxy.csr kube-proxy-key.pem server.csr server-key.pem 4.8部署kubernetes serverkubernetes master 节点运行如下组件：kube-apiserverkube-schedulerkube-controller-managerkube-scheduler 和 kube-controller-manager 以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。 1）解压缩文件 tar -zxf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin/cp kube-scheduler kube-apiserver kube-controller-manager kubectl /k8s/kubernetes/bin/ 2）部署kube-apiserver组件（==注意保留此KEY，下面还会需要==） 创建TLS Bootstrapping Token[root@master01 bin]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;af93a4194e7bcf7f05dc0bab3a6e97cd vim /k8s/kubernetes/cfg/token.csvaf93a4194e7bcf7f05dc0bab3a6e97cd,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; 创建Apiserver配置文件 注：–bind-address=当前节点ip–advertise-address=当前节点ipvim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.81 \--secure-port=6443 \--advertise-address=10.8.13.81 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; 创建apiserver systemd文件vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-apiserverExecStart=/k8s/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kubernetes的证书、配置文件、启动文件到master02和master03对应路径下scp /k8s/kubernetes/ssl/* 10.8.13.82:/k8s/kubernetes/ssl/scp /k8s/kubernetes/ssl/* 10.8.13.83:/k8s/kubernetes/ssl/scp /k8s/kubernetes/cfg/* 10.8.13.82:/k8s/kubernetes/cfg/scp /k8s/kubernetes/cfg/* 10.8.13.83:/k8s/kubernetes/cfg/scp /k8s/kubernetes/bin/* 10.8.13.82:/k8s/kubernetes/bin/scp /k8s/kubernetes/bin/* 10.8.13.83:/k8s/kubernetes/bin/scp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.82:/usr/lib/systemd/systemscp /usr/lib/systemd/system/kube-apiserver.service 10.8.13.83:/usr/lib/systemd/system 5)、修改master02、master03 etcd的conf配置文件 matser02 etcd.conf配置如下：ssh 10.8.13.82vim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.82 \--secure-port=6443 \--advertise-address=10.8.13.82 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; matser03 etcd.conf配置如下：ssh 10.8.13.83vim /k8s/kubernetes/cfg/kube-apiserver KUBE_APISERVER_OPTS=&quot;--logtostderr=true \--v=4 \--etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 \--bind-address=10.8.13.83 \--secure-port=6443 \--advertise-address=10.8.13.83 \--allow-privileged=true \--service-cluster-ip-range=10.254.0.0/16 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth \--token-auth-file=/k8s/kubernetes/cfg/token.csv \--service-node-port-range=30000-50000 \--tls-cert-file=/k8s/kubernetes/ssl/server.pem \--tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem \--client-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem \--etcd-cafile=/k8s/etcd/ssl/ca.pem \--etcd-certfile=/k8s/etcd/ssl/server.pem \--etcd-keyfile=/k8s/etcd/ssl/server-key.pem&quot; 启动服务systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserver[root@elasticsearch01 bin]# systemctl status kube-apiserver● kube-apiserver.service - Kubernetes API Server Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 705 (kube-apiserver) CGroup: /system.slice/kube-apiserver.service └─705 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --s...5月 13 16:00:43 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:43.495504 705 wrap.go:47] GET /api/v1/namespaces/default/endpoints/kubernetes: (3.700854ms) 200 [kube-apiserver/v1.13.1 (linux/amd64) kubernetes/eec55b9 10.8.13.81:56744]5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.955530 705 wrap.go:47] GET /api/v1/services?resourceVersion=37540&amp;timeout=6m29s&amp;timeoutSeconds=389&amp;watch=true: (6m29.001574609s) 200 [kube-proxy/v1.13.1 (linux/amd64) kub... 10.8.13.81:56844]5月 13 16:00:45 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:45.958607 705 get.go:247] Starting watch for /api/v1/services, rv=37540 labels= fields= timeout=8m28s5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.323978 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.410282ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.371766 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (3.606335ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.376888 705 wrap.go:47] GET /apis/apiregistration.k8s.io/v1/apiservices?resourceVersion=32859&amp;timeout=5m5s&amp;timeoutSeconds=305&amp;watch=true: (5m5.001015872s) 200 [kube-apiser... 10.8.13.81:56744]5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.377312 705 reflector.go:357] k8s.io/kube-aggregator/pkg/client/informers/internalversion/factory.go:117: Watch close - *apiregistration.APIService total 0 items received5月 13 16:00:46 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:46.378469 705 get.go:247] Starting watch for /apis/apiregistration.k8s.io/v1/apiservices, rv=32859 labels= fields= timeout=8m12s5月 13 16:00:49 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:49.206602 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-controller-manager?timeout=10s: (4.541086ms) 200 [kube-controller-manager/v1.13.1 (linux/amd64) k...n 127.0.0.1:43776]5月 13 16:00:50 hwzx-test-cmpmaster01 kube-apiserver[705]: I0513 16:00:50.027213 705 wrap.go:47] GET /api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s: (4.418662ms) 200 [kube-scheduler/v1.13.1 (linux/amd64) kubernetes/eec55b9/...n 127.0.0.1:43276]Hint: Some lines were ellipsized, use -l to show in full.[root@master01 bin]# ps -ef |grep kube-apiserverroot 705 1 3 5月10 ? 02:35:10 /k8s/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379 --bind-address=10.8.13.81 --secure-port=6443 --advertise-address=10.8.13.81 --allow-privileged=true --service-cluster-ip-range=10.254.0.0/16 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --enable-bootstrap-token-auth --token-auth-file=/k8s/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/k8s/kubernetes/ssl/server.pem --tls-private-key-file=/k8s/kubernetes/ssl/server-key.pem --client-ca-file=/k8s/kubernetes/ssl/ca.pem --service-account-key-file=/k8s/kubernetes/ssl/ca-key.pem --etcd-cafile=/k8s/etcd/ssl/ca.pem --etcd-certfile=/k8s/etcd/ssl/server.pem --etcd-keyfile=/k8s/etcd/ssl/server-key.pemroot 7098 24767 0 15:57 pts/0 00:00:00 grep --color=auto kube-apiserver[root@master01 bin]# netstat -tulpn |grep kube-apiservetcp 0 0 10.8.13.81:6443 0.0.0.0:* LISTEN 705/kube-apiserver tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 705/kube-apiserver 3）部署kube-scheduler组件创建kube-scheduler配置文件vim /k8s/kubernetes/cfg/kube-scheduler KUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot; 参数备注：--address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；--kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；--leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；创建kube-scheduler systemd文件 vim /usr/lib/systemd/system/kube-scheduler.service [Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-schedulerExecStart=/k8s/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kube-scheduler配置文件、启动文件到master02和master03对应路径下scp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.82:/k8s/kubernetes/cfg/kube-schedulerscp /k8s/kubernetes/cfg/kube-scheduler 10.8.13.83:/k8s/kubernetes/cfg/kube-schedulerscp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.82:/usr/lib/systemd/system/kube-scheduler.servicescp /usr/lib/systemd/system/kube-scheduler.service 10.8.13.83:/usr/lib/systemd/system/kube-scheduler.service 启动服务systemctl daemon-reloadsystemctl enable kube-scheduler.service systemctl start kube-scheduler.service[root@master01 bin]# systemctl status kube-scheduler.service● kube-scheduler.service - Kubernetes Scheduler Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 693 (kube-scheduler) CGroup: /system.slice/kube-scheduler.service └─693 /k8s/kubernetes/bin/kube-scheduler --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024121 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:49 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:49.024161 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151743 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:51 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:51.151799 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434965 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:53 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:53.434999 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571674 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:10:57 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:10:57.571707 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914369 693 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_7601efea-7319-11e9-8964-0050569059b4 and has not yet expired5月 13 16:11:01 hwzx-test-cmpmaster01 kube-scheduler[693]: I0513 16:11:01.914411 693 leaderelection.go:210] failed to acquire lease kube-system/kube-scheduler 4）部署kube-controller-manager组件 创建kube-controller-manager配置文件vim /k8s/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \--v=4 \--master=127.0.0.1:8080 \--leader-elect=true \--address=127.0.0.1 \--service-cluster-ip-range=10.254.0.0/16 \--cluster-name=kubernetes \--cluster-signing-cert-file=/k8s/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/k8s/kubernetes/ssl/ca-key.pem \--root-ca-file=/k8s/kubernetes/ssl/ca.pem \--service-account-private-key-file=/k8s/kubernetes/ssl/ca-key.pem&quot; 创建kube-controller-manager systemd文件vim /usr/lib/systemd/system/kube-controller-manager.service [Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-controller-managerExecStart=/k8s/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 拷贝master01 kube-controller-manager配置文件、启动文件到master02和master03对应路径下scp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.82:/k8s/kubernetes/cfg/kube-controller-managerscp /k8s/kubernetes/cfg/kube-controller-manager 10.8.13.83:/k8s/kubernetes/cfg/kube-controller-managerscp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.82:/usr/lib/systemd/system/kube-controller-manager.servicescp /usr/lib/systemd/system/kube-controller-manager.service 10.8.13.83:/usr/lib/systemd/system/kube-controller-manager.service 启动服务systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-manager[root@master01 bin]# systemctl status kube-controller-manager● kube-controller-manager.service - Kubernetes Controller Manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since 五 2019-05-10 20:33:32 CST; 2 days ago Docs: https://github.com/kubernetes/kubernetes Main PID: 685 (kube-controller) CGroup: /system.slice/kube-controller-manager.service └─685 /k8s/kubernetes/bin/kube-controller-manager --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect=true --address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/k8s/kubernetes/ssl/ca...5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539102 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:45 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:45.539136 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767187 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:48 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:48.767221 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939294 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:50 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:50.939329 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212185 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:53 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:53.212218 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291399 685 leaderelection.go:289] lock is held by hwzx-test-cmpmaster03_823f19e6-7319-11e9-94be-0050569059b4 and has not yet expired5月 13 16:16:57 hwzx-test-cmpmaster01 kube-controller-manager[685]: I0513 16:16:57.291430 685 leaderelection.go:210] failed to acquire lease kube-system/kube-controller-manager 4.9、验证kubeserver服务设置环境变量(==所有服务器都执行此步==) vim /etc/profilePATH=/k8s/kubernetes/bin:$PATHsource /etc/profile 查看master服务状态[root@master01 ~]# kubectl get cs,nodesNAME STATUS MESSAGE ERRORcomponentstatus/scheduler Healthy ok componentstatus/controller-manager Healthy ok componentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 至此master组件安装完毕 五、Node部署(node01、node02安装)kubernetes work 节点运行如下组件：dockerkubeletkube-proxyflannel 5.1 Docker环境安装yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -ryum install docker-ce -ysystemctl start docker &amp;&amp; systemctl enable docker 5.2 部署kubelet组件kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如exec、run、logs 等;kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况;为确保安全，只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如apiserver、heapster)。 1)、安装二进制文件wget https://dl.k8s.io/v1.13.1/kubernetes-node-linux-amd64.tar.gztar zxvf kubernetes-node-linux-amd64.tar.gzcd kubernetes/node/bin/cp kube-proxy kubelet kubectl /k8s/kubernetes/bin/ 2)、从master01复制相关证书到node01和node02节点[root@master01 ssl]# cd /k8s/kubernetes/ssl/[root@master01 ssl]# scp *.pem 10.8.13.84:/k8s/kubernetes/ssl/root@10.8.13.84&apos;s password: ca-key.pem 100% 1679 914.6KB/s 00:00 ca.pem 100% 1359 1.0MB/s 00:00 kube-proxy-key.pem 100% 1675 1.2MB/s 00:00 kube-proxy.pem 100% 1403 1.1MB/s 00:00 server-key.pem 100% 1679 809.1KB/s 00:00 server.pem 100% 1675 1.2MB/s 00:00[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.84:/k8s/etcd/ssl/[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.84:/k8s/etcd/bin/ [root@master01 ssl]# scp *.pem 10.8.13.85:/k8s/kubernetes/ssl/root@10.8.13.85&apos;s password: ca-key.pem 100% 1679 914.6KB/s 00:00 ca.pem 100% 1359 1.0MB/s 00:00 kube-proxy-key.pem 100% 1675 1.2MB/s 00:00 kube-proxy.pem 100% 1403 1.1MB/s 00:00 server-key.pem 100% 1679 809.1KB/s 00:00 server.pem 100% 1675 1.2MB/s 00:00[root@master01 ssl]# scp /k8s/etcd/ssl/* 10.8.13.85:/k8s/etcd/ssl/[root@master01 ssl]# scp /k8s/etcd/bin/* 10.8.13.85:/k8s/etcd/bin/ 3)、创建kubelet bootstrap kubeconfig文件 通过脚本实现KUBE_APISERVER=vip:haproxy中自定义的端口BOOTSTRAP_TOKEN=部署kube-apiserver中生成的tokenvim /k8s/kubernetes/cfg/environment.sh#!/bin/bash#创建kubelet bootstrapping kubeconfig BOOTSTRAP_TOKEN=af93a4194e7bcf7f05dc0bab3a6e97cdKUBE_APISERVER=&quot;https://10.8.13.80:16443&quot;#设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig #设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig # 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig # 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig #---------------------- # 创建kube-proxy kubeconfig文件 kubectl config set-cluster kubernetes \ --certificate-authority=/k8s/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \ --client-certificate=/k8s/kubernetes/ssl/kube-proxy.pem \ --client-key=/k8s/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 执行脚本[root@node01 cfg]# cd /k8s/kubernetes/cfg/[root@node01 cfg]# sh environment.sh Cluster &quot;kubernetes&quot; set.User &quot;kubelet-bootstrap&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;.Cluster &quot;kubernetes&quot; set.User &quot;kube-proxy&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;.[root@node01 cfg]# lsbootstrap.kubeconfig environment.sh kube-proxy.kubeconfig 4)、创建kubelet参数配置模板文件 address:node节点IPvim /k8s/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 10.8.13.84port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.254.0.10&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true 5)、创建kubelet配置文件 –hostname-override=node节点IPvim /k8s/kubernetes/cfg/kubelet KUBELET_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.84 \--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \--config=/k8s/kubernetes/cfg/kubelet.config \--cert-dir=/k8s/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; 6)、创建kubelet systemd文件vim /usr/lib/systemd/system/kubelet.service [Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service [Service]EnvironmentFile=/k8s/kubernetes/cfg/kubeletExecStart=/k8s/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process [Install]WantedBy=multi-user.target 7)、将kubelet-bootstrap用户绑定到系统集群角色(==在master01执行==)kubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrap 注意这个默认连接localhost:8080端口，可以在master上操作[root@master01 ssl]# kubectl create clusterrolebinding kubelet-bootstrap \&gt; --clusterrole=system:node-bootstrapper \&gt; --user=kubelet-bootstrapclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created 8)、复制node01kubelet配置和启动服务文件到node02相对应路径scp /k8s/kubernetes/cfg/* 10.8.13.85:/k8s/kubernetes/cfg/scp /usr/lib/systemd/system/kubelet.service 10.8.13.85:/usr/lib/systemd/system/kubelet.service 9)、修改node02中kubelet.config和kubelet文件中的nodeIP node02中kubelet.config配置 address:node节点IPvim /k8s/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 10.8.13.85port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.254.0.10&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true node02中kubelet配置 –hostname-override=node节点IPvim /k8s/kubernetes/cfg/kubelet KUBELET_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.85 \--kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig \--config=/k8s/kubernetes/cfg/kubelet.config \--cert-dir=/k8s/kubernetes/ssl \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot; 10)、启动服务 systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubelet [root@node01 ~]# systemctl status kubelet● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2019-05-10 20:31:30 CST; 3 days ago Main PID: 8583 (kubelet) Memory: 45.5M CGroup: /system.slice/kubelet.service └─8583 /k8s/kubernetes/bin/kubelet --logtostderr=true --v=4 --hostname-override=10.8.13.84 --kubeconfig=/k8s/kubernetes/cfg/kubelet.kubeconfig --bootstrap-kubeconfig=/k8s/kubernetes/cfg/bootstrap.kubeconfig --config=/k8s/kubernetes/cfg/kubelet.config --cer... 11)、Master接受kubelet CSR请求(master01操作，接受两个node节点)可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书，如下是手动 approve CSR请求操作方法 查看CSR列表 [root@master01 ssl]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc 102s kubelet-bootstrap Pending 接受node [root@master01 ssl]# kubectl certificate approve node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdccertificatesigningrequest.certificates.k8s.io/node-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc approved 再查看CSR [root@master01 ssl]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-ij3py9j-yi-eoa8sOHMDs7VeTQtMv0N3Efj3ByZLMdc 5m13s kubelet-bootstrap Approved,Issued 5.3部署kube-proxy组件(node01执行)kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡 1)、创建 kube-proxy 配置文件 –hostname-override=node节点IP vim /k8s/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.84 \--cluster-cidr=10.254.0.0/16 \--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot; 2)、创建kube-proxy systemd文件vim /usr/lib/systemd/system/kube-proxy.service [Unit]Description=Kubernetes ProxyAfter=network.target [Service]EnvironmentFile=-/k8s/kubernetes/cfg/kube-proxyExecStart=/k8s/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failure [Install]WantedBy=multi-user.target 3)、复制node01kube-proxy配置和服务启动文件到node02相对应路径scp /k8s/kubernetes/cfg/kube-proxy 10.8.13.85:/k8s/kubernetes/cfg/kube-proxyscp /usr/lib/systemd/system/kube-proxy.service 10.8.13.85:/usr/lib/systemd/system/kube-proxy.service 4)、修改node02kube-proxy配置文件如下 –hostname-override=node节点IPvim /k8s/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \--v=4 \--hostname-override=10.8.13.85 \--cluster-cidr=10.254.0.0/16 \--kubeconfig=/k8s/kubernetes/cfg/kube-proxy.kubeconfig&quot; 5)、启动服务 systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxy [root@node01 ~]# systemctl status kube-proxy.service ● kube-proxy.service - Kubernetes Proxy Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2019-05-10 20:31:31 CST; 3 days ago Main PID: 8669 (kube-proxy) Memory: 9.9M CGroup: /system.slice/kube-proxy.service ‣ 8669 /k8s/kubernetes/bin/kube-proxy --logtostderr=true --v=4 --hostname-override=10.8.13.84 --cluster-cidr=10.254.0.0/16 --kubeconfig...May 14 09:07:50 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:50.634641 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:51 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:51.365166 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:52 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:52.647317 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:53 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:53.375833 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:54 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:54.658691 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:55 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:55.387881 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:56 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:56.670562 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:57 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:57.398763 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:58 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:58.682049 8669 config.go:141] Calling handler.OnEndpointsUpdateMay 14 09:07:59 hwzx-test-cmpnode01 kube-proxy[8669]: I0514 09:07:59.411141 8669 config.go:141] Calling handler.OnEndpointsUpdate 6)、查看集群状态[root@master01 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSION10.8.13.84 Ready &lt;none&gt; 3d13h v1.14.110.8.13.85 Ready &lt;none&gt; 3d13h v1.14.1 至此node组件安装完成 六、Flanneld网络部署(以node01为例，node02同样操作)默认没有flanneld网络，Node节点间的pod不能通信，只能Node内通信，为了部署步骤简洁明了，故flanneld放在后面安装flannel服务需要先于docker启动。flannel服务启动时主要做了以下几步的工作： 从etcd中获取network的配置信息 划分subnet，并在etcd中进行注册 将子网信息记录到/run/flannel/subnet.env中 6.1 etcd注册网段[root@node01 ~]# /k8s/etcd/bin/etcdctl --ca-file=/k8s/etcd/ssl/ca.pem --cert-file=/k8s/etcd/ssl/server.pem --key-file=/k8s/etcd/ssl/server-key.pem --endpoints=&quot;https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379&quot; set /k8s/network/config &apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;&#123; &quot;Network&quot;: &quot;10.254.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125; flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致； 6.2 flannel安装1)、解压安装tar -zxf flannel-v0.11.0-linux-amd64.tar.gzmv flanneld mk-docker-opts.sh /k8s/kubernetes/bin/ 2)、配置flanneldvim /k8s/kubernetes/cfg/flanneldFLANNEL_OPTIONS=&quot;--etcd-endpoints=https://10.8.13.81:2379,https://10.8.13.82:2379,https://10.8.13.83:2379,https://10.8.13.84:2379,https://10.8.13.85:2379 -etcd-cafile=/k8s/etcd/ssl/ca.pem -etcd-certfile=/k8s/etcd/ssl/server.pem -etcd-keyfile=/k8s/etcd/ssl/server-key.pem -etcd-prefix=/k8s/network&quot; 3)、创建flanneld systemd文件vim /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service [Service]Type=notifyEnvironmentFile=/k8s/kubernetes/cfg/flanneldExecStart=/k8s/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/k8s/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure [Install]WantedBy=multi-user.target ==注意：== mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥； flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口; flanneld 运行时需要 root 权限； 3）配置Docker启动指定子网 添加EnvironmentFile=/run/flannel/subnet.env，修改ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS即可 vim /usr/lib/systemd/system/docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target [Service]Type=notifyEnvironmentFile=/run/flannel/subnet.envExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s [Install]WantedBy=multi-user.target 4)、启动服务 注意启动flannel前要关闭docker及相关的kubelet这样flannel才会覆盖docker0网桥systemctl daemon-reloadsystemctl stop dockersystemctl start flanneldsystemctl enable flanneldsystemctl start dockersystemctl restart kubeletsystemctl restart kube-proxy 5)、验证服务[root@node01 bin]# cat /run/flannel/subnet.env DOCKER_OPT_BIP=&quot;--bip=10.254.88.1/24&quot;DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;DOCKER_OPT_MTU=&quot;--mtu=1450&quot;DOCKER_NETWORK_OPTIONS=&quot; --bip=10.254.88.1/24 --ip-masq=false --mtu=1450&quot; 注意查看docker0和flannel是不是属于同一网段[root@node01 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:50:56:90:67:d1 brd ff:ff:ff:ff:ff:ff inet 10.8.13.84/24 brd 10.8.13.255 scope global ens160 valid_lft forever preferred_lft forever inet6 fe80::802:2c0f:a197:38a7/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:5c:18:5b:93 brd ff:ff:ff:ff:ff:ff inet 10.254.88.1/24 brd 10.254.88.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:5cff:fe18:5b93/64 scope link valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 8e:f6:f8:87:47:ee brd ff:ff:ff:ff:ff:ff inet 10.254.88.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::8cf6:f8ff:fe87:47ee/64 scope link valid_lft forever preferred_lft forever 至此flannel安装完成 查看NODE和etcd[root@hwzx-test-cmpmaster01 ~]# kubectl get nodes,csNAME STATUS ROLES AGE VERSIONnode/10.8.13.84 Ready &lt;none&gt; 3d13h v1.14.1node/10.8.13.85 Ready &lt;none&gt; 3d13h v1.14.1NAME STATUS MESSAGE ERRORcomponentstatus/controller-manager Healthy ok componentstatus/scheduler Healthy ok componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器网络 calico 基本原理和模拟]]></title>
    <url>%2F2019%2F04%2F22%2F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%20calico%20%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E6%A8%A1%E6%8B%9F%2F</url>
    <content type="text"><![CDATA[摘要在容器网络跨 Host 互联方案中，除了 Flannel 的隧道实现方案，还有一种比较主流的纯三层路由的方案 Calico ，与 Flannel 不同的是 Calico 不使用隧道或 NAT 来实现转发，而是巧妙的把所有二三层流量转换成三层流量，并通过 host 上路由配置完成跨 Host 转发，本文对 Calico 的方案进行基本原理分析和模拟验证。 简介Calico 官方定义如下：Calico provides secure network connectivity for containers and virtual machine workloads.Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.Calico also provides dynamic enforcement of network security rules. Using Calico’s simple policy language, you can achieve fine-grained control over communications between containers, virtual machine workloads, and bare metal host endpoints. 总结如下： Calico 为容器和 vm 等提供一个安全的网路互联方法，我们把 VM、Container、白盒等实例统称为 workloads，通过给 workload 分配一个扁平的三层路由可达 IP 地址实现转发，是一种纯三层转发的方案，workload 之间不使用隧道或 NAT 技术，这种方式提供更好的网络性能，提高易维护和可交互性。同时也支持 IPIP 隧道和与 Flannel 集成能力。Calico 提供动态实施的网络安全策略，可使用简单的安全模型语言实现细粒度的安全控制。 相对 Overlay，为什么用 Calico？Calico 是一种 workloads 之间互通的网络方案，并支持以上任意一种场景。在虚拟化平台中，比如 OpenStack、Docker 等都需要实现 workloads 之间互连，但同时也需要对 workloads 做隔离控制，就像在 Internet 中的服务仅开放80端口、公有云的多租户一样，提供隔离和管控机制。 而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现 workloads 的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。 那么有更好的方案吗？我们在审视了二层方案并思考如何支持大型网络时从 Internet 网络实现中获得了灵感。我们知道在 Internet 的网络中，路由器作为网关连接着自己的子网络，之间通过 BGP 相互学习，并使用防火墙控制不同子网之间安全策略，所有这些子网络共同组成了 Internet 网络，那么，这种方式能否也应用到虚拟化基础平台中呢？ 借鉴这种思路，我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案，整个方案的优势为： 更优的资源利用： 二层网络通讯需要依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，Calico 使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。 另外，二层网络使用 VLAN 隔离技术，天生有 4096 个规格限制，即便可以使用 vxlan 解决，但 vxlan 又带来了隧道开销的新问题。而 Calico 不使用 vlan 或 vxlan 技术，使资源利用率更高。 可扩展性： Calico 使用与 Internet 类似的方案，Internet 的网络比任何数据中心都大，Calico 同样天然具有可扩展性。 简单而更容易 debug： 因为没有隧道，意味着 workloads 之间路径更短更简单，配置更少，在 host 上更容易进行 debug 调试。 更少的依赖： Calico 仅依赖三层路由可达。 可适配性： Calico 较少的依赖性使它能适配所有 VM、Container、白盒或者混合环境场景。 除了以上，还有更多其他优势，因此，如果你在为 OpenStack 或 docker 构建虚拟化网络环境的话，可以好好考虑下 Calico 的方案。 Calico 由 5 部分组件组成，整体构架如下： Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等 Orchestrator Plugin：编排插件，并不是独立运行的某些进程，而是设计与 k8s、OpenStack 等平台集成的插件，如 Neutron’s ML2 plugin 用于用户使用 Neutron API 来管理 Calico，本质是要解决模型和 API 间的兼容性问题。 Etcd：Calico 模型的存储引擎。 BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。 BGP Route Reflector(BIRD)：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。 模拟组网组网如下： guest 配置 169.254.1.1 的默认路由； host 上配置 10.20.2.0/24 和 10.20.1.3/32 路由； 开启 arp proxy 和 ip_forward 能力； 网络连通性测试： # HOST0[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.2.2PING 10.20.2.2 (10.20.2.2) 56(84) bytes of data. bytes from 10.20.2.2: icmp_seq=1 ttl=62 time=0.774 ms bytes from 10.20.2.2: icmp_seq=2 ttl=62 time=0.332 ms 10.20.1.2 与跨 Host 跨子网 10.20.2.2 互通成功# HOST0[root@i-7dlclo08 ~]# ip netns exec ns0 ping 10.20.1.3PING 10.20.1.3 (10.20.1.3) 56(84) bytes of data. bytes from 10.20.1.3: icmp_seq=1 ttl=62 time=957 ms bytes from 10.20.1.3: icmp_seq=2 ttl=62 time=0.432 ms bytes from 10.20.1.3: icmp_seq=3 ttl=62 time=0.563 ms 10.20.1.2 与跨 Host 同子网 10.20.1.3 互通成功 [root@i-7dlclo08 ~]# ip netns exec ns0 ping 192.168.100.3PING 192.168.100.3 (192.168.100.3) 56(84) bytes of data. bytes from 192.168.100.3: icmp_seq=1 ttl=63 time=1.00 ms bytes from 192.168.100.3: icmp_seq=2 ttl=63 time=0.695 ms 在未做安全策略下，10.20.1.2 与 Host 192.168.100.3 互通成功 转发过程： guest0 本地所有数据包都转发到一个虚假的地址 169.254.1.1，发送 ARP Req。 Host0 的 veth 端收到 ARP Req 时通过开启网卡的 proxy arp 代理功能直接把自己的 MAC 地址返回给 guest0 guest0 发送目的地址为 guest1 的 IP 数据包 因为使用了 169.254.1.1 这样的地址，Host 判断为三层路由转发，查询本地路由 10.20.2.0/24 via 192.168.0.3 dev eth0 发送给对端 host1，如果配置 BGP，这里会看到 proto 协议为 BIRD 在发送之前匹配本地的 iptables 规则进行安全策略控制，这里略 当 host1 收到 10.20.2.2 的数据包时查找本地路由表匹配 10.20.2.2/32 dev veth0 scope link 转发到对应的 veth0 端从而到达 guest1 回程类似，略 整体转发流程简单清晰。因此可以看到，Calico 需要给所有 guest 配置一条特别的路由并利用 veth 的 proxy arp 的能力让 guest 出来的所有转发都变成三层路由转发，再利用 host 的路由表进行转发，这种方式不仅仅实现了同 host 的二三层转发，也能实现跨 host 的转发。 遗留问题 1、租户隔离问题 Calico 的三层方案是直接在 host 上进行路由寻址，那么对于多租户如果使用同一个 CIDR 网络就面临着地址冲突的问题。 2、路由规模问题 通过路由规则可以看出，路由规模和 guest 分布有关，如果 guest 离散分布在 host 集群中，势必会产生较多的路由项。 3、iptables 规则规模问题 1台 Host 上可能虚拟化十几或几十个容器实例，过多的 iptables 规则造成复杂性和不可调试性，同时也存在性能损耗。 4、跨子网时的网关路由问题 当对端网络不为二层可达时，需要通过三层路由机时，需要网关支持自定义路由配置，即 guest 的目的地址为本网段的网关地址，再由网关进行跨三层转发。 总结： 1、Calico 通过巧妙的引导 workload 所有的流量至一个特殊的网关 169.254.1.1，从而引流到 host 的 calixxx 网络设备上，形成了二三层流量全部转换 host 的三层流量转发。 2、在 Host 上通过开启 arp proxy 的能力实现 arp 代答，arp 广播被抑制在 host 里，arp 记录变成“无效记录”，抑制了广播风暴和不会有 arp 表膨胀的问题。 3、使用 iptables 在 host 做 policy 实现的复杂的安全模型，安全策略应用在每一台虚拟路由器上，最终形成了一个分布式的安全系统。]]></content>
      <categories>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>calico</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：URL与视图函数（九）]]></title>
    <url>%2F2019%2F04%2F13%2F%E4%B9%9D%E3%80%81URL%E4%B8%8E%E8%A7%86%E5%9B%BE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在讲URL与视图函数之前我们先给大家简单介绍一下用户访问网站的流程。我们访问一个网站的时候，一般先打开浏览器，然后在浏览器的地址栏里输入一个网址，也就是URL，然后回车，我们就可以在浏览器里看到这个网址返回的内容。这是我们能看得见的过程，还有一些我们看不见的过程，那就是：++当我们在浏览器里输入网址（URL）时，回车，然后浏览器就会向目标网址发送一个HTTP请求，服务器收到请求之后就会给这个请求做出一个响应，这个响应就是把对应的内容通过浏览器渲染出来，呈现给我们看++。这个过程就是请求与响应。 下图，就是请求响应的过程。 上面我们提到了URL，这个URL在我们的Django中，其实是由我们自己构造的。(这个说法不太严谨，但为了方便大家理解之后的内容，先当这说辞是正确的。) Django中，我们约定URL是在项目同名目录下的urls.py文件里urlpatterns列表构造的。 myblog/myblog/urls.py 表现形式如下： urlpatterns = [ path(正则表达式, views视图函数，参数，别名),]括号里的参数说明：1、一个正则表达式字符串2、一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串3、可选的要传递给视图函数的默认参数（字典形式）4、一个可选的name参数(别名) 完整的URL应该要这么写：path(正则表达式, views视图函数，参数，别名)：里面的正则表达式, views视图函数，是必须要写的，而参数，别名是可选的。我们在有特殊需要的时候才写。 通过上面我们可以看到，每个URL都对应一个views视图函数名，视图函数名不能相同，否则会报错。视图函数，Django中约定写在APP应用里的views.py文件里。然后在urls.py文件里通过下面的方式导入：from APP应用名 import viewsfrom APP应用名.vews import 函数名或类名 视图函数是一个简单的Python 函数，它接受Web请求并且返回Web响应。响应可以是一张网页的HTML内容，一个重定向，一个404错误，一个XML文档，或者一张图片. . . 是任何东西都可以。无论视图本身包含什么逻辑，都要返回响应。这个视图函数代码一般约定是放置在项目或应用程序目录中的名为views.py的文件中。 http请求中产生两个核心对象： 1、http请求—-&gt;HttpRequest对象，用户请求相关的所有信息（对象） 2、http响应—-&gt;HttpResponse对象，响应字符串 首先，打开打开bolg目录下的views.py文件，写一个hello视图函数，在里面输入：from django.http import HttpResponsedef hello(request): &quot;&quot;&quot; 写一个hello函数，通过request接收URL或者说是http请求信息， 然后给这个请求返回一个HttpResponse对象 &quot;&quot;&quot; return HttpResponse(&apos;欢迎使用Django！&apos;) 例子里，我们用到的request，就是HttpRequest对象。HttpResponse(“欢迎使用Django！”)，就是HttpRequest对象，它向http请求响应了一段字符串对象。 我们打开myblog目录下的urls.py文件中先导入视图函数，然后构造一个URL，代码如下： from blog import views #导入视图函数urlpatterns = [ ... path(&apos;&apos;, views.hello), #这个是我们构造的URL] 代码写完之后，启动项目就可以在浏览器里看到视图函数返回的字符串”欢迎使用Django！” 每一个URL都会对应一个视图函数，当一个用户请求访问Django站点的一个页面时，然后就由Django路由系统（URL配置文件）去决定要执行哪个视图函数使用的算法。 通过URL对应关系匹配 -&gt;找到对应的函数（或者类）-&gt;返回字符串(或者读取Html之后返回渲染的字符串）这个过程也就是我们Django请求的生命周期。 视图函数，就是围绕着HttpRequest和HttpResponse这两个对象进行的。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：使用富文本编辑器（八）]]></title>
    <url>%2F2019%2F04%2F12%2F%E5%85%AB%E3%80%81%E4%BD%BF%E7%94%A8%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[在Django admin后台添加数据的时候，文章内容文本框想发布一篇图文并茂的文章需就得手写Html代码，这十分吃力，也没法上传图片和文件。这显然不是我等高大上程序猿想要的。 为提升效率，我们可以使用富文本编辑器添加数据。支持Django的富文本编辑器很多，这里我推荐使用DjangoUeditor，Ueditor是百度开发的一个富文本编辑器，功能强大。下面教大家安装如何使用DjangoUeditor。 1、首先我们先下载DjangoUeditor包点击下面的链接进行下载！下载完成然后解压到项目根目录里。 https://www.django.cn/media/upfile/DjangoUeditor_20181010013851_248.zip 2、settings.py里注册APP在INSTALLED_APPS里添加’DjangoUeditor’ myblog/settings.yINSTALLED_APPS = [ &apos;django.contrib.admin&apos;, .... &apos;DjangoUeditor&apos;, #注册APP应用]##验证是否已经增加正确，按住 ctrl 点击DjangoUdeitor，只要能跳转到目录即可； 3、myblog/urls.py里添加url。myblog/urls.py...from django.urls import path, include#留意上面这行比原来多了一个includeurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;&apos;, views.hello), path(&apos;ueditor/&apos;, include(&apos;DjangoUeditor.urls&apos;)), #添加DjangoUeditor的URL] 4、修改blog/models.py里需要使用富文本编辑器渲染的字段。这里面我们要修改的是Article表里的body字段。 把原来的： blog/models.pybody = models.TextField() 修改成：blog/models.pyfrom DjangoUeditor.models import UEditorField #头部增加这行代码导入UEditorFieldbody = UEditorField(&apos;内容&apos;, width=800, height=500, toolbars=&quot;full&quot;, imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot;, upload_settings=&#123;&quot;imageMaxSize&quot;: 1204000&#125;, settings=&#123;&#125;, command=None, blank=True ) 留意里面的imagePath=&quot;upimg/&quot;, filePath=&quot;upfile/&quot; 这两个是图片和文件上传的路径，我们上传文件，++会自动上传到项目根目录media文件夹下对应的upimg和upfile目录里++，这个目录名可以自行定义。有的人问，为什么会上传到media目录里去呢？那是因为之前我们在基础配置文章里，设置了上传文件目录media。 上面步骤完成后，我们启动项目，进入文章发布页面。提示出错： 错误一：File &quot;C:\Python37\lib\site-packages\django\views\debug.py&quot;, line 332, in get_traceback_html t = DEBUG_ENGINE.from_string(fh.read())UnicodeDecodeError: &apos;gbk&apos; codec can&apos;t decode byte 0xa6 in position 9737: illegal multibyte sequence 查看错误栈最后一行发现是 编码问题找到django 源码 “C:\Python37\lib\site-packages\django\views\debug.py” 332行位置 ,增加utf-8编码 open( encoding=’utf-8’)，问题解决：def get_traceback_html(self): &quot;&quot;&quot;Return HTML version of debug 500 HTTP error page.&quot;&quot;&quot; with Path(CURRENT_DIR, &apos;templates&apos;, &apos;technical_500.html&apos;).open( encoding=&apos;utf-8&apos;) as fh: t = DEBUG_ENGINE.from_string(fh.read()) c = Context(self.get_traceback_data(), use_l10n=False) return t.render(c) 错误二：render() got an unexpected keyword argument &apos;renderer&apos; 我这里使用的是最新版本的Django2.1.1所以报错，解决办法很简单。打开这个文件的93行，注释这行即可。 修改成之后，重新刷新页面，就可以看到我们的富文本编辑器正常显示。 留意，如果我们在富文本编辑器里，上传图片，在编辑器内容里不显示上传的图片。那我们还需要进行如下设置，打开myblog/urls.py文件，在里面输入如下代码：myblog/urls.py....from django.urls import path, include, re_path#上面这行多加了一个re_pathfrom django.views.static import serve#导入静态文件模块from django.conf import settings#导入配置文件里的文件上传配置urlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), .... re_path(&apos;^media/(?P&lt;path&gt;.*)$&apos;, serve, &#123;&apos;document_root&apos;: settings.MEDIA_ROOT&#125;),#增加此行] 设置好了之后，图片就会正常显示。这样我们就可以用DjangoUeditor富文本编辑器发布图文并茂的文章了。 随便测试了一篇文章：]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：用Admin管理后台数据（七）]]></title>
    <url>%2F2019%2F04%2F11%2F%E4%B8%83%E3%80%81%E7%94%A8Admin%E7%AE%A1%E7%90%86%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[django的admin后台管理它可以让我们快速便捷管理数据，我们可以在各个app目录下的admin.py文件中对其进行控制。想要对APP应用进行管理，最基本的前提是要先在settings里对其进行注册，就是在INSTALLED_APPS里把APP名添加进去。 注册APP应用之后，我们想要在admin后台里对数据库表进行操作，我们还得在应用APP下的admin.py文件里对数据库表先进行注册。我们的APP应用是blog，所以我们需要在blog/admin.py文件里进行注册： blog/admin.pyfrom django.contrib import adminfrom .models import Banner, Category, Tag, Tui, Article, Link #导入需要管理的数据库表@admin.register(Article)class ArticleAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;category&apos;, &apos;title&apos;, &apos;tui&apos;, &apos;user&apos;, &apos;views&apos;, &apos;created_time&apos;) # 文章列表里显示想要显示的字段 list_per_page = 50 # 满50条数据就自动分页 ordering = (&apos;-created_time&apos;,) #后台数据列表排序方式 list_display_links = (&apos;id&apos;, &apos;title&apos;) # 设置哪些字段可以点击进入编辑界面@admin.register(Banner)class BannerAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;text_info&apos;, &apos;img&apos;, &apos;link_url&apos;, &apos;is_active&apos;)@admin.register(Category)class CategoryAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;, &apos;index&apos;)@admin.register(Tag)class TagAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;)@admin.register(Tui)class TuiAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;)@admin.register(Link)class LinkAdmin(admin.ModelAdmin): list_display = (&apos;id&apos;, &apos;name&apos;,&apos;linkurl&apos;) 登录管理后台 http://127.0.0.1:8000/admin/ 多出了之前我们在models里创建的表。我们可以在后台里面对这些表进行增、删、改方面的操作。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：创建数据库模型（六）]]></title>
    <url>%2F2019%2F04%2F10%2F%E5%85%AD%E3%80%81%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Django是通过Model操作数据库，不管你数据库的类型是MySql或者Sqlite，Django它自动帮你生成相应数据库类型的SQL语句，所以不需要关注SQL语句和类型，对数据的操作Django帮我们自动完成。只要回写Model就可以了！ django根据代码中定义的类来自动生成数据库表。我们写的类表示数据库的表，如果根据这个类创建的对象是数据库表里的一行数据，对象.id 对象.value是每一行里的数据。 基本的原则如下： 每个模型在Django中的存在形式为一个Python类 每个模型都是django.db.models.Model的子类 模型里的每个类代表数据库中的一个表 模型的每个字段（属性）代表数据表的某一列 Django将自动为你生成数据库访问API 完成博客，我们需要存储六种数据：文章分类、文章、文章标签、幻灯图、推荐位、友情链接。每种数据一个表。 分类表结构设计： 表名：Category、分类名：name 标签表设计： 表名：Tag、标签名：name 文章表结构设计： 表名：Article、标题：title、摘要：excerpt、分类：category、标签：tags、推荐位、内容：body、创建时间：created_time、作者：user、文章封面图片img 幻灯图表结构设计： 表名：Banner、图片文本text_info、图片img、图片链接link_url、图片状态is_active。 推荐位表结构设计： 表名：Tui、推荐位名name。 友情链接表结构设计： 表名：Link、链接名name、链接网址linkurl。 其中： ++文章和分类是一对多的关系，文章和标签是多对多的关系，文章和作者是一对多的关系，文章和推荐位是一对多关系(看自己的需求，也可以设计成多对多)。++ 打开blog/models.py,输入代码：from django.db import modelsfrom django.contrib.auth.models import User #导入Django自带用户模块# 文章分类class Category(models.Model): name = models.CharField(&apos;博客分类&apos;, max_length=100) index = models.IntegerField(default=999, verbose_name=&apos;分类排序&apos;) class Meta: verbose_name = &apos;博客分类&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#文章标签class Tag(models.Model): name = models.CharField(&apos;文章标签&apos;,max_length=100) class Meta: verbose_name = &apos;文章标签&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#推荐位class Tui(models.Model): name = models.CharField(&apos;推荐位&apos;,max_length=100) class Meta: verbose_name = &apos;推荐位&apos; verbose_name_plural = verbose_name def __str__(self): return self.name#文章class Article(models.Model): title = models.CharField(&apos;标题&apos;, max_length=70) excerpt = models.TextField(&apos;摘要&apos;, max_length=200, blank=True) category = models.ForeignKey(Category, on_delete=models.DO_NOTHING, verbose_name=&apos;分类&apos;, blank=True, null=True) #使用外键关联分类表与分类是一对多关系 tags = models.ManyToManyField(Tag,verbose_name=&apos;标签&apos;, blank=True) #使用外键关联标签表与标签是多对多关系 img = models.ImageField(upload_to=&apos;article_img/%Y/%m/%d/&apos;, verbose_name=&apos;文章图片&apos;, blank=True, null=True) body = models.TextField() user = models.ForeignKey(User, on_delete=models.CASCADE, verbose_name=&apos;作者&apos;) &quot;&quot;&quot; 文章作者，这里User是从django.contrib.auth.models导入的。 这里我们通过 ForeignKey 把文章和 User 关联了起来。 &quot;&quot;&quot; views = models.PositiveIntegerField(&apos;阅读量&apos;, default=0) tui = models.ForeignKey(Tui, on_delete=models.DO_NOTHING, verbose_name=&apos;推荐位&apos;, blank=True, null=True) created_time = models.DateTimeField(&apos;发布时间&apos;, auto_now_add=True) modified_time = models.DateTimeField(&apos;修改时间&apos;, auto_now=True) class Meta: verbose_name = &apos;文章&apos; verbose_name_plural = &apos;文章&apos; def __str__(self): return self.title#Bannerclass Banner(models.Model): text_info = models.CharField(&apos;标题&apos;, max_length=50, default=&apos;&apos;) img = models.ImageField(&apos;轮播图&apos;, upload_to=&apos;banner/&apos;) link_url = models.URLField(&apos;图片链接&apos;, max_length=100) is_active = models.BooleanField(&apos;是否是active&apos;, default=False) def __str__(self): return self.text_info class Meta: verbose_name = &apos;轮播图&apos; verbose_name_plural = &apos;轮播图&apos;#友情链接class Link(models.Model): name = models.CharField(&apos;链接名称&apos;, max_length=20) linkurl = models.URLField(&apos;网址&apos;,max_length=100) def __str__(self): return self.name class Meta: verbose_name = &apos;友情链接&apos; verbose_name_plural = &apos;友情链接&apos; 这里面我们多增加了一个img图片封面字段，用于上传文章封面图片的，article_img/为上传目录，%Y/%m/%d/为自动在上传的图片上加上文件上传的时间。 我们已经编写了博客数据库模型的代码，但那还只是 Python 代码而已，Django 还没有把它翻译成数据库语言，因此实际上这些数据库表还没有真正的在数据库中创建。我们需要进行数据库迁移。 在迁移之前，我们先需要设置数据库，如果我们使用默认的sqlite数据库的话，就不需要设置，Django默认使用； sqlite3数据库，如果我们想使用Mysql数据库的话，则需要我们单独配置。我们打开settings.py文件，找到DATABASES，然后把它修改成如下代码： ############修改成mysql如下：DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;NAME&apos;: &apos;test&apos;, #你的数据库名称 &apos;USER&apos;: &apos;root&apos;, #你的数据库用户名 &apos;PASSWORD&apos;: &apos;445813&apos;, #你的数据库密码 &apos;HOST&apos;: &apos;&apos;, #你的数据库主机，留空默认为localhost &apos;PORT&apos;: &apos;3306&apos;, #你的数据库端口 &#125;&#125;#由于mysql默认引擎为MySQLdb，在__init__.py文件中添加下面代码#在python3中须替换为pymysql,可在主配置文件（和项目同名的文件下，不是app配置文件）中增加如下代码#import pymysql#pymysql.install_as_MySQLdb()#如果找不到pymysql板块，则通过pip install pymysql进行安装。 数据库设置好之后，我们就依次输入下面的命令进行数据库迁移： python manage.py makemigrationspython manage.py migrate 迁移的时候，会有如下提示： 出现这个原因是因为我们的幻灯图使用到图片字段，我们需要引入图片处理包。提示里也给了我们处理方案，输入如下命令，安装Pillow模块即可：pip install Pillow 然后再次迁移即可；]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：欢迎页面（五）]]></title>
    <url>%2F2019%2F04%2F09%2F%E4%BA%94%E3%80%81%E6%AC%A2%E8%BF%8E%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[基础配置做好了之后，我们就可以先迁移数据到数据库，然后启动我们的项目，感受Django的魅力。 python manage.py makemigrationspython manage.py migrate 迁移数据之后，网站目录里自动会创建一个数据库文件db.sqlite3，里面存放着我们的数据。 之后输入下面命令创建管理帐号和密码： python manage.py createsuperuser 最后，我们设置下启动命令，启动我们的Django项目： 启动后登陆 http://0.0.0.0:8000/ 就可以看到我们的欢迎页。 另外一点，我们当然也可以设置自定义的主页： 首先，打开打开bolg目录下的views.py文件，在里面输入： myblog/blog/views.pyfrom django.http import HttpResponsedef hello(request): return HttpResponse(&apos;Welcome to Django Zone！&apos;) 再打开myblog目录下的urls.py文件，在文件里添加两行代码： myblog/myblog/urls.pyfrom django.contrib import adminfrom django.urls import pathfrom blog import viewsurlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;&apos;, views.hello),] 再次刷新启动后，即可看到新的欢迎页面。 当然，我们在浏览器里面访问：http://127.0.0.1:8000/admin 就可以进入Django自带的后台管理。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：基础配置（四）]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%9B%9B%E3%80%81%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[打开myblog目录下的settings.py文件： 一、设置域名访问权限myblog/settings.pyALLOWED_HOSTS = [] #修改前ALLOWED_HOSTS = [&apos;*&apos;]#修改后，表示任何域名都能访问。如果指定域名的话，在&apos;&apos;里放入指定的域名即可 二、设置TEMPLATES里的’DIRS’添加模板目录templates的路径，后面我们做网站模板的时候用得着。 myblog/settings.py#修改前&apos;DIRS&apos;: []#修改后&apos;DIRS&apos;: [os.path.join(BASE_DIR, &apos;templates&apos;)]注：使用pycharm创建的话会自动添加 三、找到DATABASES设置网站数据库类型。这里使用默认的sqlite3。四、在INSTALLED_APPS添加APP应用名称。myblog/settings.pyINSTALLED_APPS = [ &apos;django.contrib.admin&apos;, .... &apos;blog.apps.BlogConfig&apos;,#注册APP应用]#使用pycharm创建的话，这里自动添加了，如果是终端命令创建的话，需要手动添加应用名称如&apos;blog&apos;, 五、修改项目语言和时区myblog/settings.py#修改前为英文LANGUAGE_CODE = &apos;en-us&apos;#修改后LANGUAGE_CODE = &apos;zh-hans&apos; #语言修改为中文#时区，修改前TIME_ZONE = &apos;UTC&apos;#修改后TIME_ZONE = &apos;Asia/Shanghai&apos; 六、在项目根目录里创建static和mediastatic用来存放模板CSS、JS、图片等静态资源，media用来存放上传的文件，后面我们在讲解数据库创建的时候有说明。 myblog/settings.py#设置静态文件目录和名称STATIC_URL = &apos;/static/&apos;#加入下面代码#这个是设置静态文件夹目录的路径STATICFILES_DIRS = ( os.path.join(BASE_DIR, &apos;static&apos;),)#设置文件上传路径，图片上传、文件上传都会存放在此目录里MEDIA_URL = &apos;/media/&apos;MEDIA_ROOT = os.path.join(BASE_DIR, &apos;media&apos;)]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：创建项目（三）]]></title>
    <url>%2F2019%2F04%2F07%2F%E4%B8%89%E3%80%81%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[基础环境： Python3.6 django2.1.1 开发工具为Pycharm 说明：1、为项目保存路径，myblog为项目名。 2、为选择使用的虚拟环境软件，这里选virtualenv。 3、为虚拟环境保存目录，我把它保存在项目里，虚拟环境默认名为env，我系统里有多个项目为了区分出来命名为myblogenv 4、为使用的模板语言，我们默认用django模板语言。 5、为创建项目的时候建立一个模板文件目录，用来存放模板文件。用CMD命令创建项目的话，模板目录需要自己手动创建。 6、为创建一个名为blog的APP应用。同样的用CMD命令创建的话，需要手动通过python manage.py startapp blog命令来进行创建。 点击创建之后，Pycharm自动帮我们完成Django软件下载安装和Django的项目创建。 注意：如果对需要指定Django版本的话，不能直接使用这个方法，这个方法会直接下载最新版本的Django。指定版本的话，请使用CMD通过命令如：pip install django==2.0.1 第一个黑色的 myblog 为项目文件夹目录。 blog为APP应用目录，也是我们上面设置第6项才创建的。myblog为项目配置目录，myblogvenv为Pycharm创建的虚拟环境目录，与项目无关，不用理会。 目录里的文件含义如下：blog #APP应用名和目录│ admin.py #对应应用后台管理配置文件。│ apps.py #对应应用的配置文件。│ models.py #数据模块，数据库设计就在此文件中设计。后面重点讲解│ tests.py #自动化测试模块，可在里面编写测试脚本自动化测试│ views.py #视图文件，用来执行响应代码的。你在浏览器所见所得都是它处理的。│ __init__.py│├─migrations #数据迁移、移植文目录，记录数据库操作记录，内容自动生成。│ │ __init__.pymyblog #项目配置目录│ __init__.py #初始化文件，一般情况下不用做任何修改。│ settings.py #项目配置文件，具体如何配置后面有介绍。│ url.py #项目URL设置文件，可理解为路由，可以控制你访问去处。│ wsgi.py #为Python服务器网关接口，是Python与WEB服务器之间的接口。myblogvenv #Pycharm创建的虚拟环境目录，和项目无关，不需要管它。templates #项目模板文件目录，用来存放模板文件manage.py #命令行工具，通过可以与项目与行交互。在终端输入python manege.py help，可以查看功能。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：数据库设计分析（二）]]></title>
    <url>%2F2019%2F04%2F06%2F%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[++从网站需求分析及网站功能、页面设计可以知道，我们的Blog主要以文章内容为主。所以我们在设计数据库的时候，我们主要以文章信息为核心数据，然后逐步向外扩展相关联的数据信息。++ 从如下图片中可以看到，文章有标题、分类、作者、浏览次数、发布时间、文章标签等信息。 这其中，文章与分类的关系是一对多的关系，什么是一对多？就是一篇文章只能有一个分类，而一个分类里可以有多篇文章。文章与标签的关系是多对多的关系，多对多简单理解就是，一篇文章可以有多个标签，一个标签里同样可以有多篇文章。 我们将文章表命名为Article，通过前面的分析得出文章信息表Article的数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 title CharField类型，长度为100 文章标题 category ForeignKey 外键，关联文章分类表 tags ManyToManyField 多对多，关联标签列表 body TextField 文章内容 user ForeignKey 外键，文章作者关联用户模型，系统自带的 views PositiveIntegerField 文章浏览数，正的整数，不能为负 tui ForeignKey 外键，关联推荐位表 created_time DateTimeField 文章发布时间 从文章表里，我们关联了一个分类表，我们把这个分类表命名为category，category表的数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 分类名 文章关联的标签表，我们命名为tag，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 标签名 文章关联的推荐位表，命名为tui，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为30 标签名 除此之外，我们还有两个独立的表，和文章没有关联的，一个是幻灯图片的表，一个是友情链接的表。 幻灯图表，命名为banner，数据库结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 text_info CharField类型，长度为100 标题，图片文本信息 img ImageField类型 图片类型，保存传图片的路径 link_url URLField类型 图片链接的URL is_active BooleanField布尔类型 有True 和False两个值，意思为是否激活 友情链接表命名为link，结构如下： 表字段 字段类型 备注 id int类型，长度为11 主键，由系统自动生成 name CharField类型，长度为70 友情链接的名称 linkurl URLField类型 友情链接的URL 至此，我们的数据库构造大致完成，后期如果还有其它的需求，我们可以在这基础上进行增加或者删除。下面我们就开始进行项目的创建与开发。]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django博客开发：项目需求分析（一）]]></title>
    <url>%2F2019%2F04%2F05%2F%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[当我们要开发一个项目的时候，首先需要了解我们项目的具体需求，根据需求类型划分网站功能，并详细了解这些需求的业务流程。然后更具需求和业务流程进行数据库设计。 blog的功能相对比较简单，主要以文章为主。 ==从功能需求来看==，这个Blog的功能分为：网站首页、文章分类、文章内容、幻灯图片、文章推荐、文章排行、热门推荐、文章搜索、友情链接。 1、网站首页：网站首页是整个网站的主界面，也是网站入口界面，里面主要展示Blog的动态信息及Blog功能导。网站动态信息以文章为主，如最新文章、幻灯图片、推荐阅读、文章排行、热门推荐、友情链接等。导航栏主要是将文章的分类的链接展示在首页，方便用户浏览。 2、文章分类：主要展示文章分类信息及链接，方便用户按需查看。文章分类可以在后台添加删除。 3、文章内容：主要展示文章所属分类、文章所属标签、文章内容、作者信息，发布时间信息。可以通过后台增、删、改。 4、幻灯图片：在网站首页，通过图片和文字展示一些重要信息，可以通过后台添加图片、图片描述、图片链接。 5、文章推荐：推荐一些重要的文章，可以在后台进行推荐。 6、文章排行：可根据文章浏览数，按时间段进行查询，然后展示出来。具体可根据自己的需求修改。 7、热门推荐：同样的推荐一些需要推荐的文章，可以在后台按需求或推荐位进行设置。 8、文章搜索：通过关键词搜索文章。 9、友情链接：展示相互链接的网站的名称与链接，可以通过后台添加与删除。 10、单页面：展示网站介绍，作者联系方式等信息，此类信息不经常变动，可以通过后台实现修改，也可以通过修改模板实现。 了解需求之后，就由UI设计师根据网站需求来设计网站页面，然后由前端工程师根据设计好的页面进行切图，实现HTML静态页面，最后由后端根据HTML页面和需求实现数据库构建和网站后台开发。 从设计方面来看，Blog主要分为六个页面，分别是：网站首页、文章分类列表页、文章内容页、搜索列表页、标签列表页、单页面。 1、网站首页：信息聚合的地方，展示多种信息； 2、文章分类列表页：点击分类，进入一个同一分类文章展示的列表页面； 3、文章内容页：文章内容展示页面，对应演示站这个地址； 4、搜索列表页：通过首页搜索按钮，展示出与搜索 词相关的文章列表； 5、标签列表页：展示同一个标签下的所有文章； 6、单页面：展示网站介绍、作者介绍或者联系方式等信息；]]></content>
      <categories>
        <category>Django2</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django2.0</tag>
        <tag>django_blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup中find和find_all的用法]]></title>
    <url>%2F2019%2F04%2F04%2FBeautifulSoup%E4%B8%ADfind%E5%92%8Cfind_all%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在爬取网页中有用的信息时，通常是对存在于网页中的文本或各种不同标签的属性值进行查找，Beautiful Soup中内置了一些查找方式，最常用的是 find() 和 find_all() 函数。 同时通过soup.find_all()得到的所有符合条件的结果和soup.select()一样都是列表list，而soup.find()只返回第一个符合条件的结果，所以soup.find()后面可以直接接.text或者get_text()来获得标签中的文本。 一、find()用法find(name,attrs,recursive,text,**wargs) 例子：&lt;ul id=&quot;producers&quot;&gt; &lt;li class=&quot;producerlist&quot;&gt; &lt;div class=&quot;name&quot;&gt;plants&lt;/div&gt; &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt; &lt;/li&gt; &lt;li class=&quot;producerlist&quot;&gt; &lt;div class=&quot;name&quot;&gt;algae&lt;/div&gt; &lt;div class=&quot;number&quot;&gt;100000&lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; (1)ul,li,div这些就是标签； 用法p=soup.find(&apos;ul&apos;) ，那么返回结果是第一个ul标签以及&lt;xx&gt;...&lt;/xx&gt;的所有内容，即上面的代码；注意若用p=soup.find(&apos;ul&apos;).get_text()那么结果不是...的所有内 容，而应该是plants 10000 algae 10000，即...中的标签不算text文本。 (2)…之间的内容就是文本；基于文本内容的查找也可以用soup.find()，但必须用到参数text， 用法p=soup.find(text=&apos;algae&apos;)，print(p)得到的结果就是algae (3)正则表达式后面自己另外去学习； (4)ul id=”producers”&gt;中的id即标签属性，那么我们可以查找具有特定标签的属性； 用法p=soup.find(&apos;ul&apos;, id=&quot;producers&quot;)，那么可以得到&lt;xx&gt;...&lt;/xx&gt;的所有结果，其特点是把标签更一步精确化以便于查找。 对于大多数的情况可以用上面的方法解决，但是有两种情况则要用到参数attrs:一是标签字符中带有-，比如data-custom;二是class不能看作标签属性。解决的办法是在attrs属性用字典进行传递参数： soup.find(attrs=&#123;&apos;data-custom&apos;:&apos;xxx&apos;&#125;)以及：soup.find(attrs=&#123;&apos;class&apos;:&apos;xxx&apos;&#125;) 二、find_all()用法应用到find()中的不同过滤参数同理可以用到find_all()中，相比find()，find_all()有个额外的参数limit； 如下所示： p=soup.find_all(text=&apos;algae&apos;,limit=2)实际上find()也就是当limit=1时的find_all()。 [参考文档引自]]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python3</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3爬取墨迹天气并发送给微信好友]]></title>
    <url>%2F2019%2F04%2F03%2Fpython3%E7%88%AC%E5%8F%96%E5%A2%A8%E8%BF%B9%E5%A4%A9%E6%B0%94%E5%B9%B6%E5%8F%91%E9%80%81%E7%BB%99%E5%BE%AE%E4%BF%A1%E5%A5%BD%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[需求： 爬取墨迹天气的信息，包括温湿度、风速，生活tips等信息； 可选输入需要查询的城市，自动爬取相应信息； 链接微信，发送给指定好友或群； 思路比较清晰，主要分两块，一是爬虫，二是用python链接微信（非企业版微信） 先随便观察一个城市的墨迹天气，例如苏州市的url https://tianqi.moji.com/weather/china/jiangsu/suzhou 多观察几个城市的url可发现共同点就是，前面的都一样，后面的是以省拼音/市拼音结尾的。当然直辖市两者拼音一样。当然还有一些额外情况，比如山西和陕西，后者的拼音是Shaanxi，这个用户输入的时候注意一下； 第一部分： 将汉字转换为拼音； 安装了第三方库xpinyin；# prov = input(&quot;请输入省份：&quot;)# city = input(&quot;请输入城市：&quot;)prov = &quot;江苏&quot;city = &quot;苏州&quot;pin = Pinyin()prov_pin = pin.get_pinyin(prov,&apos;&apos;) #将汉字转为拼音city_pin = pin.get_pinyin(city,&apos;&apos;)url = &quot;https://tianqi.moji.com/weather/china/&quot;weather_url = url+prov_pin+&quot;/&quot;+city_pin# print(weather_url) 第二部分： 定位需要获取的信息； 获取今日天气信息； 使用select筛选的的是class名或者id名，注意同级和下一级的书写形式；find和find_all是查找的标签；htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;) #打开页面源代码bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)# print(bs4.prettify())weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;) #定位需要拿到的信息# print(weather)temp1 = weather.find(&apos;em&apos;).get_text()temp2 = weather.find(&apos;b&apos;).get_text()AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()# print(AQI)SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text() #湿度FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text() #风速TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text() #今日天气提示DATE = str(datetime.date.today()) ##今天的日期WEEK = time.strftime(&quot;%w&quot;, time.localtime())INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos; &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPSprint(INFO) 第三部分： 获取明日天气信息；##获取明日天气tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)# print(tomorrow)t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text() ##明日风速t_AQI = tomorrow[-1].get_text().strip() ##明日空气质量t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;print(t_info) 第四部分： 链接微信需要安装第三方库itchat，链接只需要这一句话，很简单。初次链接会弹出二维码，手机扫二维码登陆；info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;itchat.auto_login(hotReload=True) #在一段时间内运行不需要扫二维码登陆def sendToPersion(nickName): user = itchat.search_friends(name=nickName) print(user) userName = user[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&apos;send it to HJ succeed&apos;)def sendToRoom(nickName): group = itchat.search_chatrooms(name=nickName) print(group) userName = group[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&quot;send it to Group succeed&quot;)if __name__ == &apos;__main__&apos;: sendToPersion(&quot;微信好友备注名&quot;) sendToRoom(&quot;微信群组备注名&quot;) - 给自己的文件助手filehelper发送信息,此时无需访问通讯录- #itchat.send(&apos;❤来自XXX的天气问候❤&apos;,toUserName=&apos;filehelper&apos;)- #I = itchat.search_friends()# 获取自己的信息，返回自己的属性字典- #friends = itchat.get_friends(update=True)#返回值类型&lt;class &apos;itchat.storage.templates.ContactList&apos;&gt;。可以看做是列表，列表里的每个元素是一个字典，对应一个好友信息 全部代码：from urllib import requestfrom bs4 import BeautifulSoupfrom xpinyin import Pinyinimport timeimport itchat# prov = input(&quot;请输入省份：&quot;)# city = input(&quot;请输入城市：&quot;)prov = &quot;江苏&quot;city = &quot;苏州&quot;pin = Pinyin()prov_pin = pin.get_pinyin(prov,&apos;&apos;) #将汉字转为拼音city_pin = pin.get_pinyin(city,&apos;&apos;)url = &quot;https://tianqi.moji.com/weather/china/&quot;weather_url = url+prov_pin+&quot;/&quot;+city_pin# print(weather_url)htmldata = request.urlopen(weather_url).read().decode(&apos;utf-8&apos;) #打开页面源代码bs4 = BeautifulSoup(htmldata, &apos;lxml&apos;)# print(bs4.prettify())weather = bs4.find(&apos;div&apos;, attrs=&#123;&apos;class&apos;:&quot;wea_weather clearfix&quot;&#125;) #定位需要拿到的信息# print(weather)temp1 = weather.find(&apos;em&apos;).get_text()temp2 = weather.find(&apos;b&apos;).get_text()AQI = bs4.select(&apos;.wea_alert.clearfix &gt; ul &gt; li &gt; a &gt; em&apos;)[0].get_text()# print(AQI)SHIDU = bs4.select(&apos;.wea_about.clearfix &gt; span&apos;)[0].get_text() #湿度FENGSU = bs4.select(&apos;.wea_about.clearfix &gt; em&apos;)[0].get_text() #风速TIPS = bs4.select(&apos;.wea_tips.clearfix &gt; em&apos;)[0].get_text() #今日天气提示DATE = str(datetime.date.today()) ##今天的日期WEEK = time.strftime(&quot;%w&quot;, time.localtime())INFO = &apos;来自毛亚的天气问候\n&apos; + city + &apos;市&apos; + &apos;,&apos; + DATE + &apos;,&apos; + &apos;周&apos;+WEEK + &apos;\n&apos;+ &apos;实时温度：&apos; + temp1 + &apos;℃&apos; + &apos;,&apos; + temp2 + &apos;\n&apos; &apos;湿度：&apos; + SHIDU + &apos;\n&apos; &apos;风速：&apos; + FENGSU +&apos;\n&apos; &apos;今日提示：&apos; + TIPSprint(INFO)##获取明日天气tomorrow = bs4.select(&apos;.days.clearfix &apos;)[1].find_all(&apos;li&apos;)# print(tomorrow)t_temp1 = tomorrow[2].get_text().replace(&apos;°&apos;, &apos;℃&apos;)+ &apos;,&apos; + tomorrow[1].find(&apos;img&apos;).attrs[&apos;alt&apos;]t_fengsu = tomorrow[3].find(&apos;em&apos;).get_text()+ &apos;:&apos; + tomorrow[3].find(&apos;b&apos;).get_text() ##明日风速t_AQI = tomorrow[-1].get_text().strip() ##明日空气质量t_info = &apos;\n明日天气：\n&apos; + &apos;温度：&apos; + t_temp1 + &apos;\n&apos; + &apos;风速：&apos; + t_fengsu + &apos;\n&apos; &apos;空气质量：&apos; + t_AQI + &apos;\n&apos;print(t_info)info_all = &apos;❤❤❤❤❤❤❤❤❤❤❤\n&apos;+INFO + &apos;\n&apos; + t_info + &apos;❤❤❤❤❤❤❤❤❤❤❤&apos;itchat.auto_login(hotReload=True) #在一段时间内运行不需要扫二维码登陆def sendToPersion(nickName): user = itchat.search_friends(name=nickName) print(user) userName = user[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&apos;send it to HJ succeed&apos;)def sendToRoom(nickName): group = itchat.search_chatrooms(name=nickName) print(group) userName = group[0][&apos;UserName&apos;] itchat.send(info_all, toUserName=userName) print(&quot;send it to Group succeed&quot;)if __name__ == &apos;__main__&apos;: sendToPersion(&quot;微信好友备注名&quot;) sendToRoom(&quot;微信群组备注名&quot;) 微信好友： 微信群组： 抓取墨迹天气的源码：&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;/&gt; &lt;meta content=&quot;width=device-width, initial-scale=1&quot; name=&quot;viewport&quot;/&gt; &lt;meta content=&quot;苏州市今天实况：15度 阴，湿度：53%，东南风：2级。白天：16度,阴。 夜间：阴，9度，天气偏凉了，墨迹天气建议您穿上厚些的外套或是保暖的羊毛衫，年老体弱者可以选择保暖的摇粒绒外套。&quot; name=&quot;description&quot;/&gt; &lt;meta content=&quot;苏州市天气预报，苏州市天气查询&quot; name=&quot;keywords&quot;/&gt; &lt;meta content=&quot;Moji Weather Web Dev Team&quot; name=&quot;author&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/favicon.ico&quot; rel=&quot;shortcut icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/custom_icon.png&quot; rel=&quot;apple-touch-icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-60.png&quot; rel=&quot;apple-touch-icon&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-76.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;76x76&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-iphone-retina-120.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;120x120&quot;/&gt; &lt;link href=&quot;https://h5tq.moji.com/f5/assets/tianqi/touch-icon-ipad-retina-152.png&quot; rel=&quot;apple-touch-icon&quot; sizes=&quot;152x152&quot;/&gt; &lt;meta content=&quot;format=html5;url=https://m.moji.com/weather/china/jiangsu/suzhou&quot; name=&quot;mobile-agent&quot;/&gt; &lt;link href=&quot;https://m.moji.com/weather/china/jiangsu/suzhou&quot; media=&quot;only screen and(max-width: 640px)&quot; rel=&quot;alternate&quot;/&gt; &lt;meta content=&quot;IE=EmulateIE8; charset=UTF-8&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt; &lt;meta content=&quot;IE=edge,chrome=1&quot; http-equiv=&quot;X-UA-Compatible&quot;/&gt; &lt;title&gt; 【苏州市天气】_苏州市天气预报_天气查询 - 墨迹天气 &lt;/title&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/reset.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/index.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;link charset=&quot;utf-8&quot; href=&quot;https://h5tq.moji.com/tianqi/assets//styles/chanle.css&quot; rel=&quot;stylesheet&quot;/&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;head_box&quot;&gt; &lt;div class=&quot;head clearfix&quot;&gt; &lt;a class=&quot;logo&quot; href=&quot;http://www.moji.com/&quot;&gt; &lt;img alt=&quot;墨迹天气&quot; height=&quot;31&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/index/i_logo.png&quot;/&gt; &lt;/a&gt; &lt;div class=&quot;phone&quot;&gt; &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt; &lt;i class=&quot;shake shake-rotate&quot; id=&quot;head_shake&quot;&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/icon/phone.png&quot;/&gt; &lt;/i&gt; &lt;span&gt; 随时随地 想查就查 &lt;/span&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;nav&quot;&gt; &lt;a href=&quot;http://www.moji.com/&quot;&gt; 首页 &lt;/a&gt; &lt;a href=&quot;https://tianqi.moji.com&quot;&gt; 天气 &lt;/a&gt; &lt;a href=&quot;http://www.moji.com/mjsoft/&quot;&gt; 下载 &lt;/a&gt; &lt;!-- &lt;a href=&quot;http://www.moji.com/tob/&quot;&gt;天气服务&lt;sup style=&quot;margin: -32px 0 0 56px;&quot;&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/head_hot.png&quot;&gt;&lt;/sup&gt;--&gt; &lt;a href=&quot;https://tianqi.moji.com/news/index&quot;&gt; 资讯 &lt;/a&gt; &lt;a href=&quot;http://www.moji.com/about/&quot;&gt; 关于墨迹 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-data=&quot;13&quot; data-url=&quot;https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg&quot; id=&quot;skin&quot; style=&quot;background: url(https://h5tq.moji.com/tianqi/assets/images/skin/day_1.jpg) no-repeat center top;background-size: 100% 100%;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix&quot;&gt; &lt;div class=&quot;comm_box&quot;&gt; &lt;!--面包屑--&gt; &lt;div class=&quot;crumb clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com&quot;&gt; 天气 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china&quot;&gt; 中国 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu&quot;&gt; 江苏省 &lt;/a&gt; &lt;i&gt; &lt;/i&gt; &lt;/li&gt; &lt;li&gt; 苏州市 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;search&quot;&gt; &lt;div class=&quot;search&quot;&gt; &lt;div class=&quot;search_default&quot;&gt; &lt;em&gt; 苏州市， 江苏省， 中国 &lt;/em&gt; &lt;strong id=&quot;locate&quot;&gt; &lt;/strong&gt; &lt;b&gt; &lt;!--icon--&gt; &lt;/b&gt; &lt;input placeholder=&quot;输入你要查找的城市&quot; type=&quot;text&quot;/&gt; &lt;i&gt; &lt;/i&gt; &lt;/div&gt; &lt;div class=&quot;search_more&quot;&gt; &lt;a href=&quot;https://tianqi.moji.com/findmycity&quot;&gt; 更多城市 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;search_city&quot; style=&quot;display: none;&quot;&gt; &lt;ul&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix wea_info&quot;&gt; &lt;div class=&quot;left&quot;&gt; &lt;div class=&quot;wea_alert clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu/suzhou&quot;&gt; &lt;span class=&quot;level level_2&quot;&gt; &lt;img alt=&quot;63 良&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/aqi/2.png&quot;/&gt; &lt;/span&gt; &lt;em&gt; 63 良 &lt;/em&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;wea_weather clearfix&quot;&gt; &lt;em&gt; 15 &lt;/em&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; &lt;b&gt; 阴 &lt;/b&gt; &lt;strong class=&quot;info_uptime&quot;&gt; 今天8:45更新 &lt;/strong&gt; &lt;/div&gt; &lt;div class=&quot;wea_about clearfix&quot;&gt; &lt;span&gt; 湿度 53% &lt;/span&gt; &lt;em&gt; 东南风2级 &lt;/em&gt; &lt;/div&gt; &lt;div class=&quot;wea_tips clearfix&quot;&gt; &lt;span&gt; 今日天气提示 &lt;/span&gt; &lt;em&gt; 略微偏凉，还是蛮舒适的。 &lt;/em&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;right&quot;&gt; &lt;div class=&quot;wea_info_avator&quot;&gt; &lt;img alt=&quot;墨迹天气 小墨哥&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/avator/icon/7.png&quot;/&gt; &lt;div id=&quot;windows_download&quot;&gt; &lt;img alt=&quot;Windows 下载&quot; height=&quot;35&quot; src=&quot;https://h5tq.moji.com/tianqi/assets//images/icon/avator_windows.png&quot;/&gt; &lt;a href=&quot;http://download.moji001.com/mojiapp/windoz/MoWeatherInstall_1.8.1.1.exe&quot; target=&quot;_blank&quot;&gt; Windows 下载 &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix&quot;&gt; &lt;div class=&quot;left&quot;&gt; &lt;div class=&quot;forecast clearfix&quot;&gt; &lt;div class=&quot;g_title&quot;&gt; &lt;span&gt; 预报 &lt;/span&gt; &lt;ul class=&quot;nav&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu/suzhou&quot;&gt; 7天预报 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu/suzhou&quot;&gt; 10天预报 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu/suzhou&quot;&gt; 15天预报 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu/suzhou&quot;&gt; 今天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; 阴 &lt;/li&gt; &lt;li&gt; 9° / 16° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 63 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu/suzhou&quot;&gt; 明天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;多云&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/span&gt; 多云 &lt;/li&gt; &lt;li&gt; 10° / 19° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 62 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;ul class=&quot;days clearfix&quot;&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu/suzhou&quot;&gt; 后天 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;span&gt; &lt;img alt=&quot;阴&quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/span&gt; 阴 &lt;/li&gt; &lt;li&gt; 11° / 15° &lt;/li&gt; &lt;li&gt; &lt;em&gt; 东南风 &lt;/em&gt; &lt;b&gt; 3级 &lt;/b&gt; &lt;/li&gt; &lt;li&gt; &lt;strong class=&quot;level_2&quot;&gt; 68 良 &lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;hours&quot;&gt; &lt;div class=&quot;g_title&quot;&gt; &lt;span&gt; 24小时预报 &lt;/span&gt; &lt;ul class=&quot;nav&quot;&gt; &lt;li class=&quot;active&quot;&gt; 温度 &lt;/li&gt; &lt;li&gt; 风力 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;charts clearfix&quot;&gt; &lt;div class=&quot;chart chart_temp clearfix&quot; id=&quot;chart_temp&quot;&gt; &lt;div class=&quot;prev&quot;&gt; &lt;/div&gt; &lt;div class=&quot;next&quot;&gt; &lt;/div&gt; &lt;div class=&quot;num&quot;&gt; &lt;span&gt; 30° &lt;/span&gt; &lt;span&gt; 20° &lt;/span&gt; &lt;span&gt; 10° &lt;/span&gt; &lt;span&gt; 0° &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;canvas&quot;&gt; &lt;div class=&quot;canvas_box&quot;&gt; &lt;canvas height=&quot;300&quot; id=&quot;temp&quot; width=&quot;4000&quot;&gt; &lt;/canvas&gt; &lt;div class=&quot;canvas_point&quot;&gt; &lt;span&gt; &lt;/span&gt; &lt;div&gt; &lt;em&gt; 29° &lt;/em&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;chart chart_wind clearfix&quot; id=&quot;chart_wind&quot; style=&quot;display: none;&quot;&gt; &lt;div class=&quot;prev&quot;&gt; &lt;/div&gt; &lt;div class=&quot;next&quot;&gt; &lt;/div&gt; &lt;div class=&quot;num&quot;&gt; &lt;span&gt; 30° &lt;/span&gt; &lt;span&gt; 20° &lt;/span&gt; &lt;span&gt; 10° &lt;/span&gt; &lt;span&gt; 0° &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;canvas&quot;&gt; &lt;div class=&quot;canvas_box&quot;&gt; &lt;canvas height=&quot;300&quot; id=&quot;wind&quot; width=&quot;4000&quot;&gt; &lt;/canvas&gt; &lt;div class=&quot;canvas_point&quot;&gt; &lt;span&gt; &lt;/span&gt; &lt;div&gt; &lt;em&gt; 29° &lt;/em&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets//images/weather/w1.png&quot;/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!--生活指数--&gt; &lt;div id=&quot;live_index&quot;&gt; &lt;div class=&quot;live_index_title&quot;&gt; &lt;h2&gt; 生活指数 &lt;/h2&gt; &lt;span&gt; &lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;live_index_grid&quot;&gt; &lt;ul class=&quot;clearfix&quot;&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/2.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 适宜 &lt;/dt&gt; &lt;dd&gt; 旅游 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/cold/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/12.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 易发 &lt;/dt&gt; &lt;dd&gt; 感冒 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/fish/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/28.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较适宜 &lt;/dt&gt; &lt;dd&gt; 钓鱼 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/makeup/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/7.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 控油 &lt;/dt&gt; &lt;dd&gt; 化妆 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/sport/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/26.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 不适宜 &lt;/dt&gt; &lt;dd&gt; 运动 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;javascript:&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/5.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 很差 &lt;/dt&gt; &lt;dd&gt; 交通 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/car/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/17.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较适宜 &lt;/dt&gt; &lt;dd&gt; 洗车 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/pollution/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/0.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 较差 &lt;/dt&gt; &lt;dd&gt; 空气污染扩散 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/dress/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/20.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 温凉 &lt;/dt&gt; &lt;dd&gt; 穿衣 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&quot;clearfix&quot; href=&quot;https://tianqi.moji.com/uray/china/jiangsu/suzhou&quot;&gt; &lt;span&gt; &lt;img src=&quot;https://h5tq.moji.com/tianqi/assets/images/script/21.png&quot;/&gt; &lt;/span&gt; &lt;dl&gt; &lt;dt&gt; 最弱 &lt;/dt&gt; &lt;dd&gt; 紫外线 &lt;/dd&gt; &lt;/dl&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;/li&gt; &lt;li&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;right&quot;&gt; &lt;!--热门时景--&gt; &lt;div class=&quot;liveview liveview_index&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 热门时景 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81736828&quot;&gt; &lt;span&gt; &lt;img alt=&quot;四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区&quot; data-height=&quot;1301&quot; data-width=&quot;1080&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/06/15072215250.83494900.1182_android.jpg&quot;/&gt; &lt;/span&gt; &lt;h2&gt; 四川省凉山彝族自治州盐源县泸沽湖镇亚泸路泸沽湖风景名胜区 &lt;/h2&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/picture/81749154&quot;&gt; &lt;span&gt; &lt;img alt=&quot;安徽省黄山市休宁县溪口镇詹家山&quot; data-height=&quot;720&quot; data-width=&quot;960&quot; src=&quot;https://cdn.moji002.com/images/sthumb/2017/10/08/15074223300.94412000.1764_android.jpg&quot;/&gt; &lt;/span&gt; &lt;h2&gt; 安徽省黄山市休宁县溪口镇詹家山 &lt;/h2&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;near&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 附近地区 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/nearcity/weather/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuzhong-district&quot;&gt; 吴中区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/changshu&quot;&gt; 常熟市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/zhangjiagang&quot;&gt; 张家港市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/kunshan&quot;&gt; 昆山市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wujiang-district&quot;&gt; 吴江区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/taicang&quot;&gt; 太仓市 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dongshan-town&quot;&gt; 东山镇 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wuxian&quot;&gt; 吴县市（现吴中区、相城区） &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;near&quot;&gt; &lt;div class=&quot;title&quot;&gt; &lt;em&gt; 附近景点 &lt;/em&gt; &lt;a href=&quot;https://tianqi.moji.com/nearscenic/weather/china/jiangsu/suzhou&quot;&gt; 更多 &lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;item clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/dong-mountain-scenic-spot&quot;&gt; 东山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/huqiu-mountain-scenic-spot&quot;&gt; 虎丘山风景名胜区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/jinji-lake-scenic-spot&quot;&gt; 金鸡湖景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-lingering-garden&quot;&gt; 留园 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/qionku-mountain-scenic-spot&quot;&gt; 穹窿山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/tianping-mountain-scenic-spot&quot;&gt; 天平山风景名胜区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/wang-mountain-scenic-spot&quot;&gt; 旺山景区 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/weather/china/jiangsu/the-humble-administrator&apos;s-garden&quot;&gt; 拙政园 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;wrap clearfix calendar&quot;&gt; &lt;div class=&quot;g_title clearfix&quot;&gt; &lt;span&gt; 天气日历 &lt;/span&gt; &lt;em&gt; &lt;!--今天8:05更新--&gt; &lt;/em&gt; &lt;/div&gt; &lt;div class=&quot;grid_title clearfix&quot;&gt; &lt;ul&gt; &lt;li&gt; 星期日 &lt;/li&gt; &lt;li&gt; 星期一 &lt;/li&gt; &lt;li&gt; 星期二 &lt;/li&gt; &lt;li&gt; 星期三 &lt;/li&gt; &lt;li&gt; 星期四 &lt;/li&gt; &lt;li&gt; 星期五 &lt;/li&gt; &lt;li&gt; 星期六 &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;grid clearfix&quot; id=&quot;calendar_grid&quot;&gt; &lt;ul&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 01 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;晴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w0.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 7/18° &lt;/p&gt; &lt;p&gt; 西南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item active&quot;&gt; &lt;em&gt; 02 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/16° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 03 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 10/19° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 04 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/15° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 05 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/22° &lt;/p&gt; &lt;p&gt; 南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 06 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/22° &lt;/p&gt; &lt;p&gt; 南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 07 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 11/25° &lt;/p&gt; &lt;p&gt; 北风 3-4级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 08 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/15° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 09 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/15° &lt;/p&gt; &lt;p&gt; 东风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 10 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 9/18° &lt;/p&gt; &lt;p&gt; 东北风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 11 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 12/18° &lt;/p&gt; &lt;p&gt; 东南风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 12 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;阴 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w2.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/23° &lt;/p&gt; &lt;p&gt; 东南风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 13 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 13/24° &lt;/p&gt; &lt;p&gt; 东风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 14 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 15/21° &lt;/p&gt; &lt;p&gt; 东南风 4-5级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 15 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;雨 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w8.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 8/19° &lt;/p&gt; &lt;p&gt; 西北风 5-6级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 16 &lt;/em&gt; &lt;b&gt; &lt;img alt=&quot;多云 &quot; src=&quot;https://h5tq.moji.com/tianqi/assets/images/weather/w1.png&quot;/&gt; &lt;/b&gt; &lt;p&gt; 8/19° &lt;/p&gt; &lt;p&gt; 东北风 3级 &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 17 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 18 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 19 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 20 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 21 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 22 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 23 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 24 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 25 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 26 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 27 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 28 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 29 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;em&gt; 30 &lt;/em&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;li class=&quot;item&quot;&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;!--新闻列表--&gt; &lt;input id=&quot;staticdomain&quot; type=&quot;hidden&quot; value=&quot;https://h5tq.moji.com/tianqi&quot;/&gt; &lt;input id=&quot;staticmd5&quot; type=&quot;hidden&quot; value=&quot;&quot;/&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets/scripts/libs/jquery.min.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.charts.js&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://h5tq.moji.com/tianqi/assets//scripts/pages/index.js&quot;&gt; &lt;/script&gt; &lt;iframe frameborder=&quot;0&quot; height=&quot;0&quot; src=&quot;http://miniweb.cntv.cn/hezuo/mo.html&quot; style=&quot;display: none; overflow: hidden;&quot; width=&quot;0&quot;&gt; &lt;/iframe&gt; &lt;div class=&quot;foot_box clearfix&quot;&gt; &lt;div class=&quot;foot clearfix&quot;&gt; &lt;div class=&quot;related_link&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 今天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china&quot;&gt; 今天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/today/china/jiangsu&quot;&gt; 今天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 明天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china&quot;&gt; 明天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tommorrow/china/jiangsu&quot;&gt; 明天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 后天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china&quot;&gt; 后天省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/tdat/china/jiangsu&quot;&gt; 后天城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 7天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china&quot;&gt; 7天预报省份表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast7/china/jiangsu&quot;&gt; 7天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 10天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china&quot;&gt; 10天预报省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast10/china/jiangsu&quot;&gt; 10天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 15天预报 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china&quot;&gt; 15天预报省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/forecast15/china/jiangsu&quot;&gt; 15天预报城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 空气指数 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china&quot;&gt; 空气指数省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/aqi/china/jiangsu&quot;&gt; 空气指数城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; pm2.5 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pm/china&quot;&gt; pm2.5省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pm/china/jiangsu&quot;&gt; pm2.5城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 污染指数 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pollution/china&quot;&gt; 污染指数省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/pollution/china/jiangsu&quot;&gt; 污染指数城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt; &lt;ol&gt; &lt;li&gt; 时景 &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china&quot;&gt; 时景省份列表 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https://tianqi.moji.com/liveview/china/jiangsu&quot;&gt; 时景城市列表 &lt;/a&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;foot clearfix&quot;&gt; &lt;div class=&quot;address&quot;&gt; &lt;p&gt; 公司地址：北京市朝阳区酒仙桥路14号兆维华灯大厦A1区3门A216 联系电话：400-880-0599 &lt;/p&gt; &lt;ul class=&quot;f_nav&quot;&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/updata/android/&quot; rel=&quot;nofollow&quot;&gt; 升级日志 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/faq/android/&quot; rel=&quot;nofollow&quot;&gt; 常见问题 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://designer.moji.com/signin&quot; rel=&quot;nofollow&quot;&gt; 设计师平台 &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;http://www.moji.com/about/agreement/&quot; rel=&quot;nofollow&quot;&gt; 服务协议 &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;copyright&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; var _hmt = _hmt || [];(function() &#123; var hm = document.createElement(&quot;script&quot;); hm.src = &quot;//hm.baidu.com/hm.js?49e9e3e54ae5bf8f8c637e11b3994c74&quot;; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(hm, s);&#125;)(); &lt;/script&gt; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&apos;GoogleAnalyticsObject&apos;]=r;i[r]=i[r]||function()&#123; (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) &#125;)(window,document,&apos;script&apos;,&apos;//www.google-analytics.com/analytics.js&apos;,&apos;ga&apos;); ga(&apos;create&apos;, &apos;UA-49812585-12&apos;, &apos;auto&apos;); ga(&apos;send&apos;, &apos;pageview&apos;); &lt;/script&gt; &lt;script&gt; //自动推送(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)(); &lt;/script&gt; &lt;script&gt; (function()&#123; //手机抖动动画 function phoneAnimate() &#123; $(&quot;#head_shake&quot;).addClass(&quot;shake&quot;); var phone = setTimeout(&apos;$(&quot;#head_shake&quot;).removeClass(&quot;shake&quot;)&apos;,3000); &#125; var d = new Date(); var nowYear = d.getFullYear(); var html = &quot;Copyright© 2009-&quot;+nowYear+&quot; 北京墨迹风云科技股份有限公司 All Rights Reserved&lt;br /&gt;京ICP备10021324号 京公网安备11010502023583&lt;br /&gt; 客服服务热线：400-880-0599 违法和不良信息举报电话：400-880-0599 举报邮箱：AS@moji.com&quot;; $(&quot;.copyright&quot;).html(html); phoneAnimate();&#125;)(); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python3</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Python3爬虫】常见反爬虫措施及解决办法]]></title>
    <url>%2F2019%2F03%2F31%2F%E3%80%90Python3%E7%88%AC%E8%99%AB%E3%80%91%E5%B8%B8%E8%A7%81%E5%8F%8D%E7%88%AC%E8%99%AB%E6%8E%AA%E6%96%BD%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、UserAgentUserAgent中文名为用户代理，它使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本等信息。对于一些网站来说，它会检查我们发送的请求中所携带的UserAgent字段，如果非浏览器，就会被识别为爬虫，一旦被识别出来， 我们的爬虫也就无法正常爬取数据了。这里先看一下在不设置UserAgent字段时该字段的值会是什么：import requestsurl = &quot;http://www.baidu.com&quot;res = requests.get(url) 解决办法：1、收集整理常见的UserAgent以供使用ua_list = [&quot;Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;, &quot;Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5&quot;, &quot;MQQBrowser/25 (Linux; U; 2.3.3; zh-cn; HTC Desire S Build/GRI40;480*800)&quot;, &quot;Mozilla/5.0 (Linux; U; Android 2.3.3; zh-cn; HTC_DesireS_S510e Build/GRI40) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1&quot;, &quot;Mozilla/5.0 (SymbianOS/9.3; U; Series60/3.2 NokiaE75-1 /110.48.125 Profile/MIDP-2.1 Configuration/CLDC-1.1 ) AppleWebKit/413 (KHTML, like Gecko) Safari/413&quot; ...] 2、使用第三方库–fake_useragent使用方法如下：from fake_useragent import UserAgentua = UserAgent()for i in range(3): print(ua.random) 剖析：print(ua.ie) #随机打印ie浏览器任意版本print(ua.firefox) #随机打印firefox浏览器任意版本print(ua.chrome) #随机打印chrome浏览器任意版本print(ua.random) #随机打印任意厂家的浏览器 二、IP对于一些网站来说，如果某个IP在单位时间里的访问次数超过了某个阈值，那么服务器就会ban掉这个IP了，它就会返回给你一些错误的数据。一般来说，当我们的IP被ban了，我们的爬虫也就无法正常获取数据了，但是用浏览器还是可以正常访问，但是如果用浏览器都无法访问，那就真的GG了。很多网站都会对IP进行检测，比如知乎，如果单个IP访问频率过高就会被封掉。 解决办法：使用代理IP。网上有很多免费代理和付费代理可供选择，免费代理比如：西刺代理、快代理等等，付费代理比如：代理云、阿布云等等。 三、Referer防盗链防盗链主要是针对客户端请求过程中所携带的一些关键信息来验证请求的合法性，而防盗链又有很多种，比如Referer防盗链，还有Cookie防盗链和时间戳防盗链。 Cookie防盗链常见于论坛、社区。当访客请求一个资源的时候，他会检查这个访客的Cookie，如果不是他自己的用户的Cookie，就不会给这个访客正确的资源，也就达到了防盗的目的。时间戳防盗链指的是在他的url后面加上一个时间戳参数，所以如果你直接请求网站的url是无法得到真实的页面的，只有带上时间戳才可以。 这次的例子是天涯社区的图片分社区： 这里我们先打开开发者工具，然后任意选择一张图片，得到这个图片的链接，然后用requests来下载一下这张图片，注意带上Referer字段，看结果如何：import requestsurl = &quot;http://img3.laibafile.cn/p/l/305989961.jpg&quot;headers = &#123; &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;, &quot;UserAgent&quot;:&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36&quot;&#125;res = requests.get(url)with open(&apos;test.jpg&apos;, &apos;wb&apos;) as f: f.write(res.content) 我们的爬虫正常运行了，也看到生成了一个test.jpg文件，先别急着高兴，打开图片看一下： 解决办法：既然他说仅供天涯社区用户分享，那我们也成为他的用户不就行了吗？二话不说就去注册了个账号，然后登录，再拿到登录后的Cookie： 注意：Cookie是有时效性的，具体多久就会失效我没测试。紧接着把Cookie添加到代码中，然后运行，可以看到成功把图片下载下来了； 搞了这么久才下了一张图片，我们怎么可能就这么满足呢？分析页面可知一个页面上有十五张图片，然后往下拉的时候会看到”正在加载，请稍后”： 我们立马反应过来这是通过AJAX来加载的，于是打开开发者工具查看，可以找到如下内容： 可以看到每个链接“？”前面的部分都是基本一样的，“list_”后面跟的数字表示页数，而“_=”后面这一串数字是什么呢？有经验的人很快就能意识到这是一个时间戳，所以我们来测试一下：import timeimport requestst = time.time()*1000url = &quot;http://pp.tianya.cn/qt/list_4.shtml?_=&#123;&#125;&quot;.format(t)res = requests.get(url)print(res.text) 最后编写程序并运行：import reimport timeimport requestsfrom fake_useragent import UserAgentua = UserAgent()headers = &#123; &quot;Referer&quot;: &quot;http://pp.tianya.cn/&quot;, &quot;Cookie&quot;: &quot;user=w=ASD9577&amp;id=139400111&amp;f=1; right=web4=n&amp;portal=n; __u_a=v2.2.4; sso=r=2037194054&amp;sid=&amp;wsid=54DECCFD58BB393C19060A02D963FFFC; temp=k=32072170&amp;s=&amp;t=1553817544&amp;b=bdacde9b28f59113991fd271bdba3f65&amp;ct=1553817544&amp;et=1556409544; temp4=rm=0e74c53c8e0cfe3ec18596583e42c38f; ty_msg=1553817664458_139400111_2_0_0_0_0_0_2_0_0_0_0_0; bbs_msg=1553817664463_139400111_0_0_0_0; time=ct=1553817677.647&quot;, &quot;UserAgent&quot;: ua.random&#125;def crawl(url): res = requests.get(url, headers=headers) res.encoding = &quot;utf-8&quot; print(&quot;status_code:&quot;, res.status_code) # print(&quot;res.text:&quot;, res.text) if res.status_code == 200: # result = re.findall(r&apos;src=&quot;.*?&quot; alt=&quot;.*?&quot;&apos;, res.text) result = re.findall(&apos;&lt;img.+src=&quot;(.*?)&quot; alt=&quot;(.*?)&quot;.+&gt;&apos;, res.text) print(result) for i in result: # print(&quot;iiiiiiiiiii&quot;, i[0], i[1]) download(i[0], i[1]) else: print(&quot;Error Request!&quot;)def download(href, name): try: res = requests.get(href, headers=headers) with open(&apos;&#123;&#125;.jpg&apos;.format(name), &apos;wb&apos;) as f: f.write(res.content) print(&apos;[INFO]&#123;&#125;.jpg已下载！&apos;.format(name)) except: print(&quot;Error Download!&quot;)if __name__ == &apos;__main__&apos;: for num in range(1, 3): # 最大页数2页 time.sleep(2) t = int(time.time() * 1000) # 获取13位时间戳 print(t) page_url = &quot;http://pp.tianya.cn/qt/list_&#123;&#125;.shtml?_=&#123;&#125;&quot;.format(num, t) # 构造链接 crawl(page_url)]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[urllib.parse.urlencode转换get请求参数]]></title>
    <url>%2F2019%2F03%2F22%2Furllib.parse.urlencode%E8%BD%AC%E6%8D%A2get%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[浏览器地址栏搜索 刘若英https://www.baidu.com/s?word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 但是复制到文件中是这样的：https://www.baidu.com/s?word=%E5%88%98%E8%8B%A5%E8%8B%B1&amp;tn=71069079_1_hao_pg&amp;ie=utf-8 这是因为浏览器对中文请求参数进行了转码用代码访问网站所发的请求中如果有中文也必须是转码之后的。这里需要用到urllib.parse.urlencode 方法。这个方法的作用就是将字典里面所有的键值转化为query-string格式（key=value&amp;key=value），并且将中文转码。 import urllib.requestimport urllib.parseimport osurl = &apos;http://www.baidu.com/s?&apos;wd = input(&apos;请输入要搜索关键字： &apos;)&quot;&quot;&quot;word=刘若英&amp;tn=71069079_1_hao_pg&amp;ie=utf-8&quot;&quot;&quot;data = &#123; &apos;word&apos;: wd, &apos;tn&apos;: &apos;71069079_1_hao_pg&apos;, &apos;ie&apos;: &apos;utf-8&apos;&#125;query_string = urllib.parse.urlencode(data)# 拼接获取完整urlurl += query_string# 发起请求，获取响应response = urllib.request.urlopen(url=url)filename = wd + &apos;.html&apos;dirname = &apos;./html&apos;if not os.path.exists(dirname): os.mkdir(dirname)filepath = dirname + &apos;/&apos; + filename# 以二进制写入文件# with open(filepath, &apos;wb&apos;) as fp:# fp.write(response.read())# 或者以utf8编码写入文件with open (filepath, &apos;w&apos;, encoding=&apos;utf8&apos;) as fp: fp.write(response.read().decode(&apos;utf8&apos;))]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-dashboard仪表盘（十二）]]></title>
    <url>%2F2019%2F03%2F15%2F12%E3%80%81dashboard%E4%BB%AA%E8%A1%A8%E7%9B%98%2F</url>
    <content type="text"><![CDATA[对于运维管理平台，一个总览的dashboard仪表盘界面是必须有的，不但提升整体格调，也有利于向老板‘邀功请赏’。 dashboard页面必须酷炫吊炸天，所以界面元素应当美观、丰富、富有冲击力。 完整的dashboard.html文件代码如下：dashboard.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html 一、资产状态占比图首先，制作一个资产状态百分比表盘，用于显示上线、下线、未知、故障和备用五种资产在总资产中的占比。注意是占比，不是数量！ 按照AdminLTE中提供的示例，在HTML中添加相应的标签，在script中添加相应的JS代码（jQueryKnob）。JS代码基本照抄，不需要改动。对于显示的圆圈，可以修改其颜色、大小、形态、是否只读等属性，可以参照AdminLTE中的范例。 最重要的是，需要从数据库中获取相应的数据，修改assets/views.py中的dashboard视图，最终如下：def dashboard(request): total = models.Asset.objects.count() upline = models.Asset.objects.filter(status=0).count() offline = models.Asset.objects.filter(status=1).count() unknown = models.Asset.objects.filter(status=2).count() breakdown = models.Asset.objects.filter(status=3).count() backup = models.Asset.objects.filter(status=4).count() up_rate = round(upline/total*100) o_rate = round(offline/total*100) un_rate = round(unknown/total*100) bd_rate = round(breakdown/total*100) bu_rate = round(backup/total*100) server_number = models.Server.objects.count() networkdevice_number = models.NetworkDevice.objects.count() storagedevice_number = models.StorageDevice.objects.count() securitydevice_number = models.SecurityDevice.objects.count() software_number = models.Software.objects.count() return render(request, &apos;assets/dashboard.html&apos;, locals()) 代码很简单，分别获取资产总数量，上线、下线、未知、故障和备用资产的数量，然后计算出各自的占比，例如上线率up_rate。同时获取服务器、网络设备、安全设备和软件设备的数量，后面需要使用。 在dashboard.html中修改各input框的value属性为value=&quot;&#123;&#123; up_rate &#125;&#125;&quot; （以上线率为例），这是最关键的步骤，前端会根据这个值的大小，决定圆圈的幅度。 完成后的页面如下图所示： 二、不同状态资产数量统计柱状图要绘制柱状图，不可能我们自己一步步从无到有写起，建议使用第三方插件。AdminLTE中内置的是Chartjs插件，但更建议大家使用百度开源的Echarts插件，功能更强大，更容易学习。 百度Echarts的网址，提供插件下载和说明文档、在线帮助等功能。 教程提供了一个echarts.js源文件，当然你也可以自行下载并安装。 使用Echarts的柱状图很简单，首先生成一个用于放置图形的容器：&lt;div class=&quot;col-md-6&quot;&gt; &lt;!-- BAR CHART --&gt; &lt;div class=&quot;box box-success&quot;&gt; &lt;div class=&quot;box-header with-border&quot;&gt; &lt;h3 class=&quot;box-title&quot;&gt;各状态资产数量统计：&lt;/h3&gt; &lt;div class=&quot;box-tools pull-right&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;collapse&quot;&gt;&lt;i class=&quot;fa fa-minus&quot;&gt;&lt;/i&gt; &lt;/button&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-box-tool&quot; data-widget=&quot;remove&quot;&gt;&lt;i class=&quot;fa fa-times&quot;&gt;&lt;/i&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;box-body&quot;&gt; &lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;!-- /.box-body --&gt; &lt;/div&gt;&lt;/div&gt; 上面的核心是&lt;div id=&quot;barChart&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; 这句，它指明了图表的id和容器大小。其它的都是AdminLTE框架需要的元素，用于生成表头和折叠、关闭动作按钮。我们的容器是可以折叠和删除的，也是移动端自适应的。 构造了容器后，需要在页面底部首先引入 echarts.js 文件： &lt;script src=&quot;&#123;% static &apos;plugins/echarts.js&apos; %&#125;&quot;&gt;&lt;/script&gt; 然后在中，添加初始化的js代码：$(function () &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&apos;barChart&apos;)); // 指定图表的配置项和数据 var option = &#123; color: [&apos;#3398DB&apos;], title: &#123; text: &apos;数量&apos; &#125;, tooltip: &#123;&#125;, legend: &#123; data:[&apos;&apos;] &#125;, xAxis: &#123; data: [&quot;在线&quot;, &quot;下线&quot;,&quot;故障&quot;,&quot;备用&quot;,&quot;未知&quot;] &#125;, yAxis: &#123;&#125;, series: [&#123; name: &apos;数量&apos;, type: &apos;bar&apos;, barWidth: &apos;50%&apos;, data: [&#123;&#123; upline &#125;&#125;, &#123;&#123; offline &#125;&#125;, &#123;&#123; breakdown &#125;&#125;, &#123;&#123; backup &#125;&#125;, &#123;&#123; unknown &#125;&#125;] &#125;] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &#125;); 上面的js代码中，中文文字部分很容易理解，就是x轴的说明文字。还可以设置柱状图的颜色、宽度等特性。关键是series列表，其中的type指定该charts是什么类型，bar表示柱状图，而data就是至关重要的具体数据了，利用模板语言，将从数据库中获取的具体数值传入进来，Echarts插件会根据数值进行动态调整。 三、各类型资产数量统计饼图类似上面的柱状图，在HTML中需要先添加一个容器。不同之处在于初始化的JS代码：//资产类型数量统计 饼图 $(function () &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&apos;donutChart&apos;)); // 指定图表的配置项和数据 option = &#123; title : &#123; x:&apos;center&apos; &#125;, tooltip : &#123; trigger: &apos;item&apos;, formatter: &quot;&#123;a&#125; &lt;br/&gt;&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot; &#125;, legend: &#123; orient: &apos;vertical&apos;, left: &apos;left&apos;, data: [&apos;服务器&apos;,&apos;网络设备&apos;,&apos;存储设备&apos;,&apos;安全设备&apos;,&apos;软件资产&apos;] &#125;, series : [ &#123; name: &apos;资产类型&apos;, type: &apos;pie&apos;, radius : &apos;55%&apos;, center: [&apos;50%&apos;, &apos;60%&apos;], data:[ &#123;value:&#123;&#123; server_number &#125;&#125;, name:&apos;服务器&apos;&#125;, &#123;value:&#123;&#123; networkdevice_number &#125;&#125;, name:&apos;网络设备&apos;&#125;, &#123;value:&#123;&#123; storagedevice_number &#125;&#125;, name:&apos;存储设备&apos;&#125;, &#123;value:&#123;&#123; securitydevice_number &#125;&#125;, name:&apos;安全设备&apos;&#125;, &#123;value:&#123;&#123; software_number &#125;&#125;, name:&apos;软件资产&apos;&#125; ], itemStyle: &#123; emphasis: &#123; shadowBlur: 10, shadowOffsetX: 0, shadowColor: &apos;rgba(0, 0, 0, 0.5)&apos; &#125; &#125; &#125; ] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &#125;); series中的type指定为pie类型表示饼图，data列表动态传入各种资产类型的数量。其它的设置可参考官方文档。 为了展示的方便，我们在admin中新建一些网络设备、安全设备、软件资产等其它类型的资产，然后查看资产总表和饼图。这里我分别添加了一台网络、安全和存储设备和两个软件资产。 查看资产总表如下图所示： 查看dashboard如下图所示： 四、项目总结至此，CMDB项目就基本讲解完毕。 还是要强调的是，这是一个demo版都不能算的教学版，很多内容和细节没有实现，必然存在bug和不足。但不管怎么样，它至少包含CMDB资产管理的主体内容，如果你能从中有点收获，那么教程的目的就达到了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-资产详细页面（十一）]]></title>
    <url>%2F2019%2F03%2F14%2F11%E3%80%81%E8%B5%84%E4%BA%A7%E8%AF%A6%E7%BB%86%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[在资产的详细页面，我们将尽可能地将所有的信息都显示出来，并保持美观、整齐。 教程中实现了主要的服务器资产页面，对于其它类型的资产详细页面，可参照完成，并不复杂。 完整的detail.html页面代码如下：detail.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html 主要代码全部集中在：&lt;section class=&quot;content&quot;&gt; 分别用几个表格将概览、服务器、CPU、内存、硬盘和网卡的信息展示出来了。并且，AdminLTE为我们提供了一个折叠的功能，也是非常酷的。 这个HTML文件没有太多需要额外解释的内容，都是一些很基础的模板语言，构造&lt;table&gt; 然后插入数据。如果没有数据，就以‘N/A’代替。最后在底部添加一个返回资产总表的链接。 下面是一个展示图：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-资产总表（十）]]></title>
    <url>%2F2019%2F03%2F13%2F10%E3%80%81%E8%B5%84%E4%BA%A7%E6%80%BB%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[当前，我们的资产总表如下图所示，还没有任何数据： 这需要我们从数据库中查询数据，然后渲染到前端页面中。 数据的获取很简单，一句：assets = models.Asset.objects.all() 就搞定。当然，你也可以设置过滤条件，添加分页等等。 而在前端，我们往往需要以表格的形式，规整、美观、可排序的展示出来。这里推荐一个前端插件 datatables，是一个非常好的表格插件，功能强大、配置简单。 其官网为：https://datatables.net/ 中文网站：http://datatables.club/ 在AdminLTE中，集成了datatables插件，无需额外下载和安装，直接引入使用就可以。 下面给出一个完整的index.html模板代码：index.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html 主要是新增了表格相关的html代码和初始化表格的js代码。 &lt;table id=&quot;assets_table&quot; class=&quot;table table-bordered table-striped&quot;&gt; id属性非常重要，用于关联相应的初始化js代码。 表格中，循环每一个资产： 首先生成一个排序的列； 再根据资产类型的不同，用不同的颜色生成不同的资产类型名和子类型名； 通过asset.get_asset_type_display的模板语法，拿到资产类型的直观名称，比如‘服务器’，而不是显示呆板的‘server’； 通过asset.server.get_sub_asset_type_display，获取资产对应类型的子类型。这是Django特有的模板语法，非常类似其ORM的语法； 在资产名的栏目，增加了超级链接，用于显示资产的详细内容。这里只实现了服务器类型资产的详细页面，其它类型请自行完善； 根据资产状态的不同，用不同的颜色显示； 利用asset.m_time|date:”Y/m/d [H:m:s]”调整时间的显示格式； 由于资产和tas标签属于多对多的关系，所以需要一个循环，遍历每个tas并打印其名称； 通过asset.tags.all可以获取一个资产对应的多对多字段的全部对象，很类似ORM的做法。 表格的初始化JS代码如下：&lt;script&gt; $(function () &#123; $(&apos;#assets_table&apos;).DataTable(&#123; &quot;paging&quot;: true, &lt;!-- 允许分页 --&gt; &quot;lengthChange&quot;: true, &lt;!-- 允许改变每页显示的行数 --&gt; &quot;searching&quot;: true, &lt;!-- 允许内容搜索 --&gt; &quot;ordering&quot;: true, &lt;!-- 允许排序 --&gt; &quot;info&quot;: true, &lt;!-- 显示信息 --&gt; &quot;autoWidth&quot;: false &lt;!-- 固定宽度 --&gt; &#125;); &#125;);&lt;/script&gt; 其中可定义是否允许分页、改变显示的行数、搜索、排序、显示信息、固定宽度等等，通过表格的id进行关联。 下面，我们通过后台admin界面，多增加几个服务器实例，并修改其类型、业务线、状态、厂商、机房、标签，再刷新资产总表，可以看到效果如下： 试着使用一下排序和搜索功能吧！datatables还是相当强大的！ 现在点击资产名称，可以链接到资产详细页面，但没有任何数据显示，在下一节中，我们来实现它。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-前端框架AdminLTE（九）]]></title>
    <url>%2F2019%2F03%2F12%2F9%E3%80%81%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6AdminLTE%2F</url>
    <content type="text"><![CDATA[作为CMDB资产管理项目，必须有一个丰富、直观、酷炫的前端页面。 适合运维平台的前端框架有很多，开源的也不少，这里选用的是AdminLTE。 AdminLTE托管在GitHub上，可以通过下面的地址下载： https://github.com/almasaeed2010/AdminLTE/releases AdminLTE自带JQuery和Bootstrap3框架，无需另外下载。 AdminLTE自带多种配色皮肤，可根据需要实时调整。 AdminLTE是移动端自适应的，无需单独考虑。 AdminLTE自带大量插件，比如表格、Charts等等，可根据需要载入。 但是AdminLTE的源文件包内，缺少font-awesome-4.6.3和ionicons-2.0.1这两个图标插件，它是通过CDN的形式加载的，如果网络不太好，加载可能比较困难或者缓慢，最好用本地静态文件的形式。教程在Github的包内附带上了这两个插件，可以直接使用，当然你自己下载安装也行。 一、创建base.htmlAdminLTE源文件包里有个index.html页面文件，可以利用它修改出我们CMDB项目需要的基本框架。 在项目的根目录cmdb下新建static目录，在settings文件中添加下面的配置： STATIC_URL = &apos;/static/&apos;STATICFILES_DIRS = [ os.path.join(BASE_DIR, &quot;mycmdb/static&quot;),] 为了以后扩展的方便，将AdminLTE源文件包里的 bootstrap、dist 和 plugins 三个文件夹，全部拷贝到 static 目录中，这样做的话文件会比较大，比较多，但可以防止出现引用文件找不到、插件缺失等情况的发生，等以后对AdminLTE非常熟悉了，可以对static中无用的文件进行删减。 在cmdb根目录下的templates目录下，新建base.html文件，将AdminLTE源文件包中的index.html中的内容拷贝过去。然后，根据我们项目的具体情况修改文件引用、页面框架、title、CSS、主体和script块。 base.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/base.html 这是一个适合当前CMDB的精简版本。 二、创建路由、视图这里设计了三个视图和页面，分别是： dashboard：仪表盘，图形化的数据展示 index：资产总表，表格的形式展示资产信息 detail：单个资产的详细信息页面 将assets/urls.py修改成下面的样子：from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;), url(r&apos;^dashboard/&apos;, views.dashboard, name=&apos;dashboard&apos;), url(r&apos;^index/&apos;, views.index, name=&apos;index&apos;), url(r&apos;^detail/(?P&lt;asset_id&gt;[0-9]+)/$&apos;, views.detail, name=&quot;detail&quot;), url(r&apos;^$&apos;, views.dashboard),] 在 assets/views.py 中，增加下面三个视图：from django.shortcuts import get_object_or_404def index(request): assets = models.Asset.objects.all() return render(request, &apos;assets/index.html&apos;, locals())def dashboard(request): pass return render(request, &apos;assets/dashboard.html&apos;, locals())def detail(request, asset_id): &quot;&quot;&quot; 以显示服务器类型资产详细为例，安全设备、存储设备、网络设备等参照此例。 :param request: :param asset_id: :return: &quot;&quot;&quot; asset = get_object_or_404(models.Asset, id=asset_id) return render(request, &apos;assets/detail.html&apos;, locals()) 注意需要提前from django.shortcuts import get_object_or_404 导入get_object_or_404()方法，这是一个非常常用的内置方法。 三、创建模版1.dashboard.html在assets目录下创建 templates/assets/dashboard.html 文件，写入下面的代码：dashboard.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/dashboard.html 2.index.html在assets目录下创建 templates/assets/index.html 文件，写入下面的代码：index.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/index.html 3.detail.html在assets目录下创建 templates/assets/detail.html 文件，写入下面的代码： detail.htmlhttps://github.com/zhangduanya/mycmdb/blob/master/templates/assets/detail.html 以上三个模板都很简单，就是下面的流程： extends继承‘base.html’； load staticfiles：载入静态文件； block title：资产详细endblock，定制title; block css：载入当前页面的专用CSS文件； block script：载入当前页面的专用js文件； 最后在block content：中，编写一个当前页面的面包屑导航； 页面的主体内容在后面的章节进行充实。 四、访问页面重启CMDB服务器，访问http://192.168.1.3:8000/assets/dashboard/，可以看到下面的页面。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-已上线资产信息更新（八）]]></title>
    <url>%2F2019%2F03%2F11%2F8%E3%80%81%E5%B7%B2%E4%B8%8A%E7%BA%BF%E8%B5%84%E4%BA%A7%E4%BF%A1%E6%81%AF%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前面，我们已经实现了资产进入待审批区、更新待审批区的资产信息以及审批资产上线三个主要功能，还剩下一个最主要的实时更新已上线资产信息的功能。 在assets/views.py中的report视图，目前是把已上线资产的数据更新流程‘pass’了，现在将其替换成下面的语句： update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) report视图变成了下面的样子：views.py from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 update_asset = asset_handler.UpdateAsset(request, asset_obj[0], data) return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) 然后，进入assets/asset_handler.py模块，修改log()方法，增加UpdateAsset类，最终的asset_handler.py如下：#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\n%s&quot; % msg event.user = request.user elif log_type == &quot;update&quot;: event.name = &quot;%s &lt;%s&gt; ： 数据更新！&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新成功！&quot; elif log_type == &quot;update_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 更新失败&quot; % (asset.asset_type, asset.sn) event.asset = asset event.detail = &quot;更新失败！\n%s&quot; % msg # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;create_manufacturer--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete()class UpdateAsset: &quot;&quot;&quot; 自动更新已上线的资产。 如果想让记录的日志更详细，可以逐条对比数据项，将更新过的项目记录到log信息中。 &quot;&quot;&quot; def __init__(self, request, asset, report_data): self.request = request self.asset = asset self.report_data = report_data # 此处的数据是由客户端发送过来的整个数据字符串 self.asset_update() def asset_update(self): # 为以后的其它类型资产扩展留下接口 func = getattr(self, &quot;_%s_update&quot; % self.report_data[&apos;asset_type&apos;]) func() def _server_update(self): try: self._update_manufacturer() # 更新厂商 self._update_server() # 更新服务器 self._update_CPU() # 更新CPU self._update_RAM() # 更新内存 self._update_disk() # 更新硬盘 self._update_nic() # 更新网卡 self.asset.save() except Exception as e: log(&apos;update_failed&apos;, msg=e, asset=self.asset, request=self.request) print(e) else: # 添加日志 log(&quot;update&quot;, asset=self.asset) print(&quot;资产数据被更新!&quot;) def _update_manufacturer(self): &quot;&quot;&quot; 更新厂商 &quot;&quot;&quot; m = self.report_data.get(&apos;manufacturer&apos;) if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) self.asset.manufacturer = manufacturer_obj else: self.asset.manufacturer = None self.asset.manufacturer.save() def _update_server(self): &quot;&quot;&quot; 更新服务器 &quot;&quot;&quot; self.asset.server.model = self.report_data.get(&apos;model&apos;) self.asset.server.os_type = self.report_data.get(&apos;os_type&apos;) self.asset.server.os_distribution = self.report_data.get(&apos;os_distribution&apos;) self.asset.server.os_release = self.report_data.get(&apos;os_release&apos;) self.asset.server.save() def _update_CPU(self): &quot;&quot;&quot; 更新CPU信息 :return: &quot;&quot;&quot; self.asset.cpu.cpu_model = self.report_data.get(&apos;cpu_model&apos;) self.asset.cpu.cpu_count = self.report_data.get(&apos;cpu_count&apos;) self.asset.cpu.cpu_core_count = self.report_data.get(&apos;cpu_core_count&apos;) self.asset.cpu.save() def _update_RAM(self): &quot;&quot;&quot; 更新内存信息。 使用集合数据类型中差的概念，处理不同的情况。 如果新数据有，但原数据没有，则新增； 如果新数据没有，但原数据有，则删除原来多余的部分； 如果新的和原数据都有，则更新。 在原则上，下面的代码应该写成一个复用的函数， 但是由于内存、硬盘、网卡在某些方面的差别，导致很难提取出重用的代码。 :return: &quot;&quot;&quot; # 获取已有内存信息，并转成字典格式 old_rams = models.RAM.objects.filter(asset=self.asset) old_rams_dict = dict() if old_rams: for ram in old_rams: old_rams_dict[ram.slot] = ram # 获取新数据中的内存信息，并转成字典格式 new_rams_list = self.report_data[&apos;ram&apos;] new_rams_dict = dict() if new_rams_list: for item in new_rams_list: new_rams_dict[item[&apos;slot&apos;]] = item # 利用set类型的差集功能，获得需要删除的内存数据对象 need_deleted_keys = set(old_rams_dict.keys()) - set(new_rams_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_rams_dict[key].delete() # 需要新增或更新的 if new_rams_dict: for key in new_rams_dict: defaults = &#123; &apos;sn&apos;: new_rams_dict[key].get(&apos;sn&apos;), &apos;model&apos;: new_rams_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_rams_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_rams_dict[key].get(&apos;capacity&apos;, 0), &#125; models.RAM.objects.update_or_create(asset=self.asset, slot=key, defaults=defaults) def _update_disk(self): &quot;&quot;&quot; 更新硬盘信息。类似更新内存。 &quot;&quot;&quot; old_disks = models.Disk.objects.filter(asset=self.asset) old_disks_dict = dict() if old_disks: for disk in old_disks: old_disks_dict[disk.sn] = disk new_disks_list = self.report_data[&apos;physical_disk_driver&apos;] new_disks_dict = dict() if new_disks_list: for item in new_disks_list: new_disks_dict[item[&apos;sn&apos;]] = item # 需要删除的 need_deleted_keys = set(old_disks_dict.keys()) - set(new_disks_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_disks_dict[key].delete() # 需要新增或更新的 if new_disks_dict: for key in new_disks_dict: interface_type = new_disks_dict[key].get(&apos;iface_type&apos;, &apos;unknown&apos;) if interface_type not in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: interface_type = &apos;unknown&apos; defaults = &#123; &apos;slot&apos;: new_disks_dict[key].get(&apos;slot&apos;), &apos;model&apos;: new_disks_dict[key].get(&apos;model&apos;), &apos;manufacturer&apos;: new_disks_dict[key].get(&apos;manufacturer&apos;), &apos;capacity&apos;: new_disks_dict[key].get(&apos;capacity&apos;, 0), &apos;interface_type&apos;: interface_type, &#125; models.Disk.objects.update_or_create(asset=self.asset, sn=key, defaults=defaults) def _update_nic(self): &quot;&quot;&quot; 更新网卡信息。类似更新内存。 &quot;&quot;&quot; old_nics = models.NIC.objects.filter(asset=self.asset) old_nics_dict = dict() if old_nics: for nic in old_nics: old_nics_dict[nic.model+nic.mac] = nic new_nics_list = self.report_data[&apos;nic&apos;] new_nics_dict = dict() if new_nics_list: for item in new_nics_list: new_nics_dict[item[&apos;model&apos;]+item[&apos;mac&apos;]] = item # 需要删除的 need_deleted_keys = set(old_nics_dict.keys()) - set(new_nics_dict.keys()) if need_deleted_keys: for key in need_deleted_keys: old_nics_dict[key].delete() # 需要新增或更新的 if new_nics_dict: for key in new_nics_dict: if new_nics_dict[key].get(&apos;net_mask&apos;) and len(new_nics_dict[key].get(&apos;net_mask&apos;)) &gt; 0: net_mask = new_nics_dict[key].get(&apos;net_mask&apos;)[0] else: net_mask = &quot;&quot; defaults = &#123; &apos;name&apos;: new_nics_dict[key].get(&apos;name&apos;), &apos;ip_address&apos;: new_nics_dict[key].get(&apos;ip_address&apos;), &apos;net_mask&apos;: net_mask, &#125; models.NIC.objects.update_or_create(asset=self.asset, model=new_nics_dict[key][&apos;model&apos;], mac=new_nics_dict[key][&apos;mac&apos;], defaults=defaults) print(&apos;更新成功！&apos;) 对于log()函数，只是增加了两种数据更新的日志类型，分别记录不同的日志情况，没什么特别的。 对于UpdateAsset类，类似前面的ApproveAsset类： 首先初始化动作，自动执行asset_update()方法； 依然是通过反射，决定要调用的更新方法； 教程实现了主要的服务器类型资产的更新，对于网络设备、安全设备等请自行完善，基本类似； _server_update(self)方法中，分别更新厂商、服务器本身、CPU、内存、网卡、硬盘等信息。然后保存数据，这些事务应该是原子性的，所以要抓取异常； 不管成功还是失败，都要记录日志。 最主要的，对于_update_CPU(self)等方法，以内存为例，由于内存可能有多条，新的数据中可能出现三种情况，拔除、新增、信息变更，因此要分别对待和处理。 首先，获取已有内存信息，并转成字典格式； 其次，获取新数据中的内存信息，并转成字典格式； 利用set类型的差集功能，获得需要删除的内存数据对象 对要删除的对象，执行delete()方法； 对于需要新增或更新的内存对象，首先生成defaults数据字典； 然后，使用update_or_create(asset=self.asset, slot=key, defaults=defaults)方法，一次性完成新增或者更新数据的操作，不用写两个方法的代码； 硬盘和网卡的操作类同内存的操作。 数据更新完毕后，需要保存asset对象，也就是self.asset.save()，否则前面的工作无法关联保存下来。 现在，可以测试一下资产数据的更新了。重启CMDB，然后转到Client/report_assetss.py脚本，直接运行： 修改其中的一些数据，删除或增加一些内存、硬盘、网卡的条目。注意数据格式必须正确，sn必须不能变。 再次运行脚本，报告数据。进入admin中查看相关内容，可以看到数据已经得到更新了。 至此，CMDB自动资产管理系统的后台部分已经完成了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-审批新资产（七）]]></title>
    <url>%2F2019%2F03%2F10%2F7%E3%80%81%E5%AE%A1%E6%89%B9%E6%96%B0%E8%B5%84%E4%BA%A7%2F</url>
    <content type="text"><![CDATA[一、自定义admin的actions需要有专门的审批员来审批新资产，对资产的合法性、健全性、可用性等更多方面进行审核，如果没有问题，那么就批准上线。 批准上线这一操作是通过admin的自定义actions来实现的。 Django的admin默认有一个delete操作的action，所有在admin中的模型都有这个action，更多的就需要我们自己编写了。 修改/assets/admin.py的代码，新的代码如下： from django.contrib import admin# Register your models here.from assets import modelsfrom . import asset_handlerclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,) actions = [&apos;approve_selected_new_assets&apos;] def approve_selected_new_assets(self, request, queryset): # 获得被打钩的checkbox对应的资产 selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME) success_upline_number = 0 for asset_id in selected: obj = asset_handler.ApproveAsset(request, asset_id) ret = obj.asset_upline() if ret: success_upline_number += 1 # 顶部绿色提示信息 self.message_user(request, &quot;成功批准 %s 条新资产上线！&quot; % success_upline_number) approve_selected_new_assets.short_description = &quot;批准选择的新资产&quot;class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &quot;m_time&quot;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 说明： 通过actions = [‘approve_selected_new_assets’]定义当前模型的新acitons列表； approve_selected_new_assets()方法包含具体的动作逻辑； 自定义的action接收至少三个参数，第一个是self，第二个是request即请求，第三个是被选中的数据对象集合queryset。 首先通过request.POST.getlist()方法获取被打钩的checkbox对应的资产； 可能同时有多个资产被选择，所以这是个批量操作，需要进行循环； selected是一个包含了被选中资产的id值的列表； 对于每一个资产，创建一个asset_handler.ApproveAsset()的实例，然后调用实例的asset_upline()方法，并获取返回值。如果返回值为True，说明该资产被成功批准，那么success_upline_number变量+1，保存成功批准的资产数； 最后，在admin中给与提示信息。 approve_selected_new_assets.short_description = “批准选择的新资产”用于在admin界面中为action提供中文显示。你可以尝试去掉这条，看看效果。 重新启动CMDB，进入admin的待审批资产区，查看上方的acitons动作条，如下所示： 二、创建测试用例由于没有真实的服务器供测试，这里需要手动创建一些虚假的服务器用例，方便后面的使用和展示。 首先，将先前的所有资产条目全部从admin中删除，确保数据库内没有任何数据。 然后，在Client/bin/目录下新建一个report_assets脚本，其内容如下：#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonimport osimport sysimport urllib.requestimport urllib.parseBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from conf import settingsdef update_test(data): &quot;&quot;&quot; 创建测试用例 :return: &quot;&quot;&quot; # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(data)&#125; # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() ##转码 response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\033[31;1m发送完毕！\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e)if __name__ == &apos;__main__&apos;: windows_data = &#123; &quot;os_type&quot;: &quot;Windows&quot;, &quot;os_release&quot;: &quot;7 64bit 6.1.7601 &quot;, &quot;os_distribution&quot;: &quot;Microsoft&quot;, &quot;asset_type&quot;: &quot;server&quot;, &quot;cpu_count&quot;: 2, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;cpu_core_count&quot;: 8, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &quot;model&quot;: &quot;Physical Memory&quot;, &quot;manufacturer&quot;: &quot;kingstone &quot;, &quot;sn&quot;: &quot;456&quot; &#125;, ], &quot;manufacturer&quot;: &quot;Intel&quot;, &quot;model&quot;: &quot;P67X-UD3R-B3&quot;, &quot;wake_up_type&quot;: 6, &quot;sn&quot;: &quot;00426-OEM-8992662-111111&quot;, &quot;physical_disk_driver&quot;: [ &#123; &quot;iface_type&quot;: &quot;unknown&quot;, &quot;slot&quot;: 0, &quot;sn&quot;: &quot;3830414130423230343234362020202020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 128 &#125;, &#123; &quot;iface_type&quot;: &quot;SATA&quot;, &quot;slot&quot;: 1, &quot;sn&quot;: &quot;383041413042323023234362020102020202020&quot;, &quot;model&quot;: &quot;KINGSTON SV100S264G ATA Device&quot;, &quot;manufacturer&quot;: &quot;(标准磁盘驱动器)&quot;, &quot;capacity&quot;: 2048 &#125;, ], &quot;nic&quot;: [ &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000011] Realtek RTL8192CU Wireless LAN 802.11n USB 2.0 Network Adapter&quot;, &quot;name&quot;: 11, &quot;ip_address&quot;: &quot;192.168.1.110&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;0A:01:27:00:00:00&quot;, &quot;model&quot;: &quot;[00000013] VirtualBox Host-Only Ethernet Adapter&quot;, &quot;name&quot;: 13, &quot;ip_address&quot;: &quot;192.168.56.1&quot;, &quot;net_mask&quot;: [ &quot;255.255.255.0&quot;, &quot;64&quot; ] &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;[00000017] Microsoft Virtual WiFi Miniport Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, &#123; &quot;mac&quot;: &quot;14:CF:22:FF:48:34&quot;, &quot;model&quot;: &quot;Intel Adapter&quot;, &quot;name&quot;: 17, &quot;ip_address&quot;: &quot;192.1.1.1&quot;, &quot;net_mask&quot;: &quot;&quot; &#125;, ] &#125; linux_data = &#123; &quot;asset_type&quot;: &quot;server&quot;, &quot;manufacturer&quot;: &quot;innotek GmbH&quot;, &quot;sn&quot;: &quot;00001&quot;, &quot;model&quot;: &quot;VirtualBox&quot;, &quot;uuid&quot;: &quot;E8DE611C-4279-495C-9B58-502B6FCED076&quot;, &quot;wake_up_type&quot;: &quot;Power Switch&quot;, &quot;os_distribution&quot;: &quot;Ubuntu&quot;, &quot;os_release&quot;: &quot;Ubuntu 16.04.3 LTS&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &quot;cpu_count&quot;: &quot;2&quot;, &quot;cpu_core_count&quot;: &quot;4&quot;, &quot;cpu_model&quot;: &quot;Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz&quot;, &quot;ram&quot;: [ &#123; &quot;slot&quot;: &quot;A1&quot;, &quot;capacity&quot;: 8, &#125; ], &quot;ram_size&quot;: 3.858997344970703, &quot;nic&quot;: [], &quot;physical_disk_driver&quot;: [ &#123; &quot;model&quot;: &quot;VBOX HARDDISK&quot;, &quot;size&quot;: &quot;50&quot;, &quot;sn&quot;: &quot;VBeee1ba73-09085302&quot; &#125; ] &#125; update_test(linux_data) update_test(windows_data) 该脚本的作用很简单，人为虚构了两台服务器（一台windows，一台Linux）的信息，并发送给CMDB。单独执行该脚本，在admin的新资产待审批区可以看到添加了两条新资产信息。 要添加更多的资产，只需修改脚本中windows_data和linux_data的数据即可。但是要注意的是，如果不修改sn，那么会变成资产数据更新，而不是增加新资产，这一点一定要注意。 三、批准资产上线在/assets/asset_handler.py中添加下面的代码： #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data print(&quot;asset_handler--&gt;&quot;, self.data) def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;capacity&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos;def log(log_type, msg=None, asset=None, new_asset=None, request=None): &quot;&quot;&quot; 记录日志，被程序调用 &quot;&quot;&quot; event = models.EventLog() if log_type == &quot;upline&quot;: event.name = &quot;%s &lt;%s&gt; ： 上线&quot; % (asset.name, asset.sn) event.asset = asset event.detail = &quot;资产成功上线！&quot; event.user = request.user elif log_type == &quot;approve_failed&quot;: event.name = &quot;%s &lt;%s&gt; ： 审批失败&quot; % (new_asset.asset_type, new_asset.sn) event.new_asset = new_asset event.detail = &quot;审批失败！\n%s&quot; % msg event.user = request.user # 更多日志类型..... event.save()class ApproveAsset: &quot;&quot;&quot; 审批资产并上线。 &quot;&quot;&quot; def __init__(self, request, asset_id): self.request = request self.new_asset = models.NewAssetApprovalZone.objects.get(id=asset_id) self.data = json.loads(self.new_asset.data) def asset_upline(self): # 为以后的其它类型资产扩展留下接口(假如不是server，是firewall 或者其他类型，我们就不需要重复的去写) func = getattr(self, &quot;_%s_upline&quot; % self.new_asset.asset_type) ret = func() return ret and True def _server_upline(self): asset = self._create_asset() try: self._create_manufacturer(asset) # 创建厂商 self._create_server(asset) # 创建服务器 self._create_CPU(asset) # 创建CPU self._create_RAM(asset) # 创建内存 self._create_disk(asset) # 创建硬盘 self._create_nic(asset) # 创建网卡 self._delete_original_asset() # 从待审批资产区删除已审批上线的资产 except Exception as e: asset.delete() log(&apos;approve_failed&apos;, msg=e, new_asset=self.new_asset, request=self.request) print(e) return False else: log(&apos;upline&apos;, asset=asset, request=self.request) print(&quot;新服务器上线&quot;) return True def _create_asset(self): &quot;&quot;&quot; 创建资产并上线 :return: &quot;&quot;&quot; # 利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 asset = models.Asset.objects.create(asset_type=self.new_asset.asset_type, name=&quot;%s: %s&quot; % (self.new_asset.asset_type, self.new_asset.sn), sn=self.new_asset.sn, approved_by=self.request.user, ) return asset def _create_manufacturer(self, asset): &quot;&quot;&quot; 创建厂商 :param asset: :return: &quot;&quot;&quot; # 判断厂商数据是否存在。如果存在，看看数据库里是否已经有该厂商，再决定是获取还是创建。 m = self.new_asset.manufacturer if m: manufacturer_obj, _ = models.Manufacturer.objects.get_or_create(name=m) print(&quot;asset_handler--&gt;&quot;, manufacturer_obj, _) asset.manufacturer = manufacturer_obj asset.save() def _create_server(self, asset): &quot;&quot;&quot; 创建服务器 :param asset: :return: &quot;&quot;&quot; models.Server.objects.create(asset=asset, model=self.new_asset.model, os_type=self.new_asset.os_type, os_distribution=self.new_asset.os_distribution, os_release=self.new_asset.os_release, ) def _create_CPU(self, asset): &quot;&quot;&quot; 创建CPU. 教程这里对发送过来的数据采取了最大限度的容忍， 实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测， 根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。 这里的业务逻辑非常复杂，不可能面面俱到。 :param asset: :return: &quot;&quot;&quot; cpu = models.CPU.objects.create(asset=asset) cpu.cpu_model = self.new_asset.cpu_model cpu.cpu_count = self.new_asset.cpu_count cpu.cpu_core_count = self.new_asset.cpu_core_count cpu.save() def _create_RAM(self, asset): &quot;&quot;&quot; 创建内存。通常有多条内存 :param asset: :return: &quot;&quot;&quot; ram_list = self.data.get(&apos;ram&apos;) if not ram_list: # 万一一条内存数据都没有 return for ram_dict in ram_list: if not ram_dict.get(&apos;slot&apos;): raise ValueError(&quot;未知的内存插槽！&quot;) # 使用虚拟机的时候，可能无法获取内存插槽，需要你修改此处的逻辑。 ram = models.RAM() ram.asset = asset ram.slot = ram_dict.get(&apos;slot&apos;) ram.sn = ram_dict.get(&apos;sn&apos;) ram.model = ram_dict.get(&apos;model&apos;) ram.manufacturer = ram_dict.get(&apos;manufacturer&apos;) ram.capacity = ram_dict.get(&apos;capacity&apos;, 0) ram.save() def _create_disk(self, asset): &quot;&quot;&quot; 存储设备种类多，还有Raid情况，需要根据实际情况具体解决。 这里只以简单的SATA硬盘为例子。可能有多块硬盘。 :param asset: :return: &quot;&quot;&quot; disk_list = self.data.get(&apos;physical_disk_driver&apos;) if not disk_list: # 一条硬盘数据都没有 return for disk_dict in disk_list: if not disk_dict.get(&apos;sn&apos;): raise ValueError(&quot;未知sn的硬盘！&quot;) # 根据sn确定具体某块硬盘。 disk = models.Disk() disk.asset = asset disk.sn = disk_dict.get(&apos;sn&apos;) disk.model = disk_dict.get(&apos;model&apos;) disk.manufacturer = disk_dict.get(&apos;manufacturer&apos;), disk.slot = disk_dict.get(&apos;slot&apos;) disk.capacity = disk_dict.get(&apos;capacity&apos;, 0) iface = disk_dict.get(&apos;iface_type&apos;) if iface in [&apos;SATA&apos;, &apos;SAS&apos;, &apos;SCSI&apos;, &apos;SSD&apos;, &apos;unknown&apos;]: disk.interface_type = iface disk.save() def _create_nic(self, asset): &quot;&quot;&quot; 创建网卡。可能有多个网卡，甚至虚拟网卡。 :param asset: :return: &quot;&quot;&quot; nic_list = self.data.get(&quot;nic&quot;) if not nic_list: return for nic_dict in nic_list: if not nic_dict.get(&apos;mac&apos;): raise ValueError(&quot;网卡缺少mac地址！&quot;) if not nic_dict.get(&apos;model&apos;): raise ValueError(&quot;网卡型号未知！&quot;) nic = models.NIC() nic.asset = asset nic.name = nic_dict.get(&apos;name&apos;) nic.model = nic_dict.get(&apos;model&apos;) nic.mac = nic_dict.get(&apos;mac&apos;) nic.ip_address = nic_dict.get(&apos;ip_address&apos;) if nic_dict.get(&apos;net_mask&apos;): if len(nic_dict.get(&apos;net_mask&apos;)) &gt; 0: nic.net_mask = nic_dict.get(&apos;net_mask&apos;)[0] nic.save() def _delete_original_asset(self): &quot;&quot;&quot; 这里的逻辑是已经审批上线的资产，就从待审批区删除。 也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示。 不过这样可能导致待审批区越来越大。 :return: &quot;&quot;&quot; self.new_asset.delete() 核心就是增加了一个记录日志的log()函数以及审批资产的ApproveAsset类。 log()函数很简单，根据日志类型的不同，保存日志需要的各种信息，比如日志名称、关联的资产对象、日志详细内容和审批人员等等。所有的日志都被保存在数据库中，可以在admin中查看。 对于关键的ApproveAsset类，说明如下： 初始化方法接收reqeust和待审批资产的id； 分别提前获取资产对象和所有数据data； asset_upline()是入口方法，通过反射，获取一个类似_server_upline的方法。之所以这么做，是为后面的网络设别、安全设备、存储设备等更多类型资产的审批留下扩展接口。本教程里只实现了服务器类型资产的审批方法，更多的请自行完善，过程基本类似。 _server_upline()是服务器类型资产上线的核心方法： 它首先新建了一个Asset资产对象（注意要和待审批区的资产区分开）； 然后利用该对象，分别创建了对应的厂商、服务器、CPU、内存、硬盘和网卡，并删除待审批区的对应资产； 在实际的生产环境中，上面的操作应该是原子性的整体事务，任何一步出现异常，所有操作都要回滚； 如果任何一步出现错误，上面的操作全部撤销，也就是asset.delete()。记录错误日志，返回False； 如果没问题，那么记录正确日志，返回True。 对于_create_asset(self)方法，利用request.user自动获取当前管理人员的信息，作为审批人添加到资产数据中。 对于_create_manufacturer(self, asset)方法，先判断厂商数据是否存在，再决定是获取还是创建。 对于_create_CPU(self, asset)等方法，教程这里对数据采取了最大限度的容忍，实际情况下你可能还要对数据的完整性、合法性、数据类型进行检测，根据不同的检测情况，是被动接收，还是打回去要求重新收集，请自行决定。这里的业务逻辑非常复杂，不可能面面俱到。后面的内存、硬盘和网卡也是一样的。 对于_delete_original_asset(self)方法，这里的逻辑是已经审批上线的资产，就从待审批区删除。也可以设置为修改成已审批状态但不删除，只是在管理界面特别处理，不让再次审批，灰色显示，不过这样可能导致待审批区越来越大。 四、测试资产上线功能运行 report_assets.py 脚本： 在admin的新资产待审批区选择刚才的3条资产，然后选择上线action并点击‘执行’按钮，稍等片刻，显示成功批准 3 条新资产上线！的绿色提示信息，同时新资产也从待审批区被删除了，如下图所示： 往后，如果我们再次发送这3个服务器资产的信息，那就不是在待审批区了，而是已上线资产了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-新资产待审批区（六）]]></title>
    <url>%2F2019%2F03%2F09%2F6%E3%80%81%E6%96%B0%E8%B5%84%E4%BA%A7%E5%BE%85%E5%AE%A1%E6%89%B9%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[一、启用admin前面，我们已经完成了数据收集客户端的编写和测试，下面我们就可以在admin中展示和管理资产数据了。 首先，通过python manage.py createsuperuser 创建一个管理员账户。 然后，进入/assets/admin.py文件，写入下面的代码：from django.contrib import admin# Register your models here.from assets import modelsclass NewAssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;sn&apos;, &apos;model&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;, &apos;m_time&apos;] list_filter = [&apos;asset_type&apos;, &apos;manufacturer&apos;, &apos;c_time&apos;] search_fields = (&apos;sn&apos;,)class AssetAdmin(admin.ModelAdmin): list_display = [&apos;asset_type&apos;, &apos;name&apos;, &apos;status&apos;, &apos;approved_by&apos;, &apos;c_time&apos;, &apos;m_time&apos;]admin.site.register(models.Asset, AssetAdmin)admin.site.register(models.Server)admin.site.register(models.StorageDevice)admin.site.register(models.SecurityDevice)admin.site.register(models.BusinessUnit)admin.site.register(models.Contract)admin.site.register(models.CPU)admin.site.register(models.Disk)admin.site.register(models.EventLog)admin.site.register(models.IDC)admin.site.register(models.Manufacturer)admin.site.register(models.NetworkDevice)admin.site.register(models.NIC)admin.site.register(models.RAM)admin.site.register(models.Software)admin.site.register(models.Tag)admin.site.register(models.NewAssetApprovalZone, NewAssetAdmin) 利用刚才创建的管理员用户，登录admin站点： 这里略微对admin界面做了些简单地配置，但目前还没有数据。 二、创建新资产前面我们只是在Pycharm中获取并打印数据，并没有将数据保存到数据库里。下面我们来实现这一功能。 修改/assets/views.py文件，代码如下：from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exemptimport jsonfrom . import modelsfrom . import asset_handler# Create your views here.@csrf_exemptdef report(request): &quot;&quot;&quot; 通过csrf_exempt装饰器，跳过Django的csrf安全机制，让post的数据能被接收，但这又会带来新的安全问题。 可以在客户端，使用自定义的认证token，进行身份验证。这部分工作，请根据实际情况，自己进行。 :param request: :return: &quot;&quot;&quot; if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) data = json.loads(asset_data) # 各种数据检查，请自行添加和完善！ if not data: return HttpResponse(&quot;没有数据！&quot;) if not issubclass(dict, type(data)): return HttpResponse(&quot;数据必须为字典格式！&quot;) # 是否携带了关键的sn号 sn = data.get(&apos;sn&apos;, None) if sn: # 进入审批流程 # 首先判断是否在上线资产中存在该sn asset_obj = models.Asset.objects.filter(sn=sn) if asset_obj: # 进入已上线资产的数据更新流程 pass return HttpResponse(&quot;资产数据已经更新！&quot;) else: # 如果已上线资产中没有，那么说明是未批准资产，进入新资产待审批区，更新或者创建资产。 obj = asset_handler.NewAsset(request, data) response = obj.add_to_new_assets_zone() return HttpResponse(response) else: return HttpResponse(&quot;没有资产sn序列号，请检查数据！&quot;) report视图的逻辑是这样的： sn是标识一个资产的唯一字段，必须携带，不能重复！ 从POST中获取发送过来的数据； 使用json转换数据类型； 进行各种数据检查（比如身份验证等等，请自行完善）； 判断数据是否为空，空则返回错误信息，结束视图； 判断data的类型是否字典类型，否则返回错误信息； 之所以要对data的类型进行判断是因为后面要大量的使用字典的get方法和中括号操作； 如果没有携带sn号，返回错误信息； 当前面都没问题时，进入下面的流程： 首先，利用sn值尝试在已上线的资产进行查找，如果有，则进入已上线资产的更新流程，具体实现，这里暂且跳过; 如果没有，说明这是个新资产，需要添加到新资产区； 这里又分两种情况，一种是彻底的新资产，那没得说，需要新增；另一种是新资产区已经有了，但是审批员还没来得及审批，资产数据的后续报告就已经到达了，那么需要更新数据。 创建一个asset_handler.NewAsset()对象，然后调用它的obj.add_to_new_assets_zone()方法，进行数据保存，并接收返回结果； asset_handler是下面我们要新建的资产处理模块，NewAsset是其中的一个类。 为了不让views.py文件过于庞大，通常会建立新的py文件，专门处理一些核心业务。 在assets下新建asset_handler.py文件，并写入下面的代码：#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-8 07:49# @Author : zhdya@zhdya.cn# @File : asset_handler.pyimport jsonfrom . import modelsclass NewAsset(object): def __init__(self, request, data): self.request = request self.data = data def add_to_new_assets_zone(self): defaults = &#123; &apos;data&apos;: json.dumps(self.data), &apos;asset_type&apos;: self.data.get(&apos;asset_type&apos;), &apos;manufacturer&apos;: self.data.get(&apos;manufacturer&apos;), &apos;model&apos;: self.data.get(&apos;model&apos;), &apos;ram_size&apos;: self.data.get(&apos;ram_size&apos;), &apos;cpu_model&apos;: self.data.get(&apos;cpu_model&apos;), &apos;cpu_count&apos;: self.data.get(&apos;cpu_count&apos;), &apos;cpu_core_count&apos;: self.data.get(&apos;cpu_core_count&apos;), &apos;os_distribution&apos;: self.data.get(&apos;os_distribution&apos;), &apos;os_release&apos;: self.data.get(&apos;os_release&apos;), &apos;os_type&apos;: self.data.get(&apos;os_type&apos;), &#125; models.NewAssetApprovalZone.objects.update_or_create(sn=self.data[&apos;sn&apos;], defaults=defaults) return &apos;资产已经加入或更新待审批区！&apos; NewAsset类接收两个参数，request和data，分别封装了请求和资产数据，它的唯一方法：obj.add_to_new_assets_zone() 首先构造了一个defaults字典，分别将资产数据包的各种数据打包进去，然后利用Django中特别好用的update_or_create()方法，进行数据保存！ update_or_create()方法的机制：如果数据库内没有该数据，那么新增，如果有，则更新，这就大大减少了我们的代码量，不用写两个方法。该方法的参数必须为一些用于查询的指定字段（这里是sn），以及需要新增或者更新的defaults字典。而其返回值，则是一个查询对象和是否新建对象布尔值的二元元组。 三、测试数据重启CMDB，在linux中给Client下的main.py客户端，添加一个report_data的运行参数，然后运行main.py，发送一个资产数据给CMDB服务器，结果如下： 再进入admin后台，查看新资产待审批区，可以看到资产已经成功进入待审批区： 这里我们显示了资产的汇报和更新日期，过几分钟后，重新汇报该资产数据，然后刷新admin中的页面，可以看到，待审批区的资产数据也一并被更新了。]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-Linux下收集数据（五）]]></title>
    <url>%2F2019%2F03%2F04%2F5%E3%80%81Linux%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[Linux下收集数据就有很多命令和工具了，比Windows方便多了。 但是要在Python的进程中运行操作系统级别的命令，我们通常需要使用subprocess模块。这个模块的具体用法，请查看Python教程中相关部分的内容。 下面，我们在Client/plugins下创建一个linux包，再到包里创建一个sys_info.py文件，写入下面的代码： 前提需要现在被收集的虚机上面安装必须的组件：yum install -y lsb 关于disk的获取与原著有差别，更容易理解：##获取厂商：[root@python_master pythontest]# dmidecode -s system-manufacturerVMware, Inc.##获取型号：[root@python_master pythontest]# dmidecode -s system-product-nameVMware Virtual Platform##获取sn：[root@python_master pythontest]# dmidecode -s system-serial-numberVMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60 #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-3 13:16# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport subprocessdef collect(): filter_keys = [&apos;Manufacturer&apos;, &apos;Serial Number&apos;, &apos;Product Name&apos;, &apos;UUID&apos;, &apos;Wake-up Type&apos;] raw_data = &#123;&#125; for key in filter_keys: try: res = subprocess.Popen(&quot;sudo dmidecode -t system|grep &apos;%s&apos;&quot; % key, stdout=subprocess.PIPE, shell=True) result = res.stdout.read().decode() data_list = result.split(&apos;:&apos;) if len(data_list) &gt; 1: raw_data[key] = data_list[1].strip() else: raw_data[key] = -1 except Exception as e: print(e) raw_data[key] = -2 data = dict() data[&apos;asset_type&apos;] = &apos;server&apos; data[&apos;manufacturer&apos;] = raw_data[&apos;Manufacturer&apos;] data[&apos;sn&apos;] = raw_data[&apos;Serial Number&apos;] data[&apos;model&apos;] = raw_data[&apos;Product Name&apos;] data[&apos;uuid&apos;] = raw_data[&apos;UUID&apos;] data[&apos;wake_up_type&apos;] = raw_data[&apos;Wake-up Type&apos;] data.update(get_os_info()) data.update(get_cpu_info()) data.update(get_ram_info()) data.update(get_nic_info()) data.update(get_disk_info()) return datadef get_os_info(): &quot;&quot;&quot; 获取操作系统信息 :return: &quot;&quot;&quot; distributor = subprocess.Popen(&quot;lsb_release -a|grep &apos;Distributor ID&apos;&quot;, stdout=subprocess.PIPE, shell=True) distributor = distributor.stdout.read().decode().split(&quot;:&quot;) release = subprocess.Popen(&quot;lsb_release -a|grep &apos;Description&apos;&quot;, stdout=subprocess.PIPE, shell=True) release = release.stdout.read().decode().split(&quot;:&quot;) data_dic = &#123; &quot;os_distribution&quot;: distributor[1].strip() if len(distributor) &gt; 1 else &quot;&quot;, &quot;os_release&quot;: release[1].strip() if len(release) &gt; 1 else &quot;&quot;, &quot;os_type&quot;: &quot;Linux&quot;, &#125; return data_dicdef get_cpu_info(): &quot;&quot;&quot; 获取cpu信息 :return: &quot;&quot;&quot; base_cmd = &apos;cat /proc/cpuinfo&apos; raw_data = &#123; &apos;cpu_model&apos;: &quot;%s |grep &apos;model name&apos; |head -1 &quot; % base_cmd, &apos;cpu_count&apos;: &quot;%s |grep &apos;processor&apos;|wc -l &quot; % base_cmd, &apos;cpu_core_count&apos;: &quot;%s |grep &apos;cpu cores&apos; |awk -F: &apos;&#123;SUM +=$2&#125; END &#123;print SUM&#125;&apos;&quot; % base_cmd, &#125; for key, cmd in raw_data.items(): try: cmd_res = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True) raw_data[key] = cmd_res.stdout.read().decode().strip() except ValueError as e: print(e) raw_data[key] = &quot;&quot; data = &#123; &quot;cpu_count&quot;: raw_data[&quot;cpu_count&quot;], &quot;cpu_core_count&quot;: raw_data[&quot;cpu_core_count&quot;] &#125; cpu_model = raw_data[&quot;cpu_model&quot;].split(&quot;:&quot;) if len(cpu_model) &gt; 1: data[&quot;cpu_model&quot;] = cpu_model[1].strip() else: data[&quot;cpu_model&quot;] = -1 return datadef get_ram_info(): &quot;&quot;&quot; 获取内存信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;sudo dmidecode -t memory&quot;, stdout=subprocess.PIPE, shell=True) raw_list = raw_data.stdout.read().decode().split(&quot;\n&quot;) raw_ram_list = [] item_list = [] for line in raw_list: if line.startswith(&quot;Memory Device&quot;): raw_ram_list.append(item_list) item_list = [] else: item_list.append(line.strip()) ram_list = [] for item in raw_ram_list: item_ram_size = 0 ram_item_to_dic = &#123;&#125; for i in item: data = i.split(&quot;:&quot;) if len(data) == 2: key, v = data if key == &apos;Size&apos;: if v.strip() != &quot;No Module Installed&quot;: ram_item_to_dic[&apos;capacity&apos;] = v.split()[0].strip() item_ram_size = round(float(v.split()[0])) else: ram_item_to_dic[&apos;capacity&apos;] = 0 if key == &apos;Type&apos;: ram_item_to_dic[&apos;model&apos;] = v.strip() if key == &apos;Manufacturer&apos;: ram_item_to_dic[&apos;manufacturer&apos;] = v.strip() if key == &apos;Serial Number&apos;: ram_item_to_dic[&apos;sn&apos;] = v.strip() if key == &apos;Asset Tag&apos;: ram_item_to_dic[&apos;asset_tag&apos;] = v.strip() if key == &apos;Locator&apos;: ram_item_to_dic[&apos;slot&apos;] = v.strip() if item_ram_size == 0: pass else: ram_list.append(ram_item_to_dic) raw_total_size = subprocess.Popen(&quot;cat /proc/meminfo|grep MemTotal &quot;, stdout=subprocess.PIPE, shell=True) raw_total_size = raw_total_size.stdout.read().decode().split(&quot;:&quot;) ram_data = &#123;&apos;ram&apos;: ram_list&#125; if len(raw_total_size) == 2: total_gb_size = int(raw_total_size[1].split()[0]) / 1024**2 ram_data[&apos;ram_size&apos;] = total_gb_size return ram_datadef get_nic_info(): &quot;&quot;&quot; 获取网卡信息 :return: &quot;&quot;&quot; raw_data = subprocess.Popen(&quot;ifconfig -a&quot;, stdout=subprocess.PIPE, shell=True) raw_data = raw_data.stdout.read().decode().split(&quot;\n&quot;) nic_dic = dict() next_ip_line = False last_mac_addr = None for line in raw_data: if next_ip_line: next_ip_line = False nic_name = last_mac_addr.split()[0] mac_addr = last_mac_addr.split(&quot;HWaddr&quot;)[1].strip() raw_ip_addr = line.split(&quot;inet addr:&quot;) raw_bcast = line.split(&quot;Bcast:&quot;) raw_netmask = line.split(&quot;Mask:&quot;) if len(raw_ip_addr) &gt; 1: ip_addr = raw_ip_addr[1].split()[0] network = raw_bcast[1].split()[0] netmask = raw_netmask[1].split()[0] else: ip_addr = None network = None netmask = None if mac_addr not in nic_dic: nic_dic[mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 0, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; else: if &apos;%s_bonding_addr&apos; % (mac_addr,) not in nic_dic: random_mac_addr = &apos;%s_bonding_addr&apos; % (mac_addr,) else: random_mac_addr = &apos;%s_bonding_addr2&apos; % (mac_addr,) nic_dic[random_mac_addr] = &#123;&apos;name&apos;: nic_name, &apos;mac&apos;: random_mac_addr, &apos;net_mask&apos;: netmask, &apos;network&apos;: network, &apos;bonding&apos;: 1, &apos;model&apos;: &apos;unknown&apos;, &apos;ip_address&apos;: ip_addr, &#125; if &quot;HWaddr&quot; in line: next_ip_line = True last_mac_addr = line nic_list = [] for k, v in nic_dic.items(): nic_list.append(v) return &#123;&apos;nic&apos;: nic_list&#125;def get_disk_info(): &quot;&quot;&quot; 获取存储信息。 本脚本只针对centos7.6中使用sda2，且只有一块虚拟硬盘的情况。 具体查看硬盘信息的命令，请根据实际情况，实际调整。 如果需要查看Raid信息，可以尝试MegaCli工具。 :return: &quot;&quot;&quot; sn_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-serial-number&quot;, stdout=subprocess.PIPE, shell=True) sn = sn_raw_data.stdout.read().decode() model_raw_data = subprocess.Popen(&quot;sudo dmidecode -s system-product-name&quot;, stdout=subprocess.PIPE, shell=True) model = model_raw_data.stdout.read().decode() #size_data = subprocess.Popen(&quot;sudo fdisk -l /dev/sda2 | grep Disk|head -1&quot;, stdout=subprocess.PIPE, shell=True) #size_data = size_data.stdout.read().decode() #size = size_data.split(&quot;:&quot;)[1].strip().split(&quot; &quot;)[0] size_raw_data = subprocess.Popen(&quot;sudo smartctl -a /dev/sda2 |grep Capacity&quot;, stdout=subprocess.PIPE, shell=True) raw_data = size_raw_data.stdout.read().decode() data_list = raw_data.split()[4] size = data_list.split(&apos;[&apos;)[1] result = &#123;&apos;physical_disk_driver&apos;: []&#125; disk_dict = dict() disk_dict[&quot;model&quot;] = model disk_dict[&quot;size&quot;] = size disk_dict[&quot;sn&quot;] = sn result[&apos;physical_disk_driver&apos;].append(disk_dict) return resultif __name__ == &quot;__main__&quot;: # 收集信息功能测试 d = collect() print(d) 先来个输出在 centos7.6虚机上面的测试（可以读出所有数据）：&#123;&apos;asset_type&apos;: &apos;server&apos;, &apos;manufacturer&apos;: &apos;VMware, Inc.&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60&apos;, &apos;model&apos;: &apos;VMware Virtual Platform&apos;, &apos;uuid&apos;: &apos;68254d56-ee5c-fbdc-a15e-776a5fe76660&apos;, &apos;wake_up_type&apos;: &apos;Power Switch&apos;, &apos;os_distribution&apos;: &apos;CentOS&apos;, &apos;os_release&apos;: &apos;CentOS Linux release 7.6.1810 (Core)&apos;, &apos;os_type&apos;: &apos;Linux&apos;, &apos;cpu_count&apos;: &apos;1&apos;, &apos;cpu_core_count&apos;: &apos;1&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;ram&apos;: [&#123;&apos;capacity&apos;: &apos;1024&apos;, &apos;slot&apos;: &apos;RAM slot #0&apos;, &apos;model&apos;: &apos;DRAM&apos;, &apos;manufacturer&apos;: &apos;Not Specified&apos;, &apos;sn&apos;: &apos;Not Specified&apos;, &apos;asset_tag&apos;: &apos;Not Specified&apos;&#125;], &apos;ram_size&apos;: 0.9497604370117188, &apos;nic&apos;: [], &apos;physical_disk_driver&apos;: [&#123;&apos;model&apos;: &apos;VMware Virtual Platform\n&apos;, &apos;size&apos;: &apos;32.2&apos;, &apos;sn&apos;: &apos;VMware-56 4d 25 68 5c ee dc fb-a1 5e 77 6a 5f e7 66 60\n&apos;&#125;]&#125; 代码整体没有什么难点，无非就是使用subprocess.Popen()方法执行Linux的命令，然后获取返回值，并以规定的格式打包到data字典里。 需要说明的问题有： 当Linux中存在好几个Python解释器版本时，要注意调用方式，前面已经强调过了； 不同的Linux发行版，有些命令可能没有，需要额外安装； 所使用的查看硬件信息的命令并不一定必须和这里的一样，只要能获得数据就行； 有一些命令在ubuntu中涉及sudo的问题，需要特别对待； 最终数据字典的格式一定要正确。 可以在Linux下配置cronb或其它定时服务，设置定期的数据收集、报告任务。下面，我们在Linux虚拟机上，测试一下客户端。 将Pycharm中的Client客户端文件夹，拷贝到Linux虚拟机中，我这里是centos7.6 进入bin目录，运行：python3 main.py report_data 一切顺利的话应该能得到如下的反馈：正在将数据发送至： [http://10.101.120.34:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22asset_type%22%3A+%22server%22%2C+%22manufacturer%22%3A+%22VMware%2C+Inc.%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%22%2C+%22model%22%3A+%22VMware+Virtual+Platform%22%2C+%22uuid%22%3A+%2268254d56-ee5c-fbdc-a15e-776a5fe76660%22%2C+%22wake_up_type%22%3A+%22Power+Switch%22%2C+%22os_distribution%22%3A+%22CentOS%22%2C+%22os_release%22%3A+%22CentOS+Linux+release+7.6.1810+%28Core%29%22%2C+%22os_type%22%3A+%22Linux%22%2C+%22cpu_count%22%3A+%221%22%2C+%22cpu_core_count%22%3A+%221%22%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22ram%22%3A+%5B%7B%22capacity%22%3A+%221024%22%2C+%22slot%22%3A+%22RAM+slot+%230%22%2C+%22model%22%3A+%22DRAM%22%2C+%22manufacturer%22%3A+%22Not+Specified%22%2C+%22sn%22%3A+%22Not+Specified%22%2C+%22asset_tag%22%3A+%22Not+Specified%22%7D%5D%2C+%22ram_size%22%3A+0.9497604370117188%2C+%22nic%22%3A+%5B%5D%2C+%22physical_disk_driver%22%3A+%5B%7B%22model%22%3A+%22VMware+Virtual+Platform%5Cn%22%2C+%22size%22%3A+%2232.2%22%2C+%22sn%22%3A+%22VMware-56+4d+25+68+5c+ee+dc+fb-a1+5e+77+6a+5f+e7+66+60%5Cn%22%7D%5D%7D&apos;发送完毕！返回结果：成功收到数据！日志记录成功！ 然后我们可以在pycharm 页面收到： 规整如下：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB-Windows下收集数据（四）]]></title>
    <url>%2F2019%2F03%2F03%2F4%E3%80%81Windows%E4%B8%8B%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[一、windows中收集硬件信息为了收集运行Windows操作系统的服务器的硬件信息，我们需要编写一个专门的脚本。 在Pycharm的Client目录下的plugins包中，新建一个windows包，然后创建一个sys_info.py文件，写入下面的代码： ==&lt;如下打印data的语句，是为了调试查看输出，可以去掉&gt;==#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 16:41# @Author : zhdya@zhdya.cn# @File : sys_info.pyimport platformimport win32comimport wmi&quot;&quot;&quot;本模块基于windows操作系统，依赖wmi和win32com库，需要提前使用pip进行安装，或者下载安装包手动安装。&quot;&quot;&quot;def collect(): data = &#123; &apos;os_type&apos;: platform.system(), &apos;os_release&apos;: &quot;%s %s %s &quot; % (platform.release(), platform.architecture()[0], platform.version()), &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos; &#125; # 分别获取各种硬件信息 win32obj = Win32Info() data.update(win32obj.get_cpu_info()) data.update(win32obj.get_ram_info()) data.update(win32obj.get_motherboard_info()) data.update(win32obj.get_disk_info()) data.update(win32obj.get_nic_info()) # 最后返回一个数据字典 print(&quot;data11&quot;, data) return dataclass Win32Info(object): def __init__(self): # 固定用法，更多内容请参考模块说明 self.wmi_obj = wmi.WMI() self.wmi_service_obj = win32com.client.Dispatch(&quot;WbemScripting.SWbemLocator&quot;) self.wmi_service_connector = self.wmi_service_obj.ConnectServer(&quot;.&quot;, &quot;root\cimv2&quot;) def get_cpu_info(self): &quot;&quot;&quot; 获取CPU的相关数据，这里只采集了三个数据，实际有更多，请自行选择需要的数据 :return: &quot;&quot;&quot; data = &#123;&#125; cpu_lists = self.wmi_obj.Win32_Processor() cpu_core_count = 0 for cpu in cpu_lists: cpu_core_count += cpu.NumberOfCores cpu_model = cpu_lists[0].Name # CPU型号（所有的CPU型号都是一样的） data[&quot;cpu_count&quot;] = len(cpu_lists) # CPU个数 data[&quot;cpu_model&quot;] = cpu_model data[&quot;cpu_core_count&quot;] = cpu_core_count # CPU总的核数 print(&quot;data22&quot;, data) return data def get_ram_info(self): &quot;&quot;&quot; 收集内存信息 :return: &quot;&quot;&quot; data = [] # 这个模块用SQL语言获取数据 ram_collections = self.wmi_service_connector.ExecQuery(&quot;Select * from Win32_PhysicalMemory&quot;) for item in ram_collections: # 主机中存在很多根内存，要循环所有的内存数据 ram_size = int(int(item.Capacity) / (1024**3)) # 转换内存单位为GB item_data = &#123; &quot;slot&quot;: item.DeviceLocator.strip(), &quot;capacity&quot;: ram_size, &quot;model&quot;: item.Caption, &quot;manufacturer&quot;: item.Manufacturer, &quot;sn&quot;: item. SerialNumber, &#125; data.append(item_data) # 将每条内存的信息，添加到一个列表里 print(&quot;data33&quot;, data) return &#123;&quot;ram&quot;: data&#125; # 再对data列表封装一层，返回一个字典，方便上级方法的调用 def get_motherboard_info(self): &quot;&quot;&quot; 获取主板信息 :return: &quot;&quot;&quot; computer_info = self.wmi_obj.Win32_ComputerSystem()[0] system_info = self.wmi_obj.Win32_OperatingSystem()[0] data = dict() data[&apos;manufacturer&apos;] = computer_info.Manufacturer data[&apos;model&apos;] = computer_info.Model data[&apos;wake_up_type&apos;] = computer_info.WakeUpType data[&apos;sn&apos;] = system_info.SerialNumber print(&quot;data44&quot;, data) return data def get_disk_info(self): &quot;&quot;&quot; 硬盘信息 :return: &quot;&quot;&quot; data = [] for disk in self.wmi_obj.Win32_DiskDrive(): # 每块硬盘都要获取相应信息 item_data = dict() iface_choices = [&quot;SAS&quot;, &quot;SCSI&quot;, &quot;SATA&quot;, &quot;SSD&quot;] for iface in iface_choices: if iface in disk.Model: item_data[&apos;iface_type&apos;] = iface break else: item_data[&apos;iface_type&apos;] = &apos;unknown&apos; item_data[&apos;slot&apos;] = disk.Index item_data[&apos;sn&apos;] = disk.SerialNumber item_data[&apos;model&apos;] = disk.Model item_data[&apos;manufacturer&apos;] = disk.Manufacturer item_data[&apos;capacity&apos;] = int(int(disk.Size) / (1024**3)) data.append(item_data) print(&quot;data55&quot;, data) return &#123;&apos;physical_disk_driver&apos;: data&#125; def get_nic_info(self): &quot;&quot;&quot; 网卡信息 :return: &quot;&quot;&quot; data = [] for nic in self.wmi_obj.Win32_NetworkAdapterConfiguration(): if nic.MACAddress is not None: item_data = dict() item_data[&apos;mac&apos;] = nic.MACAddress item_data[&apos;model&apos;] = nic.Caption item_data[&apos;name&apos;] = nic.Index if nic.IPAddress is not None: item_data[&apos;ip_address&apos;] = nic.IPAddress[0] item_data[&apos;net_mask&apos;] = nic.IPSubnet else: item_data[&apos;ip_address&apos;] = &apos;&apos; item_data[&apos;net_mask&apos;] = &apos;&apos; data.append(item_data) print(&quot;data66&quot;, data) return &#123;&apos;nic&apos;: data&#125;if __name__ == &quot;__main__&quot;: # 测试代码(仅限于在当前页面运行获取本机的信息） dic = collect() print(dic) windows中没有方便的命令可以获取硬件信息，但是有额外的模块可以帮助我们实现目的，这个模块叫做wmi。可以使用pip install wmi 的方式安装，当前版本是1.4.9。但是wmi安装后，import wmi依然会出错，因为它依赖一个叫做win32com的模块。 我们依然可以通过pip install pypiwin32 来安装win32com模块，但是不幸的是，据反映，有些机器无法通过pip成功安装。所以，这里我在github中提供了一个手动安装包==pywin32-220.win-amd64-py3.5(配合wmi模块，获取主机信息的模块).exe==，方便大家。链接：https://pan.baidu.com/s/14ZUKPJnmlwuUHsYLUUApKg 提取码：nq5i 依赖包的问题解决后，我们来看一下==sys_info.py==脚本的代码。 核心在于collect()方法！ 该方法首先通过platform模块获取平台的信息，然后保存到一个data字典中。 然后创建一个Win32Info对象，并调用win32的各种功能方法，分别获取CPU、RAM、主板、硬盘和网卡的信息。 类Win32Info是我们编写的封装了具体数据收集逻辑的类； 该类中有很多方法，每个方法针对一项数据； 其中对Win32模块的调用方式是固定的，有兴趣的可以自行学习这个模块的官方文档 每一类的数据收集完成后都会作为一个新的子字典，update到开始的data字典中，最终形成完整的信息字典。 最后在脚本末尾有一个测试入口。 整个脚本的代码其实很简单，我们只要将Win32的方法调用当作透明的空气，剩下的不过就是将获得的数据，按照我们指定的格式打包成一个数据字典。 ==强调：数据字典的格式和键值是非常重要的，是预设的，不可以随意改变！== 二、信息收集测试单独运行一下该脚本（注意不是运行CMDB项目），查看一下生成的数据：data22 &#123;&apos;cpu_core_count&apos;: 4, &apos;cpu_count&apos;: 1, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;&#125;data33 [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;]data44 &#123;&apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;wake_up_type&apos;: 6&#125;data55 [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;]data66 [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;]data11 &#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125;&#123;&apos;manufacturer&apos;: &apos;Dell Inc.&apos;, &apos;model&apos;: &apos;Latitude 5480&apos;, &apos;cpu_core_count&apos;: 4, &apos;os_distribution&apos;: &apos;Microsoft&apos;, &apos;asset_type&apos;: &apos;server&apos;, &apos;cpu_model&apos;: &apos;Intel(R) Core(TM) i5-7300HQ CPU @ 2.50GHz&apos;, &apos;nic&apos;: [&#123;&apos;name&apos;: 1, &apos;mac&apos;: &apos;00:50:56:C0:00:08&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000001] VMware Virtual Ethernet Adapter for VMnet8&apos;, &apos;ip_address&apos;: &apos;192.168.171.1&apos;&#125;, &#123;&apos;name&apos;: 2, &apos;mac&apos;: &apos;00:50:56:C0:00:01&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000002] VMware Virtual Ethernet Adapter for VMnet1&apos;, &apos;ip_address&apos;: &apos;192.168.209.1&apos;&#125;, &#123;&apos;name&apos;: 3, &apos;mac&apos;: &apos;00:FF:E2:52:B5:09&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000003] TAP-Windows Adapter V9&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 4, &apos;mac&apos;: &apos;10:65:30:11:8A:8A&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000004] Intel(R) Ethernet Connection (5) I219-LM&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 5, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.240.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000005] Intel(R) Dual Band Wireless-AC 8265&apos;, &apos;ip_address&apos;: &apos;10.10.7.26&apos;&#125;, &#123;&apos;name&apos;: 6, &apos;mac&apos;: &apos;68:EC:C5:86:F1:DD&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000006] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 7, &apos;mac&apos;: &apos;68:EC:C5:86:F1:E0&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000007] Bluetooth Device (Personal Area Network)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 13, &apos;mac&apos;: &apos;14:74:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000013] WAN Miniport (IP)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 14, &apos;mac&apos;: &apos;14:FC:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000014] WAN Miniport (IPv6)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 15, &apos;mac&apos;: &apos;16:0F:20:52:41:53&apos;, &apos;net_mask&apos;: &apos;&apos;, &apos;model&apos;: &apos;[00000015] WAN Miniport (Network Monitor)&apos;, &apos;ip_address&apos;: &apos;&apos;&#125;, &#123;&apos;name&apos;: 16, &apos;mac&apos;: &apos;6A:EC:C5:86:F1:DC&apos;, &apos;net_mask&apos;: (&apos;255.255.255.0&apos;, &apos;64&apos;), &apos;model&apos;: &apos;[00000016] Microsoft Wi-Fi Direct Virtual Adapter&apos;, &apos;ip_address&apos;: &apos;192.168.137.1&apos;&#125;], &apos;physical_disk_driver&apos;: [&#123;&apos;slot&apos;: 0, &apos;capacity&apos;: 238, &apos;manufacturer&apos;: &apos;(标准磁盘驱动器)&apos;, &apos;model&apos;: &apos;SAMSUNG SSD PM871b M.2 2280 256GB&apos;, &apos;iface_type&apos;: &apos;SSD&apos;, &apos;sn&apos;: &apos; S3U0NY0K244546&apos;&#125;], &apos;wake_up_type&apos;: 6, &apos;sn&apos;: &apos;00330-80000-00000-AA069&apos;, &apos;cpu_count&apos;: 1, &apos;os_release&apos;: &apos; 64bit 10.0.17763 &apos;, &apos;ram&apos;: [&#123;&apos;slot&apos;: &apos;DIMM A&apos;, &apos;capacity&apos;: 8, &apos;manufacturer&apos;: &apos;802C0000802C&apos;, &apos;model&apos;: &apos;物理内存&apos;, &apos;sn&apos;: &apos;1B458788&apos;&#125;], &apos;os_type&apos;: &apos;Windows&apos;&#125; 上面的信息包含操作系统、主板、CPU、内存、硬盘、网卡等各种信息。可以看到我有1条内存，1块SSD硬盘，以及4块网卡。四块网卡有出现mac地址相同的情况，因为那是虚拟机的。 你自己的数据和我的肯定不一样，但是数据格式和键值必须一样，我们后面自动分析数据、填充数据，都依靠这个固定格式的数据字典。 通过测试我们发现数据可以收集到了，那么再测试一下数据能否正常发送到服务器。 三、数据发送测试由于我这里采用了Linux虚拟机作为测试用例，我们的Django服务器就不能再运行在127.0.0.1:8000上面了。 查看一下当前机器的IP，发现是192.168.1.100，修改项目的settings.py文件，将ALLOWED_HOSTS修改如下： ALLOWED_HOSTS = [&quot;*&quot;] 这表示接收所有同一局域网内的网络访问。 然后以0.0.0.0:8000的参数启动CMDB，表示对局域网内所有ip开放服务。 回到客户端，进入Client/bin目录，运行python main.py report_data 可以看到如下结果：正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22cpu_core_count%22%3A+4%2C+%22cpu_model%22%3A+%22Intel%28R%29+Core%28TM%29+i5-7300HQ+CPU+%40+2.50GHz%22%2C+%22wake_up...&lt;中间信息省略&gt;...71b+M.2+2280+256GB%22%2C+%22capacity%22%3A+238%7D%5D%2C+%22cpu_count%22%3A+1%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22asset_type%22%3A+%22server%22%7D&apos;?[31;1m发送失败，HTTP Error 404: Not Found?[0m日志记录成功！ 这是一个404错误，表示服务器地址没找到，这是因为我们还没有为Django编写接收数据的视图和路由。 这时，打开log目录下的日志文件，内容如下：发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 四、接收数据进入==cmdb/urls.py==文件中，编写一个二级路由，将所有++assets相关的数据都转发到assets.urls++中，如下所示：from django.contrib import adminfrom django.conf.urls import url, includeurlpatterns = [ url(&apos;admin/&apos;, admin.site.urls), url(r&apos;^assets/&apos;, include(&apos;assets.urls&apos;)),] 然后，我们在assets中新建一个urls.py文件，写入下面的代码：from django.conf.urls import urlfrom assets import viewsapp_name = &apos;assets&apos;urlpatterns = [ url(r&apos;^report/&apos;, views.report, name=&apos;report&apos;),] 这样，我们的路由就写好了。 转过头，我们进入++assets/views.py++文件，写一个简单的视图。from django.shortcuts import render# Create your views here.from django.shortcuts import render, HttpResponsedef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 代码很简单，接收POST过来的数据，打印出来，然后返回成功的消息。 重新运行python main.py report_data 可以看到： 正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22ram%22%3A+%5B%7B%22manufacturer%22%3A+%22802C0...&lt;中间部分省略&gt;...%22%3A+%22Latitude+5480%22%2C+%22sn%22%3A+%2200330-80000-00000-AA069%22%2C+%22os_distribution%22%3A+%22Microsoft%22%2C+%22manufacturer%22%3A+%22Dell+Inc.%22%7D&apos;?[31;1m发送失败，&lt;urlopen error [WinError 10061] 由于目标计算机积极拒绝，无法连接。&gt;?[0m日志记录成功！ 遇到拒绝服务的错误了。 原因在于我们模拟浏览器发送了一个POST请求给Django，但是请求中没有携带Django需要的csrf安全令牌，所以拒绝了请求。 为了解决这个问题，我们需要在这个report视图上忽略csrf验证，可以通过Django的@csrf_exempt装饰器。修改代码如下：from django.shortcuts import render, HttpResponsefrom django.views.decorators.csrf import csrf_exempt# Create your views here.@csrf_exemptdef report(request): if request.method == &quot;POST&quot;: asset_data = request.POST.get(&apos;asset_data&apos;) print(asset_data) return HttpResponse(&quot;成功收到数据！&quot;) 重启CMDB服务器，再次从客户端报告数据，可以看到返回结果如下：正在将数据发送至： [http://10.10.7.26:8000/assets/report/] ......handler_data_encode--&gt;&gt; b&apos;asset_data=%7B%22nic%22%3A+%5B%7B%22name%22%3A+1%2C+%22model%22%3A+%22%5B00000001%5D+VMware+Virt...&lt;中间部分省略&gt;...facturer%22%3A+%22802C0000802C%22%2C+%22slot%22%3A+%22DIMM+A%22%2C+%22sn%22%3A+%221B458788%22%2C+%22model%22%3A+%22%5Cu7269%5Cu7406%5Cu5185%5Cu5b58%22%7D%5D%7D&apos;?[31;1m发送完毕！?[0m返回结果：成功收到数据！日志记录成功！ 看下日志记录：发送时间：2019-03-03 10:49:26 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:00:53 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:05:36 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：发送失败 发送时间：2019-03-03 11:06:51 服务器地址：http://10.10.7.26:8000/assets/report/ 返回结果：成功收到数据！ 这表明数据发送成功了。 再看Pycharm中，也打印出了接收到的数据，一切OK！ CSRF验证的问题解决了，但是又带来新的安全问题。我们可以通过增加用户名、密码，或者md5验证或者自定义安全令牌的方式解决，这部分内容就不展开了。 Windows下的客户端已经验证完毕了，然后我们就可以通过各种方式让脚本定时运行、收集和报告数据，一切都自动化。 最后补充：++CMDB系统是部署在A服务器上，那么客户端CLIENT是需要部署在B服务器上的，那就意味着每台需要被采集数据的服务器都要安装PYTHON及所用到的包。++]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB数据收集客户端（三）]]></title>
    <url>%2F2019%2F03%2F02%2F3%E3%80%81%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[CMDB最主要的管理对象：服务器，其数据信息自然不可能通过手工收集，必须以客户端的方式，定时自动收集并报告给远程的服务器。 下面，让我们暂时忘掉Django，进入Python运维的世界…… 一、客户端程序组织编写客户端，不能一个py脚本包打天下，要有组织有目的，通常我们会采取下面的结构： 在Pycharm下，创建一个Client文件夹，作为客户端的根目录。 在Client下，创建上面的包。注意是包，不是文件夹： bin是客户端启动脚本的所在目录 conf是配置文件目录 core是核心代码目录 log是日志文件保存目录 plugins是插件或工具目录 二、开发数据收集客户端1.程序入口脚本++在bin目录中新建main.py文件++，写入下面的代码： #!/usr/bin/env python# -*- coding:utf-8 -*-import osimport sysBASE_DIR = os.path.dirname(os.getcwd())# 设置工作目录，使得包和模块能够正常导入sys.path.append(BASE_DIR)from core import handlerif __name__ == &apos;__main__&apos;: handler.ArgvHandler(sys.argv) ##获取参数，传入到ArgvHandler() 通过os和sys模块的配合，将当前客户端所在目录设置为工作目录，如果不这么做，会无法导入其它模块； handler模块是核心代码模块，在core目录中，我们一会来实现它。 以后调用客户端就只需要执行python main.py 参数就可以了 ++这里有个问题一定要强调一下，那就是Python解释器的调用，执行命令的方式和代码第一行#!/usr/bin/env python的指定方式一定不能冲突，要根据你的实际情况实际操作和修改代码，很多新手连Python本身都没搞明白就上来执行脚本，碰到各种解释器不合法的错误，请回去补足基础！++ 2.主功能模块++在core下，创建handler.py文件++，写入下面的代码： #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 13:51# @Author : zhdya@zhdya.cn# @File : handler.pyimport jsonimport timeimport urllib.parseimport urllib.requestfrom core import info_collectionfrom conf import settingsclass ArgvHandler(object): def __init__(self, args): self.args = args self.parse_args() def parse_args(self): &quot;&quot;&quot; 分析参数，如果有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 :return: &quot;&quot;&quot; if len(self.args) &gt; 1 and hasattr(self, self.args[1]): func = getattr(self, self.args[1]) func() else: self.help_msg() ##如果执行程序没有带参数就会提示如下信息：help_msg() @staticmethod #静态方法 类或实例均可调用,静态方法函数里不传入self，这样如上self.help_msg()就可以调用了 def help_msg(): &quot;&quot;&quot; 帮助说明 :return: &quot;&quot;&quot; msg = &apos;&apos;&apos; collect_data 收集硬件信息 report_data 收集硬件信息并汇报 &apos;&apos;&apos; print(msg) @staticmethod def collect_data(): &quot;&quot;&quot;收集硬件信息,用于测试！&quot;&quot;&quot; info = info_collection.InfoCollection() asset_data = info.collect() print(asset_data) @staticmethod def report_data(): &quot;&quot;&quot; 收集硬件信息，然后发送到服务器。 :return: &quot;&quot;&quot; # 收集信息 info = info_collection.InfoCollection() asset_data = info.collect() # 将数据打包到一个字典内，并转换为json格式 data = &#123;&quot;asset_data&quot;: json.dumps(asset_data)&#125; print(&quot;handler_data--&gt;&gt;&quot;, data) # 根据settings中的配置，构造url url = &quot;http://%s:%s%s&quot; % (settings.Params[&apos;server&apos;], settings.Params[&apos;port&apos;], settings.Params[&apos;url&apos;]) print(&quot;handler_url--&gt;&gt;&quot;, url) print(&apos;正在将数据发送至： [%s] ......&apos; % url) try: # 使用Python内置的urllib.request库，发送post请求。 # 需要先将数据进行封装，并转换成bytes类型 data_encode = urllib.parse.urlencode(data).encode() print(&quot;handler_data_encode--&gt;&gt;&quot;, data_encode) response = urllib.request.urlopen(url=url, data=data_encode, timeout=settings.Params[&apos;request_timeout&apos;]) print(&quot;\033[31;1m发送完毕！\033[0m &quot;) message = response.read().decode() print(&quot;返回结果：%s&quot; % message) except Exception as e: message = &quot;发送失败&quot; print(&quot;\033[31;1m发送失败，%s\033[0m&quot; % e) # 记录发送日志 with open(settings.PATH, &apos;ab&apos;) as f: ##a追加,b二进制文件 string = &apos;发送时间：%s \t 服务器地址：%s \t 返回结果：%s \n&apos; % (time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;), url, message) f.write(string.encode()) print(&quot;日志记录成功！&quot;) 说明： handler模块中只有一个ArgvHandler类； 在main模块中也是实例化了一个ArgvHandler类的对象，并将调用参数传递进去； 首先，初始化方法会保存调用参数，然后执行parse_args()方法分析参数； 如果ArgvHandler类有参数指定的功能，则执行该功能，如果没有，打印帮助说明。 目前ArgvHandler类只有两个核心方法：collect_data和report_dataa； 这两个方法一个是收集数据并打印到屏幕，用于测试；report_data方法才会将实际的数据发往服务器。 数据的收集由info_collection.InfoCollection类负责，一会再看； report_data方法会将收集到的数据打包到一个字典内，并转换为json格式； 然后通过settings中的配置，构造发送目的地url； 通过Python内置的urllib.parse对数据进行封装； 通过urllib.request将数据发送到目的url； 接收服务器返回的信息； 将成功或者失败的信息写入日志文件中。 以后，我们要测试数据收集，执行：python main.py collect_data 要实际往服务器发送收集到的数据，则执行：python main.py report_data 3.配置文件要将所有可能修改的数据、常量、配置等都尽量以配置文件的形式组织起来，尽量不要在代码中写死任何数据。 ++在conf中，新建settings.py文件++，写入下面的代码：#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:21# @Author : zhdya@zhdya.cn# @File : settings.pyimport os# 远端服务器配置Params = &#123; &quot;server&quot;: &quot;10.10.7.26&quot;, &quot;port&quot;: 8000, &apos;url&apos;: &apos;/assets/report/&apos;, &apos;request_timeout&apos;: 30,&#125;# 日志文件配置PATH = os.path.join(os.path.dirname(os.getcwd()), &apos;log&apos;, &apos;cmdb.log&apos;)print(&quot;conf_settings--&gt;&gt;&quot;, PATH)# 更多配置，请都集中在此文件中 这里，配置了服务器地址、端口、发送的url、请求的超时时间，以及日志文件路径。请根据你的实际情况进行修改。 ==如上server端我是直接启动的django服务 也就是如上10.10.7.26是我笔记本的IP地址，这样默认我笔记本的10.10.7.26:8000就对外开放了！== 4.信息收集模块++在core中新建info_collection.py文件++，写入下面的代码： ==&lt;关于如下：from plugins.linux import sys_info 以及 from plugins.windows import sys_info as win_sys_info 稍后章节我们会建立一系列的目录！！&gt;==#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019-3-2 14:48# @Author : zhdya@zhdya.cn# @File : info_collection.pyimport sysimport platformdef linux_sys_info(): from plugins.linux import sys_info return sys_info.collect()def windows_sys_info(): from plugins.windows import sys_info as win_sys_info return win_sys_info.collect()class InfoCollection(object): def collect(self): # 收集平台信息 # 首先判断当前平台，根据平台的不同，执行不同的方法 try: func = getattr(self, platform.system()) info_data = func() formatted_data = self.build_report_data(info_data) return formatted_data except AttributeError: sys.exit(&quot;不支持当前操作系统： [%s]! &quot; % platform.system()) def Linux(self): return linux_sys_info() def Windows(self): return windows_sys_info() def build_report_data(self, data): # 留下一个接口，方便以后增加功能或者过滤数据 return data 该模块的作用很简单： 首先通过Python内置的platform模块获取执行main脚本的操作系统类别，通常是windows和Linux； 根据操作系统的不同，反射获取相应的信息收集方法，并执行 如果是客户端不支持的操作系统，比如苹果系统，则提示并退出客户端。 因为windows和Linux两大操作系统的巨大平台差异，我们必须写两个收集信息的脚本。 到目前未知，我们的客户端结构如下图所示：]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB模型设计（二）]]></title>
    <url>%2F2019%2F03%2F01%2F2%E3%80%81%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[一、创建项目创建Django项目cmdb，配置好settings中的语言和时区，最后新建一个app，名字就叫做assets。这些基本过程以后就不再赘述了，不熟悉的请参考教程的前面部分。 django版本：1.11.11 创建成功后，初始状态如下图所示： 二、模型设计说明：本项目依然采用SQLite数据库，等下一个项目再使用Mysql。 模型设计是整个项目的重中之重，其它所有的内容其实都是围绕它展开的。 而我们设计数据模型的原则和参考依据是前一节分析的项目需求和数据分类表。 1.资产共有数据模型打开assets/models.py文件，首先我们要设计一张资产表： from django.db import modelsfrom django.contrib.auth.models import User# Create your models here.class Asset(models.Model): &quot;&quot;&quot; 所有资产的共有数据表 &quot;&quot;&quot; asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;software&apos;, &apos;软件资产&apos;) ) asset_status = ( (0, &apos;在线&apos;), (1, &apos;下线&apos;), (2, &apos;未知&apos;), (3, &apos;故障&apos;), (4, &apos;备用&apos;), ) asset_type = models.CharField(choices=asset_type_choice, max_length=64, default=&apos;server&apos;, verbose_name=&quot;资产类型&quot;) name = models.CharField(max_length=64, unique=True, verbose_name=&quot;资产名称&quot;) ##唯一，不可重复 sn = models.CharField(max_length=128, unique=True, verbose_name=&quot;资产序列号&quot;) ##唯一，不可重复 business_unit = models.ForeignKey(&apos;BusinessUnit&apos;, null=True, blank=True, verbose_name=&quot;所属业务线&quot;) status = models.SmallIntegerField(choices=asset_status, default=0, verbose_name=&quot;设备状态&quot;) manufacturer = models.ForeignKey(&apos;Manufacturer&apos;, null=True, blank=True, verbose_name=&quot;制造商&quot;) manage_ip = models.GenericIPAddressField(null=True, blank=True, verbose_name=&quot;管理IP&quot;) tags = models.ManyToManyField(&apos;Tag&apos;, blank=True, verbose_name=&quot;标签&quot;) admin = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;资产管理员&quot;, related_name=&apos;admin&apos;) idc = models.ForeignKey(&apos;IDC&apos;, null=True, blank=True, verbose_name=&quot;所在机房&quot;) contract = models.ForeignKey(&apos;Contract&apos;, null=True, blank=True, verbose_name=&quot;合同&quot;) purchase_day = models.DateField(null=True, blank=True, verbose_name=&quot;购买日期&quot;) expire_day = models.DateField(null=True, blank=True, verbose_name=&quot;过保日期&quot;) price = models.FloatField(null=True, blank=True, verbose_name=&quot;价格&quot;) approved_by = models.ForeignKey(User, null=True, blank=True, verbose_name=&quot;批准人&quot;, related_name=&quot;approved_by&quot;) memo = models.TextField(null=True, blank=True, verbose_name=&quot;备注&quot;) c_time = models.DateTimeField(auto_now_add=True, verbose_name=&quot;批准日期&quot;) ##auto_add_now默认=False：储存当对象被创建时的时间，可以用来存储比如说博客什么时候创建的，后来你再更改博客，它的值也不会变。 m_time = models.DateTimeField(auto_now=True, verbose_name=&quot;更新日期&quot;) ##auto_now默认=False:当对象被存储时自动将对象的时间更新为当前时间 def __str__(self): return &apos;&lt;%s&gt; %s&apos; %(self.get_asset_type_display(), self.name) class Meta: verbose_name = &quot;资产总表&quot; verbose_name_plural = &quot;资产总表&quot; ordering = [&apos;-c_time&apos;] 说明： sn这个数据字段是所有资产都必须有，并且唯一不可重复的！通常来自自动收集的数据中； name和sn一样，也是唯一的； asset_type_choice和asset_status分别设计为两个选择类型 admin和approved_by是分别是当前资产的管理员和将该资产上线的审批员； 导入Django内置的User表，作为我们CMDB项目的用户表，用于保存管理员和审判员等人员信息； asset表中的很多字段内容都无法自动获取，需要我们手动输入，比如合同、备注。 2.服务器模型服务器作为资产的一种，而且是最主要的管理对象，包含了一些主要的信息，其模型结构如下：class Server(models.Model): &quot;&quot;&quot;服务器设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;PC服务器&apos;), (1, &apos;刀片型&apos;), (2, &apos;小型机&apos;), ) created_by_choice = ( (&apos;auto&apos;, &apos;自动添加&apos;), (&apos;manual&apos;, &apos;手动录入&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) # 非常关键的一对一关联！ sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=1, verbose_name=&quot;服务器类型&quot;) created_by = models.CharField(choices=created_by_choice, max_length=32, default=&apos;auto&apos;, verbose_name=&quot;添加方式&quot;) hosted_on = models.ForeignKey(&apos;self&apos;, related_name=&apos;hosted_on_server&apos;, blank=True, null=True, verbose_name=&quot;宿主机&quot;) ##虚拟机专用字段 model = models.CharField(max_length=512, blank=True, null=True, verbose_name=&quot;Raid类型&quot;) os_type = models.CharField(&apos;操作系统类型&apos;, max_length=64, blank=True, null=True) os_distribution = models.CharField(&apos;发行版本&apos;, max_length=64, blank=True, null=True) os_release = models.CharField(&apos;操作系统版本&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; %(self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;服务器&apos; verbose_name_plural = &quot;服务器&quot; 说明： 每台服务器都唯一关联着一个资产对象，因此使用OneToOneField构建了一个一对一字段，这非常重要! 服务器又可分为几种子类型，这里定义了三种； 服务器添加的方式可以分为手动和自动； 有些服务器是虚拟机或者docker生成的，没有物理实体，存在于宿主机中，因此需要增加一个hosted_on字段； 服务器有型号信息，如果硬件信息中不包含，那么指的就是主板型号； Raid类型在采用了Raid的时候才有，否则为空; 操作系统相关信息包含类型、发行版本和具体版本。 3.安全、网络、存储设备和软件资产的模型这部分内容不是项目的主要内容，而且数据大多数不能自动收集和报告，很多都需要手工录入。我这里给出了范例，更多的数据字段，可以自行添加。class SecurityDevice(models.Model): &quot;&quot;&quot;安全设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;防火墙&apos;), (1, &apos;入侵检测设备&apos;), (2, &apos;互联网网关&apos;), (4, &apos;运维审计系统&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;安全设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;安全设备&apos; verbose_name_plural = &quot;安全设备&quot;class StorageDevice(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;磁盘阵列&apos;), (1, &apos;网络存储器&apos;), (2, &apos;磁带库&apos;), (4, &apos;磁带机&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;存储设备类型&quot;) def __str__(self): return self.asset.name + &quot;--&quot; + self.get_sub_asset_type_display() + &quot; id:%s&quot; % self.id class Meta: verbose_name = &apos;存储设备&apos; verbose_name_plural = &quot;存储设备&quot;class NetworkDevice(models.Model): &quot;&quot;&quot;网络设备&quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;路由器&apos;), (1, &apos;交换机&apos;), (2, &apos;负载均衡&apos;), (4, &apos;VPN设备&apos;), ) asset = models.OneToOneField(&apos;Asset&apos;) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;网络设备类型&quot;) vlan_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;VLanIP&quot;) intranet_ip = models.GenericIPAddressField(blank=True, null=True, verbose_name=&quot;内网IP&quot;) model = models.CharField(max_length=128, null=True, blank=True, verbose_name=&quot;网络设备型号&quot;) firmware = models.CharField(max_length=128, blank=True, null=True, verbose_name=&quot;设备固件版本&quot;) port_num = models.SmallIntegerField(null=True, blank=True, verbose_name=&quot;端口个数&quot;) device_detail = models.TextField(null=True, blank=True, verbose_name=&quot;详细配置&quot;) def __str__(self): return &apos;%s--%s--%s &lt;sn:%s&gt;&apos; % (self.asset.name, self.get_sub_asset_type_display(), self.model, self.asset.sn) class Meta: verbose_name = &apos;网络设备&apos; verbose_name_plural = &quot;网络设备&quot;class Software(models.Model): &quot;&quot;&quot; 只保存付费购买的软件 &quot;&quot;&quot; sub_asset_type_choice = ( (0, &apos;操作系统&apos;), (1, &apos;办公\开发软件&apos;), (2, &apos;业务软件&apos;), ) sub_asset_type = models.SmallIntegerField(choices=sub_asset_type_choice, default=0, verbose_name=&quot;软件类型&quot;) license_num = models.IntegerField(default=1, verbose_name=&quot;授权数量&quot;) version = models.CharField(max_length=64, unique=True, help_text=&apos;例如: CentOS release 6.7 (Final)&apos;, verbose_name=&apos;软件/系统版本&apos;) def __str__(self): return &apos;%s--%s&apos; % (self.get_sub_asset_type_display(), self.version) class Meta: verbose_name = &apos;软件/系统&apos; verbose_name_plural = &quot;软件/系统&quot; 说明： 每台安全、网络、存储设备都通过一对一的方式唯一关联这一个资产对象。 通过sub_asset_type又细分设备的子类型 对于软件，它没有物理形体，因此无须关联一个资产对象； 软件只管理那些大型的收费软件，关注点是授权数量和软件版本。对于那些开源的或者免费的软件，显然不算公司的资产。 4.机房、制造商、业务线、合同、资产标签等数据模型这一部分是CMDB中相关的内容，数据表建立后，可以通过手动添加。class IDC(models.Model): &quot;&quot;&quot;机房&quot;&quot;&quot; name = models.CharField(max_length=64, unique=True, verbose_name=&quot;机房名称&quot;) memo = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;备注&apos;) def __str__(self): return self.name class Meta: verbose_name = &apos;机房&apos; verbose_name_plural = &quot;机房&quot;class Manufacturer(models.Model): &quot;&quot;&quot;厂商&quot;&quot;&quot; name = models.CharField(&apos;厂商名称&apos;, max_length=64, unique=True) telephone = models.CharField(&apos;支持电话&apos;, max_length=30, blank=True, null=True) memo = models.CharField(&apos;备注&apos;, max_length=128, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;厂商&apos; verbose_name_plural = &quot;厂商&quot;class BusinessUnit(models.Model): &quot;&quot;&quot;业务线&quot;&quot;&quot; parent_unit = models.ForeignKey(&apos;self&apos;, blank=True, null=True, related_name=&apos;parent_level&apos;) name = models.CharField(&apos;业务线&apos;, max_length=64, unique=True) memo = models.CharField(&apos;备注&apos;, max_length=64, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;业务线&apos; verbose_name_plural = &quot;业务线&quot;class Contract(models.Model): &quot;&quot;&quot;合同&quot;&quot;&quot; sn = models.CharField(&apos;合同号&apos;, max_length=128, unique=True) name = models.CharField(&apos;合同名称&apos;, max_length=64) memo = models.TextField(&apos;备注&apos;, blank=True, null=True) price = models.IntegerField(&apos;合同金额&apos;) detail = models.TextField(&apos;合同详细&apos;, blank=True, null=True) start_day = models.DateField(&apos;开始日期&apos;, blank=True, null=True) end_day = models.DateField(&apos;失效日期&apos;, blank=True, null=True) license_num = models.IntegerField(&apos;license数量&apos;, blank=True, null=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) m_day = models.DateField(&apos;修改日期&apos;, auto_now=True) def __str__(self): return self.name class Meta: verbose_name = &apos;合同&apos; verbose_name_plural = &quot;合同&quot;class Tag(models.Model): &quot;&quot;&quot;标签&quot;&quot;&quot; name = models.CharField(&apos;标签名&apos;, max_length=32, unique=True) c_day = models.DateField(&apos;创建日期&apos;, auto_now_add=True) def __str__(self): return self.name class Meta: verbose_name = &apos;标签&apos; verbose_name_plural = &quot;标签&quot; 说明： 机房可以有很多其它字段，比如城市、楼号、楼层和未知等等，如有需要可自行添加； 业务线可以有子业务线，因此使用一个外键关联自身模型； 合同模型主要存储财务部门关心的数据； 资产标签模型与资产是多对多的关系。 5.CPU模型通常一台服务器中只能有一种CPU型号，所以这里使用OneToOneField唯一关联一个资产对象，而不是外键关系。服务器上可以有多个物理CPU，它们的型号都是一样的。每个物理CPU又可能包含多核。class CPU(models.Model): &quot;&quot;&quot;CPU组件&quot;&quot;&quot; asset = models.OneToOneField(&apos;Asset&apos;) # 设备上的cpu肯定都是一样的，所以不需要建立多个cpu数据，一条就可以，因此使用一对一。 cpu_model = models.CharField(&apos;CPU型号&apos;, max_length=128, blank=True, null=True) cpu_count = models.PositiveSmallIntegerField(&apos;物理CPU个数&apos;, default=1) cpu_core_count = models.PositiveSmallIntegerField(&apos;CPU核数&apos;, default=1) def __str__(self): return self.asset.name + &quot;: &quot; + self.cpu_model class Meta: verbose_name = &apos;CPU&apos; verbose_name_plural = &quot;CPU&quot; 6.RAM模型某个资产中可能有多条内存，所以这里必须是外键关系。其次，内存的sn号可能无法获得，就必须通过内存所在的插槽未知来唯一确定每条内存。因此，unique_together = (‘asset’, ‘slot’)这条设置非常关键，相当于内存的主键了，每条内存数据必须包含slot字段，否则就不合法。class RAM(models.Model): &quot;&quot;&quot;内存组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 只能通过外键关联Asset。否则不能同时关联服务器、网络设备等等。 sn = models.CharField(&apos;SN号&apos;, max_length=128, blank=True, null=True) model = models.CharField(&apos;内存型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;内存制造商&apos;, max_length=128, blank=True, null=True) slot = models.CharField(&apos;插槽&apos;, max_length=64) capacity = models.IntegerField(&apos;内存大小(GB)&apos;, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s: %s&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;内存&apos; verbose_name_plural = &quot;内存&quot; unique_together = (&apos;asset&apos;, &apos;slot&apos;) #unique_together，也就是联合唯一，同一资产下的内存，根据插槽slot的不同，必须唯一 7. 硬盘模型与内存相同的是，硬盘也可能有很多块，所以也是外键关系。不同的是，硬盘通常都能获取到sn号，使用sn作为唯一值比较合适，也就是unique_together = (‘asset’, ‘sn’)。硬盘有不同的接口，这里设置了4种以及unknown，可自行添加其它类别。class Disk(models.Model): &quot;&quot;&quot;存储设备&quot;&quot;&quot; disk_interface_type_choice = ( (&apos;SATA&apos;, &apos;SATA&apos;), (&apos;SAS&apos;, &apos;SAS&apos;), (&apos;SCSI&apos;, &apos;SCSI&apos;), (&apos;SSD&apos;, &apos;SSD&apos;), (&apos;unknown&apos;, &apos;unknown&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;) sn = models.CharField(&apos;硬盘SN号&apos;, max_length=128) slot = models.CharField(&apos;所在插槽位&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;磁盘型号&apos;, max_length=128, blank=True, null=True) manufacturer = models.CharField(&apos;磁盘制造商&apos;, max_length=128, blank=True, null=True) capacity = models.FloatField(&apos;磁盘容量(GB)&apos;, blank=True, null=True) interface_type = models.CharField(&apos;接口类型&apos;, max_length=16, choices=disk_interface_type_choice, default=&apos;unknown&apos;) def __str__(self): return &apos;%s: %s: %s: %sGB&apos; % (self.asset.name, self.model, self.slot, self.capacity) class Meta: verbose_name = &apos;硬盘&apos; verbose_name_plural = &quot;硬盘&quot; unique_together = (&apos;asset&apos;, &apos;sn&apos;) 8.网卡模型一台设备中可能有很多块网卡，所以网卡与资产也是外键的关系。另外，由于虚拟机的存在，网卡的mac地址可能会发生重复，无法唯一确定某块网卡，因此通过网卡型号加mac地址的方式来唯一确定网卡。class NIC(models.Model): &quot;&quot;&quot;网卡组件&quot;&quot;&quot; asset = models.ForeignKey(&apos;Asset&apos;) # 注意要用外键 name = models.CharField(&apos;网卡名称&apos;, max_length=64, blank=True, null=True) model = models.CharField(&apos;网卡型号&apos;, max_length=128) mac = models.CharField(&apos;MAC地址&apos;, max_length=64) # 虚拟机有可能会出现同样的mac地址 ip_address = models.GenericIPAddressField(&apos;IP地址&apos;, blank=True, null=True) net_mask = models.CharField(&apos;掩码&apos;, max_length=64, blank=True, null=True) bonding = models.CharField(&apos;绑定地址&apos;, max_length=64, blank=True, null=True) def __str__(self): return &apos;%s: %s: %s&apos; % (self.asset.name, self.model, self.mac) class Meta: verbose_name = &apos;网卡&apos; verbose_name_plural = &quot;网卡&quot; unique_together = (&apos;asset&apos;, &apos;model&apos;, &apos;mac&apos;) # 资产、型号和mac必须联合唯一。防止虚拟机中的特殊情况发生错误。 9.日志模型CMDB必须记录各种日志，这是毫无疑问的！我们通常要记录事件名称、类型、关联的资产、子事件、事件详情、谁导致的、发生时间。这些都很重要！ 尤其要注意的是，事件日志不能随着关联资产的删除被一并删除，也就是我们设置on_delete=models.SET_NULL的意义！class EventLog(models.Model): &quot;&quot;&quot; 日志. 在关联对象被删除的时候，不能一并删除，需保留日志。 因此，on_delete=models.SET_NULL &quot;&quot;&quot; name = models.CharField(&apos;事件名称&apos;, max_length=128) event_type_choice = ( (0, &apos;其它&apos;), (1, &apos;硬件变更&apos;), (2, &apos;新增配件&apos;), (3, &apos;设备下线&apos;), (4, &apos;设备上线&apos;), (5, &apos;定期维护&apos;), (6, &apos;业务上线\更新\变更&apos;), ) asset = models.ForeignKey(&apos;Asset&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批成功时有这项数据 new_asset = models.ForeignKey(&apos;NewAssetApprovalZone&apos;, blank=True, null=True, on_delete=models.SET_NULL) # 当资产审批失败时有这项数据 event_type = models.SmallIntegerField(&apos;事件类型&apos;, choices=event_type_choice, default=4) component = models.CharField(&apos;事件子项&apos;, max_length=256, blank=True, null=True) detail = models.TextField(&apos;事件详情&apos;) date = models.DateTimeField(&apos;事件时间&apos;, auto_now_add=True) user = models.ForeignKey(User, blank=True, null=True, verbose_name=&apos;事件执行人&apos;, on_delete=models.SET_NULL) # 自动更新资产数据时没有执行人 memo = models.TextField(&apos;备注&apos;, blank=True, null=True) def __str__(self): return self.name class Meta: verbose_name = &apos;事件纪录&apos; verbose_name_plural = &quot;事件纪录&quot; 10.新资产待审批区模型新资产的到来，并不能直接加入CMDB数据库中，而是要通过管理员审批后，才可以上线的。这就需要一个新资产的待审批区。在该区中，以资产的sn号作为唯一值，确定不同的资产。除了关键的包含资产所有信息的data字段，为了方便审批员查看信息，我们还设计了一些厂商、型号、内存大小、CPU类型等字段。同时，有可能出现资产还未审批，更新数据就已经发过来的情况，所以需要一个数据更新日期字段。class NewAssetApprovalZone(models.Model): &quot;&quot;&quot;新资产待审批区&quot;&quot;&quot; sn = models.CharField(&apos;资产SN号&apos;, max_length=128, unique=True) # 此字段必填 asset_type_choice = ( (&apos;server&apos;, &apos;服务器&apos;), (&apos;networkdevice&apos;, &apos;网络设备&apos;), (&apos;storagedevice&apos;, &apos;存储设备&apos;), (&apos;securitydevice&apos;, &apos;安全设备&apos;), (&apos;IDC&apos;, &apos;机房&apos;), (&apos;software&apos;, &apos;软件资产&apos;), ) asset_type = models.CharField(choices=asset_type_choice, default=&apos;server&apos;, max_length=64, blank=True, null=True, verbose_name=&apos;资产类型&apos;) manufacturer = models.CharField(max_length=64, blank=True, null=True, verbose_name=&apos;生产厂商&apos;) model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;型号&apos;) ram_size = models.PositiveIntegerField(blank=True, null=True, verbose_name=&apos;内存大小&apos;) cpu_model = models.CharField(max_length=128, blank=True, null=True, verbose_name=&apos;CPU型号&apos;) cpu_count = models.PositiveSmallIntegerField(blank=True, null=True) cpu_core_count = models.PositiveSmallIntegerField(blank=True, null=True) os_distribution = models.CharField(max_length=64, blank=True, null=True) os_type = models.CharField(max_length=64, blank=True, null=True) os_release = models.CharField(max_length=64, blank=True, null=True) data = models.TextField(&apos;资产数据&apos;) # 此字段必填 c_time = models.DateTimeField(&apos;汇报日期&apos;, auto_now_add=True) m_time = models.DateTimeField(&apos;数据更新日期&apos;, auto_now=True) approved = models.BooleanField(&apos;是否批准&apos;, default=False) def __str__(self): return self.sn class Meta: verbose_name = &apos;新上线待批准资产&apos; verbose_name_plural = &quot;新上线待批准资产&quot; ordering = [&apos;-c_time&apos;] 11.总结通过前面的内容，我们可以看出CMDB数据模型的设计非常复杂，我们这里还是省略了很多不太重要的部分，就这样总共都有400多行代码。其中每个模型需要保存什么字段、采用什么类型、什么关联关系、定义哪些参数、数据是否可以为空，这些都是踩过各种坑后总结出来的，不是随便就能定义的。所以，请务必详细阅读和揣摩这些模型的内容。 一切没有问题之后，注册app，然后makemigrations以及migrate! 注册app： cmdb/settings.pyINSTALLED_APPS = [ &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;assets&apos;, ##此处] 创建数据库表单：python manage.py makemigrationspython manage.py migrate]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMDB项目需求分析（一）]]></title>
    <url>%2F2019%2F02%2F28%2F1%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、CMDB简介CMDB (Configuration Management Database)配置管理数据库: CMDB用于存储与管理企业IT架构中设备的各种配置信息，它与所有服务支持和服务交付流程都紧密相联，支持这些流程的运转、发挥配置信息的价值，同时依赖于相关流程保证数据的准确性。 CMDB是ITIL(Information Technology Infrastructure Library，信息技术基础架构库)的基础，常常被认为是构建其它ITIL流程的先决条件而优先考虑，ITIL项目的成败与是否成功建立CMDB有非常大的关系。 CMDB的核心是对整个公司的IT硬件/软件资源进行自动/手动收集、变更操作，说白了也就是对IT资产进行自动化管理，这也是本项目的重点。 二、项目需求分析本项目不是一个完整的的CMDB系统，重点针对服务器资产的自动数据收集、报告、接收、审批、更新和展示，搭建一个基础的面向运维的主机管理平台。 下面是项目需求的总结： 尽可能存储所有的IT资产数据，但不包括外设、优盘、显示器这种属于行政部门管理的设备； 硬件信息可自动收集、报告、分析、存储和展示； 具有后台管理人员的工作界面； 具有前端可视化展示的界面； 具有日志记录功能； 数据可手动添加、修改和删除。 当然，实际的CMDB项目需求绝对不止这些，还有诸如用户管理、权限管理、API安全认证、REST设计等等。 三、资产分类资产种类众多，不是所有的都需要CMDB管理，也不是什么都是CMDB能管理的。 下面是一个大致的分类，不一定准确、全面： 资产类型包括： 服务器 存储设备 安全设备 网络设备 软件资产 服务器又可分为： 刀片服务器 PC服务器 小型机 大型机 其它 存储设备包括： 磁盘阵列 网络存储器 磁带库 磁带机 其它 安全设备包括： 防火墙 入侵检测设备 互联网网关 漏洞扫描设备 数字签名设备 上网行为管理设备 运维审计设备 加密机 其它 网络设备包括： 路由器 交换器 负载均衡 VPN 流量分析 其它 软件资产包括： 操作系统授权 大型软件授权 数据库授权 其它 其中，服务器是运维部门最关心的，也是CMDB中最主要、最方便进行自动化管理的资产。 服务器又可以包含下面的部件： CPU 硬盘 内存 网卡 除此之外，我们还要考虑下面的一些内容： 机房 业务线 合同 管理员 审批员 资产标签 其它未尽事宜 大概对资产进行了分类之后，就要详细考虑各细分数据条目了。 共有数据条目： 有一些数据条目是所有资产都应该有的，比如： 资产名称 资产sn 所属业务线 设备状态 制造商 管理IP 所在机房 资产管理员 资产标签 合同 价格 购买日期 过保日期 批准人 批准日期 数据更新日期 备注 另外，不同类型的资产还有各自不同的数据条目，例如服务器： 服务器： 服务器类型 添加方式 宿主机 服务器型号 Raid类型 操作系统类型 发行版本 操作系统版本 其实，在开始正式编写CMDB项目代码之前，对项目的需求分析准确与否，数据条目的安排是否合理，是决定整个CMDB项目成败的关键。这一部分工作看似简单其实复杂，看似无用其实关键，做好了，项目基础就牢固，没做好，推到重来好几遍很正常！]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
        <tag>CMDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django基础学习Ⅰ]]></title>
    <url>%2F2019%2F02%2F11%2FDjango%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E2%85%A0%2F</url>
    <content type="text"><![CDATA[一、Django简介 Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的框架模式，即模型M，视图V和控制器C。它最初是被开发来用于管理劳伦斯出版集团旗下的一些以新闻内容为主的网站的，即是CMS（内容管理系统）软件。并于2005年7月在BSD许可证下发布。这套框架是以比利时的吉普赛爵士吉他手Django Reinhardt来命名的。 Django是一个处理网络请求的webweb应用框架 Django是开源的 Django有四个核心组件： 1.数据模型和数据库之间的媒介ORM 2.基于正则表达式的URL分发器 3.视图处理系统 4.模板系统 MVC： m modules 模型， 和数据库字段对应v views 视图 用来展示给用户的，就是我们所学到的前端c controll url控制， 一个url，对应一个方法或者类 二、Django特点1) 强大的数据库功能：用python的类继承，几行代码就可以拥有一个动态的数据库操作API，如果需要也能执行SQL语句。2) 自带的强大的后台功能：几行代码就让网站拥有一个强大的后台，轻松管理内容。3) 优雅的网址：用正则匹配网址，传递到对应函数。4) 模板系统：强大，易扩展的模板系统，设计简易，代码和样式分开设计，更易管理。5) 缓存系统：与memcached或其它缓存系统联用，表现更出色，加载速度更快。6) 国际化：完全支持多语言应用，允许你定义翻译的字符，轻松翻译成不同国家的语言 三、安装Django使用pip工具来安装Django，直接通过下面命令来安装就可以。# pip install Django 用一下测试django是否安装成功：C:\Users\ZHDYA&gt;pythonPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import django&gt;&gt;&gt; print (django.VERSION)(2, 0, 6, &apos;final&apos;, 0) 四、创建项目首先，我们先通过django来创建一个项目，命令如下：# django-admin startproject firstproject 然后在当前目录下就自动生成了一个firstproject的项目然后就可以启动这个项目了：# python manage.py runserver 127.0.0.1:8080 默认不写ip绑定的是本机的所以ip地址，端口默认为8000 也可以通过在pycharm中直接创建一个Django项目，就自动创建好了文件，然后配置manage.py脚本的参数。 直接再次运行manage.py文件就好。 然后访问url：http://127.0.0.1:8080 有一个欢迎的首页 4.1、项目目录结构第一层：DjangoTest 项目名称第二层： DjangoTest目录和__init__.py文件，声明是一个包，表示项目实际的python包，不要随意更改该目录，与配置有关联settings.py 项目的全局（所有项目）配置中心urls.py 项目的url配置中心wsgi.py 项目的wsgi配置中心templates 模板目录manage.py django命令管理脚本 setting.pyBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ## 类似于环境变量SECRET_KEY = ##密钥DEBUG = True ##当报错时显示的错误信息ALLOWED_HOSTS = [] ##允许哪些机器可以访问 五、创建app打个比方，jd网站：http://www.jd.com/，上面有各种各样的不同模块，我们划分为不同的app，那我们就需要在django里面创建不通的app啦。接下来，我们就来看看如何创建app如果是命令行模式： #python manage.py startapp linux#python manage.py startapp python 如果是pycharm，你就需要点击：Tools-&gt;&gt;Run manage.py Task 然后出现的交互命令行上输入：startapp newapp 这样就创建了newapp的app。 六、Django的解析顺序既然我们知道Django是使用的MVC的架构，那我们先来聊聊MVC是什么样的原理，首先，通过MVC中的C就是control，用来接收url请求，对应我们django中的url.py模块，M就代表Model，调用数据库的Model层，就是Django的model.py模块，然后经过业务逻辑层的处理，最后渲染到前端界面。前端就是MVC中的view层，对应django的view模块。 其实所有的参数定义都是以setting.py为准，++首先django先去setting.py中找到ROOT_URLCONF = ‘firstproject.urls’找到总url++。然后在firstproject下的urls.py文件中的urlpatterns列表变量，然后根据里面的URL的正则表达式进行解析，如果解析到，就调用第二个参数，第二个参数对应一个类或者一个函数，或者直接是一个前端页面，在经过类或者函数处理完以后，在展现在前端界面。而前端是单独的html文件，前端界面和后端处理分开，架构更加清晰。 在上面的目录结构中，每一个app都会有一个view.py， model.py，我们自己还要在创建一个url.py，通过include函数，在firstproject项目中的总url.py分出去，把属于各自的app的url分配到不通的APP的urls.py文件中，这样可以降低耦合度，增加代码的健壮性。。 6.1、创建urls.py文件urls作为程序的url入口，支持正则匹配，讲访问的url映射到view中的函数中。为了能调用每个app管理自己的url，我们首先需要在DjangoTest的urls.py文件中做如下修改： from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^newapp/&apos;, include(&apos;newapp.urls&apos;)),] 注意事项：为了避免和别的app取同样的名字，一般我们会在名字前加一个app名称作为前缀url匹配，主url不需要/反斜杠：==因为django已经给域名加了一个反斜杠，如：http://127.0.0.1/主url后面要加/， app的url前面就不需要加/了，主url后面一般不要加$符号， app的url后面要加$符号== 然后在创建newapp/urls.py文件，编辑如下： from django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&apos;^$&apos;, views.index)] 配置 view.py文件而以上：http://127.0.0.1:8080/newapp的url对应的就是view模块中的index函数，在linux的view.py中定义index函数 from django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;) 然后在浏览器上访问：http://127.0.0.1:8080/newapp/，如下图所示： 6.2、如果再次增加内容：urls.pyfrom django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index), url(r&apos;newapp/&apos;, views.index), url(r&apos;hello/&apos;, views.hello)] views.pyfrom django.http import HttpResponsedef index(request): return HttpResponse(&quot;This is a test Django index!!!&quot;)def hello(request): return HttpResponse(&quot;&lt;h1 style=&apos;text-align:center&apos;&gt; hheello world!!!&lt;/h1&gt;&quot;) 当访问：http://127.0.0.1:8000/newapp/hello/ 这会出现一个一级标题且居中的字体。 6.3、urls捕获参数（匹配正则表达式）在urls.py中增加如下：url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=hello), 在views.py中增加如下：def hello(request, p1, p2): return HttpResponse(&quot;hello &#123;0&#125;, hello &#123;1&#125;&quot;.format(p1, p2)) url参数的捕获有两种方式：b 捕获关键字参数：在url函数中，正则表达是用（?P）进行捕获，然后在views.py中定义即可 在urls.py中增加如下：url(r&apos;keyword/(?P&lt;ip&gt;\S+)/$&apos;, views.keyword, name=keyword), 在views.py中增加如下：def keyword(request, ip): return HttpResponse(&quot;the ip is &#123;0&#125;&quot;.format(ip)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/keyword/1.1.1.1/ 七、urls重定向在学习url重定向之前，我们先来看看定义url的函数是怎样一个表达形式。 注意如下，有个参数 name= 这个是必须要写的，类似于起个别名。def url(regex, view, kwargs=None, name=None): regex：url匹配的正则字符串view：一个可以调用的类型函数，或者使用include函数kwargs：关键字参数，必须是一个字典，可以通过这个传递更多参数给views.py，views通过kwargs.get(“key”)得到对应的valuename：给URL取得名字，以后可以通过reverse函数进行重定向 对于kwargs如何传递参数，我们来看一个例子： 在urls.py中增加如下：url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), 在views.py中增加如下： def test(request, **kwargs): return HttpResponse(&quot;the name is : &#123;0&#125;&quot;.format(kwargs.get(&quot;name&quot;))) 在浏览器上访问url：http://127.0.0.1:8080/newapp/test/ 既然已经知道name属性的用法，现在我们就来说重定向，重定向常用name属性来进行重定向 修改urls.pyurl(r&quot;^$&quot;, views.index, name=&quot;index&quot;),url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;), 修改views.pyfrom django.http import HttpResponse, HttpResponseRedirectfrom django.urls import reversedef redirect(request): return HttpResponseRedirect(reverse(&quot;index&quot;)) 在浏览器上访问url：http://127.0.0.1:8080/newapp/redirect，直接跳转到http://127.0.0.1:8080/newapp/， 当然也可以指定返回数据的具体类型，例如：Json格式返回 urls.pyfrom django.conf.urls import url, includefrom newapp import viewsurlpatterns = [ url(r&quot;^$&quot;, views.index, name=&quot;index&quot;), # url(r&apos;hello/&apos;, views.hello) url(r&apos;hello/p1(\w+)p2(.+)/$&apos;, views.hello, name=&quot;hello&quot;), url(r&apos;test/$&apos;, view=views.test, kwargs=&#123;&quot;name&quot;: &quot;zhdya&quot;&#125;, name=&quot;test&quot;), url(r&quot;redirect/$&quot;, view=views.redirect, name=&quot;redirect&quot;)] 在views.py中修改主页为：def index(request): test = dict() test[&apos;name&apos;] = &quot;zhdya&quot; test[&apos;sex&apos;] = &quot;man&quot; test[&apos;age&apos;] = 28 # return HttpResponse(&quot;This is a test Django index!!!&quot;) return HttpResponse(json.dumps(test))]]></content>
      <categories>
        <category>Python3</category>
      </categories>
      <tags>
        <tag>django</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅲ]]></title>
    <url>%2F2019%2F02%2F10%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A2%2F</url>
    <content type="text"><![CDATA[测试集群# 创建一个 nginx deplymentapiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ---apiVersion: v1 kind: Servicemetadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx 创建testnginx deployment[root@master1 ~]# kubectl create -f testnginx.yamldeployment.extensions/nginx-dm createdservice/nginx-svc created [root@master1 ~]# kubectl get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-dm-fff68d674-j7dlk 1/1 Running 0 9m 10.254.108.115 node2 &lt;none&gt;nginx-dm-fff68d674-r5hb6 1/1 Running 0 9m 10.254.102.133 node1 &lt;none&gt; 在 安装了 calico 网络的node节点 里 curl[root@node2 ~]# curl 10.254.102.133&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 查看 ipvs 规则[root@node2 ssl]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 1 0 -&gt; 192.168.161.162:6443 Masq 1 0 0TCP 10.254.18.37:80 rr -&gt; 10.254.75.1:80 Masq 1 0 0 -&gt; 10.254.102.133:80 Masq 1 0 0 配置 CoreDNS官方 地址 https://coredns.io 下载 yaml 文件wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedmv coredns.yaml.sed coredns.yaml 修改配置文件中的部分配置：# vi coredns.yaml第一处：...data: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/18 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 &#125;... 第二处：搜索 /clusterIP 即可 clusterIP: 10.254.0.2 配置说明1）errors官方没有明确解释，后面研究2）health:健康检查，提供了指定端口（默认为8080）上的HTTP端点，如果实例是健康的，则返回“OK”。3）cluster.local：CoreDNS为kubernetes提供的域，10.254.0.0/18这告诉Kubernetes中间件它负责为反向区域提供PTR请求0.0.254.10.in-addr.arpa ..换句话说，这是允许反向DNS解析服务（我们经常使用到得DNS服务器里面有两个区域，即“正向查找区域”和“反向查找区域”，正向查找区域就是我们通常所说的域名解析，反向查找区域即是这里所说的IP反向解析，它的作用就是通过查询IP地址的PTR记录来得到该IP地址指向的域名，当然，要成功得到域名就必需要有该IP地址的PTR记录。PTR记录是邮件交换记录的一种，邮件交换记录中有A记录和PTR记录，A记录解析名字到地址，而PTR记录解析地址到名字。地址是指一个客户端的IP地址，名字是指一个客户的完全合格域名。通过对PTR记录的查询，达到反查的目的。）4）proxy:这可以配置多个upstream 域名服务器，也可以用于延迟查找 /etc/resolv.conf 中定义的域名服务器5）cache:这允许缓存两个响应结果，一个是肯定结果（即，查询返回一个结果）和否定结果（查询返回“没有这样的域”），具有单独的高速缓存大小和TTLs。# 这里 kubernetes cluster.local 为 创建 svc 的 IP 段kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IPclusterIP: 10.254.0.2 创建coreDNS[root@master1 src]# kubectl apply -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.extensions/coredns createdservice/kube-dns created 查看创建：[root@master1 src]# kubectl get pod,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 23s 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 23s 10.254.75.21 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 23s k8s-app=kube-dns 检查日志[root@master1 src]# kubectl logs coredns-55f86bf584-hsrbp -n kube-system.:532018/09/22 02:03:06 [INFO] CoreDNS-1.2.22018/09/22 02:03:06 [INFO] linux/amd64, go1.11, eb51e8bCoreDNS-1.2.2linux/amd64, go1.11, eb51e8b 验证 dns 服务在验证 dns 之前，在 dns 未部署++之前创建的 pod 与 deployment 等，都必须删除++，重新部署，否则无法解析。 创建一个 pods 来测试一下 dnsapiVersion: v1kind: Podmetadata: name: alpinespec: containers: - name: alpine image: alpine command: - sleep - &quot;3600&quot; 查看 创建的服务[root@master1 ~]# kubectl get po,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/alpine 1/1 Running 0 52s 10.254.102.141 node1 &lt;none&gt;pod/nginx-dm-fff68d674-fzhqk 1/1 Running 0 3m 10.254.102.140 node1 &lt;none&gt;pod/nginx-dm-fff68d674-h8n79 1/1 Running 0 3m 10.254.75.22 node2 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 20d &lt;none&gt;service/nginx-svc ClusterIP 10.254.10.144 &lt;none&gt; 80/TCP 3m name=nginx 测试[root@master1 ~]# kubectl exec -it alpine nslookup nginx-svcnslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: nginx-svcAddress 1: 10.254.10.144 nginx-svc.default.svc.cluster.local 部署 DNS 自动伸缩按照 node 数量 自动伸缩 dns 数量vim dns-auto-scaling.yamlkind: ServiceAccountapiVersion: v1metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilerules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;list&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;replicationcontrollers/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;deployments/scale&quot;, &quot;replicasets/scale&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] verbs: [&quot;get&quot;, &quot;create&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcilesubjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-systemroleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: &quot;20m&quot; memory: &quot;10Mi&quot; command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params=&#123;&quot;linear&quot;:&#123;&quot;coresPerReplica&quot;:256,&quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true&#125;&#125; - --logtostderr=true - --v=2 tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot; serviceAccountName: kube-dns-autoscaler 导入文件[root@master1 ~]# kubectl apply -f dns-auto-scaling.yamlserviceaccount/kube-dns-autoscaler createdclusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler createddeployment.apps/kube-dns-autoscaler created ++如下是上面所用到的镜像，如果不可以下载使用如下的即可++：registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:coredns-1.2.2registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cluster-proportional-autoscaler-amd64_1.1.2-r2 部署 Ingress 与 Dashboard部署 heapster官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster 下载 heapster 相关 yaml 文件wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ==如上官方镜像一直在更新，修改的时候需要把如下的版本号也修改下↓== 下载 heapster 镜像下载# 官方镜像k8s.gcr.io/heapster-grafana-amd64:v4.4.3k8s.gcr.io/heapster-amd64:v1.5.3k8s.gcr.io/heapster-influxdb-amd64:v1.3.3# 个人的镜像jicki/heapster-grafana-amd64:v4.4.3jicki/heapster-amd64:v1.5.3jicki/heapster-influxdb-amd64:v1.3.3# 备用阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-grafana-amd64-v4.4.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-amd64-v1.5.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:heapster-influxdb-amd64-v1.3.3# 替换所有yaml 镜像地址sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; *.yaml 修改 yaml 文件# heapster.yaml 文件#### 修改如下部分 #####因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true # heapster-rbac.yaml 文件#### 修改为部分 #####将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapsterroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapstersubjects:- kind: ServiceAccount name: heapster namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapster-kubelet-apiroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-adminsubjects:- kind: ServiceAccount name: heapster namespace: kube-system 创建：[root@master1 dashboard180922]# kubectl apply -f .deployment.extensions/monitoring-grafana createdservice/monitoring-grafana createdclusterrolebinding.rbac.authorization.k8s.io/heapster createdclusterrolebinding.rbac.authorization.k8s.io/heapster-kubelet-api createdserviceaccount/heapster createddeployment.extensions/heapster createdservice/heapster createddeployment.extensions/monitoring-influxdb createdservice/monitoring-influxdb created 这儿可能需要等待一下，这个取决于自己server的网络情况：[root@node1 ~]# journalctl -u kubelet -f-- Logs begin at 六 2018-09-22 09:07:48 CST. --9月 22 10:34:55 node1 kubelet[2301]: I0922 10:34:55.701016 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=======&gt; ] 7.617MB/50.21MB&quot;9月 22 10:35:05 node1 kubelet[2301]: I0922 10:35:05.700868 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [========&gt; ] 8.633MB/50.21MB&quot;9月 22 10:35:15 node1 kubelet[2301]: I0922 10:35:15.701193 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==========&gt; ] 10.66MB/50.21MB&quot;9月 22 10:35:25 node1 kubelet[2301]: I0922 10:35:25.700980 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [============&gt; ] 12.69MB/50.21MB&quot;9月 22 10:35:35 node1 kubelet[2301]: I0922 10:35:35.700779 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [===============&gt; ] 15.74MB/50.21MB&quot;9月 22 10:35:45 node1 kubelet[2301]: I0922 10:35:45.701359 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================&gt; ] 18.28MB/50.21MB&quot;9月 22 10:35:55 node1 kubelet[2301]: I0922 10:35:55.701618 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [====================&gt; ] 20.82MB/50.21MB&quot;9月 22 10:36:05 node1 kubelet[2301]: I0922 10:36:05.701611 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [=========================&gt; ] 25.39MB/50.21MB&quot;9月 22 10:36:15 node1 kubelet[2301]: I0922 10:36:15.700926 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==============================&gt; ] 30.99MB/50.21MB&quot;9月 22 10:36:25 node1 kubelet[2301]: I0922 10:36:25.700931 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot;9月 22 10:36:35 node1 kubelet[2301]: I0922 10:36:35.701950 2301 kube_docker_client.go:345] Pulling image &quot;jicki/heapster-grafana-amd64:v4.4.3&quot;: &quot;a05a7a3d2d4f: Downloading [==================================&gt; ] 34.55MB/50.21MB&quot; 查看部署情况[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEpod/calico-kube-controllers-79cfd7887-scnnp 1/1 Running 1 2d 192.168.161.78 node2 &lt;none&gt;pod/calico-node-pwlq4 2/2 Running 2 2d 192.168.161.77 node1 &lt;none&gt;pod/calico-node-vmrrq 2/2 Running 2 2d 192.168.161.78 node2 &lt;none&gt;pod/coredns-55f86bf584-fqjf2 1/1 Running 0 44m 10.254.102.139 node1 &lt;none&gt;pod/coredns-55f86bf584-hsrbp 1/1 Running 0 44m 10.254.75.21 node2 &lt;none&gt;pod/heapster-745d7bc8b7-zk65c 1/1 Running 0 13m 10.254.75.51 node2 &lt;none&gt;pod/kube-dns-autoscaler-66d448df8f-4zvw6 1/1 Running 0 32m 10.254.102.142 node1 &lt;none&gt;pod/monitoring-grafana-558c44f948-m2tzz 1/1 Running 0 1m 10.254.75.6 node2 &lt;none&gt;pod/monitoring-influxdb-f6bcc9795-496jd 1/1 Running 0 13m 10.254.102.147 node1 &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/heapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 13m k8s-app=heapsterservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 44m k8s-app=kube-dnsservice/monitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 1m k8s-app=grafanaservice/monitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 13m k8s-app=influxdb 部署 dashboard下载 dashboard 镜像# 官方镜像k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3# 个人的镜像jicki/kubernetes-dashboard-amd64:v1.8.3# 阿里的镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kubernetes-dashboard-amd64-v1.8.3 下载 yaml 文件curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 导入 yaml # 替换所有的 images，注意修改镜像版本号为1.8.3sed -i &apos;s/k8s\.gcr\.io/jicki/g&apos; kubernetes-dashboard.yaml 创建dashboard[root@master1 dashboard180922]# kubectl apply -f kubernetes-dashboard.yamlsecret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created 查看创建的dashboard[root@master1 dashboard180922]# kubectl get po,svc -n kube-system -o wide | grep dashboardpod/kubernetes-dashboard-65666d4586-bb66s 1/1 Running 0 7m 10.254.102.151 node1 &lt;none&gt;service/kubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 7m k8s-app=kubernetes-dashboard 部署 Nginx Ingress++Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。++ 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ 配置 调度 node# ingress 有多种方式 1. deployment 自由调度 replicas2. daemonset 全局调度 分配到所有node里# deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签# 默认如下:[root@master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONnode1 Ready &lt;none&gt; 20d v1.11.2node2 Ready &lt;none&gt; 8d v1.11.2# 对 node1 与 node2 打上 label[root@master1 ~]# kubectl label nodes node1 ingress=proxynode/node1 labeled[root@master1 ~]# kubectl label nodes node2 ingress=proxynode/node2 labeled# 打完标签以后[root@master1 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSnode1 Ready &lt;none&gt; 20d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node1node2 Ready &lt;none&gt; 9d v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node2 下载镜像# 官方镜像gcr.io/google_containers/defaultbackend:1.4quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2# 国内镜像jicki/defaultbackend:1.4jicki/nginx-ingress-controller:0.16.2# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:defaultbackend-1.4registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:nginx-ingress-controller-0.16.2 下载 yaml 文件部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yamlcurl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml# 部署 Ingress RBAC 认证curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml# 部署 Ingress Controller 组件curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml# tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务# 为了更加方便理解，如下两个例子：# tcp 例子apiVersion: v1kind: ConfigMapmetadata: name: tcp-services namespace: ingress-nginxdata: 9000: &quot;default/tomcat:8080&quot; # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中# udp 例子apiVersion: v1kind: ConfigMapmetadata: name: udp-services namespace: ingress-nginxdata: 53: &quot;kube-system/kube-dns:53&quot;# 替换所有的 imagessed -i &apos;s/gcr\.io\/google_containers/jicki/g&apos; *sed -i &apos;s/quay\.io\/kubernetes-ingress-controller/jicki/g&apos; *# 上面 对 两个 node 打了 label 所以配置 replicas: 2# 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。vim with-rbac.yaml第一处：↓spec: replicas: 2 第二处：↓（搜索 /nginx-ingress-serviceaccount 即可，在其下添加） .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... 第三处：↓ # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 导入 yaml 文件[root@master1 ingress-service]# kubectl apply -f namespace.yamlnamespace/ingress-nginx created[root@master1 ingress-service]# kubectl get nsNAME STATUS AGEdefault Active 20dingress-nginx Active 6skube-public Active 20dkube-system Active 20d[root@master1 ingress-service]# kubectl apply -f .configmap/nginx-configuration createddeployment.extensions/default-http-backend createdservice/default-http-backend creatednamespace/ingress-nginx configuredserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createdconfigmap/tcp-services createdconfigmap/udp-services createddeployment.extensions/nginx-ingress-controller created# 查看服务，可以看到这两个 pods 被分别调度到 77 与 78 中[root@master1 ingress-service]# kubectl get pods -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEdefault-http-backend-6b89c8bdcb-vvl9f 1/1 Running 0 9m 10.254.102.163 node1 &lt;none&gt;nginx-ingress-controller-cf8d4564d-5vz7h 1/1 Running 0 9m 10.254.75.16 node2 &lt;none&gt;nginx-ingress-controller-cf8d4564d-z7q4b 1/1 Running 0 9m 10.254.102.158 node1 &lt;none&gt;# 查看我们原有的 svc[root@master1 ingress-service]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEalpine 1/1 Running 3 6h 10.254.102.141 node1 &lt;none&gt;nginx-dm-fff68d674-fzhqk 1/1 Running 0 6h 10.254.102.140 node1 &lt;none&gt;nginx-dm-fff68d674-h8n79 1/1 Running 0 6h 10.254.75.22 node2 &lt;none&gt; 创建一个 基于 nginx-dm 的 ingressvi nginx-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-ingressspec: rules: - host: nginx.zhdya.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 理解如下:- host指虚拟出来的域名，具体地址(我理解应该是Ingress-controller那台Pod所在的主机的地址)应该加入/etc/hosts中,这样所有去nginx.zhdya.cn的请求都会发到nginx- servicePort主要是定义服务的时候的端口，不是NodePort.# 查看服务[root@master1 ingress-service]# kubectl create -f nginx-ingress.yamlingress.extensions/nginx-ingress created[root@master1 ingress-service]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEnginx-ingress nginx.zhdya.cn 80 10s# 测试访问[root@node1 ~]# curl nginx.zhdya.cn&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 当然如果本地浏览器访问的话 我们也需要绑定hosts # 创建一个基于 dashboard 的 https 的 ingress# 新版本的 dashboard 默认就是 ssl ,所以这里使用 tcp 代理到 443 端口# 查看 dashboard svc[root@master1 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheapster ClusterIP 10.254.4.11 &lt;none&gt; 80/TCP 2dkube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 3dkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2dmonitoring-grafana ClusterIP 10.254.25.50 &lt;none&gt; 80/TCP 2dmonitoring-influxdb ClusterIP 10.254.37.83 &lt;none&gt; 8086/TCP 2d# 修改 tcp-services-configmap.yaml 文件[root@master1 src]# vim tcp-services-configmap.yamlkind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginxdata: 8888: &quot;kube-system/kubernetes-dashboard:443&quot;# 载入配置文件[root@master1 src]# kubectl apply -f tcp-services-configmap.yamlconfigmap/tcp-services configured# 查看服务[root@master1 src]# kubectl get configmap/tcp-services -n ingress-nginxNAME DATA AGEtcp-services 1 2d[root@master1 src]# kubectl describe configmap/tcp-services -n ingress-nginxName: tcp-servicesNamespace: ingress-nginxLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;8888&quot;:&quot;kube-system/kubernetes-dashboard:443&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;tcp-services&quot;,&quot;namesp...Data====8888:----kube-system/kubernetes-dashboard:443Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 2d nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 20m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal CREATE 19m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services Normal UPDATE 1m nginx-ingress-controller ConfigMap ingress-nginx/tcp-services# 测试访问[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888curl: (6) Could not resolve host: dashboard.zhdya.cn; 未知的名称或服务当然如上报错很正常，咱们需要绑定下hosts在master 上查询下：[root@master1 src]# kubectl get svc -n kube-system -o wide | grep dashboardkubernetes-dashboard ClusterIP 10.254.3.42 &lt;none&gt; 443/TCP 2d k8s-app=kubernetes-dashboard然后再node端绑定hosts [root@node1 ~]# vim /etc/hosts10.254.3.42 dashboard.zhdya.cn[root@node1 ~]# curl -I -k https://dashboard.zhdya.cn:8888HTTP/1.1 200 OKAccept-Ranges: bytesCache-Control: no-storeContent-Length: 990Content-Type: text/html; charset=utf-8Last-Modified: Tue, 13 Feb 2018 11:17:03 GMTDate: Tue, 25 Sep 2018 02:51:18 GMT # 配置一个基于域名的 https , ingress# 创建一个 基于 自身域名的 证书[root@master1 dashboard-keys]# openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.zhdya.cn-key.key -out dashboard.zhdya.cn.pem -subj &quot;/CN=dashboard.zhdya.cn&quot;Generating a 2048 bit RSA private key.......+++..............+++writing new private key to &apos;dashboard.zhdya.cn-key.key&apos;-----[root@master1 dashboard-keys]# kubectl create secret tls dashboard-secret --namespace=kube-system --cert dashboard.zhdya.cn.pem --key dashboard.zhdya.cn-key.keysecret/dashboard-secret created# 查看 secret[root@master1 dashboard-keys]# kubectl get secret -n kube-system | grep dashboarddashboard-secret kubernetes.io/tls 2 55skubernetes-dashboard-certs Opaque 0 2dkubernetes-dashboard-key-holder Opaque 2 2dkubernetes-dashboard-token-r98wk kubernetes.io/service-account-token 3 2d# 创建一个 ingressvi dashboard-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubernetes-dashboard namespace: kube-system annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot; nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;spec: tls: - hosts: - dashboard.zhdya.cn secretName: dashboard-secret rules: - host: dashboard.zhdya.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443# 创建配置文件[root@master1 src]# kubectl apply -f dashboard-ingress.yamlingress.extensions/kubernetes-dashboard created[root@master1 src]# kubectl get ingress -n kube-systemNAME HOSTS ADDRESS PORTS AGEkubernetes-dashboard dashboard.zhdya.cn 80, 443 37s 测试访问 # 登录认证# 首先创建一个 dashboard rbac 超级用户vi dashboard-admin-rbac.yaml---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system# 导入配置文件[root@master1 src]# kubectl apply -f dashboard-admin-rbac.yamlserviceaccount/kubernetes-dashboard-admin createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created# 查看超级用户的 token 名称[root@master1 src]# kubectl -n kube-system get secret | grep kubernetes-dashboard-adminkubernetes-dashboard-admin-token-kq27d kubernetes.io/service-account-token 3 38s# 查看 token 部分[root@master1 src]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-kq27d 然后我们登录 web ui 选择 令牌登录然后就发现了还是那熟悉的味道：]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅱ]]></title>
    <url>%2F2019%2F02%2F09%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A1%2F</url>
    <content type="text"><![CDATA[配置 kubelet 认证kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 # RBAC 只需创建一次就可以kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes 创建 bootstrap kubeconfig 文件++注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token++ 创建 集群所有 kubelet 的 token==注意修改hostname==[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master1 --kubeconfig ~/.kube/configof2phx.v39lq3ofeh0w6f3m[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master2 --kubeconfig ~/.kube/configb3stk9.edz2iylppqjo5qbc[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:master3 --kubeconfig ~/.kube/configck2uqr.upeu75jzjj1ko901[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node1 --kubeconfig ~/.kube/config1ocjm9.7qa3rd5byuft9gwr[root@master1 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:node2 --kubeconfig ~/.kube/confightsqn3.z9z6579gxw5jdfzd 查看生成的 token[root@master1 kubernetes]# kubeadm token list --kubeconfig ~/.kube/configTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS1ocjm9.7qa3rd5byuft9gwr 23h 2018-09-02T16:06:32+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node1b3stk9.edz2iylppqjo5qbc 23h 2018-09-02T16:03:46+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master2ck2uqr.upeu75jzjj1ko901 23h 2018-09-02T16:05:16+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master3htsqn3.z9z6579gxw5jdfzd 23h 2018-09-02T16:06:34+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node2of2phx.v39lq3ofeh0w6f3m 23h 2018-09-02T16:03:40+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:master1 以下为了区分 会先生成 hostname 名称加 bootstrap.kubeconfig 生成 master1 的 bootstrap.kubeconfig# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=of2phx.v39lq3ofeh0w6f3m \ --kubeconfig=master1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master1-bootstrap.kubeconfig# 拷贝生成的 master1-bootstrap.kubeconfig 文件mv master1-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 master2 的 bootstrap.kubeconfig# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=b3stk9.edz2iylppqjo5qbc \ --kubeconfig=master2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master2-bootstrap.kubeconfig# 拷贝生成的 master2-bootstrap.kubeconfig 文件scp master2-bootstrap.kubeconfig 192.168.161.162:/etc/kubernetes/bootstrap.kubeconfig 生成 master3 的 bootstrap.kubeconfig# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=master3-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=ck2uqr.upeu75jzjj1ko901 \ --kubeconfig=master3-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=master3-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=master3-bootstrap.kubeconfig# 拷贝生成的 master3-bootstrap.kubeconfig 文件scp master3-bootstrap.kubeconfig 192.168.161.163:/etc/kubernetes/bootstrap.kubeconfig 生成 node1 的 bootstrap.kubeconfig# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=node1-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=1ocjm9.7qa3rd5byuft9gwr \ --kubeconfig=node1-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=node1-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node1-bootstrap.kubeconfig# 拷贝生成的 node1-bootstrap.kubeconfig 文件scp node1-bootstrap.kubeconfig 192.168.161.77:/etc/kubernetes/bootstrap.kubeconfig 生成 node2 的 bootstrap.kubeconfig# 配置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=node2-bootstrap.kubeconfig# 配置客户端认证kubectl config set-credentials kubelet-bootstrap \ --token=htsqn3.z9z6579gxw5jdfzd \ --kubeconfig=node2-bootstrap.kubeconfig# 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=node2-bootstrap.kubeconfig # 配置默认关联kubectl config use-context default --kubeconfig=node2-bootstrap.kubeconfig# 拷贝生成的 node2-bootstrap.kubeconfig 文件scp node2-bootstrap.kubeconfig 192.168.161.78:/etc/kubernetes/bootstrap.kubeconfig 配置 bootstrap RBAC 权限kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers# 否则报如下错误failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:1jezb7&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope 创建自动批准相关 CSR 请求的 ClusterRolevi /etc/kubernetes/tls-instructs-csr.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/selfnodeserver&quot;] verbs: [&quot;create&quot;]# 创建 yaml 文件[root@master1 kubernetes]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yamlclusterrole.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver created[root@master1 kubernetes]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserverName: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration=&#123;&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ClusterRole&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;system:certificates.k8s.io:certificatesigningreq...PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes Node 端单 Node 部分 需要部署的组件有docker， calico， kubelet， kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server;node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口;当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ++这种模式和我之前所接触的不太一样，之前所做的架构是基于KUBE-APISERVER 的负载均衡，所有的node节点都会去连接负载均衡的虚拟VIP。++ 创建Nginx 代理在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.161.161:6443; server 192.168.161.162:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\ -v /etc/nginx:/etc/nginx \\ --name nginx-proxy \\ --net=host \\ --restart=on-failure:5 \\ --memory=512M \\ nginx:1.13.7-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 Nginxsystemctl daemon-reloadsystemctl start nginx-proxysystemctl enable nginx-proxysystemctl status nginx-proxyjournalctl -u nginx-proxy -f ##查看实时日志9月 01 17:34:55 node1 docker[4032]: 1.13.7-alpine: Pulling from library/nginx9月 01 17:34:57 node1 docker[4032]: 128191993b8a: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: 655cae3ea06e: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: dbc72c3fd216: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Pulling fs layer9月 01 17:34:57 node1 docker[4032]: f391a4589e37: Waiting9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Verifying Checksum9月 01 17:35:03 node1 docker[4032]: dbc72c3fd216: Download complete9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Verifying Checksum9月 01 17:35:07 node1 docker[4032]: f391a4589e37: Download complete9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Verifying Checksum9月 01 17:35:15 node1 docker[4032]: 128191993b8a: Download complete9月 01 17:35:17 node1 docker[4032]: 128191993b8a: Pull complete9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Verifying Checksum9月 01 17:35:50 node1 docker[4032]: 655cae3ea06e: Download complete9月 01 17:35:51 node1 docker[4032]: 655cae3ea06e: Pull complete9月 01 17:35:51 node1 docker[4032]: dbc72c3fd216: Pull complete9月 01 17:35:51 node1 docker[4032]: f391a4589e37: Pull complete9月 01 17:35:51 node1 docker[4032]: Digest: sha256:34aa80bb22c79235d466ccbbfa3659ff815100ed21eddb1543c6847292010c4d9月 01 17:35:51 node1 docker[4032]: Status: Downloaded newer image for nginx:1.13.7-alpine9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: using the &quot;epoll&quot; event method9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: nginx/1.13.79月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: built by gcc 6.2.1 20160822 (Alpine 6.2.1)9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: OS: Linux 3.10.0-514.el7.x86_649月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:10485769月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker processes9月 01 17:35:54 node1 docker[4032]: 2018/09/01 09:35:54 [notice] 1#1: start worker process 5 创建 kubelet.service 文件==注意修改节点的hostname↓==# 创建 kubelet 目录mkdir -p /var/lib/kubeletvi /etc/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \ --hostname-override=node1 \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 \ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --config=/etc/kubernetes/kubelet.config.json \ --cert-dir=/etc/kubernetes/ssl \ --logtostderr=true \ --v=2[Install]WantedBy=multi-user.target 创建 kubelet config 配置文件vi /etc/kubernetes/kubelet.config.json&#123; &quot;kind&quot;: &quot;KubeletConfiguration&quot;, &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;, &quot;authentication&quot;: &#123; &quot;x509&quot;: &#123; &quot;clientCAFile&quot;: &quot;/etc/kubernetes/ssl/ca.pem&quot; &#125;, &quot;webhook&quot;: &#123; &quot;enabled&quot;: true, &quot;cacheTTL&quot;: &quot;2m0s&quot; &#125;, &quot;anonymous&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;authorization&quot;: &#123; &quot;mode&quot;: &quot;Webhook&quot;, &quot;webhook&quot;: &#123; &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;, &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot; &#125; &#125;, &quot;address&quot;: &quot;192.168.161.77&quot;, &quot;port&quot;: 10250, &quot;readOnlyPort&quot;: 0, &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;, &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;, &quot;serializeImagePulls&quot;: false, &quot;RotateCertificates&quot;: true, &quot;featureGates&quot;: &#123; &quot;RotateKubeletClientCertificate&quot;: true, &quot;RotateKubeletServerCertificate&quot;: true &#125;, &quot;MaxPods&quot;: &quot;512&quot;, &quot;failSwapOn&quot;: false, &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;, &quot;containerLogMaxFiles&quot;: 5, &quot;clusterDomain&quot;: &quot;cluster.local.&quot;, &quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;]&#125;##其它node节点记得修改如上的IP地址 # 如上配置:node1 本机hostname10.254.0.2 预分配的 dns 地址cluster.local. 为 kubernetes 集群的 domainregistry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:pause-amd64_3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。&quot;clusterDNS&quot;: [&quot;10.254.0.2&quot;] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ++同理修改其它node节点++ 启动 kubeletsystemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubeletjournalctl -u kubelet -f 创建 kube-proxy 证书# 证书方面由于我们node端没有装 cfssl# 我们回到 master 端 机器 去配置证书，然后拷贝过来cd /opt/sslvi kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 kube-proxy 证书和私钥/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成ls kube-proxy*kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem# 拷贝到目录cp kube-proxy* /etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.77:/etc/kubernetes/ssl/scp ca.pem kube-proxy* 192.168.161.78:/etc/kubernetes/ssl/ 创建 kube-proxy kubeconfig 文件# 配置集群kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=kube-proxy.kubeconfig# 配置客户端认证kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig # 配置关联kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 配置默认关联kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig# 拷贝到需要的 node 端里scp kube-proxy.kubeconfig 192.168.161.77:/etc/kubernetes/scp kube-proxy.kubeconfig 192.168.161.78:/etc/kubernetes/ 创建 kube-proxy.service 文件1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 ==node== 中安装yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go cd /etc/kubernetes/vi kube-proxy.config.yamlapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 192.168.161.77clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfigclusterCIDR: 10.254.64.0/18healthzBindAddress: 192.168.161.77:10256hostnameOverride: node1 ##注意修改此处的hostnamekind: KubeProxyConfigurationmetricsBindAddress: 192.168.161.77:10249mode: &quot;ipvs&quot; # 创建 kube-proxy 目录mkdir -p /var/lib/kube-proxyvi /etc/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/usr/local/bin/kube-proxy \ --config=/etc/kubernetes/kube-proxy.config.yaml \ --logtostderr=true \ --v=1Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 kube-proxysystemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 检查 ipvs 情况[root@node1 kubernetes]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.161.161:6443 Masq 1 0 0 -&gt; 192.168.161.162:6443 Masq 1 0 0 配置 Calico 网络官方文档 https://docs.projectcalico.org/v3.1/introduction 下载 Calico yaml# 下载 yaml 文件wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yamlwget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml 下载镜像# 下载 镜像# 国外镜像 有墙quay.io/calico/node:v3.1.3quay.io/calico/cni:v3.1.3quay.io/calico/kube-controllers:v3.1.3# 国内镜像jicki/node:v3.1.3jicki/cni:v3.1.3jicki/kube-controllers:v3.1.3# 阿里镜像registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:node_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:cni_v3.1.3registry.cn-hangzhou.aliyuncs.com/zhdya_centos_docker/zhdya_cc:kube-controllers_v3.1.3# 替换镜像sed -i &apos;s/quay\.io\/calico/jicki/g&apos; calico.yaml 修改配置vi calico.yaml# 注意修改如下选项:# etcd 地址 etcd_endpoints: &quot;https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379&quot; # etcd 证书路径 # If you&apos;re using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot; # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面)data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d &apos;\n&apos;) etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d &apos;\n&apos;) etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d &apos;\n&apos;) ## 如上需要去掉() 只需要填写生成的编码即可 # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: &quot;10.254.64.0/18&quot; 查看服务[root@master1 kubernetes]# kubectl get po -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcalico-kube-controllers-79cfd7887-xbsd4 1/1 Running 5 11d 192.168.161.77 node1 &lt;none&gt;calico-node-2545t 2/2 Running 0 29m 192.168.161.78 node2 &lt;none&gt;calico-node-tbptz 2/2 Running 7 11d 192.168.161.77 node1 &lt;none&gt;[root@master1 kubernetes]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEnode1 Ready &lt;none&gt; 11d v1.11.2 192.168.161.77 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2node2 Ready &lt;none&gt; 29m v1.11.2 192.168.161.78 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-514.el7.x86_64 docker://17.3.2 修改 kubelet 配置==两台node节点都需要配置== # kubelet 需要增加 cni 插件 --network-plugin=cnivim /etc/systemd/system/kubelet.service --network-plugin=cni \# 重新加载配置systemctl daemon-reloadsystemctl restart kubelet.servicesystemctl status kubelet.service 检查网络的互通性：[root@node1 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.102.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@node2 ~]# ifconfigtunl0: flags=193&lt;UP,RUNNING,NOARP&gt; mtu 1440 inet 10.254.75.0 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 直接在node2上面ping：[root@node2 ~]# ping 10.254.102.128PING 10.254.102.128 (10.254.102.128) 56(84) bytes of data.64 bytes from 10.254.102.128: icmp_seq=1 ttl=64 time=72.3 ms64 bytes from 10.254.102.128: icmp_seq=2 ttl=64 time=0.272 ms 安装 calicoctl++calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。++ # 下载 二进制文件curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctlmv calicoctl /usr/local/bin/chmod +x /usr/local/bin/calicoctl# 创建 calicoctl.cfg 配置文件mkdir /etc/calicovim /etc/calico/calicoctl.cfgapiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: datastoreType: &quot;kubernetes&quot; kubeconfig: &quot;/root/.kube/config&quot;# 查看 calico 状态[root@node1 src]# calicoctl node statusCalico process is running.IPv4 BGP status+----------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+----------------+-------------------+-------+----------+-------------+| 192.168.161.78 | node-to-node mesh | up | 06:54:19 | Established |+----------------+-------------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.[root@node1 src]# calicoctl get node ##当然我这边是在node节点操作的，node节点是没有/root/.kube/config 这个文件的，只需要从master节点copy过来即可！！NAMEnode1node2]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 1.11.2整理Ⅰ]]></title>
    <url>%2F2019%2F02%2F08%2Fkubernetes%201.11.2%E6%95%B4%E7%90%86%E2%85%A0%2F</url>
    <content type="text"><![CDATA[1:服务器信息以及节点介绍 初次使用 ==CoreDNS==， ==Ingress==， ==Calico== 系统信息：centos7 主机名称 IP 备注 master1 192.168.161.161 master and etcd master2 192.168.161.162 master and etcd master3 192.168.161.163 etcd node1 192.168.161.77 node1 node2 192.168.161.78 node2 我这边将数据盘挂载了 /opt 目录下 一、环境初始化1：分别在4台主机设置主机名称hostnamectl set-hostname master1hostnamectl set-hostname master2hostnamectl set-hostname master3hostnamectl set-hostname node1hostnamectl set-hostname node2 2:配置主机映射 cat &lt;&lt;EOF &gt; /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.161.161 master1192.168.161.162 master2192.168.161.163 master3192.168.161.77 node1192.168.161.78 node2EOF 3：node01上执行ssh免密码登陆配置 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.161.XXX 4：四台主机配置、停防火墙、关闭Swap、关闭Selinux、设置内核、K8S的yum源、安装依赖包、配置ntp（配置完后建议重启一次） systemctl stop firewalldsystemctl disable firewalldswapoff -a sed -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstabsetenforce 0 sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/sysconfig/selinux sed -i &quot;s/^SELINUX=permissive/SELINUX=disabled/g&quot; /etc/selinux/config modprobe br_netfiltercat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.d/k8s.confls /proc/sys/net/bridgeyum install -y epel-releaseyum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl systemctl enable ntpdate.serviceecho &apos;*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com &gt;/dev/null 2&gt;&amp;1&apos; &gt; /tmp/crontab2.tmpcrontab /tmp/crontab2.tmpsystemctl start ntpdate.service echo &quot;* soft nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nofile 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard nproc 65536&quot; &gt;&gt; /etc/security/limits.confecho &quot;* soft memlock unlimited&quot; &gt;&gt; /etc/security/limits.confecho &quot;* hard memlock unlimited&quot; &gt;&gt; /etc/security/limits.conf 二、环境说明基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler这里配置2个Master 2个node, Master-161、Master-162 做 Master + etcd, master3 仅仅etcd， node-01 node-02 只做单纯 Node 创建 验证这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 安装 cfsslmkdir -p /opt/local/cfsslcd /opt/local/cfsslwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64mv cfssl_linux-amd64 cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64mv cfssljson_linux-amd64 cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 cfssl-certinfochmod +x * 创建 CA 证书配置mkdir /opt/sslcd /opt/ssl config.json 文件vi config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; csr.json 文件vi csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 CA 证书和私钥 cd /opt/ssl//opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca[root@master1 ssl]# ls -lt总用量 20-rw-r--r-- 1 root root 1005 9月 1 13:36 ca.csr-rw------- 1 root root 1679 9月 1 13:36 ca-key.pem-rw-r--r-- 1 root root 1363 9月 1 13:36 ca.pem-rw-r--r-- 1 root root 210 9月 1 13:35 csr.json-rw-r--r-- 1 root root 292 9月 1 13:35 config.json 分发证书创建证书目录mkdir -p /etc/kubernetes/ssl 拷贝所有文件到目录下cp *.pem /etc/kubernetes/sslcp ca.csr /etc/kubernetes/ssl 这里要将文件拷贝到所有的k8s机器上scp *.pem *.csr 192.168.161.162:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.163:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.77:/etc/kubernetes/ssl/scp *.pem *.csr 192.168.161.78:/etc/kubernetes/ssl/ 三、安装 docker所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源# 安装 yum-config-manageryum -y install yum-utils# 导入yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo# 更新 repoyum makecache# 查看yum 版本yum list docker-ce.x86_64 --showduplicates |sort -r# 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinuxwget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmrpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmyum -y install docker-ce-17.03.2.cedocker version 更改docker 配置# 添加配置vi /etc/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.comAfter=network.target docker-storage-setup.serviceWants=docker-storage-setup.service[Service]Type=notifyEnvironment=GOTRACEBACK=crashExecReload=/bin/kill -s HUP $MAINPIDDelegate=yesKillMode=processExecStart=/usr/bin/dockerd \ $DOCKER_OPTS \ $DOCKER_STORAGE_OPTIONS \ $DOCKER_NETWORK_OPTIONS \ $DOCKER_DNS_OPTIONS \ $INSECURE_REGISTRYLimitNOFILE=1048576LimitNPROC=1048576LimitCORE=infinityTimeoutStartSec=1minRestart=on-abnormal[Install]WantedBy=multi-user.target 修改其他配置# 低版本内核， kernel 3.10.x 配置使用 overlay2vi /etc/docker/daemon.json&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;mkdir -p /etc/systemd/system/docker.service.d/vi /etc/systemd/system/docker.service.d/docker-options.conf# 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载)# docker 版本 17.03.2 之前配置为 --graph=/opt/docker# docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service]Environment=&quot;DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \ --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5&quot;vi /etc/systemd/system/docker.service.d/docker-dns.conf# 添加如下 : [Service]Environment=&quot;DOCKER_DNS_OPTIONS=\ --dns 10.254.0.2 --dns 114.114.114.114 \ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot; 重新读取配置，启动 dockersystemctl daemon-reloadsystemctl start dockersystemctl enable docker 如果报错 请使用systemctl status docker -l 或 journalctl -u docker 来定位问题 etcd 集群etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.2 etcd 支持最新版本为 v3.2.18 安装 etcd官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件（3台master机器都需要）wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gztar zxvf etcd-v3.2.18-linux-amd64.tar.gzcd etcd-v3.2.18-linux-amd64mv etcd etcdctl /usr/bin/ 创建 etcd 证书etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发。 cd /opt/ssl/vi etcd-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 etcd 密钥/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成[root@master1 ssl]# ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem# 检查证书[root@master1 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem# 拷贝到etcd服务器# etcd-1 cp etcd*.pem /etc/kubernetes/ssl/# etcd-2scp etcd*.pem 192.168.161.162:/etc/kubernetes/ssl/# etcd-3scp etcd*.pem 192.168.161.163:/etc/kubernetes/ssl/# 如果 etcd 非 root 用户，读取证书会提示没权限chmod 644 /etc/kubernetes/ssl/etcd-key.pem 修改 etcd 配置由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 创建 etcd data 目录， 并授权useradd etcdmkdir -p /opt/etcdchown -R etcd:etcd /opt/etcd etcd-1vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd1 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.161:2380 \ --listen-peer-urls=https://192.168.161.161:2380 \ --listen-client-urls=https://192.168.161.161:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.161:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-2vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd2 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.162:2380 \ --listen-peer-urls=https://192.168.161.162:2380 \ --listen-client-urls=https://192.168.161.162:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.162:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target etcd-3vi /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/opt/etcd/User=etcd# set GOMAXPROCS to number of processorsExecStart=/usr/bin/etcd \ --name=etcd3 \ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \ --initial-advertise-peer-urls=https://192.168.161.163:2380 \ --listen-peer-urls=https://192.168.161.163:2380 \ --listen-client-urls=https://192.168.161.163:2379,http://127.0.0.1:2379 \ --advertise-client-urls=https://192.168.161.163:2379 \ --initial-cluster-token=k8s-etcd-cluster \ --initial-cluster=etcd1=https://192.168.161.161:2380,etcd2=https://192.168.161.162:2380,etcd3=https://192.168.161.163:2380 \ --initial-cluster-state=new \ --data-dir=/opt/etcd/Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 启动 etcd分别启动 所有节点的 etcd 服务 systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcdjournalctl -u etcd -f ##用此命令来动态查看具体日志 验证 etcd 集群状态etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ cluster-health member 60ce394098258c3 is healthy: got healthy result from https://192.168.161.163:2379member afe2d07db38fa5e2 is healthy: got healthy result from https://192.168.161.162:2379member ba8a716d98dac47b is healthy: got healthy result from https://192.168.161.161:2379cluster is healthy 查看 etcd 集群成员：etcdctl --endpoints=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379\ --cert-file=/etc/kubernetes/ssl/etcd.pem \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ member list60ce394098258c3: name=etcd3 peerURLs=https://192.168.161.163:2380 clientURLs=https://192.168.161.163:2379 isLeader=falseafe2d07db38fa5e2: name=etcd2 peerURLs=https://192.168.161.162:2380 clientURLs=https://192.168.161.162:2379 isLeader=falseba8a716d98dac47b: name=etcd1 peerURLs=https://192.168.161.161:2380 clientURLs=https://192.168.161.161:2379 isLeader=true 配置 Kubernetes 集群kubectl 安装在所有需要进行操作的机器上 Master and NodeMaster 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 安装组件# 从github 上下载版本 (在两台master上节点执行)cd /usr/local/srcwget https://dl.k8s.io/v1.11.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetescp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm&#125; /usr/local/bin/scp server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm&#125; 192.168.161.162:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.77:/usr/local/bin/scp server/bin/&#123;kube-proxy,kubelet&#125; 192.168.161.78:/usr/local/bin/ 创建 admin 证书kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。cd /opt/ssl/vi admin-csr.json&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; # 生成 admin 证书和私钥cd /opt/ssl//opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin# 查看生成[root@master1 ssl]# ls admin*admin.csr admin-csr.json admin-key.pem admin.pemcp admin*.pem /etc/kubernetes/ssl/scp admin*.pem 192.168.161.162:/etc/kubernetes/ssl/ 生成 kubernetes 配置文件生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443# 配置 客户端认证kubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/ssl/admin.pem \ --embed-certs=true \ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=adminkubectl config use-context kubernetes 创建 kubernetes 证书cd /opt/sslvi kubernetes-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.161.161&quot;, &quot;192.168.161.162&quot;, &quot;192.168.161.163&quot;, &quot;192.168.161.77&quot;, &quot;192.168.161.78&quot;, &quot;10.254.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 192.168.161.161 和 172.16.161.162 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 生成 kubernetes 证书和私钥/opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \ -ca-key=/etc/kubernetes/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes# 查看生成[root@master1 ssl]# ls -lt kubernetes*-rw-r--r-- 1 root root 1277 9月 1 15:31 kubernetes.csr-rw------- 1 root root 1679 9月 1 15:31 kubernetes-key.pem-rw-r--r-- 1 root root 1651 9月 1 15:31 kubernetes.pem-rw-r--r-- 1 root root 531 9月 1 15:31 kubernetes-csr.json# 拷贝到目录cp kubernetes*.pem /etc/kubernetes/ssl/scp kubernetes*.pem 192.168.161.162:/etc/kubernetes/ssl/ 配置 kube-apiserverkubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。# 生成 token[root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;97606de41d5ee3c3392aae432eb3143d# 创建 encryption-config.yaml 配置cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 97606de41d5ee3c3392aae432eb3143d - identity: &#123;&#125;EOF# 拷贝cp encryption-config.yaml /etc/kubernetes/scp encryption-config.yaml 192.168.161.162:/etc/kubernetes/ # 生成高级审核配置文件&gt; 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&gt;&gt; 如下为最低限度的日志审核cd /etc/kubernetescat &gt;&gt; audit-policy.yaml &lt;&lt;EOF# Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF# 拷贝scp audit-policy.yaml 192.168.161.162:/etc/kubernetes/ 创建 kube-apiserver.service 文件# 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下# 配置为 各自的本地 IPvi /etc/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]User=rootExecStart=/usr/local/bin/kube-apiserver \ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \ --anonymous-auth=false \ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \ --advertise-address=192.168.161.161 \ --allow-privileged=true \ --apiserver-count=3 \ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kubernetes/audit.log \ --authorization-mode=Node,RBAC \ --bind-address=0.0.0.0 \ --secure-port=6443 \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \ --enable-swagger-ui=true \ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \ --etcd-servers=https://192.168.161.161:2379,https://192.168.161.162:2379,https://192.168.161.163:2379 \ --event-ttl=1h \ --kubelet-https=true \ --insecure-bind-address=127.0.0.1 \ --insecure-port=8080 \ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \ --service-cluster-ip-range=10.254.0.0/18 \ --service-node-port-range=30000-32000 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --enable-bootstrap-token-auth \ --v=1Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target# --experimental-encryption-provider-config ，替代之前 token.csv 文件# 这里面要注意的是 --service-node-port-range=30000-32000# 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。记得在另外一台master上修改IP地址 启动 kube-apiserversystemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 查看启动端口[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-controller-manager两台master都需要配置：新增几个配置，用于自动 续期证书–feature-gates=RotateKubeletServerCertificate=true–experimental-cluster-signing-duration=86700h0m0s # 创建 kube-controller-manager.service 文件vi /etc/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-controller-manager \ --address=0.0.0.0 \ --master=http://127.0.0.1:8080 \ --allocate-node-cidrs=true \ --service-cluster-ip-range=10.254.0.0/18 \ --cluster-cidr=10.254.64.0/18 \ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \ --feature-gates=RotateKubeletServerCertificate=true \ --controllers=*,tokencleaner,bootstrapsigner \ --experimental-cluster-signing-duration=86700h0m0s \ --cluster-name=kubernetes \ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s \ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-controller-managersystemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 查看启动端口[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 配置 kube-scheduler# 创建 kube-cheduler.service 文件vi /etc/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-scheduler \ --address=0.0.0.0 \ --master=http://127.0.0.1:8080 \ --leader-elect=true \ --v=1Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动 kube-schedulersystemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 查看启动端口[root@master1 kubernetes]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 192.168.161.161:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 192.168.161.161:2380 0.0.0.0:* LISTEN 3605/etcdtcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 3844/kube-apiservertcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1715/sshdtcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2066/mastertcp6 0 0 :::10251 :::* LISTEN 4023/kube-schedulertcp6 0 0 :::6443 :::* LISTEN 3844/kube-apiservertcp6 0 0 :::10252 :::* LISTEN 3970/kube-controlletcp6 0 0 :::22 :::* LISTEN 1715/sshdtcp6 0 0 ::1:25 :::* LISTEN 2066/master 验证 Master 节点[root@master1 kubernetes]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;[root@master2 bin]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>Kubernets</tag>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome @all!!]]></content>
  </entry>
</search>
